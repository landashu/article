---
title: Hive 函数 MySQL联合
date: 2023-06-25 09:22:36
permalink:  /hive/4
sidebar: true
article: false #  是否未非文章页，非文章不显示 面包屑和作者、时间，不显示最近更新栏，不会参与到最近更新文章的数据计算中
comment: false #  评论区
editLink: false
---

> 本文及后续所有文章都以3.1.2做为版本讲解和入门学习

## 函数

| 返回类型 | 函数名 | 描述 |
| -- | -- | -- |
| int &emsp;&emsp;&emsp;&emsp; | length(string a) | 返回字符串a的长度 select length(a) from table_name  | 
| string | reverse(string a) | 返回字符串a的反转结果 select reverse(a) from table_name  |
| string | concat(string a,string b...) | 字符串连接函数 select concat(id,name) from table_name |
| string | concat_ws(string sep,string a,string b...) | 带分隔符的字符串连接函数 select concat_ws(',',id,name) from table_name |
| string | substr(string a,index int,lastindex int) | 截取字符串 |
| string | upper(string a) | 转大写 |
| string | lower(string a) | 转小写 |
| string | trim(string a) | 去两边空格 |
| string | ltrim(string a) | 左边去空格 |
| string | rtrim(string a) | 右边去空格 |
| string | regexp_replace(string a,string b,string c) | 将字符串a中的符合java正则表达式b的部分替换为c，有些情况下要使用转义字符如 [] 为 [*]，类似 oracle 的regexp_replace函数 |
| string | regexp_extract(string a,string patten,int index) | 将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符 |
| string | repeat(string a,int n) | 返回重复n次后的str字符串 |
| array | split(string a,string pat) | 分割字符串函数 |
| row | explode(array a) | 命令可以将一行数据，按指定规则切分多行 select explode(split(a,',')) from table_name |

###  regexp_replace
正则替换
```shell
hive> select regexp_replace('foobar','oo|ar','');
OK
fb
Time taken: 0.317 seconds, Fetched: 1 row(s)
```
### regexp_extract
会提取 () 里的内容，下标从1开始，0默认全部
```shell
hive> select regexp_extract('foothebar','foo(.*)(bar)',1);
OK
the
Time taken: 0.307 seconds, Fetched: 1 row(s)
hive> select regexp_extract('foothebar','foo(.*)(bar)',2);
OK
bar
Time taken: 0.314 seconds, Fetched: 1 row(s)
hive> select regexp_extract('foothebar','foo(.*)(bar)',0);
OK
foothebar
Time taken: 0.658 seconds, Fetched: 1 row(s)
```

## UDF
如果hive的内置函数不够用，我们也可以自定义函数来使用，这样的函数称为hive的用户自定义函数，简称UDF。

### 实现步骤
maven
```xml
<dependencies>
    <!-- hive -->
    <dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-exec</artifactId>
        <version>3.1.2</version>
    </dependency>
    <!-- hadoop -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-core</artifactId>
        <version>1.2.1</version>
    </dependency>
</dependencies>
```
实现
```java
package com.example.demo;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

/**
 * @author big uncle
 * @date 2021/5/19 14:52
 * 自定义一个函数，接收两个参数，如果参数1为 null值，则用第二个参数做为返回值
 **/
public class Ifv extends GenericUDF{

    /**
     * 执行一次 检查参数个数 和 参数类型
    **/
    @Override
    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
        System.out.println("initialize 被调用了 ");
        if(arguments.length != 2){
            throw new UDFArgumentException("arg length must is 2");
        }
        // 返回一个String对象检查器
        ObjectInspector outputOI = PrimitiveObjectInspectorFactory.javaStringObjectInspector;
        return outputOI;
    }

    /**
     * 处理数据
    **/
    @Override
    public Object evaluate(DeferredObject[] arguments) throws HiveException {
        System.out.println("evaluate 被调用了");
        if(arguments[0].get() != null){
            return arguments[0].get().toString();
        }
        String arg1 = arguments[1].get().toString();
        return arg1;
    }

    /**
     * explain 详细计划
    **/
    @Override
    public String getDisplayString(String[] children) {
        System.out.println("getDisplayString 被调用了");
        return getStandardDisplayString(getFuncName(), children);
    }

}

```

添加到 hive，在hive中输入如下
```shell
add jar /root/udf-1.0.0.RELEASE.jar;
```
创建成一个函数
```shell
create temporary function ifv as 'com.example.demo.Ifv';
```
调用
```shell
hive> select ifv(null,"aaaa");
initialize 被调用了 
evaluate 被调用了
initialize 被调用了 
evaluate 被调用了
initialize 被调用了 
evaluate 被调用了
OK
aaaa
Time taken: 6.55 seconds, Fetched: 1 row(s)
```
> 谁都不要在告诉我 initialize 只调用一次，mmp官网这么说我都不信


## 配置Mysql

1. 进入mysql数据库先创建 hive库
```shell
create database hive character set latin1;
```
2. 在 hive/conf 目录下创建 hive-site.xml
```xml
<configuration>

  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://node113:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>Admin@123</value>
  </property>

</configuration>
```
3. 下载mysql对应的驱动，丢到 hive/lib/ 目录下
4. 删除 hive/bin/metastore_db
5. 在 hive/bin/ 目录下执行如下
```shell
./schematool -dbType mysql -initSchema
```
6. 启动  hive/bin/
```shell
./hive
```
当 hive 启动成功后，会在mysql中创建很多的表

|表 | 作用 |
| -- | -- |
| DBS | 管理hive中创建的库 |
| TBLS | 管理hive中创建的表，其中TBL_TYPE：MANAGED_TABLE(内部表)，EXTERNAL_TABLE(外部表) |
| PARTITIONS |  |