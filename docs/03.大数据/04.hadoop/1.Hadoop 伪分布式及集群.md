---
title: Hadoop 伪分布式及集群
date: 2023-06-25 09:22:36
permalink:  /hadoop/1
sidebar: true
article: false #  是否未非文章页，非文章不显示 面包屑和作者、时间，不显示最近更新栏，不会参与到最近更新文章的数据计算中
comment: false #  评论区
editLink: false
---

> 本文及后续所有文章都以3.1.3做为版本讲解和入门学习

> 在开始做hadoop的时候，首先要做以下几点：1. 修改hostname 2. 修改机器之间免密登录

目前我有三台机器，node103，node104，node113。先用 node113 搭建一台伪分布式学习用，如果是为了 集群，跳过这里就行。

## Hadoop 伪分布式
下载连接 http://www.apache.org/dyn/closer.cgi/hadoop/common/ 可以自己选择版本进行下载。解压完成后看如下操作：
1. 进入 /hadoop-3.1.3/etc/hadoop 目录，配置 hadoop-env.sh
```shell
# 进入目录
cd /opt/software/hadoop-3.1.3/etc/hadoop
# 编辑文件
vim hadoop-env.sh
# 修改jdk安装位置
export JAVA_HOME=/opt/software/java8
# 修改hadoop安装位置
export HADOOP_CONF_DIR=/opt/software/hadoop-3.1.3/etc/hadoop
```
2. 修改 core-site.xml
```xml
<configuration> 
    <!-- 指定HDFS中NameNode的地址 -->
    <property> 
        <name>fs.defaultFS</name> 
        <value>hdfs://node113:9000</value> 
    </property> 
  <!-- 指定Hadoop运行时产生元数据文件的存储目录，改目录需要自己手动创建 -->
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/software/hadoop-3.1.3/data/tmp</value>
  </property>
</configuration>
```
3. 修改 hdfs-site.xml
```xml
<configuration>
  <!-- 指定HDFS副本的数量 -->
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <!-- 设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件 -->
  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>
</configuration>
```
4. 修改 workers 文件
> hadoop3.0以后slaves更名为 workers
```text
node113
```
5. 检查配置
```shell
# 进入bin目录
cd /opt/software/hadoop-3.1.3/bin
# 启动检查命令
./hadoop namenode -format
# 查输出信息有以下代表配置正确
successfully formatted.
```
6. 启动
```shell
# 进入目录
cd /opt/software/hadoop-3.1.3/sbin
# 启动
./start-dfs.sh
# 如果启动报错，修改start-dfs.sh 和 stop-dfs.sh，并添加如下
HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```
7. 查看启动是否正确
```shell
# jps 查看，如果包含以下进程则启动正确
# 存放元数据信息
NameNode
# 管理 NameNode 的 edit logs 和 fsimage，定时把 edit logs 写道 fsimage 然后copy到 NameNode
# edit logs 是 NameNode 重启后的写日志信息，fsimage是重启前读取的快照数据
SecondaryNameNode
# 存储数据块
DataNode
```
8. 也可以访问浏览器来验证启动正确性
```shell
# hadoop 3.x 及以上 50070 改为了 9870
http://10.240.30.113:9870/dfshealth.html#tab-overview
# 允许端口访问
/sbin/iptables -I INPUT -p tcp --dport 9870 -j ACCEPT
```
![](/assets/img/hadoop/1/img.png)

9. 把hadoop放到环境变量中
```shell
vim /etc/profile
export HADOOP_HOME=/opt/software/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export PATH JAVA_HOME HADOOP_HOME
source /etc/profile
```

## Hadoop 集群

### hadoop 集群试图
![](/assets/img/hadoop/1/img_1.png)

### hadoop 集群搭建

| hostname  | zookeeper| nameNode  | ResourceManager | journalNode | dataNode/nodeManager | 
| -- | -- | -- | -- | -- | -- |
| node103 | 1 | 1 active | 1 | 1 | 1 |
| node104 | 1 | 1 standby |  | 1 | 1 |
| node113 | 1 |  | 1 | 1 | 1 |

> 把之前伪分布式全部停掉，copy 之前的伪分布式配置为以后切换伪分布式提供便利

1. 修改node103的 hadoop-env.sh 文件，和以上伪分布式一致就行
2. 修改node103的 core-site.xml 文件
```xml
<configuration>

  <!-- 为整个分布式系统起一个别名 ns=nameService-->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://ns</value>
  </property>

  <!-- 指定Hadoop运行时产生文件的存储目录 -->
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/software/hadoop-3.1.3/data/tmp</value>
  </property>

  <!-- 执行zookeeper地址-->
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>node103:2181,node104:2181,node113:2181</value>
  </property>

</configuration>

```
3. 修改node103的 hdfs-site.xml 文件
```xml
<configuration>

  <!-- 执行hdfs的nameservices为ns，和core-site.xml 保持一致 -->
  <property>
    <name>dfs.nameservices</name>
    <value>ns</value>
  </property>

  <!-- ns下有两台namenode，分别是 n103,n104（别名） -->
  <property>
    <name>dfs.ha.namenodes.ns</name>
    <value>n103,n104</value>
  </property>

  <!-- n103的RPC通信地址 -->
  <property>
    <name>dfs.namenode.rpc-address.ns.n103</name>
    <value>node103:9000</value>
  </property>

  <!-- n103的http通信地址 -->
  <property>
    <name>dfs.namenode.http-address.ns.n103</name>
    <value>node103:9870</value>
  </property>

  <!-- n104的RPC通信地址 -->
  <property>
    <name>dfs.namenode.rpc-address.ns.n104</name>
    <value>node104:9000</value>
  </property>

  <!-- n104的http通信地址 -->
  <property>
    <name>dfs.namenode.http-address.ns.n104</name>
    <value>node104:9870</value>
  </property>

  <!-- 指定namenode的元数据在JournalNode上哪些机器上存放，主要从namenode获取最新信息，达到热备 -->
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://node103:8485;node104:8485;node113:8485/ns</value>
  </property>

  <!-- 指定JournalNode存放数据的位置，需要手动创建 -->
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/opt/software/hadoop-3.1.3/data/journal</value>
  </property>

  <!-- 开启namenode故障时自动切换 -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

  <!-- 配置切换的实现方式 -->
  <property>
    <name>dfs.client.failover.proxy.provider.ns</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>

  <!-- 配置隔离机制 -->
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>

  <!-- 配置隔离机制的ssh登录密钥所在的位置 -->
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
  </property>

  <!-- 配置namenode数据存放的位置，可以不配置，如果不配置，默认使用得时core-site.xml里配置的hadoop.tmp.dir的路径 -->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///opt/software/hadoop-3.1.3/data/namenode</value>
  </property>

  <!-- 配置datanode数据存放的位置，可以不配置，如果不配置，默认使用的是core-site.xml里配置的hadoop.tmp.dir的路径 -->
  <property>
    <name>dfs.datanode.name.dir</name>
    <value>file:///opt/software/hadoop-3.1.3/data/datanode</value>
  </property>

  <!-- 指定HDFS副本的数量 -->
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>

  <!-- 设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件 -->
  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>

</configuration>

```
4. 修改node103的 mapred-site.xml 文件，和以上伪分布式一致就行
5. 修改node103的 yarn-site.xml 文件
```xml
<configuration>

  <!-- 开启YARN HA -->
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
  </property>

  <!-- 指定两个resourcemanager的名称 -->
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm1,rm2</value>
  </property>

  <!-- 配置rm1的主机 -->
  <property>
    <name>yarn.resourcemanager.hostname.rm1</name>
    <value>node103</value>
  </property>

  <!-- 配置rm2的主机 -->
  <property>
    <name>yarn.resourcemanager.hostname.rm2</name>
    <value>node113</value>
  </property>

  <!-- 开启yarn恢复机制 -->
  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>

  <!-- 执行rm恢复机制实现类 -->
  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>

  <!-- 配置zookeeper的地址 -->
  <property>
    <name>yarn.resourcemanager.zk-address</name>
    <value>node103:2181,node104:2181,node113:2181</value>
    <description>For multiple zk services, separate them with comma</description>
  </property>

  <!-- 指定YARN HA的名称 -->
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>yarn-ha</value>
  </property>

  <!-- 指定yarn 主 resourcemanager地址 -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>node103</value>
  </property>

  <!-- Reducer获取数据的方式 -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

</configuration>
```
6. 修改node103的 workers  文件，该文件会指定启动dataNode,nodeManager
```shell
node103
node104
node113
```
> 把以上配置copy到其他hadoop里覆盖配置，删除之前 data/ 下的所有文件，并新建 tmp、journal、namenode、databode。完成之后记得所有机器上的环境变量统一修改。

7. 启动 zookeeper 集群
8. 格式化zookeeper，在zookeeper的Leader节点上执行：hdfs zkfc -formatZK，在zookeeper集群上会生成 hadoop-ha 节点（ns节点）
当namenode启动，会在ns节点下注册自己
9. 启动每台的 journalnode 进程，启动成功后用jps查看是否有该进程
```shell
hadoop-daemon.sh start journalnode
```
```shell
[root@node103 bin]# hadoop-daemon.sh start journalnode 
[root@node103 bin]# jps
109297 JournalNode
```
10. 启动 namenode 进程
```shell
# node103 active 先格式化一遍，该命令只能对active使用，
# standby使用就会报错，说你的文件不是空文件
hadoop namenode -format
# node103 active 启动 namenode
hadoop-daemon.sh start namenode
# node104 设置为 standby
hdfs namenode -bootstrapStandby
# node104 standby 启动 namenode
hadoop-daemon.sh start namenode
```
11. 每台启动 datanode
```shell
hadoop-daemon.sh start datanode
```
>出现该错误 ERROR: Cannot set priority of datanode process 12297 ，需要找到  /hadoop-3.1.3/bin/hdfs 注释 HADOOP_SHELL_EXECNAME="hdfs"，并在环境变量添加  export HADOOP_SHELL_EXECNAME=用户
12. 启动zkfc（FalioverControllerActive）故障监听转移及恢复
```shell
# 只在node103和node104启动
hadoop-daemon.sh start zkfc
# 启动成功会有以下进程
111060 DFSZKFailoverController
```
13. 启动 nodemanager 以及 resourceManager
```shell
# 在 node103 active 启动，会启动其余的 nodemanager 
start-yarn.sh
# 在 node 104 启动
yarn-daemon.sh start resourcemanager
```
14. 查看启动状态

![](/assets/img/hadoop/1/img_2.png)

![](/assets/img/hadoop/1/img_3.png)

![](/assets/img/hadoop/1/img_4.png)

15. 停止/再次启动 集群
```shell
# 在任意的节点，会停止所有节点的进程，除了手动启动的 resourcemanager
stop-all.sh
# 再次启动的话只需要
start-all.sh

# stop-all.sh 会找 stop-dfs.sh、hadoop-config.sh、stop-yarn.sh三个脚本文件，与启动脚本相似
# 如果在文件内配置了HDFS的相关账号，建议全部注释，写到 /etc/profile 里
# 以下两个是 stop-all.sh 需要的
HDFS_JOURNALNODE_USER=root
HDFS_ZKFC_USER=root
```

16. 不停机新加入节点 node114
* 1. 修改其他机器的 workers
```shell
node103
node104
node113
node114
```
* 2. node114 的配置可以copy其他机器任意一个
* 3. 删除node114下的 tmp、journal、namenode、databode 文件下的所有数据
* 4. 正确配置 hadoop 环境
* 5. 启动 datanode
```shell
hadoop-daemon.sh start datanode
```

## Hadoop 特点
1. 支持超大文件：一般来说，HDFS存储的文件可以支持TB和PB级别的数据。
2. 检测和快速应对硬件故障：在集群环境中，硬件故障是常见性问题。因为有上千台服务器连在一起，故障率高，因此故障检测和自动恢复hdfs文件系统的一个设计目标。假设某一个datanode节点挂掉之后，因为数据备份，还可以从其他节点里找到。namenode通过心跳机制来检测datanode是否还存在
3. 流式数据访问：HDFS的数据处理规模比较大，应用一次需要大量的数据，同时这些应用一般都是批量处理，而不是用户交互式处理，应用程序能以流的形式访问数据库。主要的是数据的吞吐量，而不是访问速度。访问速度最终是要受制于网络和磁盘的速度，机器节点再多，也不能突破物理的局限，HDFS不适合于低延迟的数据访问，HDFS的是高吞吐量。
4. 简化的一致性模型：对于外部使用用户，不需要了解hadoop底层细节，比如文件的切块，文件的存储，节点的管理。一个文件存储在HDFS上后，**适合一次写入，多次写出的场景once-write-read-many**。因为存储在HDFS上的文件都是超大文件，当上传完这个文件到hadoop集群后，会进行文件切块，分发，复制等操作。如果文件被修改，会导致重新出发这个过程，而这个过程耗时是最长的。所以在hadoop里，**不允许对上传到HDFS上文件做修改**（随机写），在2.0版本时可以在后面追加数据。但不建议。
5. 高容错性：数据自动保存多个副本，副本丢失后自动恢复。可构建在廉价机上，实现线性（横向）扩展，当集群增加新节点之后，namenode也可以感知，将数据分发和备份到相应的节点上。
6. 商用硬件：Hadoop并不需要运行在昂贵且高可靠的硬件上，它是设计运行在商用硬件的集群上的，因此至少对于庞大的集群来说，节点故障的几率还是非常高的。HDFS遇到上述故障时，被设计成能够继续运行且不让用户察觉到明显的中断。

## Hadoop 特性
1. 高可靠性：采用冗余数据存贮方式，即使一个副本发生故障，其他副本也可以保证对外工作的正常进行。
2. 高效性：作为并行分布式计算平台，hadoop采用分布式存贮和分布式处理两大核心技术，能够高效的处理PB级别的数据
3. 高可扩展性：hadoop的设计目标是可以高效稳定的运行在廉价的计算机集群上，可以扩展到数以千计的计算机节点上。
4. 高容错性：采用冗余数据存贮方式，自动保存数据的多个副本，并且能够自动将失败的任务重新分配。
5. 成本低：hadoop采用廉价的计算机集群，普通的用户也可以pc机搭建环境
6. 运行在linux平台上，hadoop是基于java语言开发的，可以较好的运行在linux的平台上

## Haddop 注意
1. 安全模式（1）：当HDFS启动时，每个 datanode 会向 namenode 汇报自身的存储信息，比如存储了哪些文件快，块大小，块id等。namenode 收到这些信息之后，会做到汇总和检查，监测数据是否完整，副本数量是否达到要求，如果检测出现问题，HDFS会进入安全模式，该模式只允许读，不允许写， 并自动做数据或副本的复制，直到修复完成后，自动退出安全模式。
2. 安全模式（2）：如果是伪分布模式，副本只能配置1个，大于1个会导致HDFS一直安全模式不能退出。
3. HDFS不适合存储海量小文件，每当上传一个文件，namenode就会有一个元数据信息占用内存，一条元数据信息大约在150字节左右，海量小文件不值得。
4. hadoop 不能并行写，hdfs 不能并发写指的是不能同时上传同位置同名文件。两个同名文件,一个文件成功上传,再用另一个客户端上传同名文件，会提示 File exists。防止文件写乱，底层加了锁，使用类似Redssion 看门狗的概念。
5. HDFS 不能做到低延迟的数据访问（毫秒级就内给出响应）。但是Hadoop的优势在于它的吞吐率（单位时间内产生的数据流 ）。可以说HDFS的设计时牺牲了低延迟的数据访问，而获取的是数据的高吞吐率，如果想要获取低延迟的数据访问，可以通过Hbase框架来实现。

## Hadoop 回收站机制
开启回收站机制需要修改 core-site.xml 文件，添加如下：
```xml
    <!-- 开启回收站机制，周期时间单位是分钟，0是不开启该机制 -->
    <property> 
        <name>fs.trash.interval</name> 
        <value>1440</value> 
    </property> 
```
开启回收站模式后，删除的文件或目录会存储到另一个地方。

> 面对启动或停止指令，加到环境变量才是最好的选择
export HDFS_DATANODE_SECURE_USER=hdfs
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
export HDFS_JOURNALNODE_USER=root
export HDFS_ZKFC_USER=root




