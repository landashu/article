---
title: Hadoop YARN
date: 2023-06-25 09:22:36
permalink:  /hadoop/6
sidebar: true
article: false #  是否未非文章页，非文章不显示 面包屑和作者、时间，不显示最近更新栏，不会参与到最近更新文章的数据计算中
comment: false #  评论区
editLink: false
---


## YARN概述

![](/assets/img/hadoop/6/img.png)

hadoop 1.x中，一个Job任务使由 JobTracker 来分配资源和管理任务调度，虽说最终运行是在TaskTracker，但对于JobTracker来说，当大量的Job任务要分配，JobTracker 也会面临忙不过来或受限的处境。

![](/assets/img/hadoop/6/img_1.png)


hadoop 2.x中，改变了 JobTracker  的工作机制只负责资源的调度，这样的设计方式减了JobTracker 的工作负载，从而可以更专注的处理资源分配和调度的工作，所以被称为 ResorceManager。ResorceManager含有两个组件，一个是调度器（Scheduler），一个是应用程序管理器（Applications Manager，ASM）。

## YARN 工作机制

![](/assets/img/hadoop/6/img_2.png)



### Resourcemanager 

ResourceManager 拥有系统所有资源分配的决定权，负责集群中所有应用程序的资源分配，拥有集群主要资源、全局视图。因此为用户提供公平的，基于容量的，本地化资源调度。根据程序的需求，调度优先级以及可用资源情况，动态分配特定节点运行应用程序。它与每个节点上的NodeManager和每一个应用程序的ApplicationMaster协调工作。

ResourceManager的主要职责在于调度，即在竞争的应用程序之间分配系统中的可用资源，并不关注每个应用程序的状态管理。

ResourceManager主要有两个组件：Scheduler 和 ApplicationManager：Scheduler是一个资源调度器，它主要负责协调集群中各个应用的资源分配，保障整个集群的运行效率。Scheduler的角色是一个纯调度器，它只负责调度Containers，不会关心应用程序监控及其运行状态等信息。同样，它也不能重启因应用失败或者硬件错误而运行失败的任务。


### Scheduler

Scheduler是一个可插拔的插件，负责各个运行中的应用的资源分配，受到资源容量，队列以及其他因素的影响。是一个纯粹的调度器，不负责应用程序的监控和状态追踪，不保证应用程序的失败或者硬件失败的情况对task重启，而是基于应用程序的资源需求执行其调度功能，使用了叫做资源container的概念，其中包括多种资源，比如，cpu，内存，磁盘，网络等。在Hadoop的MapReduce框架中主要有三种Scheduler：FIFO Scheduler，Capacity Scheduler 和 Fair Scheduler。默认 Capacity Scheduler

FIFO Scheduler：先进先出，不考虑作业优先级和范围，适合低负载集群。
Capacity Scheduler：将资源分为多个队列，允许共享集群，有保证每个队列最小资源的使用。
Fair Scheduler：公平的将资源分给应用的方式，使得所有应用在平均情况下随着时间得到相同的资源份额。

### ApplicationManager

ApplicationManager主要负责接收job的提交请求，为应用分配第一个Container来运行ApplicationMaster，还有就是负责监控ApplicationMaster，在遇到失败时重启ApplicationMaster运行的Container

### NodeManager

NodeManager是yarn节点的一个“工作进程”代理，管理hadoop集群中独立的计算节点，主要负责与ResourceManager通信，负责启动和管理应用程序的container的生命周期，监控它们的资源使用情况（cpu和内存），跟踪节点的监控状态，管理日志等。并报告给RM。

NodeManager在启动时，NodeManager向ResourceManager注册，然后发送心跳包来等待ResourceManager的指令，主要目的是管理resourcemanager分配给它的应用程序container。NodeManager只负责管理自身的Container，它并不知道运行在它上面应用的信息。在运行期，通过NodeManager和ResourceManager协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态

主要职责：
1、接收ResourceManager的请求，分配Container给应用的某个任务
2、和ResourceManager交换信息以确保整个集群平稳运行。ResourceManager就是通过收集每个NodeManager的报告信息来追踪整个集群健康状态的，而NodeManager负责监控自身的健康状态。
3、管理每个Container的生命周期
4、管理每个节点上的日志
5、执行Yarn上面应用的一些额外的服务，比如MapReduce的shuffle过程


### Container

Container是Yarn框架的计算单元，是具体执行应用task（如map task、reduce task）的基本单位。Container和集群节点的关系是：一个节点会运行多个Container，但一个Container不会跨节点。

一个Container就是一组分配的系统资源，现阶段只包含两种系统资源（之后可能会增加磁盘、网络、GPU等资源），由NodeManager监控，Resourcemanager调度。

每一个应用程序从ApplicationMaster开始，它本身就是一个container（第0个），一旦启动，ApplicationMaster就会追加任务需求与Resourcemanager协商更多的container，在运行过程中，可以动态释放和申请container。

### ApplicationMaster

ApplicationMaster负责与scheduler协商合适的container，跟踪应用程序的状态，以及监控它们的进度，ApplicationMaster是协调集群中应用程序执行的进程。每个应用程序都有自己的ApplicationMaster，负责与ResourceManager协商资源（container）和NodeManager协同工作来执行和监控任务 。

当一个ApplicationMaster启动后，会周期性的向resourcemanager发送心跳报告来确认其健康和所需的资源情况，在建好的需求模型中，ApplicationMaster在发往resourcemanager中的心跳信息中封装偏好和限制，在随后的心跳中，ApplicationMaster会对收到集群中特定节点上绑定了一定的资源的container的租约，根据Resourcemanager发来的container，ApplicationMaster可以更新它的执行计划以适应资源不足或者过剩，container可以动态的分配和释放资源。


## Hadoop2.x job运行机制

![](/assets/img/hadoop/6/img_3.png)

和 Hadoop1.x 不同的是多了 YARN的工作机制，会为job的每个步骤（ApplicationMaster、MapTask、ReduceTask）都会申请一个container去工作。

## yarn-site.xml 相关配置
每台nodemanager服务器贡献的内存，默认8192M。
```properties
yarn.nodemanager.resource.memory-mb=8192
```

每台nodemanager服务器贡献的CPU数量，默认8核
```properties
yarn.nodemanager.resource.cpu-vcores=8
```

每个Container最小的使用内存，默认1024M
```properties
yarn.scheduler.minimum.allocation-mb=1024
```

每个Container最大的使用内存，默认8192M
```properties
yarn.scheduler.maximum.allocation-mb=8192
```

每个Container最少使用CPU，默认1
```properties
yarn.scheduler.minimum.allocation-vcores=1
```

每个Container最大使用CPU，默认4
```properties
yarn.scheduler.maximum.allocation-vcores=4
```

每个MapTask运行所用的内存大小。此参数如果在Container的上限和下限之间，就用设置的参数值，如果超过上限或下限，就是用上限或下限做值。默认1024M
```properties
maperduce.map.memory.mb=1024
```

每个ReduceTask运行所用的内存大小。此参数如果在Container的上限和下限之间，就用设置的参数值，如果超过上限或下限，就是用上限或下限做值。默认1024M
```properties
maperduce.resuce.memory.mb=1024
```

配置Yarn的调度器类型，默认容器调度器，还有另外两种调度器：FIFO调度器，Fair调度器
```properties
yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
```

## uber JVM重用
yarn的默认配置会禁用uber ，即不允许JVM重用。在以上 yarn 的工作机制中，当每一个 task 执行完毕后，container便会被nodemamager收回，而 container 所拥有的JVM也相应地被退出。

开启 uber  即在同一个 container 里面一次执行多个task，在 yarn-site.xml 文件中，改变以下几个参数的配置，即可启用 uber  的方法。
```xml
  <!-- 开启uber模式(针对小作业的优化) -->
  <property>
    <name>mapreduce.job.ubertask.enable</name>
    <value>true</value>
  </property>

  <!-- 配置启动uber模式的最大map数-->
  <property>
    <name>mapreduce.job.ubertask.maxmaps</name>
    <value>9</value>
  </property>

  <!-- 配置启动uber模式的最大reduce数-->
  <property>
    <name>mapreduce.job.ubertask.maxreduces</name>
    <value>1</value>
  </property>
``` 