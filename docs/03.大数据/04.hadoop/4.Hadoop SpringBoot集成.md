---
title: Hadoop SpringBoot集成
date: 2023-06-25 09:22:36
permalink:  /hadoop/4
sidebar: true
article: false #  是否未非文章页，非文章不显示 面包屑和作者、时间，不显示最近更新栏，不会参与到最近更新文章的数据计算中
comment: false #  评论区
editLink: false
---


## 配置环境
我使用的不是直接在Hadoop官网下载的和安装的hadoop对应hadop版本的解压包，我是用 winutils-master 的，包整个大小只有6M左右，里面提供了对hadoop 在windows 上的支持，3.x 以上使用3.0.0就行，下载可以在网上搜索 winutils-master 或 winutils。

![](/assets/img/hadoop/4/img.png)

![](/assets/img/hadoop/4/img_1.png)


windows 开发HDFS或使用 big data tools插件 都一定要配置 HADOOP_HOME，把以上下载的配置到我们系统环境变量并名命HADOOP_HOME=D:\tools\winutils-master\hadoop-3.0.0。然后在path 上加 %HADOOP_HOME%\bin 即可。但配置完毕后我建议直接重启，有时候配置完不会直接生效。

## spring boot 集成
maven 需要的包
```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <!--引入hadoop-client Jar包  -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.1.3</version>
    </dependency>
    <!-- 引入hadoop-common Jar包 -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.1.3</version>
    </dependency>
    <!-- 引入hadoop-hdfs Jar包 -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>3.1.3</version>
    </dependency>
    <!-- mapreduce 核心jar包  -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-mapreduce-client-core</artifactId>
        <version>3.1.3</version>
    </dependency>

</dependencies>
```
配置类
```java
package com.example.demo.config;

import lombok.Data;

/**
 * @author big uncle
 * @date 2021/5/10 19:53
 * @module
 **/
@Data
public class HDFSConfig {

    /**
     * hdfs 服务器地址
    **/
    private String hostname;
    /**
     * hdfs 服务器端口
    **/
    private String port;
    /**
     * hdfs 服务器账户
    **/
    private String username;

}
```

具体实现
```java
package com.example.demo.hdfs;

import com.example.demo.config.HDFSConfig;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

import java.io.*;
import java.net.URI;

/**
 * @author big uncle
 * @date 2021/5/10 19:52
 * @module
 **/
public class HDFSService {

    private static FileSystem fileSystem;

    static {
        HDFSConfig config = new HDFSConfig();
        config.setHostname("node113");
        config.setPort("9000");
        config.setUsername("root");
        try {
            // 获得FileSystem对象，指定使用root用户上传
            fileSystem = FileSystem.get(new URI(getHdfsUrl(config)), new Configuration(),
                    config.getUsername());
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * 文件上传
     * @author big uncle
     * @date 2021/5/10 20:00
     * @param source
     * @param destination
     * @return void
    **/
    public static void upload(String source, String destination) {

        try {
            // 创建输入流，参数指定文件输出地址
            InputStream in = new FileInputStream(source);
            // 调用create方法指定文件上传，参数HDFS上传路径
            OutputStream out = fileSystem.create(new Path(destination));
            // 使用Hadoop提供的IOUtils，将in的内容copy到out，设置buffSize大小，是否关闭流设置true
            IOUtils.copyBytes(in, out, 4096, true);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * 文件下载
     * @author big uncle
     * @date 2021/5/10 20:00
     * @param source
     * @param destination
     * @return void
    **/
    public static void download(String source, String destination) {

        try {
            // 调用open方法进行下载，参数HDFS路径
            InputStream in = fileSystem.open(new Path(source));
            // 创建输出流，参数指定文件输出地址
            OutputStream out = new FileOutputStream(destination);
            IOUtils.copyBytes(in, out, 4096, true);

        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * 删除文件
     * @author big uncle
     * @date 2021/5/10 20:00
     * @param target
     * @return boolean
    **/
    public static boolean delete(String target) {
        boolean flag = false;
        try {
            // 调用delete方法，删除指定的文件。参数:false:表示是否递归删除
            flag = fileSystem.delete(new Path(target), false);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
        return flag;
    }

    /**
     * 创建文件夹
     * @author big uncle
     * @date 2021/5/10 19:59
     * @param directory
     * @return boolean
    **/
    public static boolean mkdir(String directory) {
        boolean flag = false;
        try {
            // 调用mkdirs方法，在HDFS文件服务器上创建文件夹。
            flag = fileSystem.mkdirs(new Path(directory));

        } catch (Exception e) {
            e.printStackTrace();
            return false;
        };
        return flag;
    }

    /**
     * 拼接连接
     * @author big uncle
     * @date 2021/5/10 20:00
     * @param config
     * @return java.lang.String
    **/
    private static String getHdfsUrl(HDFSConfig config) {
        StringBuilder builder = new StringBuilder();
        builder.append("hdfs://").append(config.getHostname()).append(":").append(config.getPort());
        return builder.toString();
    }
}
```
遇到如下报错： 先确定端口开放
> Call From LAPTOP-9DN4GQON/192.168.xxx.xx to node113:9000 failed on connectio

使用命令 netstat -tpnl 查看hadoop 是否是你的IP地址，如果是 127.0.0.1 那肯定连不上，或者直接在 core-site.xml 指定成IP，或者修改 hosts。

如果遇到 org.apache.hadoop.conf 或 fs 等包不存在问题

![](/assets/img/hadoop/4/img_2.png)

勾选如下即可

![](/assets/img/hadoop/4/img_3.png)

## big data tools 配置
![](/assets/img/hadoop/4/img_4.png)

输入active的地址
![](/assets/img/hadoop/4/img_5.png)

输入linux的用户
![](/assets/img/hadoop/4/img_6.png)
