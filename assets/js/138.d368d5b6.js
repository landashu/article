(window.webpackJsonp=window.webpackJsonp||[]).push([[138],{615:function(s,a,t){"use strict";t.r(a);var e=t(41),n=Object(e.a)({},(function(){var s=this,a=s.$createElement,t=s._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("blockquote",[t("p",[s._v("本文及后续所有文章都以 3.1.2 做为版本讲解和入门学习")])]),s._v(" "),t("table",[t("thead",[t("tr",[t("th",[s._v("关键词")]),s._v(" "),t("th",[s._v("情形")])])]),s._v(" "),t("tbody",[t("tr",[t("td",[s._v("group by")]),s._v(" "),t("td",[s._v("当某一个表分区重复数据较多，会导致数据倾斜，非常耗时")])]),s._v(" "),t("tr",[t("td",[s._v("join")]),s._v(" "),t("td",[s._v("当小表连接大表，处理大表的 Map 会慢且分配到的 Reduce 也会慢")])]),s._v(" "),t("tr",[t("td",[s._v("count()")]),s._v(" "),t("td",[s._v("count 会统计，交由一个 Reduce 来做，数据量大会很慢")])])])]),s._v(" "),t("h2",{attrs:{id:"group-by解决方案"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#group-by解决方案"}},[s._v("#")]),s._v(" group by 解决方案")]),s._v(" "),t("p",[s._v("再 hive 里输入如下：")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.groupby.skewindata"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("或在 hive-site.xml 里添加如下：")]),s._v(" "),t("div",{staticClass:"language-xml line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-xml"}},[t("code",[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("hive.groupby.skewindata"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("true"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("p",[s._v("hive.groupby.skewindata=true：数据倾斜时负载均衡，当选项设定为 true，生成的查询计划会有两个 MRJob：")]),s._v(" "),t("p",[s._v("第一个 MRJob 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 GroupBy Key 有可能被分到不同的 Reduce 中，从而达到负载均衡的目的；")]),s._v(" "),t("p",[s._v("第二个 MRJob 再根据预处理的数据结果按照 GroupBy Key 分布到 Reduce 中（这个过程可以保证相同的 GroupBy Key 被分到统一个 Reduce 中），最后完成最终的聚合操作。")]),s._v(" "),t("p",[s._v("由上面可以看出，起到至关重要的作用其实时第二个参数的设置，它使计算变成了两个 MapReduce，现在第一个中再 shuffle 过程 partition 时随鸡给 key 打标机，使每个 key 随机均匀分布到各个 reduce 上计算，但是这样只能完成部分计算，因为相同 key 没有分配到相同 reduce 上，所以需要第二次的 MapReduce，这次就回归正常 shuffle，但是数据分布不均匀的问题再第一次有很大的改善，因此基本解决数据倾斜。")]),s._v(" "),t("h2",{attrs:{id:"join解决方案"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#join解决方案"}},[s._v("#")]),s._v(" join 解决方案")]),s._v(" "),t("p",[s._v("再 hive 里输入如下：")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.auto.convert.join"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 其中一个表大小小于25mb时，自动启用mapjoin")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.mapjoin.smalltable.filesize"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("25mb\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("p",[s._v("或在 hive-site.xml 里添加如下：")]),s._v(" "),t("div",{staticClass:"language-xml line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-xml"}},[t("code",[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("hive.auto.convert.join"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("true"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("property")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("name")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("hive.mapjoin.smalltable.filesize"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("name")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),s._v("value")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("50mb"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("value")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token tag"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("</")]),s._v("property")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("p",[s._v("当使用以上参数设置时，需要注意 sql 写的时候，小表一定要在前面（最左侧）。")]),s._v(" "),t("p",[s._v("mapjoin 的主要意思就是，当连接两个表是一个比较小的表和一个特大的表的时候，我们把比较小的 table 直接放到内存中去，然后再对比较大的表格进行 map 操作。join 就发生在 map 操作的时候，每当扫描一个大的 table 中的数据，就要去查看小表的数据，哪条与之相符，继而进行连接。这里的 join 并不会设计 reduce 操作。map 端 join 的优势就是在于没有 shuffle。")]),s._v(" "),t("p",[s._v("还有种 sql 优化的方式，就是有条件的话，先对大表进行条件过滤，后对他 join。")]),s._v(" "),t("h2",{attrs:{id:"count-distinct-优化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#count-distinct-优化"}},[s._v("#")]),s._v(" count distinct 优化")]),s._v(" "),t("p",[s._v("优化前")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" count"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("distinct "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" from table_name\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("优化后")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" count"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("*"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" from "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("select distinct "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" from table_name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" tmp"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("优化前的 sql 会启用一个 Reduce 任务，优化后的会启用两个 Reduce 任务。优化后的 Reduce 必须设置 mapred.reduce.tasks 数量才有效，否则默认还是 1 个。而 count 一定之启用 1 个 Reduce。")]),s._v(" "),t("p",[s._v("日常统计场景中，我们经常会对一段十七内的字段进行消重并统计数量，SQL 语句类似于 优化前 的语句，这条语句是从一个表的符合 where 条件的记录中统计不重复的 id 的总数。该语句转化为 MapReduce 任务后执行示意图如下：")]),s._v(" "),t("p",[t("img",{attrs:{src:"/assets/img/hive/5/img.png",alt:""}})]),s._v(" "),t("p",[s._v("由于引入了 distinct，因此在 Map 阶段无法利用 combine 对输出结果消重，必须将 id 作为 key 输出，在 Reduce 阶段再对来自于不同 Map 相同 key 的结果进行消重，计入最终统计值。")]),s._v(" "),t("p",[s._v("我们看到任务运行时 Reduce 个数为 1，对于统计大量数据量时，这会导致最终 Map 的全部输出由单个 Reuduce 处理，这唯一的 Reduce 需要 shuffle 大量的数据，并且进行排序聚合等处理，这使得它称为整个任务的 IO 和运算瓶颈。")]),s._v(" "),t("p",[s._v("经过上述分析后，我们可以尝试显示的增大 Reduce 任务个数来提高 Reduce 阶段的并发，使每一个 Reduce 的数据量控制在我们预想的范围。")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.reduce.tasks"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v('但这个调正不会影响 count 这种 "全聚合 (full aggregates)" 的任务，它会忽略用户指定的')]),s._v(" "),t("p",[s._v("我们利用 hive 对嵌套语句的支持，将原来一个 MapReduce 任务转为两个任务，在第一阶段选出全部的非重复 id，在第二阶段再对这些已消重的 id 进行统计。这样在第一阶段我们可以同过增大 Reduce 的并发数，并发处理 Map 输出。在第二阶段，由于 id 已经消重，因此 count (*) 操作 Map 阶段不需要输出原 id 数据，只输出一个合并后的计数即可。这样即使第二阶段 Hive 强制指定一个 Reduce 任务，极少量的 Map 输出数据也不会使单一的 Reduce 任务成为瓶颈，改进后的视图如下：")]),s._v(" "),t("p",[t("img",{attrs:{src:"/assets/img/hive/5/img_1.png",alt:""}})]),s._v(" "),t("h2",{attrs:{id:"调整切片数-map任务数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#调整切片数-map任务数"}},[s._v("#")]),s._v(" 调整切片数（map 任务数）")]),s._v(" "),t("p",[s._v("Hive 底层自动对小文件做了优化，用了 CombineTextInputFormat，将多个小文件切片合成一片，对应也就只会启动一个 Map 任务。"),t("br"),s._v("\n合并完成的切片大小，如果 > mapred.max.split.size 的大小，就启动一个新的切片任务。默认 mapred.max.split.size=134217728（128MB）")]),s._v(" "),t("h2",{attrs:{id:"jvm重用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#jvm重用"}},[s._v("#")]),s._v(" JVM 重用")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 默认是1个")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.job.reuse.jvm.num.task"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])]),t("p",[s._v("JVM 重用是 hadoop 调优参数内容，对 hive 的性能具有非常大的影响，特别是对于很难避免小文件的场景或者 task 特别多的场景，这类场景大多数执行事件都很短。这时 jvm 的启动过程可能会造成相当大的开销，尤其时执行的 Job 包含由成千上万个 task 任务的情况。")]),s._v(" "),t("p",[s._v("JVM 重用可以使得一个 JVM 进程在同一个 JOB 中重新使用 N 次后才会销毁。")]),s._v(" "),t("h2",{attrs:{id:"启用严格模式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#启用严格模式"}},[s._v("#")]),s._v(" 启用严格模式")]),s._v(" "),t("p",[s._v("在 hive 里面可以通过严格模式防止用户执行那些可能产生意想不到的不好的效果的查询，从而保护 hive 的集群。")]),s._v(" "),t("p",[s._v("用户可以通过以下来设置严格模式，改成 unstrict 则为非严格模式。")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.mapred.mode"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("strict\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("在严格模式下，用户在运行如下查询的时候会报错：")]),s._v(" "),t("ul",[t("li",[s._v("分区表的查询语句没有使用分区字段来限制")]),s._v(" "),t("li",[s._v("使用了 order by 但没有使用 limit 语句。（如果不使用 limit，会对查询结果进行全局排序，消耗时间长）")]),s._v(" "),t("li",[s._v("产生了笛卡尔积")])]),s._v(" "),t("h2",{attrs:{id:"关闭推测执行机制"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#关闭推测执行机制"}},[s._v("#")]),s._v(" 关闭推测执行机制")]),s._v(" "),t("p",[s._v("因为在测试环境下我们都把应用程序跑通了，如果还加上推测执行，如果有一个数据分片本来就会发生数据倾斜，执行事时间就是比其他的时间长，那么 hive 就会把这个执行时间长的 job 当作运行失败，继而又产生一个相同的 job 去运行，后果可想而知，可以通过如下设置关闭推测执行机制。该机制默认时开启的。")]),s._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.map.speculative"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("false"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.reduce.speculative"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("false"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.mapred.reduce.tasks.speculative.execution"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("false"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])])])}),[],!1,null,null,null);a.default=n.exports}}]);