(window.webpackJsonp=window.webpackJsonp||[]).push([[114],{593:function(a,s,e){"use strict";e.r(s);var t=e(41),r=Object(t.a)({},(function(){var a=this,s=a.$createElement,e=a._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h2",{attrs:{id:"mapreduce-简介"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce-简介"}},[a._v("#")]),a._v(" MapReduce 简介")]),a._v(" "),e("p",[a._v("MapReduce 分布式并行计算框架是一种可用于数据处理的编程模型，可运行由个中语言编写的 MapReduce 程序：java、Ruby、Python、R、C++ 等语言。它用于处理超大规模数据的计算，同时具有可并行计算的特性，因此可以将大规模的数据分析任务交给任何一个拥有足够多机器的集群。并采用函数式编程的思想，在各函数之间串行计算（Map 执行完毕，才会开始执行 Reduce 任务）。")]),a._v(" "),e("h2",{attrs:{id:"hadoop1-x-job运行机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hadoop1-x-job运行机制"}},[a._v("#")]),a._v(" hadoop1.x job 运行机制")]),a._v(" "),e("p",[e("strong",[a._v("在讲 YARN 后会说 hadoop2.0 job 运行机制")])]),a._v(" "),e("p",[e("img",{attrs:{src:"/assets/img/hadoop/5/img.png",alt:""}})]),a._v(" "),e("ol",[e("li",[a._v("run job 由客户端来完成，底层有一个 JobClient 类来做具体实现。run job 会做如下几件事")])]),a._v(" "),e("ul",[e("li",[a._v("做 job 环境信息的收集，比如各个组件类，输出 KV 类型，检测是否合法。")]),a._v(" "),e("li",[a._v("检测输入路径的合法性，以及输出结果路径的合法性。"),e("br"),a._v("\n如果检测未通过，直接报错返回，不会做后续的 job 提交动作。")])]),a._v(" "),e("ol",{attrs:{start:"2"}},[e("li",[a._v("run job 检测通过，JobClient 会向 JobTracker 为当前 job 申请 jobid，jobid 是全局唯一的，用于标识一个 Job。")]),a._v(" "),e("li",[a._v("copy job resource 阶段，JobClient 把 Job 的运算资源上传到 HDFS。路径为 /tmp/hadoop-yarn/staging/ 用户 /* 。运算资源包括如下：")])]),a._v(" "),e("ul",[e("li",[a._v("jar 包")]),a._v(" "),e("li",[a._v("文件的切片信息")]),a._v(" "),e("li",[a._v("job.xml 整个 job 的环境参数")])]),a._v(" "),e("ol",{attrs:{start:"4"}},[e("li",[a._v("当 jobClient 将运算资源上传到 HDFS 之后，提交 job（submit job）"),e("br"),a._v("\n5.6. 初始化 job 环境信息（init job）以及获取 job 的切片数量（get split data）目的是获取整个 Job 的 MapTask 任务数量和 ReduceTask 的任务数量。MapTask 任务数量 = 切片数量。ReduceTask 的任务数量在代码设置。")]),a._v(" "),e("li",[a._v("TaskTracker 会根据心跳去 JobTracker 获取 job 任务。TaskTracker 在领取任务时，要满足数据本地话策略（TaskTracker 属于 datanode，datanode 存的是 块数据，JobTracker 属于 namenode，拥有块的切片信息，TaskTracker 去领取任务的时候最好领取块相对应的切片信息，可以节省带宽，否则还要去其他 datanode 找属于自己的切片信息）。切块包含的是数据（文件的真是数据），切片包含的是块的描述信息，如：")])]),a._v(" "),e("ul",[e("li",[a._v("Path 文件所在分布式文件系统路径")]),a._v(" "),e("li",[a._v("Start 数据开始位置")]),a._v(" "),e("li",[a._v("Length 数据的长度")])]),a._v(" "),e("ol",{attrs:{start:"8"}},[e("li",[a._v("TaskTracker 去 HDFS 下载 job（xxx.jar） 的运算资源"),e("br"),a._v("\n 9.10. 启动 JVM 进程")])]),a._v(" "),e("h2",{attrs:{id:"输入输出机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#输入输出机制"}},[a._v("#")]),a._v(" 输入输出机制")]),a._v(" "),e("p",[a._v("以下是写一个 单词统计的 job 的输入和输出"),e("br"),a._v(" "),e("img",{attrs:{src:"/assets/img/hadoop/5/img_1.png",alt:""}})]),a._v(" "),e("ol",[e("li",[a._v("MapTask 读取文件根据代码逻辑进行输出")]),a._v(" "),e("li",[a._v("ReduceTask 读取 MapTask 的输出做为输入，会得到 一个迭代器，迭代器包含了相同单词的 value（最后图中的 hello 1 1 1 1 1 1），迭代器中包含 6 个元素，每个元素的值都是 1.")])]),a._v(" "),e("h2",{attrs:{id:"分区机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#分区机制"}},[a._v("#")]),a._v(" 分区机制")]),a._v(" "),e("p",[e("img",{attrs:{src:"/assets/img/hadoop/5/img_2.png",alt:""}})]),a._v(" "),e("p",[a._v("分区需要在代码中设置分几个区，并且可以控制分区条件")]),a._v(" "),e("h2",{attrs:{id:"合并组件-combiner机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#合并组件-combiner机制"}},[a._v("#")]),a._v(" 合并组件 (Combiner 机制)")]),a._v(" "),e("p",[e("img",{attrs:{src:"/assets/img/hadoop/5/img_3.png",alt:""}})]),a._v(" "),e("p",[a._v("我们可以看到 合并组件 提前把 MapTask 的的输出做为输入合并后再输出，引入 Combiner 的作用就是为了降低 ReduceTask 的负载。")]),a._v(" "),e("h2",{attrs:{id:"maptask工作机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#maptask工作机制"}},[a._v("#")]),a._v(" MapTask 工作机制")]),a._v(" "),e("p",[e("img",{attrs:{src:"/assets/img/hadoop/5/img_4.png",alt:""}})]),a._v(" "),e("h3",{attrs:{id:"注意"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#注意"}},[a._v("#")]),a._v(" 注意")]),a._v(" "),e("ol",[e("li",[a._v("Spill 过程不一定会发生，当 MapTask 输出的数据流 < 溢写缓冲大小 * 溢写阈值")]),a._v(" "),e("li",[a._v("当发生了 Spill 过程，最后溢写缓冲区残留的数据会 flush 到最后生成的 Spill 文件中")]),a._v(" "),e("li",[e("strong",[a._v("Spill 理论上是 80MB，但是需要考虑序列化的因素")])]),a._v(" "),e("li",[e("strong",[a._v("不能凭一个 MapTask 处理的切片数据大小，来衡量输出数据的大小，这得取决于业务")]),a._v("。")]),a._v(" "),e("li",[a._v("有一个 MapTask，就会有一个对应的溢写缓冲区")]),a._v(" "),e("li",[a._v("溢写缓冲区本质上就是一个字节数组，即内存中一块连续的地址空间")]),a._v(" "),e("li",[a._v("溢写缓冲区又叫环写（环形）缓冲区，可以重复利用同一块地址空间。")]),a._v(" "),e("li",[a._v("Spill 因为是环形缓冲区，设置 80% 的写入，是为了不去阻塞后续的写入。")]),a._v(" "),e("li",[a._v("Merge 过程不一定发生，原因是没有发送 Spill 或 发生 Spill 但只输出一个文件。")])]),a._v(" "),e("h3",{attrs:{id:"优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#优化"}},[a._v("#")]),a._v(" 优化")]),a._v(" "),e("ol",[e("li",[a._v("适当调整 溢写缓冲 区大小，建议范围 250M~350M")]),a._v(" "),e("li",[a._v("加入 Combine（合并组件），会在缓冲区和 Merge 过程进行合并，减少 Spill 溢写次数，减少 IO 次数，减少网络数据传输。Merge 过程 Combine 不一定发生，当 Spill 文件 < 3")]),a._v(" "),e("li",[a._v("对最后的结果文件进行压缩")])]),a._v(" "),e("h2",{attrs:{id:"reducetask-工作机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#reducetask-工作机制"}},[a._v("#")]),a._v(" ReduceTask 工作机制")]),a._v(" "),e("p",[e("img",{attrs:{src:"/assets/img/hadoop/5/img_5.png",alt:""}})]),a._v(" "),e("p",[a._v("ReduceTask 会把 MapTask 输出的文件按分区读取再合并排序，该分区和 ReduceTask 的分区是两回事，MapTask 是对文件内容分区，ReduceTask 是对输出结果到不同的文件分区。"),e("br"),a._v("\nReduceTask 在合并过程中，如果有大量的分区文件，按照一定的数量（可设置）合并，直到合并到少量文件到 ReduceTask 工作。")]),a._v(" "),e("h3",{attrs:{id:"优化-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#优化-2"}},[a._v("#")]),a._v(" 优化")]),a._v(" "),e("ol",[e("li",[a._v("Fetch 线程数可调整，默认是 5 个。")]),a._v(" "),e("li",[a._v("ReduceTask 启动阈值 5%，ReduceTask 不是等所有的 MapTask 完成才工作，而是根据 5% 的比例。")])]),a._v(" "),e("h2",{attrs:{id:"hadoop-常见参数调优"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-常见参数调优"}},[a._v("#")]),a._v(" Hadoop 常见参数调优")]),a._v(" "),e("h3",{attrs:{id:"hdfs-site-xml"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-site-xml"}},[a._v("#")]),a._v(" hdfs-site.xml")]),a._v(" "),e("p",[a._v("namenode 是否允许被格式化，默认为 true，生产系统要设置 false，组织任何格式化操作再一个运行的 DFS 上。格式指令：hadoop namenode -format")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("dfs.namenode.support.allow.format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("true")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("datamode 的心跳间隔，默认 3 秒，在集群网络状态不好的情况下，建议调大")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("dfs.heartbeat.interval")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("3")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("切块大小，默认是 128MB，必须得是 1024（page size）的整数倍。")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("dfs.blocksize")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("134217728")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("edits 和 fsimage 文件合并周期阈值，默认 1 小时")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("dfs.namenode.checkpoint.period")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("3600")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("文件流缓存大小。需要是硬件 page 大小的整数倍。在读写操作时，数据缓存大小，必须是 1024 整数倍")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("dfs.stream-buffer-size")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("4096")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("h3",{attrs:{id:"maperd-site-xml"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#maperd-site-xml"}},[a._v("#")]),a._v(" maperd-site.xml")]),a._v(" "),e("p",[a._v("任务内部排序缓冲区大小，默认时 100MB，调大能减少 Spill 溢写次数，减少磁盘 IO，建议 250~400MB")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("mapreduce.task.io.sort.mb")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("100")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("Map 阶段溢写文件的阈值。不建议修改此值")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("mapreduce.map.sort.spill.percent")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("0.8")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("ReduceTask 启动的并发 copy 数据的线程数，建议尽可能等于或接近 Map 任务数量，达到并行抓取的效果")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("mapreduce.reduce.shuffle.parallelcopies")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("5")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("当 Map 任务数量完成率再 5% 时，Reduce 任务启动，这个参数不建议修改")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("mapreduce.job.reduce.slowstart.completedmaps")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("0,05")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("文件合并 (Merge) 影子，如果文件数量太多，可以适当调大，减少 IO 次数")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("io.sort.factor")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("10")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("是否对 Map 的输出结果文件进行压缩，开启后 CPU 利用率会提高，但节省带宽")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("mapred.compress.map.output")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("false")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("启动 map/reduce 任务的推测执行机制，对拖后腿的任务一种优化机制。当一个作业的某些任务运行速度明显慢于同作业的其他任务时，Hadoop 会在另一个节点上为慢任务启动一个备份任务，这样两个任务同时处理一份数据，而 Hadoop 最终会将优先完成的那个任务的结果做为最终结果，并将另一个任务杀掉。")]),a._v(" "),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("mapred.map.tasks.speculative.execution")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("true")]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("mapred.reduce.tasks.speculative.execution")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("true")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br")])]),e("h2",{attrs:{id:"注意-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#注意-2"}},[a._v("#")]),a._v(" 注意")]),a._v(" "),e("ol",[e("li",[a._v("只会在被分配的 文件的 block 上启动 Map 任务")]),a._v(" "),e("li",[a._v("一个目录有多少个文件，就会分多少个 Map 任务")]),a._v(" "),e("li",[a._v("Map 读文件默认是一行一行读取，该条件可以设置。")]),a._v(" "),e("li",[a._v("Map 会按照 Key 做排序，自定义排序可以实现 WritableComparable 接口")]),a._v(" "),e("li",[a._v("Reducer 主要做聚合操作")]),a._v(" "),e("li",[a._v("Reduce 会把数据分区 (Hash)，ReduceTask (1) 的数据不会 ReduceTask (2) 的结果中出现，输出的文件根据设置的工作数有关，可以使用 hadoop fs -getmerge / 目录 / 所有文件 / 目录 / 新得文件.txt 进行整合。")]),a._v(" "),e("li",[a._v("ReduceTask 有分区的话会先分区再聚合。")]),a._v(" "),e("li",[a._v("分区可能会造成数据倾斜现象")])])])}),[],!1,null,null,null);s.default=r.exports}}]);